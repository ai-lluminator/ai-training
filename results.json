{
    "Taylor Johnson": [
        {
            "papers": [
                "Sharpness-Aware Minimization Revisited: Weighted Sharpness as a\n  Regularization Term\nDeep Neural Networks (DNNs) generalization is known to be closely related to\nthe flatness of minima, leading to the development of Sharpness-Aware\nMinimization (SAM) for seeking flatter minima and better generalization. In\nthis paper, we revisit the loss of SAM and propose a more general method,\ncalled WSAM, by incorporating sharpness as a regularization term. We prove its\ngeneralization bound through the combination of PAC and Bayes-PAC techniques,\nand evaluate its performance on various public datasets. The results\ndemonstrate that WSAM achieves improved generalization, or is at least highly\ncompetitive, compared to the vanilla optimizer, SAM and its variants. The code\nis available at\nhttps://github.com/intelligent-machine-learning/dlrover/tree/master/atorch/atorch/optimizers.",
                "Deep Learning and Ethics\nThis article appears as chapter 21 of Prince (2023, Understanding Deep\nLearning); a complete draft of the textbook is available here:\nhttp://udlbook.com. This chapter considers potential harms arising from the\ndesign and use of AI systems. These include algorithmic bias, lack of\nexplainability, data privacy violations, militarization, fraud, and\nenvironmental concerns. The aim is not to provide advice on being more ethical.\nInstead, the goal is to express ideas and start conversations in key areas that\nhave received attention in philosophy, political science, and the broader\nsocial sciences.",
                "PDE+: Enhancing Generalization via PDE with Adaptive Distributional\n  Diffusion\nThe generalization of neural networks is a central challenge in machine\nlearning, especially concerning the performance under distributions that differ\nfrom training ones. Current methods, mainly based on the data-driven paradigm\nsuch as data augmentation, adversarial training, and noise injection, may\nencounter limited generalization due to model non-smoothness. In this paper, we\npropose to investigate generalization from a Partial Differential Equation\n(PDE) perspective, aiming to enhance it directly through the underlying\nfunction of neural networks, rather than focusing on adjusting input data.\nSpecifically, we first establish the connection between neural network\ngeneralization and the smoothness of the solution to a specific PDE, namely\n\"transport equation\". Building upon this, we propose a general framework that\nintroduces adaptive distributional diffusion into transport equation to enhance\nthe smoothness of its solution, thereby improving generalization. In the\ncontext of neural networks, we put this theoretical framework into practice as\n$\\textbf{PDE+}$ ($\\textbf{PDE}$ with $\\textbf{A}$daptive\n$\\textbf{D}$istributional $\\textbf{D}$iffusion) which diffuses each sample into\na distribution covering semantically similar inputs. This enables better\ncoverage of potentially unobserved distributions in training, thus improving\ngeneralization beyond merely data-driven methods. The effectiveness of PDE+ is\nvalidated through extensive experimental settings, demonstrating its superior\nperformance compared to SOTA methods.",
                "READ: Recurrent Adaptation of Large Transformers\nFine-tuning large-scale Transformers has led to the explosion of many AI\napplications across Natural Language Processing and Computer Vision tasks.\nHowever, fine-tuning all pre-trained model parameters becomes impractical as\nthe model size and number of tasks increase. Parameter-efficient transfer\nlearning (PETL) methods aim to address these challenges. While effective in\nreducing the number of trainable parameters, PETL methods still require\nsignificant energy and computational resources to fine-tune. In this paper, we\nintroduce \\textbf{RE}current \\textbf{AD}aption (READ) -- a lightweight and\nmemory-efficient fine-tuning method -- to overcome the limitations of the\ncurrent PETL approaches. Specifically, READ inserts a small RNN network\nalongside the backbone model so that the model does not have to back-propagate\nthrough the large backbone network. Through comprehensive empirical evaluation\nof the GLUE benchmark, we demonstrate READ can achieve a $56\\%$ reduction in\nthe training memory consumption and an $84\\%$ reduction in the GPU energy usage\nwhile retraining high model quality compared to full-tuning. Additionally, the\nmodel size of READ does not grow with the backbone model size, making it a\nhighly scalable solution for fine-tuning large Transformers.",
                "Unifying gradient regularization for Heterogeneous Graph Neural Networks\nHeterogeneous Graph Neural Networks (HGNNs) are a class of powerful deep\nlearning methods widely used to learn representations of heterogeneous graphs.\nDespite the fast development of HGNNs, they still face some challenges such as\nover-smoothing, and non-robustness. Previous studies have shown that these\nproblems can be reduced by using gradient regularization methods. However, the\nexisting gradient regularization methods focus on either graph topology or node\nfeatures. There is no universal approach to integrate these features, which\nseverely affects the efficiency of regularization. In addition, the inclusion\nof gradient regularization into HGNNs sometimes leads to some problems, such as\nan unstable training process, increased complexity and insufficient coverage\nregularized information. Furthermore, there is still short of a complete\ntheoretical analysis of the effects of gradient regularization on HGNNs. In\nthis paper, we propose a novel gradient regularization method called Grug,\nwhich iteratively applies regularization to the gradients generated by both\npropagated messages and the node features during the message-passing process.\nGrug provides a unified framework integrating graph topology and node features,\nbased on which we conduct a detailed theoretical analysis of their\neffectiveness. Specifically, the theoretical analyses elaborate the advantages\nof Grug: 1) Decreasing sample variance during the training process (Stability);\n2) Enhancing the generalization of the model (Universality); 3) Reducing the\ncomplexity of the model (Simplicity); 4) Improving the integrity and diversity\nof graph information utilization (Diversity). As a result, Grug has the\npotential to surpass the theoretical upper bounds set by DropMessage (AAAI-23\nDistinguished Papers). In addition, we evaluate Grug on five public real-world\ndatasets with two downstream tasks...",
                "Deep Neural Networks in Video Human Action Recognition: A Review\nCurrently, video behavior recognition is one of the most foundational tasks\nof computer vision. The 2D neural networks of deep learning are built for\nrecognizing pixel-level information such as images with RGB, RGB-D, or optical\nflow formats, with the current increasingly wide usage of surveillance video\nand more tasks related to human action recognition. There are increasing tasks\nrequiring temporal information for frames dependency analysis. The researchers\nhave widely studied video-based recognition rather than\nimage-based(pixel-based) only to extract more informative elements from\ngeometry tasks. Our current related research addresses multiple novel proposed\nresearch works and compares their advantages and disadvantages between the\nderived deep learning frameworks rather than machine learning frameworks. The\ncomparison happened between existing frameworks and datasets, which are video\nformat data only. Due to the specific properties of human actions and the\nincreasingly wide usage of deep neural networks, we collected all research\nworks within the last three years between 2020 to 2022. In our article, the\nperformance of deep neural networks surpassed most of the techniques in the\nfeature learning and extraction tasks, especially video action recognition."
            ],
            "interesting paper": 3
        },
        {
            "papers": [
                "XGrad: Boosting Gradient-Based Optimizers With Weight Prediction\nIn this paper, we propose a general deep learning training framework XGrad\nwhich introduces weight prediction into the popular gradient-based optimizers\nto boost their convergence and generalization when training the deep neural\nnetwork (DNN) models. In particular, ahead of each mini-batch training, the\nfuture weights are predicted according to the update rule of the used optimizer\nand are then applied to both the forward pass and backward propagation. In this\nway, during the whole training period, the optimizer always utilizes the\ngradients w.r.t. the future weights to update the DNN parameters, making the\ngradient-based optimizer achieve better convergence and generalization compared\nto the original optimizer without weight prediction. XGrad is rather\nstraightforward to implement yet pretty effective in boosting the convergence\nof gradient-based optimizers and the accuracy of DNN models. Empirical results\nconcerning five popular optimizers including SGD with momentum, Adam, AdamW,\nAdaBelief, and AdaM3 demonstrate the effectiveness of our proposal. The\nexperimental results validate that XGrad can attain higher model accuracy than\nthe baseline optimizers when training the DNN models. The code of XGrad will be\navailable at: https://github.com/guanleics/XGrad.",
                "Pruning Distorted Images in MNIST Handwritten Digits\nRecognizing handwritten digits is a challenging task primarily due to the\ndiversity of writing styles and the presence of noisy images. The widely used\nMNIST dataset, which is commonly employed as a benchmark for this task,\nincludes distorted digits with irregular shapes, incomplete strokes, and\nvarying skew in both the training and testing datasets. Consequently, these\nfactors contribute to reduced accuracy in digit recognition. To overcome this\nchallenge, we propose a two-stage deep learning approach. In the first stage,\nwe create a simple neural network to identify distorted digits within the\ntraining set. This model serves to detect and filter out such distorted and\nambiguous images. In the second stage, we exclude these identified images from\nthe training dataset and proceed to retrain the model using the filtered\ndataset. This process aims to improve the classification accuracy and\nconfidence levels while mitigating issues of underfitting and overfitting. Our\nexperimental results demonstrate the effectiveness of the proposed approach,\nachieving an accuracy rate of over 99.5% on the testing dataset. This\nsignificant improvement showcases the potential of our method in enhancing\ndigit classification accuracy. In our future work, we intend to explore the\nscalability of this approach and investigate techniques to further enhance\naccuracy by reducing the size of the training data.",
                "Revisiting Structured Variational Autoencoders\nStructured variational autoencoders (SVAEs) combine probabilistic graphical\nmodel priors on latent variables, deep neural networks to link latent variables\nto observed data, and structure-exploiting algorithms for approximate posterior\ninference. These models are particularly appealing for sequential data, where\nthe prior can capture temporal dependencies. However, despite their conceptual\nelegance, SVAEs have proven difficult to implement, and more general approaches\nhave been favored in practice. Here, we revisit SVAEs using modern machine\nlearning tools and demonstrate their advantages over more general alternatives\nin terms of both accuracy and efficiency. First, we develop a modern\nimplementation for hardware acceleration, parallelization, and automatic\ndifferentiation of the message passing algorithms at the core of the SVAE.\nSecond, we show that by exploiting structure in the prior, the SVAE learns more\naccurate models and posterior distributions, which translate into improved\nperformance on prediction tasks. Third, we show how the SVAE can naturally\nhandle missing data, and we leverage this ability to develop a novel,\nself-supervised training approach. Altogether, these results show that the time\nis ripe to revisit structured variational autoencoders.",
                "SING: A Plug-and-Play DNN Learning Technique\nWe propose SING (StabIlized and Normalized Gradient), a plug-and-play\ntechnique that improves the stability and generalization of the Adam(W)\noptimizer. SING is straightforward to implement and has minimal computational\noverhead, requiring only a layer-wise standardization of the gradients fed to\nAdam(W) without introducing additional hyper-parameters. We support the\neffectiveness and practicality of the proposed approach by showing improved\nresults on a wide range of architectures, problems (such as image\nclassification, depth estimation, and natural language processing), and in\ncombination with other optimizers. We provide a theoretical analysis of the\nconvergence of the method, and we show that by virtue of the standardization,\nSING can escape local minima narrower than a threshold that is inversely\nproportional to the network's depth.",
                "Automated Search-Space Generation Neural Architecture Search\nTo search an optimal sub-network within a general deep neural network (DNN),\nexisting neural architecture search (NAS) methods typically rely on\nhandcrafting a search space beforehand. Such requirements make it challenging\nto extend them onto general scenarios without significant human expertise and\nmanual intervention. To overcome the limitations, we propose Automated\nSearch-Space Generation Neural Architecture Search (ASGNAS), perhaps the first\nautomated system to train general DNNs that cover all candidate connections and\noperations and produce high-performing sub-networks in the one shot manner.\nTechnologically, ASGNAS delivers three noticeable contributions to minimize\nhuman efforts: (i) automated search space generation for general DNNs; (ii) a\nHierarchical Half-Space Projected Gradient (H2SPG) that leverages the hierarchy\nand dependency within generated search space to ensure the network validity\nduring optimization, and reliably produces a solution with both high\nperformance and hierarchical group sparsity; and (iii) automated sub-network\nconstruction upon the H2SPG solution. Numerically, we demonstrate the\neffectiveness of ASGNAS on a variety of general DNNs, including RegNet,\nStackedUnets, SuperResNet, and DARTS, over benchmark datasets such as CIFAR10,\nFashion-MNIST, ImageNet, STL-10 , and SVNH. The sub-networks computed by ASGNAS\nachieve competitive even superior performance compared to the starting full\nDNNs and other state-of-the-arts. The library will be released at\nhttps://github.com/tianyic/only_train_once.",
                "Neural (Tangent Kernel) Collapse\nThis work bridges two important concepts: the Neural Tangent Kernel (NTK),\nwhich captures the evolution of deep neural networks (DNNs) during training,\nand the Neural Collapse (NC) phenomenon, which refers to the emergence of\nsymmetry and structure in the last-layer features of well-trained\nclassification DNNs. We adopt the natural assumption that the empirical NTK\ndevelops a block structure aligned with the class labels, i.e., samples within\nthe same class have stronger correlations than samples from different classes.\nUnder this assumption, we derive the dynamics of DNNs trained with mean squared\n(MSE) loss and break them into interpretable phases. Moreover, we identify an\ninvariant that captures the essence of the dynamics, and use it to prove the\nemergence of NC in DNNs with block-structured NTK. We provide large-scale\nnumerical experiments on three common DNN architectures and three benchmark\ndatasets to support our theory."
            ],
            "interesting paper": 1
        },
        {
            "papers": [
                "Statistically Significant Concept-based Explanation of Image Classifiers\n  via Model Knockoffs\nA concept-based classifier can explain the decision process of a deep\nlearning model by human-understandable concepts in image classification\nproblems. However, sometimes concept-based explanations may cause false\npositives, which misregards unrelated concepts as important for the prediction\ntask. Our goal is to find the statistically significant concept for\nclassification to prevent misinterpretation. In this study, we propose a method\nusing a deep learning model to learn the image concept and then using the\nKnockoff samples to select the important concepts for prediction by controlling\nthe False Discovery Rate (FDR) under a certain value. We evaluate the proposed\nmethod in our synthetic and real data experiments. Also, it shows that our\nmethod can control the FDR properly while selecting highly interpretable\nconcepts to improve the trustworthiness of the model.",
                "Are Deep Neural Networks Adequate Behavioural Models of Human Visual\n  Perception?\nDeep neural networks (DNNs) are machine learning algorithms that have\nrevolutionised computer vision due to their remarkable successes in tasks like\nobject classification and segmentation. The success of DNNs as computer vision\nalgorithms has led to the suggestion that DNNs may also be good models of human\nvisual perception. We here review evidence regarding current DNNs as adequate\nbehavioural models of human core object recognition. To this end, we argue that\nit is important to distinguish between statistical tools and computational\nmodels, and to understand model quality as a multidimensional concept where\nclarity about modelling goals is key. Reviewing a large number of\npsychophysical and computational explorations of core object recognition\nperformance in humans and DNNs, we argue that DNNs are highly valuable\nscientific tools but that as of today DNNs should only be regarded as promising\n-- but not yet adequate -- computational models of human core object\nrecognition behaviour. On the way we dispel a number of myths surrounding DNNs\nin vision science.",
                "Vision Transformers for Small Histological Datasets Learned through\n  Knowledge Distillation\nComputational Pathology (CPATH) systems have the potential to automate\ndiagnostic tasks. However, the artifacts on the digitized histological glass\nslides, known as Whole Slide Images (WSIs), may hamper the overall performance\nof CPATH systems. Deep Learning (DL) models such as Vision Transformers (ViTs)\nmay detect and exclude artifacts before running the diagnostic algorithm. A\nsimple way to develop robust and generalized ViTs is to train them on massive\ndatasets. Unfortunately, acquiring large medical datasets is expensive and\ninconvenient, prompting the need for a generalized artifact detection method\nfor WSIs. In this paper, we present a student-teacher recipe to improve the\nclassification performance of ViT for the air bubbles detection task. ViT,\ntrained under the student-teacher framework, boosts its performance by\ndistilling existing knowledge from the high-capacity teacher model. Our\nbest-performing ViT yields 0.961 and 0.911 F1-score and MCC, respectively,\nobserving a 7% gain in MCC against stand-alone training. The proposed method\npresents a new perspective of leveraging knowledge distillation over transfer\nlearning to encourage the use of customized transformers for efficient\npreprocessing pipelines in the CPATH systems.",
                "Green Runner: A tool for efficient model selection from model\n  repositories\nDeep learning models have become essential in software engineering, enabling\nintelligent features like image captioning and document generation. However,\ntheir popularity raises concerns about environmental impact and inefficient\nmodel selection. This paper introduces GreenRunnerGPT, a novel tool for\nefficiently selecting deep learning models based on specific use cases. It\nemploys a large language model to suggest weights for quality indicators,\noptimizing resource utilization. The tool utilizes a multi-armed bandit\nframework to evaluate models against target datasets, considering tradeoffs. We\ndemonstrate that GreenRunnerGPT is able to identify a model suited to a target\nuse case without wasteful computations that would occur under a brute-force\napproach to model selection.",
                "Stability of implicit neural networks for long-term forecasting in\n  dynamical systems\nForecasting physical signals in long time range is among the most challenging\ntasks in Partial Differential Equations (PDEs) research. To circumvent\nlimitations of traditional solvers, many different Deep Learning methods have\nbeen proposed. They are all based on auto-regressive methods and exhibit\nstability issues. Drawing inspiration from the stability property of implicit\nnumerical schemes, we introduce a stable auto-regressive implicit neural\nnetwork. We develop a theory based on the stability definition of schemes to\nensure the stability in forecasting of this network. It leads us to introduce\nhard constraints on its weights and propagate the dynamics in the latent space.\nOur experimental results validate our stability property, and show improved\nresults at long-term forecasting for two transports PDEs.",
                "Understanding Sparse Neural Networks from their Topology via\n  Multipartite Graph Representations\nPruning-at-Initialization (PaI) algorithms provide Sparse Neural Networks\n(SNNs) which are computationally more efficient than their dense counterparts,\nand try to avoid performance degradation. While much emphasis has been directed\ntowards \\emph{how} to prune, we still do not know \\emph{what topological\nmetrics} of the SNNs characterize \\emph{good performance}. From prior work, we\nhave layer-wise topological metrics by which SNN performance can be predicted:\nthe Ramanujan-based metrics. To exploit these metrics, proper ways to represent\nnetwork layers via Graph Encodings (GEs) are needed, with Bipartite Graph\nEncodings (BGEs) being the \\emph{de-facto} standard at the current stage.\nNevertheless, existing BGEs neglect the impact of the inputs, and do not\ncharacterize the SNN in an end-to-end manner. Additionally, thanks to a\nthorough study of the Ramanujan-based metrics, we discover that they are only\nas good as the \\emph{layer-wise density} as performance predictors, when paired\nwith BGEs. To close both gaps, we design a comprehensive topological analysis\nfor SNNs with both linear and convolutional layers, via (i) a new input-aware\nMultipartite Graph Encoding (MGE) for SNNs and (ii) the design of new\nend-to-end topological metrics over the MGE. With these novelties, we show the\nfollowing: (a) The proposed MGE allows to extract topological metrics that are\nmuch better predictors of the accuracy drop than metrics computed from current\ninput-agnostic BGEs; (b) Which metrics are important at different sparsity\nlevels and for different architectures; (c) A mixture of our topological\nmetrics can rank PaI algorithms more effectively than Ramanujan-based metrics.\nThe codebase is publicly available at https://github.com/eliacunegatti/mge-snn."
            ],
            "interesting paper": 5
        },
        {
            "papers": [
                "A Comprehensive Overview and Comparative Analysis on Deep Learning\n  Models: CNN, RNN, LSTM, GRU\nDeep learning (DL) has emerged as a powerful subset of machine learning (ML)\nand artificial intelligence (AI), outperforming traditional ML methods,\nespecially in handling unstructured and large datasets. Its impact spans across\nvarious domains, including speech recognition, healthcare, autonomous vehicles,\ncybersecurity, predictive analytics, and more. However, the complexity and\ndynamic nature of real-world problems present challenges in designing effective\ndeep learning models. Consequently, several deep learning models have been\ndeveloped to address different problems and applications. In this article, we\nconduct a comprehensive survey of various deep learning models, including\nConvolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs),\nGenerative Models, Deep Reinforcement Learning (DRL), and Deep Transfer\nLearning. We examine the structure, applications, benefits, and limitations of\neach model. Furthermore, we perform an analysis using three publicly available\ndatasets: IMDB, ARAS, and Fruit-360. We compare the performance of six renowned\ndeep learning models: CNN, Simple RNN, Long Short-Term Memory (LSTM),\nBidirectional LSTM, Gated Recurrent Unit (GRU), and Bidirectional GRU.",
                "USIM-DAL: Uncertainty-aware Statistical Image Modeling-based Dense\n  Active Learning for Super-resolution\nDense regression is a widely used approach in computer vision for tasks such\nas image super-resolution, enhancement, depth estimation, etc. However, the\nhigh cost of annotation and labeling makes it challenging to achieve accurate\nresults. We propose incorporating active learning into dense regression models\nto address this problem. Active learning allows models to select the most\ninformative samples for labeling, reducing the overall annotation cost while\nimproving performance. Despite its potential, active learning has not been\nwidely explored in high-dimensional computer vision regression tasks like\nsuper-resolution. We address this research gap and propose a new framework\ncalled USIM-DAL that leverages the statistical properties of colour images to\nlearn informative priors using probabilistic deep neural networks that model\nthe heteroscedastic predictive distribution allowing uncertainty\nquantification. Moreover, the aleatoric uncertainty from the network serves as\na proxy for error that is used for active learning. Our experiments on a wide\nvariety of datasets spanning applications in natural images (visual genome,\nBSD100), medical imaging (histopathology slides), and remote sensing (satellite\nimages) demonstrate the efficacy of the newly proposed USIM-DAL and superiority\nover several dense regression active learning methods.",
                "One Network, Many Masks: Towards More Parameter-Efficient Transfer\n  Learning\nFine-tuning pre-trained language models for multiple tasks tends to be\nexpensive in terms of storage. To mitigate this, parameter-efficient transfer\nlearning (PETL) methods have been proposed to address this issue, but they\nstill require a significant number of parameters and storage when being applied\nto broader ranges of tasks. To achieve even greater storage reduction, we\npropose PROPETL, a novel method that enables efficient sharing of a single PETL\nmodule which we call prototype network (e.g., adapter, LoRA, and prefix-tuning)\nacross layers and tasks. We then learn binary masks to select different\nsub-networks from the shared prototype network and apply them as PETL modules\ninto different layers. We find that the binary masks can determine crucial\ninformation from the network, which is often ignored in previous studies. Our\nwork can also be seen as a type of pruning method, where we find that\noverparameterization also exists in the seemingly small PETL modules. We\nevaluate PROPETL on various downstream tasks and show that it can outperform\nother PETL methods with approximately 10% of the parameter storage required by\nthe latter.",
                "Distill Gold from Massive Ores: Bi-level Data Pruning towards Efficient\n  Dataset Distillation\nData-efficient learning has garnered significant attention, especially given\nthe current trend of large multi-modal models. Recently, dataset distillation\nhas become an effective approach by synthesizing data samples that are\nessential for network training. However, it remains to be explored which\nsamples are essential for the dataset distillation process itself. In this\nwork, we study the data efficiency and selection for the dataset distillation\ntask. By re-formulating the dynamics of distillation, we provide insight into\nthe inherent redundancy in the real dataset, both theoretically and\nempirically. We propose to use the empirical loss value as a static data\npruning criterion. To further compensate for the variation of the data value in\ntraining, we find the most contributing samples based on their causal effects\non the distillation. The proposed selection strategy can efficiently exploit\nthe training dataset, outperform the previous SOTA distillation algorithms, and\nconsistently enhance the distillation algorithms, even on much larger-scale and\nmore heterogeneous datasets, e.g., full ImageNet-1K and Kinetics-400. We\nbelieve this paradigm will open up new avenues in the dynamics of distillation\nand pave the way for efficient dataset distillation. Our code is available on\nhttps://github.com/silicx/GoldFromOres-BiLP.",
                "Observation Denoising in CYRUS Soccer Simulation 2D Team For RoboCup\n  2023\nThe RoboCup competitions hold various leagues, and the Soccer Simulation 2D\nLeague is a major one among them. Soccer Simulation 2D (SS2D) match involves\ntwo teams, including 11 players and a coach, competing against each other. The\nplayers can only communicate with the Soccer Simulation Server during the game.\nThis paper presents the latest research of the CYRUS soccer simulation 2D team,\nthe champion of RoboCup 2021. We will explain our denoising idea powered by\nlong short-term memory networks (LSTM) and deep neural networks (DNN). The\nCYRUS team uses the CYRUS2D base code that was developed based on the Helios\nand Gliders bases.",
                "PuMer: Pruning and Merging Tokens for Efficient Vision Language Models\nLarge-scale vision language (VL) models use Transformers to perform\ncross-modal interactions between the input text and image. These cross-modal\ninteractions are computationally expensive and memory-intensive due to the\nquadratic complexity of processing the input image and text. We present PuMer:\na token reduction framework that uses text-informed Pruning and modality-aware\nMerging strategies to progressively reduce the tokens of input image and text,\nimproving model inference speed and reducing memory footprint. PuMer learns to\nkeep salient image tokens related to the input text and merges similar textual\nand visual tokens by adding lightweight token reducer modules at several\ncross-modal layers in the VL model. Training PuMer is mostly the same as\nfinetuning the original VL model but faster. Our evaluation for two vision\nlanguage models on four downstream VL tasks shows PuMer increases inference\nthroughput by up to 2x and reduces memory footprint by over 50% while incurring\nless than a 1% accuracy drop."
            ],
            "interesting paper": 2
        },
        {
            "papers": [
                "Intelligent gradient amplification for deep neural networks\nDeep learning models offer superior performance compared to other machine\nlearning techniques for a variety of tasks and domains, but pose their own\nchallenges. In particular, deep learning models require larger training times\nas the depth of a model increases, and suffer from vanishing gradients. Several\nsolutions address these problems independently, but there have been minimal\nefforts to identify an integrated solution that improves the performance of a\nmodel by addressing vanishing gradients, as well as accelerates the training\nprocess to achieve higher performance at larger learning rates. In this work,\nwe intelligently determine which layers of a deep learning model to apply\ngradient amplification to, using a formulated approach that analyzes gradient\nfluctuations of layers during training. Detailed experiments are performed for\nsimpler and deeper neural networks using two different intelligent measures and\ntwo different thresholds that determine the amplification layers, and a\ntraining strategy where gradients are amplified only during certain epochs.\nResults show that our amplification offers better performance compared to the\noriginal models, and achieves accuracy improvement of around 2.5% on CIFAR- 10\nand around 4.5% on CIFAR-100 datasets, even when the models are trained with\nhigher learning rates.",
                "Deep Electron Cloud-activity and Field-activity Relationships\nChemists have been pursuing the general mathematical laws to explain and\npredict molecular properties for a long time. However, most of the traditional\nquantitative structure-activity relationship (QSAR) models have limited\napplication domains, e.g., they tend to have poor generalization performance\nwhen applied to molecules with parent structures different from those of the\ntrained molecules. This paper attempts to develop a new QSAR method that is\ntheoretically possible to predict various properties of molecules with diverse\nstructures. The proposed deep electron cloud-activity relationships (DECAR) and\ndeep field-activity relationships (DFAR) methods consist of three essentials:\n(1) A large number of molecule entities with activity data as training objects\nand responses; (2) three-dimensional electron cloud density (ECD) or related\nfield data by the accurate density functional theory methods as input\ndescriptors; (3) a deep learning model that is sufficiently flexible and\npowerful to learn the large data described above. DECAR and DFAR are used to\ndistinguish 977 sweet and 1965 non-sweet molecules (with 6-fold data\naugmentation) and the classification performance is demonstrated to be\nsignificantly better than the traditional least squares support vector machine\n(LS-SVM) models using traditional descriptors. DECAR and DFAR would provide a\npossible way to establish a widely applicable, cumulative, and shareable\nartificial intelligence-driven QSAR system. They are likely to promote the\ndevelopment of an interactive platform to collect and share the accurate ECD\nand field data of millions of molecules with annotated activities. With enough\ninput data, we envision the appearance of several deep networks trained for\nvarious molecular activities. Finally, we could anticipate a single DECAR or\nDFAR network to learn and infer various properties of interest for chemical\nmolecules.",
                "On the impact of activation and normalization in obtaining isometric\n  embeddings at initialization\nIn this paper, we explore the structure of the penultimate Gram matrix in\ndeep neural networks, which contains the pairwise inner products of outputs\ncorresponding to a batch of inputs. In several architectures it has been\nobserved that this Gram matrix becomes degenerate with depth at initialization,\nwhich dramatically slows training. Normalization layers, such as batch or layer\nnormalization, play a pivotal role in preventing the rank collapse issue.\nDespite promising advances, the existing theoretical results do not extend to\nlayer normalization, which is widely used in transformers, and can not\nquantitatively characterize the role of non-linear activations. To bridge this\ngap, we prove that layer normalization, in conjunction with activation layers,\nbiases the Gram matrix of a multilayer perceptron towards the identity matrix\nat an exponential rate with depth at initialization. We quantify this rate\nusing the Hermite expansion of the activation function.",
                "Pedestrian detection with high-resolution event camera\nDespite the dynamic development of computer vision algorithms, the\nimplementation of perception and control systems for autonomous vehicles such\nas drones and self-driving cars still poses many challenges. A video stream\ncaptured by traditional cameras is often prone to problems such as motion blur\nor degraded image quality due to challenging lighting conditions. In addition,\nthe frame rate - typically 30 or 60 frames per second - can be a limiting\nfactor in certain scenarios. Event cameras (DVS -- Dynamic Vision Sensor) are a\npotentially interesting technology to address the above mentioned problems. In\nthis paper, we compare two methods of processing event data by means of deep\nlearning for the task of pedestrian detection. We used a representation in the\nform of video frames, convolutional neural networks and asynchronous sparse\nconvolutional neural networks. The results obtained illustrate the potential of\nevent cameras and allow the evaluation of the accuracy and efficiency of the\nmethods used for high-resolution (1280 x 720 pixels) footage.",
                "Dink-Net: Neural Clustering on Large Graphs\nDeep graph clustering, which aims to group the nodes of a graph into disjoint\nclusters with deep neural networks, has achieved promising progress in recent\nyears. However, the existing methods fail to scale to the large graph with\nmillion nodes. To solve this problem, a scalable deep graph clustering method\n(Dink-Net) is proposed with the idea of dilation and shrink. Firstly, by\ndiscriminating nodes, whether being corrupted by augmentations, representations\nare learned in a self-supervised manner. Meanwhile, the cluster centres are\ninitialized as learnable neural parameters. Subsequently, the clustering\ndistribution is optimized by minimizing the proposed cluster dilation loss and\ncluster shrink loss in an adversarial manner. By these settings, we unify the\ntwo-step clustering, i.e., representation learning and clustering optimization,\ninto an end-to-end framework, guiding the network to learn clustering-friendly\nfeatures. Besides, Dink-Net scales well to large graphs since the designed loss\nfunctions adopt the mini-batch data to optimize the clustering distribution\neven without performance drops. Both experimental results and theoretical\nanalyses demonstrate the superiority of our method. Compared to the runner-up,\nDink-Net achieves 9.62% NMI improvement on the ogbn-papers100M dataset with 111\nmillion nodes and 1.6 billion edges. The source code is released at\nhttps://github.com/yueliu1999/Dink-Net. Besides, a collection (papers, codes,\nand datasets) of deep graph clustering is shared at\nhttps://github.com/yueliu1999/Awesome-Deep-Graph-Clustering.",
                "InDL: A New Dataset and Benchmark for In-Diagram Logic Interpretation\n  based on Visual Illusion\nThis paper introduces a novel approach to evaluating deep learning models'\ncapacity for in-diagram logic interpretation. Leveraging the intriguing realm\nof visual illusions, we establish a unique dataset, InDL, designed to\nrigorously test and benchmark these models. Deep learning has witnessed\nremarkable progress in domains such as computer vision and natural language\nprocessing. However, models often stumble in tasks requiring logical reasoning\ndue to their inherent 'black box' characteristics, which obscure the\ndecision-making process. Our work presents a new lens to understand these\nmodels better by focusing on their handling of visual illusions -- a complex\ninterplay of perception and logic. We utilize six classic geometric optical\nillusions to create a comparative framework between human and machine visual\nperception. This methodology offers a quantifiable measure to rank models,\nelucidating potential weaknesses and providing actionable insights for model\nimprovements. Our experimental results affirm the efficacy of our benchmarking\nstrategy, demonstrating its ability to effectively rank models based on their\nlogic interpretation ability. As part of our commitment to reproducible\nresearch, the source code and datasets will be made publicly available at\nhttps://github.com/rabbit-magic-wh/InDL"
            ],
            "interesting paper": 1
        },
        {
            "papers": [
                "How Does Information Bottleneck Help Deep Learning?\nNumerous deep learning algorithms have been inspired by and understood via\nthe notion of information bottleneck, where unnecessary information is (often\nimplicitly) minimized while task-relevant information is maximized. However, a\nrigorous argument for justifying why it is desirable to control information\nbottlenecks has been elusive. In this paper, we provide the first rigorous\nlearning theory for justifying the benefit of information bottleneck in deep\nlearning by mathematically relating information bottleneck to generalization\nerrors. Our theory proves that controlling information bottleneck is one way to\ncontrol generalization errors in deep learning, although it is not the only or\nnecessary way. We investigate the merit of our new mathematical findings with\nexperiments across a range of architectures and learning settings. In many\ncases, generalization errors are shown to correlate with the degree of\ninformation bottleneck: i.e., the amount of the unnecessary information at\nhidden layers. This paper provides a theoretical foundation for current and\nfuture methods through the lens of information bottleneck. Our new\ngeneralization bounds scale with the degree of information bottleneck, unlike\nthe previous bounds that scale with the number of parameters, VC dimension,\nRademacher complexity, stability or robustness. Our code is publicly available\nat: https://github.com/xu-ji/information-bottleneck",
                "Stochastic Gradient Langevin Dynamics Based on Quantization with\n  Increasing Resolution\nStochastic learning dynamics based on Langevin or Levy stochastic\ndifferential equations (SDEs) in deep neural networks control the variance of\nnoise by varying the size of the mini-batch or directly those of injecting\nnoise. Since the noise variance affects the approximation performance, the\ndesign of the additive noise is significant in SDE-based learning and practical\nimplementation. In this paper, we propose an alternative stochastic descent\nlearning equation based on quantized optimization for non-convex objective\nfunctions, adopting a stochastic analysis perspective. The proposed method\nemploys a quantized optimization approach that utilizes Langevin SDE dynamics,\nallowing for controllable noise with an identical distribution without the need\nfor additive noise or adjusting the mini-batch size. Numerical experiments\ndemonstrate the effectiveness of the proposed algorithm on vanilla convolution\nneural network(CNN) models and the ResNet-50 architecture across various data\nsets. Furthermore, we provide a simple PyTorch implementation of the proposed\nalgorithm.",
                "E-PANNs: Sound Recognition Using Efficient Pre-trained Audio Neural\n  Networks\nSounds carry an abundance of information about activities and events in our\neveryday environment, such as traffic noise, road works, music, or people\ntalking. Recent machine learning methods, such as convolutional neural networks\n(CNNs), have been shown to be able to automatically recognize sound activities,\na task known as audio tagging. One such method, pre-trained audio neural\nnetworks (PANNs), provides a neural network which has been pre-trained on over\n500 sound classes from the publicly available AudioSet dataset, and can be used\nas a baseline or starting point for other tasks. However, the existing PANNs\nmodel has a high computational complexity and large storage requirement. This\ncould limit the potential for deploying PANNs on resource-constrained devices,\nsuch as on-the-edge sound sensors, and could lead to high energy consumption if\nmany such devices were deployed. In this paper, we reduce the computational\ncomplexity and memory requirement of the PANNs model by taking a pruning\napproach to eliminate redundant parameters from the PANNs model. The resulting\nEfficient PANNs (E-PANNs) model, which requires 36\\% less computations and 70\\%\nless memory, also slightly improves the sound recognition (audio tagging)\nperformance. The code for the E-PANNs model has been released under an open\nsource license.",
                "Simulation-Aided Deep Learning for Laser Ultrasonic Visualization\n  Testing\nIn recent years, laser ultrasonic visualization testing (LUVT) has attracted\nmuch attention because of its ability to efficiently perform non-contact\nultrasonic non-destructive testing.Despite many success reports of deep\nlearning based image analysis for widespread areas, attempts to apply deep\nlearning to defect detection in LUVT images face the difficulty of preparing a\nlarge dataset of LUVT images that is too expensive to scale. To compensate for\nthe scarcity of such training data, we propose a data augmentation method that\ngenerates artificial LUVT images by simulation and applies a style transfer to\nsimulated LUVT images.The experimental results showed that the effectiveness of\ndata augmentation based on the style-transformed simulated images improved the\nprediction performance of defects, rather than directly using the raw simulated\nimages for data augmentation.",
                "Vector-based Representation is the Key: A Study on Disentanglement and\n  Compositional Generalization\nRecognizing elementary underlying concepts from observations\n(disentanglement) and generating novel combinations of these concepts\n(compositional generalization) are fundamental abilities for humans to support\nrapid knowledge learning and generalize to new tasks, with which the deep\nlearning models struggle. Towards human-like intelligence, various works on\ndisentangled representation learning have been proposed, and recently some\nstudies on compositional generalization have been presented. However, few works\nstudy the relationship between disentanglement and compositional\ngeneralization, and the observed results are inconsistent. In this paper, we\nstudy several typical disentangled representation learning works in terms of\nboth disentanglement and compositional generalization abilities, and we provide\nan important insight: vector-based representation (using a vector instead of a\nscalar to represent a concept) is the key to empower both good disentanglement\nand strong compositional generalization. This insight also resonates the\nneuroscience research that the brain encodes information in neuron population\nactivity rather than individual neurons. Motivated by this observation, we\nfurther propose a method to reform the scalar-based disentanglement works\n($\\beta$-TCVAE and FactorVAE) to be vector-based to increase both capabilities.\nWe investigate the impact of the dimensions of vector-based representation and\none important question: whether better disentanglement indicates higher\ncompositional generalization. In summary, our study demonstrates that it is\npossible to achieve both good concept recognition and novel concept\ncomposition, contributing an important step towards human-like intelligence.",
                "Towards Machine Learning and Inference for Resource-constrained MCUs\nMachine learning (ML) is moving towards edge devices. However, ML models with\nhigh computational demands and energy consumption pose challenges for ML\ninference in resource-constrained environments, such as the deep sea. To\naddress these challenges, we propose a battery-free ML inference and model\npersonalization pipeline for microcontroller units (MCUs). As an example, we\nperformed fish image recognition in the ocean. We evaluated and compared the\naccuracy, runtime, power, and energy consumption of the model before and after\noptimization. The results demonstrate that, our pipeline can achieve 97.78%\naccuracy with 483.82 KB Flash, 70.32 KB RAM, 118 ms runtime, 4.83 mW power, and\n0.57 mJ energy consumption on MCUs, reducing by 64.17%, 12.31%, 52.42%, 63.74%,\nand 82.67%, compared to the baseline. The results indicate the feasibility of\nbattery-free ML inference on MCUs."
            ],
            "interesting paper": 1
        },
        {
            "papers": [
                "CVSNet: A Computer Implementation for Central Visual System of The Brain\nIn computer vision, different basic blocks are created around different\nmatrix operations, and models based on different basic blocks have achieved\ngood results. Good results achieved in vision tasks grants them rationality.\nHowever, these experimental-based models also make deep learning long\ncriticized for principle and interpretability. Deep learning originated from\nthe concept of neurons in neuroscience, but recent designs detached natural\nneural networks except for some simple concepts. In this paper, we build an\nartificial neural network, CVSNet, which can be seen as a computer\nimplementation for central visual system of the brain. Each block in CVSNet\nrepresents the same vision information as that in brains. In CVSNet, blocks\ndiffers from each other and visual information flows through three independent\npathways and five different blocks. Thus CVSNet is completely different from\nthe design of all previous models, in which basic blocks are repeated to build\nmodel and information between channels is mixed at the outset. In ablation\nexperiment, we show the information extracted by blocks in CVSNet and compare\nwith previous networks, proving effectiveness and rationality of blocks in\nCVSNet from experiment side. And in the experiment of object recognition,\nCVSNet achieves comparable results to ConvNets, Vision Transformers and MLPs.",
                "Are Large Kernels Better Teachers than Transformers for ConvNets?\nThis paper reveals a new appeal of the recently emerged large-kernel\nConvolutional Neural Networks (ConvNets): as the teacher in Knowledge\nDistillation (KD) for small-kernel ConvNets. While Transformers have led\nstate-of-the-art (SOTA) performance in various fields with ever-larger models\nand labeled data, small-kernel ConvNets are considered more suitable for\nresource-limited applications due to the efficient convolution operation and\ncompact weight sharing. KD is widely used to boost the performance of\nsmall-kernel ConvNets. However, previous research shows that it is not quite\neffective to distill knowledge (e.g., global information) from Transformers to\nsmall-kernel ConvNets, presumably due to their disparate architectures. We\nhereby carry out a first-of-its-kind study unveiling that modern large-kernel\nConvNets, a compelling competitor to Vision Transformers, are remarkably more\neffective teachers for small-kernel ConvNets, due to more similar\narchitectures. Our findings are backed up by extensive experiments on both\nlogit-level and feature-level KD ``out of the box\", with no dedicated\narchitectural nor training recipe modifications. Notably, we obtain the\n\\textbf{best-ever pure ConvNet} under 30M parameters with \\textbf{83.1\\%} top-1\naccuracy on ImageNet, outperforming current SOTA methods including ConvNeXt V2\nand Swin V2. We also find that beneficial characteristics of large-kernel\nConvNets, e.g., larger effective receptive fields, can be seamlessly\ntransferred to students through this large-to-small kernel distillation. Code\nis available at: \\url{https://github.com/VITA-Group/SLaK}.",
                "End-to-end Training of Deep Boltzmann Machines by Unbiased Contrastive\n  Divergence with Local Mode Initialization\nWe address the problem of biased gradient estimation in deep Boltzmann\nmachines (DBMs). The existing method to obtain an unbiased estimator uses a\nmaximal coupling based on a Gibbs sampler, but when the state is\nhigh-dimensional, it takes a long time to converge. In this study, we propose\nto use a coupling based on the Metropolis-Hastings (MH) and to initialize the\nstate around a local mode of the target distribution. Because of the propensity\nof MH to reject proposals, the coupling tends to converge in only one step with\na high probability, leading to high efficiency. We find that our method allows\nDBMs to be trained in an end-to-end fashion without greedy pretraining. We also\npropose some practical techniques to further improve the performance of DBMs.\nWe empirically demonstrate that our training algorithm enables DBMs to show\ncomparable generative performance to other deep generative models, achieving\nthe FID score of 10.33 for MNIST.",
                "Adaptation of Tongue Ultrasound-Based Silent Speech Interfaces Using\n  Spatial Transformer Networks\nThanks to the latest deep learning algorithms, silent speech interfaces (SSI)\nare now able to synthesize intelligible speech from articulatory movement data\nunder certain conditions. However, the resulting models are rather\nspeaker-specific, making a quick switch between users troublesome. Even for the\nsame speaker, these models perform poorly cross-session, i.e. after dismounting\nand re-mounting the recording equipment. To aid quick speaker and session\nadaptation of ultrasound tongue imaging-based SSI models, we extend our deep\nnetworks with a spatial transformer network (STN) module, capable of performing\nan affine transformation on the input images. Although the STN part takes up\nonly about 10% of the network, our experiments show that adapting just the STN\nmodule might allow to reduce MSE by 88% on the average, compared to retraining\nthe whole network. The improvement is even larger (around 92%) when adapting\nthe network to different recording sessions from the same speaker.",
                "Diagnosis and Prognosis of Head and Neck Cancer Patients using\n  Artificial Intelligence\nCancer is one of the most life-threatening diseases worldwide, and head and\nneck (H&N) cancer is a prevalent type with hundreds of thousands of new cases\nrecorded each year. Clinicians use medical imaging modalities such as computed\ntomography and positron emission tomography to detect the presence of a tumor,\nand they combine that information with clinical data for patient prognosis. The\nprocess is mostly challenging and time-consuming. Machine learning and deep\nlearning can automate these tasks to help clinicians with highly promising\nresults. This work studies two approaches for H&N tumor segmentation: (i)\nexploration and comparison of vision transformer (ViT)-based and convolutional\nneural network-based models; and (ii) proposal of a novel 2D perspective to\nworking with 3D data. Furthermore, this work proposes two new architectures for\nthe prognosis task. An ensemble of several models predicts patient outcomes\n(which won the HECKTOR 2021 challenge prognosis task), and a ViT-based\nframework concurrently performs patient outcome prediction and tumor\nsegmentation, which outperforms the ensemble model.",
                "KrADagrad: Kronecker Approximation-Domination Gradient Preconditioned\n  Stochastic Optimization\nSecond order stochastic optimizers allow parameter update step size and\ndirection to adapt to loss curvature, but have traditionally required too much\nmemory and compute for deep learning. Recently, Shampoo [Gupta et al., 2018]\nintroduced a Kronecker factored preconditioner to reduce these requirements: it\nis used for large deep models [Anil et al., 2020] and in production [Anil et\nal., 2022]. However, it takes inverse matrix roots of ill-conditioned matrices.\nThis requires 64-bit precision, imposing strong hardware constraints. In this\npaper, we propose a novel factorization, Kronecker Approximation-Domination\n(KrAD). Using KrAD, we update a matrix that directly approximates the inverse\nempirical Fisher matrix (like full matrix AdaGrad), avoiding inversion and\nhence 64-bit precision. We then propose KrADagrad$^\\star$, with similar\ncomputational costs to Shampoo and the same regret. Synthetic ill-conditioned\nexperiments show improved performance over Shampoo for 32-bit precision, while\nfor several real datasets we have comparable or better generalization."
            ],
            "interesting paper": 2
        },
        {
            "papers": [
                "Breast Cancer Detection and Diagnosis: A comparative study of\n  state-of-the-arts deep learning architectures\nBreast cancer is a prevalent form of cancer among women, with over 1.5\nmillion women being diagnosed each year. Unfortunately, the survival rates for\nbreast cancer patients in certain third-world countries, like South Africa, are\nalarmingly low, with only 40% of diagnosed patients surviving beyond five\nyears. The inadequate availability of resources, including qualified\npathologists, delayed diagnoses, and ineffective therapy planning, contribute\nto this low survival rate. To address this pressing issue, medical specialists\nand researchers have turned to domain-specific AI approaches, specifically deep\nlearning models, to develop end-to-end solutions that can be integrated into\ncomputer-aided diagnosis (CAD) systems. By improving the workflow of\npathologists, these AI models have the potential to enhance the detection and\ndiagnosis of breast cancer. This research focuses on evaluating the performance\nof various cutting-edge convolutional neural network (CNN) architectures in\ncomparison to a relatively new model called the Vision Trans-former (ViT). The\nobjective is to determine the superiority of these models in terms of their\naccuracy and effectiveness. The experimental results reveal that the ViT models\noutperform the other selected state-of-the-art CNN architectures, achieving an\nimpressive accuracy rate of 95.15%. This study signifies a significant\nadvancement in the field, as it explores the utilization of data augmentation\nand other relevant preprocessing techniques in conjunction with deep learning\nmodels for the detection and diagnosis of breast cancer using datasets of\nBreast Cancer Histopathological Image Classification.",
                "Sea Ice Extraction via Remote Sensed Imagery: Algorithms, Datasets,\n  Applications and Challenges\nThe deep learning, which is a dominating technique in artificial\nintelligence, has completely changed the image understanding over the past\ndecade. As a consequence, the sea ice extraction (SIE) problem has reached a\nnew era. We present a comprehensive review of four important aspects of SIE,\nincluding algorithms, datasets, applications, and the future trends. Our review\nfocuses on researches published from 2016 to the present, with a specific focus\non deep learning-based approaches in the last five years. We divided all\nrelegated algorithms into 3 categories, including classical image segmentation\napproach, machine learning-based approach and deep learning-based methods. We\nreviewed the accessible ice datasets including SAR-based datasets, the\noptical-based datasets and others. The applications are presented in 4 aspects\nincluding climate research, navigation, geographic information systems (GIS)\nproduction and others. It also provides insightful observations and inspiring\nfuture research directions.",
                "Training-free Neural Architecture Search for RNNs and Transformers\nNeural architecture search (NAS) has allowed for the automatic creation of\nnew and effective neural network architectures, offering an alternative to the\nlaborious process of manually designing complex architectures. However,\ntraditional NAS algorithms are slow and require immense amounts of computing\npower. Recent research has investigated training-free NAS metrics for image\nclassification architectures, drastically speeding up search algorithms. In\nthis paper, we investigate training-free NAS metrics for recurrent neural\nnetwork (RNN) and BERT-based transformer architectures, targeted towards\nlanguage modeling tasks. First, we develop a new training-free metric, named\nhidden covariance, that predicts the trained performance of an RNN architecture\nand significantly outperforms existing training-free metrics. We experimentally\nevaluate the effectiveness of the hidden covariance metric on the NAS-Bench-NLP\nbenchmark. Second, we find that the current search space paradigm for\ntransformer architectures is not optimized for training-free neural\narchitecture search. Instead, a simple qualitative analysis can effectively\nshrink the search space to the best performing architectures. This conclusion\nis based on our investigation of existing training-free metrics and new metrics\ndeveloped from recent transformer pruning literature, evaluated on our own\nbenchmark of trained BERT architectures. Ultimately, our analysis shows that\nthe architecture search space and the training-free metric must be developed\ntogether in order to achieve effective results.",
                "Diffused Redundancy in Pre-trained Representations\nRepresentations learned by pre-training a neural network on a large dataset\nare increasingly used successfully to perform a variety of downstream tasks. In\nthis work, we take a closer look at how features are encoded in such\npre-trained representations. We find that learned representations in a given\nlayer exhibit a degree of diffuse redundancy, ie, any randomly chosen subset of\nneurons in the layer that is larger than a threshold size shares a large degree\nof similarity with the full layer and is able to perform similarly as the whole\nlayer on a variety of downstream tasks. For example, a linear probe trained on\n$20\\%$ of randomly picked neurons from the penultimate layer of a ResNet50\npre-trained on ImageNet1k achieves an accuracy within $5\\%$ of a linear probe\ntrained on the full layer of neurons for downstream CIFAR10 classification. We\nconduct experiments on different neural architectures (including CNNs and\nTransformers) pre-trained on both ImageNet1k and ImageNet21k and evaluate a\nvariety of downstream tasks taken from the VTAB benchmark. We find that the\nloss and dataset used during pre-training largely govern the degree of diffuse\nredundancy and the \"critical mass\" of neurons needed often depends on the\ndownstream task, suggesting that there is a task-inherent\nredundancy-performance Pareto frontier. Our findings shed light on the nature\nof representations learned by pre-trained deep neural networks and suggest that\nentire layers might not be necessary to perform many downstream tasks. We\ninvestigate the potential for exploiting this redundancy to achieve efficient\ngeneralization for downstream tasks and also draw caution to certain possible\nunintended consequences. Our code is available at\n\\url{https://github.com/nvedant07/diffused-redundancy}.",
                "Adam Accumulation to Reduce Memory Footprints of both Activations and\n  Gradients for Large-scale DNN Training\nRunning out of GPU memory has become a main bottleneck for large-scale DNN\ntraining. How to reduce the memory footprint during training has received\nintensive research attention. We find that previous gradient accumulation\nreduces activation memory but fails to be compatible with gradient memory\nreduction due to a contradiction between preserving gradients and releasing\ngradients. To address this issue, we propose a novel optimizer accumulation\nmethod for Adam, named Adam Accumulation (AdamA), which enables reducing both\nactivation and gradient memory. Specifically, AdamA directly integrates\ngradients into optimizer states and accumulates optimizer states over\nmicro-batches, so that gradients can be released immediately after use. We\nmathematically and experimentally demonstrate AdamA yields the same convergence\nproperties as Adam. Evaluated on transformer-based models, AdamA achieves up to\n23% memory reduction compared to gradient accumulation with less than 2%\ndegradation in training throughput. Notably, AdamA can work together with\nmemory reduction methods for optimizer states to fit 1.26x~3.14x larger models\nover PyTorch and DeepSpeed baseline on GPUs with different memory capacities.",
                "Transformers learn to implement preconditioned gradient descent for\n  in-context learning\nSeveral recent works demonstrate that transformers can implement algorithms\nlike gradient descent. By a careful construction of weights, these works show\nthat multiple layers of transformers are expressive enough to simulate\niterations of gradient descent. Going beyond the question of expressivity, we\nask: Can transformers learn to implement such algorithms by training over\nrandom problem instances? To our knowledge, we make the first theoretical\nprogress on this question via an analysis of the loss landscape for linear\ntransformers trained over random instances of linear regression. For a single\nattention layer, we prove the global minimum of the training objective\nimplements a single iteration of preconditioned gradient descent. Notably, the\npreconditioning matrix not only adapts to the input distribution but also to\nthe variance induced by data inadequacy. For a transformer with $L$ attention\nlayers, we prove certain critical points of the training objective implement\n$L$ iterations of preconditioned gradient descent. Our results call for future\ntheoretical studies on learning algorithms by training transformers."
            ],
            "interesting paper": 3
        },
        {
            "papers": [
                "On the Weight Dynamics of Deep Normalized Networks\nRecent studies have shown that high disparities in effective learning rates\n(ELRs) across layers in deep neural networks can negatively affect\ntrainability. We formalize how these disparities evolve over time by modeling\nweight dynamics (evolution of expected gradient and weight norms) of networks\nwith normalization layers, predicting the evolution of layer-wise ELR ratios.\nWe prove that when training with any constant learning rate, ELR ratios\nconverge to 1, despite initial gradient explosion. We identify a ``critical\nlearning rate\" beyond which ELR disparities widen, which only depends on\ncurrent ELRs. To validate our findings, we devise a hyper-parameter-free\nwarm-up method that successfully minimizes ELR spread quickly in theory and\npractice. Our experiments link ELR spread with trainability, a relationship\nthat is most evident in very deep networks with significant gradient magnitude\nexcursions.",
                "Smooth Min-Max Monotonic Networks\nMonotonicity constraints are powerful regularizers in statistical modelling.\nThey can support fairness in computer-aided decision making and increase\nplausibility in data-driven scientific models. The seminal min-max (MM) neural\nnetwork architecture ensures monotonicity, but often gets stuck in undesired\nlocal optima during training because of partial derivatives of the MM\nnonlinearities being zero. We propose a simple modification of the MM network\nusing strictly-increasing smooth minimum and maximum functions that alleviates\nthis problem. The resulting smooth min-max (SMM) network module inherits the\nasymptotic approximation properties from the MM architecture. It can be used\nwithin larger deep learning systems trained end-to-end. The SMM module is\nconceptually simple and computationally less demanding than state-of-the-art\nneural networks for monotonic modelling. Our experiments show that this does\nnot come with a loss in generalization performance compared to alternative\nneural and non-neural approaches.",
                "Adaptation and Optimization of Automatic Speech Recognition (ASR) for\n  the Maritime Domain in the Field of VHF Communication\nThis paper introduces a multilingual automatic speech recognizer (ASR) for\nmaritime radio communi-cation that automatically converts received VHF radio\nsignals into text. The challenges of maritime radio communication are described\nat first, and the deep learning architecture of marFM consisting of audio\nprocessing techniques and machine learning algorithms is presented.\nSubsequently, maritime radio data of interest is analyzed and then used to\nevaluate the transcription performance of our ASR model for various maritime\nradio data.",
                "Rotating Features for Object Discovery\nThe binding problem in human cognition, concerning how the brain represents\nand connects objects within a fixed network of neural connections, remains a\nsubject of intense debate. Most machine learning efforts addressing this issue\nin an unsupervised setting have focused on slot-based methods, which may be\nlimiting due to their discrete nature and difficulty to express uncertainty.\nRecently, the Complex AutoEncoder was proposed as an alternative that learns\ncontinuous and distributed object-centric representations. However, it is only\napplicable to simple toy data. In this paper, we present Rotating Features, a\ngeneralization of complex-valued features to higher dimensions, and a new\nevaluation procedure for extracting objects from distributed representations.\nAdditionally, we show the applicability of our approach to pre-trained\nfeatures. Together, these advancements enable us to scale distributed\nobject-centric representations from simple toy to real-world data. We believe\nthis work advances a new paradigm for addressing the binding problem in machine\nlearning and has the potential to inspire further innovation in the field.",
                "The Galerkin method beats Graph-Based Approaches for Spectral Algorithms\nHistorically, the machine learning community has derived spectral\ndecompositions from graph-based approaches. We break with this approach and\nprove the statistical and computational superiority of the Galerkin method,\nwhich consists in restricting the study to a small set of test functions. In\nparticular, we introduce implementation tricks to deal with differential\noperators in large dimensions with structured kernels. Finally, we extend on\nthe core principles beyond our approach to apply them to non-linear spaces of\nfunctions, such as the ones parameterized by deep neural networks, through\nloss-based optimization procedures.",
                "Microstructure quality control of steels using deep learning\nIn quality control, microstructures are investigated rigorously to ensure\nstructural integrity, exclude the presence of critical volume defects, and\nvalidate the formation of the target microstructure. For quenched,\nhierarchically-structured steels, the morphology of the bainitic and\nmartensitic microstructures are of major concern to guarantee the reliability\nof the material under service conditions. Therefore, industries conduct small\nsample-size inspections of materials cross-sections through metallographers to\nvalidate the needle morphology of such microstructures. We demonstrate\nround-robin test results revealing that this visual grading is afflicted by\npronounced subjectivity despite the thorough training of personnel. Instead, we\npropose a deep learning image classification approach that distinguishes steels\nbased on their microstructure type and classifies their needle length alluding\nto the ISO 643 grain size assessment standard. This classification approach\nfacilitates the reliable, objective, and automated classification of\nhierarchically structured steels. Specifically, an accuracy of 96% and roughly\n91% is attained for the distinction of martensite/bainite subtypes and needle\nlength, respectively. This is achieved on an image dataset that contains\nsignificant variance and labeling noise as it is acquired over more than ten\nyears from multiple plants, alloys, etchant applications, and light optical\nmicroscopes by many metallographers (raters). Interpretability analysis gives\ninsights into the decision-making of these models and allows for estimating\ntheir generalization capability."
            ],
            "interesting paper": 4
        },
        {
            "papers": [
                "A Novel Correlation-optimized Deep Learning Method for Wind Speed\n  Forecast\nThe increasing installation rate of wind power poses great challenges to the\nglobal power system. In order to ensure the reliable operation of the power\nsystem, it is necessary to accurately forecast the wind speed and power of the\nwind turbines. At present, deep learning is progressively applied to the wind\nspeed prediction. Nevertheless, the recent deep learning methods still reflect\nthe embarrassment for practical applications due to model interpretability and\nhardware limitation. To this end, a novel deep knowledge-based learning method\nis proposed in this paper. The proposed method hybridizes pre-training method\nand auto-encoder structure to improve data representation and modeling of the\ndeep knowledge-based learning framework. In order to form knowledge and\ncorresponding absorbers, the original data is preprocessed by an optimization\nmodel based on correlation to construct multi-layer networks (knowledge) which\nare absorbed by sequence to sequence (Seq2Seq) models. Specifically, new\ncognition and memory units (CMU) are designed to reinforce traditional deep\nlearning framework. Finally, the effectiveness of the proposed method is\nverified by three wind prediction cases from a wind farm in Liaoning, China.\nExperimental results show that the proposed method increases the stability and\ntraining efficiency compared to the traditional LSTM method and LSTM/GRU-based\nSeq2Seq method for applications of wind speed forecasting.",
                "Case Studies on X-Ray Imaging, MRI and Nuclear Imaging\nThe field of medical imaging is an essential aspect of the medical sciences,\ninvolving various forms of radiation to capture images of the internal tissues\nand organs of the body. These images provide vital information for clinical\ndiagnosis, and in this chapter, we will explore the use of X-ray, MRI, and\nnuclear imaging in detecting severe illnesses. However, manual evaluation and\nstorage of these images can be a challenging and time-consuming process. To\naddress this issue, artificial intelligence (AI)-based techniques, particularly\ndeep learning (DL), have become increasingly popular for systematic feature\nextraction and classification from imaging modalities, thereby aiding doctors\nin making rapid and accurate diagnoses. In this review study, we will focus on\nhow AI-based approaches, particularly the use of Convolutional Neural Networks\n(CNN), can assist in disease detection through medical imaging technology. CNN\nis a commonly used approach for image analysis due to its ability to extract\nfeatures from raw input images, and as such, will be the primary area of\ndiscussion in this study. Therefore, we have considered CNN as our discussion\narea in this study to diagnose ailments using medical imaging technology.",
                "Publicly available datasets of breast histopathology H&E whole-slide\n  images: A scoping review\nAdvancements in digital pathology and computing resources have made a\nsignificant impact in the field of computational pathology for breast cancer\ndiagnosis and treatment. However, access to high-quality labeled\nhistopathological images of breast cancer is a big challenge that limits the\ndevelopment of accurate and robust deep learning models. In this scoping\nreview, we identified the publicly available datasets of breast H&E stained\nwhole-slide images (WSI) that can be used to develop deep learning algorithms.\nWe systematically searched nine scientific literature databases and nine\nresearch data repositories and found 17 publicly available datasets containing\n10385 H&E WSIs of breast cancer. Moreover, we reported image metadata and\ncharacteristics for each dataset to assist researchers in selecting proper\ndatasets for specific tasks in breast cancer computational pathology. In\naddition, we compiled two lists of breast H&E patches and private datasets as\nsupplementary resources for researchers. Notably, only 28% of the included\narticles utilized multiple datasets, and only 14% used an external validation\nset, suggesting that the performance of other developed models may be\nsusceptible to overestimation. The TCGA-BRCA was used in 52% of the selected\nstudies. This dataset has a considerable selection bias that can impact the\nrobustness and generalizability of the trained algorithms. There is also a lack\nof consistent metadata reporting of breast WSI datasets that can be an issue in\ndeveloping accurate deep learning models, indicating the necessity of\nestablishing explicit guidelines for documenting breast WSI dataset\ncharacteristics and metadata.",
                "MutateNN: Mutation Testing of Image Recognition Models Deployed on\n  Hardware Accelerators\nThe increased utilization of Artificial Intelligence (AI) solutions brings\nwith it inherent risks, such as misclassification and sub-optimal execution\ntime performance, due to errors introduced in their deployment infrastructure\nbecause of problematic configuration and software faults. On top of that, AI\nmethods such as Deep Neural Networks (DNNs) are utilized to perform demanding,\nresource-intensive and even safety-critical tasks, and in order to effectively\nincrease the performance of the DNN models deployed, a variety of Machine\nLearning (ML) compilers have been developed, allowing compatibility of DNNs\nwith a variety of hardware acceleration devices, such as GPUs and TPUs.\nFurthermore the correctness of the compilation process should be verified. In\norder to allow developers and researchers to explore the robustness of DNN\nmodels deployed on different hardware accelerators via ML compilers, in this\npaper we propose MutateNN, a tool that provides mutation testing and model\nanalysis features in the context of deployment on different hardware\naccelerators. To demonstrate the capabilities of MutateNN, we focus on the\nimage recognition domain by applying mutation testing to 7 well-established\nmodels utilized for image classification. We instruct 21 mutations of 6\ndifferent categories, and deploy our mutants on 4 different hardware\nacceleration devices of varying capabilities. Our results indicate that models\nare proven robust to changes related to layer modifications and arithmetic\noperators, while presenting discrepancies of up to 90.3% in mutants related to\nconditional operators. We also observed unexpectedly severe performance\ndegradation on mutations related to arithmetic types of variables, leading the\nmutants to produce the same classifications for all dataset inputs.",
                "The Information Pathways Hypothesis: Transformers are Dynamic\n  Self-Ensembles\nTransformers use the dense self-attention mechanism which gives a lot of\nflexibility for long-range connectivity. Over multiple layers of a deep\ntransformer, the number of possible connectivity patterns increases\nexponentially. However, very few of these contribute to the performance of the\nnetwork, and even fewer are essential. We hypothesize that there are sparsely\nconnected sub-networks within a transformer, called information pathways which\ncan be trained independently. However, the dynamic (i.e., input-dependent)\nnature of these pathways makes it difficult to prune dense self-attention\nduring training. But the overall distribution of these pathways is often\npredictable. We take advantage of this fact to propose Stochastically\nSubsampled self-Attention (SSA) - a general-purpose training strategy for\ntransformers that can reduce both the memory and computational cost of\nself-attention by 4 to 8 times during training while also serving as a\nregularization method - improving generalization over dense training. We show\nthat an ensemble of sub-models can be formed from the subsampled pathways\nwithin a network, which can achieve better performance than its densely\nattended counterpart. We perform experiments on a variety of NLP, computer\nvision and graph learning tasks in both generative and discriminative settings\nto provide empirical evidence for our claims and show the effectiveness of the\nproposed method.",
                "AlerTiger: Deep Learning for AI Model Health Monitoring at LinkedIn\nData-driven companies use AI models extensively to develop products and\nintelligent business solutions, making the health of these models crucial for\nbusiness success. Model monitoring and alerting in industries pose unique\nchallenges, including a lack of clear model health metrics definition, label\nsparsity, and fast model iterations that result in short-lived models and\nfeatures. As a product, there are also requirements for scalability,\ngeneralizability, and explainability. To tackle these challenges, we propose\nAlerTiger, a deep-learning-based MLOps model monitoring system that helps AI\nteams across the company monitor their AI models' health by detecting anomalies\nin models' input features and output score over time. The system consists of\nfour major steps: model statistics generation, deep-learning-based anomaly\ndetection, anomaly post-processing, and user alerting. Our solution generates\nthree categories of statistics to indicate AI model health, offers a two-stage\ndeep anomaly detection solution to address label sparsity and attain the\ngeneralizability of monitoring new models, and provides holistic reports for\nactionable alerts. This approach has been deployed to most of LinkedIn's\nproduction AI models for over a year and has identified several model issues\nthat later led to significant business metric gains after fixing."
            ],
            "interesting paper": 5
        }
    ],
    "Drew Garcia": [
        {
            "papers": [
                "Reverse Engineering Self-Supervised Learning\nSelf-supervised learning (SSL) is a powerful tool in machine learning, but\nunderstanding the learned representations and their underlying mechanisms\nremains a challenge. This paper presents an in-depth empirical analysis of\nSSL-trained representations, encompassing diverse models, architectures, and\nhyperparameters. Our study reveals an intriguing aspect of the SSL training\nprocess: it inherently facilitates the clustering of samples with respect to\nsemantic labels, which is surprisingly driven by the SSL objective's\nregularization term. This clustering process not only enhances downstream\nclassification but also compresses the data information. Furthermore, we\nestablish that SSL-trained representations align more closely with semantic\nclasses rather than random classes. Remarkably, we show that learned\nrepresentations align with semantic classes across various hierarchical levels,\nand this alignment increases during training and when moving deeper into the\nnetwork. Our findings provide valuable insights into SSL's representation\nlearning mechanisms and their impact on performance across different sets of\nclasses.",
                "Classic machine learning methods\nIn this chapter, we present the main classic machine learning methods. A\nlarge part of the chapter is devoted to supervised learning techniques for\nclassification and regression, including nearest-neighbor methods, linear and\nlogistic regressions, support vector machines and tree-based algorithms. We\nalso describe the problem of overfitting as well as strategies to overcome it.\nWe finally provide a brief overview of unsupervised learning methods, namely\nfor clustering and dimensionality reduction.",
                "Rethinking the Evaluation Protocol of Domain Generalization\nDomain generalization aims to solve the challenge of Out-of-Distribution\n(OOD) generalization by leveraging common knowledge learned from multiple\ntraining domains to generalize to unseen test domains. To accurately evaluate\nthe OOD generalization ability, it is required that test data information is\nunavailable. However, the current domain generalization protocol may still have\npotential test data information leakage. This paper examines the risks of test\ndata information leakage from two aspects of the current evaluation protocol:\nsupervised pretraining on ImageNet and oracle model selection. We propose\nmodifications to the current protocol that we should employ self-supervised\npretraining or train from scratch instead of employing the current supervised\npretraining, and we should use multiple test domains. These would result in a\nmore precise evaluation of OOD generalization ability. We also rerun the\nalgorithms with the modified protocol and introduce new leaderboards to\nencourage future research in domain generalization with a fairer comparison.",
                "Differentiable Clustering with Perturbed Spanning Forests\nWe introduce a differentiable clustering method based on stochastic\nperturbations of minimum-weight spanning forests. This allows us to include\nclustering in end-to-end trainable pipelines, with efficient gradients. We show\nthat our method performs well even in difficult settings, such as data sets\nwith high noise and challenging geometries. We also formulate an ad hoc loss to\nefficiently learn from partial clustering data using this operation. We\ndemonstrate its performance on several data sets for supervised and\nsemi-supervised tasks.",
                "How to escape sharp minima with random perturbations\nModern machine learning applications have witnessed the remarkable success of\noptimization algorithms that are designed to find flat minima. Motivated by\nthis design choice, we undertake a formal study that (i) formulates the notion\nof flat minima, and (ii) studies the complexity of finding them. Specifically,\nwe adopt the trace of the Hessian of the cost function as a measure of\nflatness, and use it to formally define the notion of approximate flat minima.\nUnder this notion, we then analyze algorithms that find approximate flat minima\nefficiently. For general cost functions, we discuss a gradient-based algorithm\nthat finds an approximate flat local minimum efficiently. The main component of\nthe algorithm is to use gradients computed from randomly perturbed iterates to\nestimate a direction that leads to flatter minima. For the setting where the\ncost function is an empirical risk over training data, we present a faster\nalgorithm that is inspired by a recently proposed practical algorithm called\nsharpness-aware minimization, supporting its success in practice.",
                "Generalizable Low-Resource Activity Recognition with Diverse and\n  Discriminative Representation Learning\nHuman activity recognition (HAR) is a time series classification task that\nfocuses on identifying the motion patterns from human sensor readings. Adequate\ndata is essential but a major bottleneck for training a generalizable HAR\nmodel, which assists customization and optimization of online web applications.\nHowever, it is costly in time and economy to collect large-scale labeled data\nin reality, i.e., the low-resource challenge. Meanwhile, data collected from\ndifferent persons have distribution shifts due to different living habits, body\nshapes, age groups, etc. The low-resource and distribution shift challenges are\ndetrimental to HAR when applying the trained model to new unseen subjects. In\nthis paper, we propose a novel approach called Diverse and Discriminative\nrepresentation Learning (DDLearn) for generalizable low-resource HAR. DDLearn\nsimultaneously considers diversity and discrimination learning. With the\nconstructed self-supervised learning task, DDLearn enlarges the data diversity\nand explores the latent activity properties. Then, we propose a diversity\npreservation module to preserve the diversity of learned features by enlarging\nthe distribution divergence between the original and augmented domains.\nMeanwhile, DDLearn also enhances semantic discrimination by learning\ndiscriminative representations with supervised contrastive learning. Extensive\nexperiments on three public HAR datasets demonstrate that our method\nsignificantly outperforms state-of-art methods by an average accuracy\nimprovement of 9.5% under the low-resource distribution shift scenarios, while\nbeing a generic, explainable, and flexible framework. Code is available at:\nhttps://github.com/microsoft/robustlearn."
            ],
            "interesting paper": 1
        },
        {
            "papers": [
                "Applications of Intelligent Systems in Green Technology\nIntelligent Systems (ISs) are technologically advanced machines, which\nperceive and respond to the environment around them. They are usually of\nvarious forms ranging from software to hardware. ISs are generally the fusion\nof Artificial Intelligence (AI), robotics and Internet of things (IoT). In\norder to strengthen ISs, one of the key technologies is green technology (GT).\nIt refers to the continuously advancing methods and materials, which cover\ntechniques for producing energy to non-toxic cleaning products. It may also be\nbroadened to saving energy, and reducing toxic and waste materials in the\nenvironment. The motto of GT can be achieved by using the ISs. In this paper,\nwe present various applications of ISs in GT. Moreover, we discuss various\npossible solutions using ISs in order to overcome the on-going real-life\nproblems.",
                "Lucy-SKG: Learning to Play Rocket League Efficiently Using Deep\n  Reinforcement Learning\nA successful tactic that is followed by the scientific community for\nadvancing AI is to treat games as problems, which has been proven to lead to\nvarious breakthroughs. We adapt this strategy in order to study Rocket League,\na widely popular but rather under-explored 3D multiplayer video game with a\ndistinct physics engine and complex dynamics that pose a significant challenge\nin developing efficient and high-performance game-playing agents. In this\npaper, we present Lucy-SKG, a Reinforcement Learning-based model that learned\nhow to play Rocket League in a sample-efficient manner, outperforming by a\nnotable margin the two highest-ranking bots in this game, namely Necto (2022\nbot champion) and its successor Nexto, thus becoming a state-of-the-art agent.\nOur contributions include: a) the development of a reward analysis and\nvisualization library, b) novel parameterizable reward shape functions that\ncapture the utility of complex reward types via our proposed Kinesthetic Reward\nCombination (KRC) technique, and c) design of auxiliary neural architectures\nfor training on reward prediction and state representation tasks in an\non-policy fashion for enhanced efficiency in learning speed and performance. By\nperforming thorough ablation studies for each component of Lucy-SKG, we showed\ntheir independent effectiveness in overall performance. In doing so, we\ndemonstrate the prospects and challenges of using sample-efficient\nReinforcement Learning techniques for controlling complex dynamical systems\nunder competitive team-based multiplayer conditions.",
                "Learning to Act through Evolution of Neural Diversity in Random Neural\n  Networks\nBiological nervous systems consist of networks of diverse, sophisticated\ninformation processors in the form of neurons of different classes. In most\nartificial neural networks (ANNs), neural computation is abstracted to an\nactivation function that is usually shared between all neurons within a layer\nor even the whole network; training of ANNs focuses on synaptic optimization.\nIn this paper, we propose the optimization of neuro-centric parameters to\nattain a set of diverse neurons that can perform complex computations.\nDemonstrating the promise of the approach, we show that evolving neural\nparameters alone allows agents to solve various reinforcement learning tasks\nwithout optimizing any synaptic weights. While not aiming to be an accurate\nbiological model, parameterizing neurons to a larger degree than the current\ncommon practice, allows us to ask questions about the computational abilities\nafforded by neural diversity in random neural networks. The presented results\nopen up interesting future research directions, such as combining evolved\nneural diversity with activity-dependent plasticity.",
                "Deep Learning and Ethics\nThis article appears as chapter 21 of Prince (2023, Understanding Deep\nLearning); a complete draft of the textbook is available here:\nhttp://udlbook.com. This chapter considers potential harms arising from the\ndesign and use of AI systems. These include algorithmic bias, lack of\nexplainability, data privacy violations, militarization, fraud, and\nenvironmental concerns. The aim is not to provide advice on being more ethical.\nInstead, the goal is to express ideas and start conversations in key areas that\nhave received attention in philosophy, political science, and the broader\nsocial sciences.",
                "TLNets: Transformation Learning Networks for long-range time-series\n  prediction\nTime series prediction is a prevalent issue across various disciplines, such\nas meteorology, traffic surveillance, investment, and energy production and\nconsumption. Many statistical and machine-learning strategies have been\ndeveloped to tackle this problem. However, these approaches either lack\nexplainability or exhibit less satisfactory performance when the prediction\nhorizon increases. To this end, we propose a novel plan for the designing of\nnetworks' architecture based on transformations, possessing the potential to\nachieve an enhanced receptive field in learning which brings benefits to fuse\nfeatures across scales. In this context, we introduce four different\ntransformation mechanisms as bases to construct the learning model including\nFourier Transform (FT), Singular Value Decomposition (SVD), matrix\nmultiplication and Conv block. Hence, we develop four learning models based on\nthe above building blocks, namely, FT-Matrix, FT-SVD, FT-Conv, and Conv-SVD.\nNote that the FT and SVD blocks are capable of learning global information,\nwhile the Conv blocks focus on learning local information. The matrix block is\nsparsely designed to learn both global and local information simultaneously.\nThe above Transformation Learning Networks (TLNets) have been extensively\ntested and compared with multiple baseline models based on several real-world\ndatasets and showed clear potential in long-range time-series forecasting.",
                "A Mini Review on the utilization of Reinforcement Learning with OPC UA\nReinforcement Learning (RL) is a powerful machine learning paradigm that has\nbeen applied in various fields such as robotics, natural language processing\nand game playing achieving state-of-the-art results. Targeted to solve\nsequential decision making problems, it is by design able to learn from\nexperience and therefore adapt to changing dynamic environments. These\ncapabilities make it a prime candidate for controlling and optimizing complex\nprocesses in industry. The key to fully exploiting this potential is the\nseamless integration of RL into existing industrial systems. The industrial\ncommunication standard Open Platform Communications UnifiedArchitecture (OPC\nUA) could bridge this gap. However, since RL and OPC UA are from different\nfields,there is a need for researchers to bridge the gap between the two\ntechnologies. This work serves to bridge this gap by providing a brief\ntechnical overview of both technologies and carrying out a semi-exhaustive\nliterature review to gain insights on how RL and OPC UA are applied in\ncombination. With this survey, three main research topics have been identified,\nfollowing the intersection of RL with OPC UA. The results of the literature\nreview show that RL is a promising technology for the control and optimization\nof industrial processes, but does not yet have the necessary standardized\ninterfaces to be deployed in real-world scenarios with reasonably low effort."
            ],
            "interesting paper": 1
        },
        {
            "papers": [
                "Revisiting Structured Variational Autoencoders\nStructured variational autoencoders (SVAEs) combine probabilistic graphical\nmodel priors on latent variables, deep neural networks to link latent variables\nto observed data, and structure-exploiting algorithms for approximate posterior\ninference. These models are particularly appealing for sequential data, where\nthe prior can capture temporal dependencies. However, despite their conceptual\nelegance, SVAEs have proven difficult to implement, and more general approaches\nhave been favored in practice. Here, we revisit SVAEs using modern machine\nlearning tools and demonstrate their advantages over more general alternatives\nin terms of both accuracy and efficiency. First, we develop a modern\nimplementation for hardware acceleration, parallelization, and automatic\ndifferentiation of the message passing algorithms at the core of the SVAE.\nSecond, we show that by exploiting structure in the prior, the SVAE learns more\naccurate models and posterior distributions, which translate into improved\nperformance on prediction tasks. Third, we show how the SVAE can naturally\nhandle missing data, and we leverage this ability to develop a novel,\nself-supervised training approach. Altogether, these results show that the time\nis ripe to revisit structured variational autoencoders.",
                "Detecting Heart Disease from Multi-View Ultrasound Images via Supervised\n  Attention Multiple Instance Learning\nAortic stenosis (AS) is a degenerative valve condition that causes\nsubstantial morbidity and mortality. This condition is under-diagnosed and\nunder-treated. In clinical practice, AS is diagnosed with expert review of\ntransthoracic echocardiography, which produces dozens of ultrasound images of\nthe heart. Only some of these views show the aortic valve. To automate\nscreening for AS, deep networks must learn to mimic a human expert's ability to\nidentify views of the aortic valve then aggregate across these relevant images\nto produce a study-level diagnosis. We find previous approaches to AS detection\nyield insufficient accuracy due to relying on inflexible averages across\nimages. We further find that off-the-shelf attention-based multiple instance\nlearning (MIL) performs poorly. We contribute a new end-to-end MIL approach\nwith two key methodological innovations. First, a supervised attention\ntechnique guides the learned attention mechanism to favor relevant views.\nSecond, a novel self-supervised pretraining strategy applies contrastive\nlearning on the representation of the whole study instead of individual images\nas commonly done in prior literature. Experiments on an open-access dataset and\nan external validation set show that our approach yields higher accuracy while\nreducing model size.",
                "Scan and Snap: Understanding Training Dynamics and Token Composition in\n  1-layer Transformer\nTransformer architecture has shown impressive performance in multiple\nresearch domains and has become the backbone of many neural network models.\nHowever, there is limited understanding on how it works. In particular, with a\nsimple predictive loss, how the representation emerges from the gradient\n\\emph{training dynamics} remains a mystery. In this paper, for 1-layer\ntransformer with one self-attention layer plus one decoder layer, we analyze\nits SGD training dynamics for the task of next token prediction in a\nmathematically rigorous manner. We open the black box of the dynamic process of\nhow the self-attention layer combines input tokens, and reveal the nature of\nunderlying inductive bias. More specifically, with the assumption (a) no\npositional encoding, (b) long input sequence, and (c) the decoder layer learns\nfaster than the self-attention layer, we prove that self-attention acts as a\n\\emph{discriminative scanning algorithm}: starting from uniform attention, it\ngradually attends more to distinct key tokens for a specific next token to be\npredicted, and pays less attention to common key tokens that occur across\ndifferent next tokens. Among distinct tokens, it progressively drops attention\nweights, following the order of low to high co-occurrence between the key and\nthe query token in the training set. Interestingly, this procedure does not\nlead to winner-takes-all, but decelerates due to a \\emph{phase transition} that\nis controllable by the learning rates of the two layers, leaving (almost) fixed\ntoken combination. We verify this \\textbf{\\emph{scan and snap}} dynamics on\nsynthetic and real-world data (WikiText).",
                "Diversify Your Vision Datasets with Automatic Diffusion-Based\n  Augmentation\nMany fine-grained classification tasks, like rare animal identification, have\nlimited training data and consequently classifiers trained on these datasets\noften fail to generalize to variations in the domain like changes in weather or\nlocation. As such, we explore how natural language descriptions of the domains\nseen in training data can be used with large vision models trained on diverse\npretraining datasets to generate useful variations of the training data. We\nintroduce ALIA (Automated Language-guided Image Augmentation), a method which\nutilizes large vision and language models to automatically generate natural\nlanguage descriptions of a dataset's domains and augment the training data via\nlanguage-guided image editing. To maintain data integrity, a model trained on\nthe original dataset filters out minimal image edits and those which corrupt\nclass-relevant information. The resulting dataset is visually consistent with\nthe original training data and offers significantly enhanced diversity. We show\nthat ALIA is able to surpasses traditional data augmentation and text-to-image\ngenerated data on fine-grained classification tasks, including cases of domain\ngeneralization and contextual bias. Code is available at\nhttps://github.com/lisadunlap/ALIA.",
                "On convex decision regions in deep network representations\nCurrent work on human-machine alignment aims at understanding machine-learned\nlatent spaces and their correspondence to human representations.\nG{\\\"a}rdenfors' conceptual spaces is a prominent framework for understanding\nhuman representations. Convexity of object regions in conceptual spaces is\nargued to promote generalizability, few-shot learning, and interpersonal\nalignment. Based on these insights, we investigate the notion of convexity of\nconcept regions in machine-learned latent spaces. We develop a set of tools for\nmeasuring convexity in sampled data and evaluate emergent convexity in layered\nrepresentations of state-of-the-art deep networks. We show that convexity is\nrobust to basic re-parametrization and, hence, meaningful as a quality of\nmachine-learned latent spaces. We find that approximate convexity is pervasive\nin neural representations in multiple application domains, including models of\nimages, audio, human activity, text, and medical images. Generally, we observe\nthat fine-tuning increases the convexity of label regions. We find evidence\nthat pretraining convexity of class label regions predicts subsequent\nfine-tuning performance.",
                "Differentiable Random Partition Models\nPartitioning a set of elements into an unknown number of mutually exclusive\nsubsets is essential in many machine learning problems. However, assigning\nelements, such as samples in a dataset or neurons in a network layer, to an\nunknown and discrete number of subsets is inherently non-differentiable,\nprohibiting end-to-end gradient-based optimization of parameters. We overcome\nthis limitation by proposing a novel two-step method for inferring partitions,\nwhich allows its usage in variational inference tasks. This new approach\nenables reparameterized gradients with respect to the parameters of the new\nrandom partition model. Our method works by inferring the number of elements\nper subset and, second, by filling these subsets in a learned order. We\nhighlight the versatility of our general-purpose approach on three different\nchallenging experiments: variational clustering, inference of shared and\nindependent generative factors under weak supervision, and multitask learning."
            ],
            "interesting paper": 2
        },
        {
            "papers": [
                "Enhancing Human Capabilities through Symbiotic Artificial Intelligence\n  with Shared Sensory Experiences\nThe merging of human intelligence and artificial intelligence has long been a\nsubject of interest in both science fiction and academia. In this paper, we\nintroduce a novel concept in Human-AI interaction called Symbiotic Artificial\nIntelligence with Shared Sensory Experiences (SAISSE), which aims to establish\na mutually beneficial relationship between AI systems and human users through\nshared sensory experiences. By integrating multiple sensory input channels and\nprocessing human experiences, SAISSE fosters a strong human-AI bond, enabling\nAI systems to learn from and adapt to individual users, providing personalized\nsupport, assistance, and enhancement. Furthermore, we discuss the incorporation\nof memory storage units for long-term growth and development of both the AI\nsystem and its human user. As we address user privacy and ethical guidelines\nfor responsible AI-human symbiosis, we also explore potential biases and\ninequalities in AI-human symbiosis and propose strategies to mitigate these\nchallenges. Our research aims to provide a comprehensive understanding of the\nSAISSE concept and its potential to effectively support and enhance individual\nhuman users through symbiotic AI systems. This position article aims at\ndiscussing poteintial AI-human interaction related topics within the scientific\ncommunity, rather than providing experimental or theoretical results.",
                "On Computing Universal Plans for Partially Observable Multi-Agent Path\n  Finding\nMulti-agent routing problems have drawn significant attention nowadays due to\ntheir broad industrial applications in, e.g., warehouse robots, logistics\nautomation, and traffic control. Conventionally, they are modelled as classical\nplanning problems. In this paper, we argue that it is beneficial to formulate\nthem as universal planning problems. We therefore propose universal plans, also\nknown as policies, as the solution concepts, and implement a system called\nASP-MAUPF (Answer Set Programming for Multi-Agent Universal Plan Finding) for\ncomputing them. Given an arbitrary two-dimensional map and a profile of goals\nfor the agents, the system finds a feasible universal plan for each agent that\nensures no collision with others. We use the system to conduct some\nexperiments, and make some observations on the types of goal profiles and\nenvironments that will have feasible policies, and how they may depend on\nagents' sensors. We also demonstrate how users can customize action preferences\nto compute more efficient policies, even (near-)optimal ones.",
                "Dendritic Integration Based Quadratic Neural Networks Outperform\n  Traditional Aritificial Ones\nIncorporating biological neuronal properties into Artificial Neural Networks\n(ANNs) to enhance computational capabilities poses a formidable challenge in\nthe field of machine learning. Inspired by recent findings indicating that\ndendrites adhere to quadratic integration rules for synaptic inputs, we propose\na novel ANN model, Dendritic Integration-Based Quadratic Neural Network\n(DIQNN). This model shows superior performance over traditional ANNs in a\nvariety of classification tasks. To reduce the computational cost of DIQNN, we\nintroduce the Low-Rank DIQNN, while we find it can retain the performance of\nthe original DIQNN. We further propose a margin to characterize the\ngeneralization error and theoretically prove this margin will increase\nmonotonically during training. And we show the consistency between\ngeneralization and our margin using numerical experiments. Finally, by\nintegrating this margin into the loss function, the change of test accuracy is\nindeed accelerated. Our work contributes a novel, brain-inspired ANN model that\nsurpasses traditional ANNs and provides a theoretical framework to analyze the\ngeneralization error in classification tasks.",
                "Individuality in Swarm Robots with the Case Study of Kilobots: Noise,\n  Bug, or Feature?\nInter-individual differences are studied in natural systems, such as fish,\nbees, and humans, as they contribute to the complexity of both individual and\ncollective behaviors. However, individuality in artificial systems, such as\nrobotic swarms, is undervalued or even overlooked. Agent-specific deviations\nfrom the norm in swarm robotics are usually understood as mere noise that can\nbe minimized, for example, by calibration. We observe that robots have\nconsistent deviations and argue that awareness and knowledge of these can be\nexploited to serve a task. We measure heterogeneity in robot swarms caused by\nindividual differences in how robots act, sense, and oscillate. Our use case is\nKilobots and we provide example behaviors where the performance of robots\nvaries depending on individual differences. We show a non-intuitive example of\nphototaxis with Kilobots where the non-calibrated Kilobots show better\nperformance than the calibrated supposedly ``ideal\" one. We measure the\ninter-individual variations for heterogeneity in sensing and oscillation, too.\nWe briefly discuss how these variations can enhance the complexity of\ncollective behaviors. We suggest that by recognizing and exploring this new\nperspective on individuality, and hence diversity, in robotic swarms, we can\ngain a deeper understanding of these systems and potentially unlock new\npossibilities for their design and implementation of applications.",
                "Attention Paper: How Generative AI Reshapes Digital Shadow Industry?\nThe rapid development of digital economy has led to the emergence of various\nblack and shadow internet industries, which pose potential risks that can be\nidentified and managed through digital risk management (DRM) that uses\ndifferent techniques such as machine learning and deep learning. The evolution\nof DRM architecture has been driven by changes in data forms. However, the\ndevelopment of AI-generated content (AIGC) technology, such as ChatGPT and\nStable Diffusion, has given black and shadow industries powerful tools to\npersonalize data and generate realistic images and conversations for fraudulent\nactivities. This poses a challenge for DRM systems to control risks from the\nsource of data generation and to respond quickly to the fast-changing risk\nenvironment. This paper aims to provide a technical analysis of the challenges\nand opportunities of AIGC from upstream, midstream, and downstream paths of\nblack/shadow industries and suggest future directions for improving existing\nrisk control systems. The paper will explore the new black and shadow\ntechniques triggered by generative AI technology and provide insights for\nbuilding the next-generation DRM system.",
                "Batch Model Consolidation: A Multi-Task Model Consolidation Framework\nIn Continual Learning (CL), a model is required to learn a stream of tasks\nsequentially without significant performance degradation on previously learned\ntasks. Current approaches fail for a long sequence of tasks from diverse\ndomains and difficulties. Many of the existing CL approaches are difficult to\napply in practice due to excessive memory cost or training time, or are tightly\ncoupled to a single device. With the intuition derived from the widely applied\nmini-batch training, we propose Batch Model Consolidation ($\\textbf{BMC}$) to\nsupport more realistic CL under conditions where multiple agents are exposed to\na range of tasks. During a $\\textit{regularization}$ phase, BMC trains multiple\n$\\textit{expert models}$ in parallel on a set of disjoint tasks. Each expert\nmaintains weight similarity to a $\\textit{base model}$ through a\n$\\textit{stability loss}$, and constructs a $\\textit{buffer}$ from a fraction\nof the task's data. During the $\\textit{consolidation}$ phase, we combine the\nlearned knowledge on 'batches' of $\\textit{expert models}$ using a\n$\\textit{batched consolidation loss}$ in $\\textit{memory}$ data that aggregates\nall buffers. We thoroughly evaluate each component of our method in an ablation\nstudy and demonstrate the effectiveness on standardized benchmark datasets\nSplit-CIFAR-100, Tiny-ImageNet, and the Stream dataset composed of 71 image\nclassification tasks from diverse domains and difficulties. Our method\noutperforms the next best CL approach by 70% and is the only approach that can\nmaintain performance at the end of 71 tasks; Our benchmark can be accessed at\nhttps://github.com/fostiropoulos/stream_benchmark"
            ],
            "interesting paper": 2
        },
        {
            "papers": [
                "Instance-based Max-margin for Practical Few-shot Recognition\nIn order to mimic the human few-shot learning (FSL) ability better and to\nmake FSL closer to real-world applications, this paper proposes a practical FSL\n(pFSL) setting. pFSL is based on unsupervised pretrained models (analogous to\nhuman prior knowledge) and recognizes many novel classes simultaneously.\nCompared to traditional FSL, pFSL is simpler in its formulation, easier to\nevaluate, more challenging and more practical. To cope with the rarity of\ntraining examples, this paper proposes IbM2, an instance-based max-margin\nmethod not only for the new pFSL setting, but also works well in traditional\nFSL scenarios. Based on the Gaussian Annulus Theorem, IbM2 converts random\nnoise applied to the instances into a mechanism to achieve maximum margin in\nthe many-way pFSL (or traditional FSL) recognition task. Experiments with\nvarious self-supervised pretraining methods and diverse many- or few-way FSL\ntasks show that IbM2 almost always leads to improvements compared to its\nrespective baseline methods, and in most cases the improvements are\nsignificant. With both the new pFSL setting and novel IbM2 method, this paper\nshows that practical few-shot learning is both viable and promising.",
                "Self-Supervised Reinforcement Learning that Transfers using Random\n  Features\nModel-free reinforcement learning algorithms have exhibited great potential\nin solving single-task sequential decision-making problems with\nhigh-dimensional observations and long horizons, but are known to be hard to\ngeneralize across tasks. Model-based RL, on the other hand, learns\ntask-agnostic models of the world that naturally enables transfer across\ndifferent reward functions, but struggles to scale to complex environments due\nto the compounding error. To get the best of both worlds, we propose a\nself-supervised reinforcement learning method that enables the transfer of\nbehaviors across tasks with different rewards, while circumventing the\nchallenges of model-based RL. In particular, we show self-supervised\npre-training of model-free reinforcement learning with a number of random\nfeatures as rewards allows implicit modeling of long-horizon environment\ndynamics. Then, planning techniques like model-predictive control using these\nimplicit models enable fast adaptation to problems with new reward functions.\nOur method is self-supervised in that it can be trained on offline datasets\nwithout reward labels, but can then be quickly deployed on new tasks. We\nvalidate that our proposed method enables transfer across tasks on a variety of\nmanipulation and locomotion domains in simulation, opening the door to\ngeneralist decision-making agents.",
                "Matrix Information Theory for Self-Supervised Learning\nThe maximum entropy encoding framework provides a unified perspective for\nmany non-contrastive learning methods like SimSiam, Barlow Twins, and MEC.\nInspired by this framework, we introduce Matrix-SSL, a novel approach that\nleverages matrix information theory to interpret the maximum entropy encoding\nloss as matrix uniformity loss. Furthermore, Matrix-SSL enhances the maximum\nentropy encoding method by seamlessly incorporating matrix alignment loss,\ndirectly aligning covariance matrices in different branches. Experimental\nresults reveal that Matrix-SSL outperforms state-of-the-art methods on the\nImageNet dataset under linear evaluation settings and on MS-COCO for transfer\nlearning tasks. Specifically, when performing transfer learning tasks on\nMS-COCO, our method outperforms previous SOTA methods such as MoCo v2 and BYOL\nup to 3.3% with only 400 epochs compared to 800 epochs pre-training. We also\ntry to introduce representation learning into the language modeling regime by\nfine-tuning a 7B model using matrix cross-entropy loss, with a margin of 3.1%\non the GSM8K dataset over the standard cross-entropy loss. Code available at\nhttps://github.com/yifanzhang-pro/Matrix-SSL.",
                "Statistically Significant Concept-based Explanation of Image Classifiers\n  via Model Knockoffs\nA concept-based classifier can explain the decision process of a deep\nlearning model by human-understandable concepts in image classification\nproblems. However, sometimes concept-based explanations may cause false\npositives, which misregards unrelated concepts as important for the prediction\ntask. Our goal is to find the statistically significant concept for\nclassification to prevent misinterpretation. In this study, we propose a method\nusing a deep learning model to learn the image concept and then using the\nKnockoff samples to select the important concepts for prediction by controlling\nthe False Discovery Rate (FDR) under a certain value. We evaluate the proposed\nmethod in our synthetic and real data experiments. Also, it shows that our\nmethod can control the FDR properly while selecting highly interpretable\nconcepts to improve the trustworthiness of the model.",
                "Visualizing Self-Regulated Learner Profiles in Dashboards: Design\n  Insights from Teachers\nFlipped Classrooms (FC) are a promising teaching strategy, where students\nengage with the learning material before attending face-to-face sessions. While\npre-class activities are critical for course success, many students struggle to\nengage effectively in them due to inadequate of self-regulated learning (SRL)\nskills. Thus, tools enabling teachers to monitor students' SRL and provide\npersonalized guidance have the potential to improve learning outcomes. However,\nexisting dashboards mostly focus on aggregated information, disregarding recent\nwork leveraging machine learning (ML) approaches that have identified\ncomprehensive, multi-dimensional SRL behaviors. Unfortunately, the complexity\nof such findings makes them difficult to communicate and act on. In this paper,\nwe follow a teacher-centered approach to study how to make thorough findings\naccessible to teachers. We design and implement FlippED, a dashboard for\nmonitoring students' SRL behavior. We evaluate the usability and actionability\nof the tool in semi-structured interviews with ten university teachers. We find\nthat communicating ML-based profiles spark a range of potential interventions\nfor students and course modifications.",
                "Causal Component Analysis\nIndependent Component Analysis (ICA) aims to recover independent latent\nvariables from observed mixtures thereof. Causal Representation Learning (CRL)\naims instead to infer causally related (thus often statistically dependent)\nlatent variables, together with the unknown graph encoding their causal\nrelationships. We introduce an intermediate problem termed Causal Component\nAnalysis (CauCA). CauCA can be viewed as a generalization of ICA, modelling the\ncausal dependence among the latent components, and as a special case of CRL. In\ncontrast to CRL, it presupposes knowledge of the causal graph, focusing solely\non learning the unmixing function and the causal mechanisms. Any impossibility\nresults regarding the recovery of the ground truth in CauCA also apply for CRL,\nwhile possibility results may serve as a stepping stone for extensions to CRL.\nWe characterize CauCA identifiability from multiple datasets generated through\ndifferent types of interventions on the latent causal variables. As a\ncorollary, this interventional perspective also leads to new identifiability\nresults for nonlinear ICA -- a special case of CauCA with an empty graph --\nrequiring strictly fewer datasets than previous results. We introduce a\nlikelihood-based approach using normalizing flows to estimate both the unmixing\nfunction and the causal mechanisms, and demonstrate its effectiveness through\nextensive synthetic experiments in the CauCA and ICA setting."
            ],
            "interesting paper": 3
        },
        {
            "papers": [
                "Meta-prediction Model for Distillation-Aware NAS on Unseen Datasets\nDistillation-aware Neural Architecture Search (DaNAS) aims to search for an\noptimal student architecture that obtains the best performance and/or\nefficiency when distilling the knowledge from a given teacher model. Previous\nDaNAS methods have mostly tackled the search for the neural architecture for\nfixed datasets and the teacher, which are not generalized well on a new task\nconsisting of an unseen dataset and an unseen teacher, thus need to perform a\ncostly search for any new combination of the datasets and the teachers. For\nstandard NAS tasks without KD, meta-learning-based computationally efficient\nNAS methods have been proposed, which learn the generalized search process over\nmultiple tasks (datasets) and transfer the knowledge obtained over those tasks\nto a new task. However, since they assume learning from scratch without KD from\na teacher, they might not be ideal for DaNAS scenarios. To eliminate the\nexcessive computational cost of DaNAS methods and the sub-optimality of rapid\nNAS methods, we propose a distillation-aware meta accuracy prediction model,\nDaSS (Distillation-aware Student Search), which can predict a given\narchitecture's final performances on a dataset when performing KD with a given\nteacher, without having actually to train it on the target task. The\nexperimental results demonstrate that our proposed meta-prediction model\nsuccessfully generalizes to multiple unseen datasets for DaNAS tasks, largely\noutperforming existing meta-NAS methods and rapid NAS baselines. Code is\navailable at https://github.com/CownowAn/DaSS",
                "MADiff: Offline Multi-agent Learning with Diffusion Models\nDiffusion model (DM) recently achieved huge success in various scenarios\nincluding offline reinforcement learning, where the diffusion planner learn to\ngenerate desired trajectories during online evaluations. However, despite the\neffectiveness in single-agent learning, it remains unclear how DMs can operate\nin multi-agent problems, where agents can hardly complete teamwork without good\ncoordination by independently modeling each agent's trajectories. In this\npaper, we propose MADiff, a novel generative multi-agent learning framework to\ntackle this problem. MADiff is realized with an attention-based diffusion model\nto model the complex coordination among behaviors of multiple agents. To the\nbest of our knowledge, MADiff is the first diffusion-based multi-agent learning\nframework, which behaves as both a decentralized policy and a centralized\ncontroller. During decentralized executions, MADiff simultaneously performs\nteammate modeling, and the centralized controller can also be applied in\nmulti-agent trajectory predictions. Our experiments show the superior\nperformance of MADiff compared to baseline algorithms in a wide range of\nmulti-agent learning tasks, which emphasizes the effectiveness of MADiff in\nmodeling complex multi-agent interactions. Our code is available at\nhttps://github.com/zbzhu99/madiff.",
                "Attention Schema in Neural Agents\nAttention has become a common ingredient in deep learning architectures. It\nadds a dynamical selection of information on top of the static selection of\ninformation supported by weights. In the same way, we can imagine a\nhigher-order informational filter built on top of attention: an Attention\nSchema (AS), namely, a descriptive and predictive model of attention. In\ncognitive neuroscience, Attention Schema Theory (AST) supports this idea of\ndistinguishing attention from AS. A strong prediction of this theory is that an\nagent can use its own AS to also infer the states of other agents' attention\nand consequently enhance coordination with other agents. As such, multi-agent\nreinforcement learning would be an ideal setting to experimentally test the\nvalidity of AST. We explore different ways in which attention and AS interact\nwith each other. Our preliminary results indicate that agents that implement\nthe AS as a recurrent internal control achieve the best performance. In\ngeneral, these exploratory experiments suggest that equipping artificial agents\nwith a model of attention can enhance their social intelligence.",
                "Stability of implicit neural networks for long-term forecasting in\n  dynamical systems\nForecasting physical signals in long time range is among the most challenging\ntasks in Partial Differential Equations (PDEs) research. To circumvent\nlimitations of traditional solvers, many different Deep Learning methods have\nbeen proposed. They are all based on auto-regressive methods and exhibit\nstability issues. Drawing inspiration from the stability property of implicit\nnumerical schemes, we introduce a stable auto-regressive implicit neural\nnetwork. We develop a theory based on the stability definition of schemes to\nensure the stability in forecasting of this network. It leads us to introduce\nhard constraints on its weights and propagate the dynamics in the latent space.\nOur experimental results validate our stability property, and show improved\nresults at long-term forecasting for two transports PDEs.",
                "Exploiting Large Neuroimaging Datasets to Create Connectome-Constrained\n  Approaches for more Robust, Efficient, and Adaptable Artificial Intelligence\nDespite the progress in deep learning networks, efficient learning at the\nedge (enabling adaptable, low-complexity machine learning solutions) remains a\ncritical need for defense and commercial applications. We envision a pipeline\nto utilize large neuroimaging datasets, including maps of the brain which\ncapture neuron and synapse connectivity, to improve machine learning\napproaches. We have pursued different approaches within this pipeline\nstructure. First, as a demonstration of data-driven discovery, the team has\ndeveloped a technique for discovery of repeated subcircuits, or motifs. These\nwere incorporated into a neural architecture search approach to evolve network\narchitectures. Second, we have conducted analysis of the heading direction\ncircuit in the fruit fly, which performs fusion of visual and angular velocity\nfeatures, to explore augmenting existing computational models with new insight.\nOur team discovered a novel pattern of connectivity, implemented a new model,\nand demonstrated sensor fusion on a robotic platform. Third, the team analyzed\ncircuitry for memory formation in the fruit fly connectome, enabling the design\nof a novel generative replay approach. Finally, the team has begun analysis of\nconnectivity in mammalian cortex to explore potential improvements to\ntransformer networks. These constraints increased network robustness on the\nmost challenging examples in the CIFAR-10-C computer vision robustness\nbenchmark task, while reducing learnable attention parameters by over an order\nof magnitude. Taken together, these results demonstrate multiple potential\napproaches to utilize insight from neural systems for developing robust and\nefficient machine learning techniques.",
                "Input-Aware Dynamic Timestep Spiking Neural Networks for Efficient\n  In-Memory Computing\nSpiking Neural Networks (SNNs) have recently attracted widespread research\ninterest as an efficient alternative to traditional Artificial Neural Networks\n(ANNs) because of their capability to process sparse and binary spike\ninformation and avoid expensive multiplication operations. Although the\nefficiency of SNNs can be realized on the In-Memory Computing (IMC)\narchitecture, we show that the energy cost and latency of SNNs scale linearly\nwith the number of timesteps used on IMC hardware. Therefore, in order to\nmaximize the efficiency of SNNs, we propose input-aware Dynamic Timestep SNN\n(DT-SNN), a novel algorithmic solution to dynamically determine the number of\ntimesteps during inference on an input-dependent basis. By calculating the\nentropy of the accumulated output after each timestep, we can compare it to a\npredefined threshold and decide if the information processed at the current\ntimestep is sufficient for a confident prediction. We deploy DT-SNN on an IMC\narchitecture and show that it incurs negligible computational overhead. We\ndemonstrate that our method only uses 1.46 average timesteps to achieve the\naccuracy of a 4-timestep static SNN while reducing the energy-delay-product by\n80%."
            ],
            "interesting paper": 4
        },
        {
            "papers": [
                "Data Minimization at Inference Time\nIn domains with high stakes such as law, recruitment, and healthcare,\nlearning models frequently rely on sensitive user data for inference,\nnecessitating the complete set of features. This not only poses significant\nprivacy risks for individuals but also demands substantial human effort from\norganizations to verify information accuracy. This paper asks whether it is\nnecessary to use \\emph{all} input features for accurate predictions at\ninference time. The paper demonstrates that, in a personalized setting,\nindividuals may only need to disclose a small subset of their features without\ncompromising decision-making accuracy. The paper also provides an efficient\nsequential algorithm to determine the appropriate attributes for each\nindividual to provide. Evaluations across various learning tasks show that\nindividuals can potentially report as little as 10\\% of their information while\nmaintaining the same accuracy level as a model that employs the full set of\nuser information.",
                "Distill Gold from Massive Ores: Bi-level Data Pruning towards Efficient\n  Dataset Distillation\nData-efficient learning has garnered significant attention, especially given\nthe current trend of large multi-modal models. Recently, dataset distillation\nhas become an effective approach by synthesizing data samples that are\nessential for network training. However, it remains to be explored which\nsamples are essential for the dataset distillation process itself. In this\nwork, we study the data efficiency and selection for the dataset distillation\ntask. By re-formulating the dynamics of distillation, we provide insight into\nthe inherent redundancy in the real dataset, both theoretically and\nempirically. We propose to use the empirical loss value as a static data\npruning criterion. To further compensate for the variation of the data value in\ntraining, we find the most contributing samples based on their causal effects\non the distillation. The proposed selection strategy can efficiently exploit\nthe training dataset, outperform the previous SOTA distillation algorithms, and\nconsistently enhance the distillation algorithms, even on much larger-scale and\nmore heterogeneous datasets, e.g., full ImageNet-1K and Kinetics-400. We\nbelieve this paradigm will open up new avenues in the dynamics of distillation\nand pave the way for efficient dataset distillation. Our code is available on\nhttps://github.com/silicx/GoldFromOres-BiLP.",
                "The Curse of Recursion: Training on Generated Data Makes Models Forget\nStable Diffusion revolutionised image creation from descriptive text. GPT-2,\nGPT-3(.5) and GPT-4 demonstrated astonishing performance across a variety of\nlanguage tasks. ChatGPT introduced such language models to the general public.\nIt is now clear that large language models (LLMs) are here to stay, and will\nbring about drastic change in the whole ecosystem of online text and images. In\nthis paper we consider what the future might hold. What will happen to GPT-{n}\nonce LLMs contribute much of the language found online? We find that use of\nmodel-generated content in training causes irreversible defects in the\nresulting models, where tails of the original content distribution disappear.\nWe refer to this effect as Model Collapse and show that it can occur in\nVariational Autoencoders, Gaussian Mixture Models and LLMs. We build\ntheoretical intuition behind the phenomenon and portray its ubiquity amongst\nall learned generative models. We demonstrate that it has to be taken seriously\nif we are to sustain the benefits of training from large-scale data scraped\nfrom the web. Indeed, the value of data collected about genuine human\ninteractions with systems will be increasingly valuable in the presence of\ncontent generated by LLMs in data crawled from the Internet.",
                "One Network, Many Masks: Towards More Parameter-Efficient Transfer\n  Learning\nFine-tuning pre-trained language models for multiple tasks tends to be\nexpensive in terms of storage. To mitigate this, parameter-efficient transfer\nlearning (PETL) methods have been proposed to address this issue, but they\nstill require a significant number of parameters and storage when being applied\nto broader ranges of tasks. To achieve even greater storage reduction, we\npropose PROPETL, a novel method that enables efficient sharing of a single PETL\nmodule which we call prototype network (e.g., adapter, LoRA, and prefix-tuning)\nacross layers and tasks. We then learn binary masks to select different\nsub-networks from the shared prototype network and apply them as PETL modules\ninto different layers. We find that the binary masks can determine crucial\ninformation from the network, which is often ignored in previous studies. Our\nwork can also be seen as a type of pruning method, where we find that\noverparameterization also exists in the seemingly small PETL modules. We\nevaluate PROPETL on various downstream tasks and show that it can outperform\nother PETL methods with approximately 10% of the parameter storage required by\nthe latter.",
                "Diagnosing Transformers: Illuminating Feature Spaces for Clinical\n  Decision-Making\nPre-trained transformers are often fine-tuned to aid clinical decision-making\nusing limited clinical notes. Model interpretability is crucial, especially in\nhigh-stakes domains like medicine, to establish trust and ensure safety, which\nrequires human engagement. We introduce SUFO, a systematic framework that\nenhances interpretability of fine-tuned transformer feature spaces. SUFO\nutilizes a range of analytic and visualization techniques, including Supervised\nprobing, Unsupervised similarity analysis, Feature dynamics, and Outlier\nanalysis to address key questions about model trust and interpretability. We\nconduct a case study investigating the impact of pre-training data where we\nfocus on real-world pathology classification tasks, and validate our findings\non MedNLI. We evaluate five 110M-sized pre-trained transformer models,\ncategorized into general-domain (BERT, TNLR), mixed-domain (BioBERT, Clinical\nBioBERT), and domain-specific (PubMedBERT) groups. Our SUFO analyses reveal\nthat: (1) while PubMedBERT, the domain-specific model, contains valuable\ninformation for fine-tuning, it can overfit to minority classes when class\nimbalances exist. In contrast, mixed-domain models exhibit greater resistance\nto overfitting, suggesting potential improvements in domain-specific model\nrobustness; (2) in-domain pre-training accelerates feature disambiguation\nduring fine-tuning; and (3) feature spaces undergo significant sparsification\nduring this process, enabling clinicians to identify common outlier modes among\nfine-tuned models as demonstrated in this paper. These findings showcase the\nutility of SUFO in enhancing trust and safety when using transformers in\nmedicine, and we believe SUFO can aid practitioners in evaluating fine-tuned\nlanguage models for other applications in medicine and in more critical\ndomains.",
                "USIM-DAL: Uncertainty-aware Statistical Image Modeling-based Dense\n  Active Learning for Super-resolution\nDense regression is a widely used approach in computer vision for tasks such\nas image super-resolution, enhancement, depth estimation, etc. However, the\nhigh cost of annotation and labeling makes it challenging to achieve accurate\nresults. We propose incorporating active learning into dense regression models\nto address this problem. Active learning allows models to select the most\ninformative samples for labeling, reducing the overall annotation cost while\nimproving performance. Despite its potential, active learning has not been\nwidely explored in high-dimensional computer vision regression tasks like\nsuper-resolution. We address this research gap and propose a new framework\ncalled USIM-DAL that leverages the statistical properties of colour images to\nlearn informative priors using probabilistic deep neural networks that model\nthe heteroscedastic predictive distribution allowing uncertainty\nquantification. Moreover, the aleatoric uncertainty from the network serves as\na proxy for error that is used for active learning. Our experiments on a wide\nvariety of datasets spanning applications in natural images (visual genome,\nBSD100), medical imaging (histopathology slides), and remote sensing (satellite\nimages) demonstrate the efficacy of the newly proposed USIM-DAL and superiority\nover several dense regression active learning methods."
            ],
            "interesting paper": 1
        },
        {
            "papers": [
                "Evolving Connectivity for Recurrent Spiking Neural Networks\nRecurrent spiking neural networks (RSNNs) hold great potential for advancing\nartificial general intelligence, as they draw inspiration from the biological\nnervous system and show promise in modeling complex dynamics. However, the\nwidely-used surrogate gradient-based training methods for RSNNs are inherently\ninaccurate and unfriendly to neuromorphic hardware. To address these\nlimitations, we propose the evolving connectivity (EC) framework, an\ninference-only method for training RSNNs. The EC framework reformulates\nweight-tuning as a search into parameterized connection probability\ndistributions, and employs Natural Evolution Strategies (NES) for optimizing\nthese distributions. Our EC framework circumvents the need for gradients and\nfeatures hardware-friendly characteristics, including sparse boolean\nconnections and high scalability. We evaluate EC on a series of standard\nrobotic locomotion tasks, where it achieves comparable performance with deep\nneural networks and outperforms gradient-trained RSNNs, even solving the\ncomplex 17-DoF humanoid task. Additionally, the EC framework demonstrates a two\nto three fold speedup in efficiency compared to directly evolving parameters.\nBy providing a performant and hardware-friendly alternative, the EC framework\nlays the groundwork for further energy-efficient applications of RSNNs and\nadvances the development of neuromorphic devices.",
                "Modeling Dynamic Environments with Scene Graph Memory\nEmbodied AI agents that search for objects in large environments such as\nhouseholds often need to make efficient decisions by predicting object\nlocations based on partial information. We pose this as a new type of link\nprediction problem: link prediction on partially observable dynamic graphs. Our\ngraph is a representation of a scene in which rooms and objects are nodes, and\ntheir relationships are encoded in the edges; only parts of the changing graph\nare known to the agent at each timestep. This partial observability poses a\nchallenge to existing link prediction approaches, which we address. We propose\na novel state representation -- Scene Graph Memory (SGM) -- with captures the\nagent's accumulated set of observations, as well as a neural net architecture\ncalled a Node Edge Predictor (NEP) that extracts information from the SGM to\nsearch efficiently. We evaluate our method in the Dynamic House Simulator, a\nnew benchmark that creates diverse dynamic graphs following the semantic\npatterns typically seen at homes, and show that NEP can be trained to predict\nthe locations of objects in a variety of environments with diverse object\nmovement dynamics, outperforming baselines both in terms of new scene\nadaptability and overall accuracy. The codebase and more can be found at\nhttps://www.scenegraphmemory.com.",
                "Distill Gold from Massive Ores: Bi-level Data Pruning towards Efficient\n  Dataset Distillation\nData-efficient learning has garnered significant attention, especially given\nthe current trend of large multi-modal models. Recently, dataset distillation\nhas become an effective approach by synthesizing data samples that are\nessential for network training. However, it remains to be explored which\nsamples are essential for the dataset distillation process itself. In this\nwork, we study the data efficiency and selection for the dataset distillation\ntask. By re-formulating the dynamics of distillation, we provide insight into\nthe inherent redundancy in the real dataset, both theoretically and\nempirically. We propose to use the empirical loss value as a static data\npruning criterion. To further compensate for the variation of the data value in\ntraining, we find the most contributing samples based on their causal effects\non the distillation. The proposed selection strategy can efficiently exploit\nthe training dataset, outperform the previous SOTA distillation algorithms, and\nconsistently enhance the distillation algorithms, even on much larger-scale and\nmore heterogeneous datasets, e.g., full ImageNet-1K and Kinetics-400. We\nbelieve this paradigm will open up new avenues in the dynamics of distillation\nand pave the way for efficient dataset distillation. Our code is available on\nhttps://github.com/silicx/GoldFromOres-BiLP.",
                "A Comprehensive Overview and Comparative Analysis on Deep Learning\n  Models: CNN, RNN, LSTM, GRU\nDeep learning (DL) has emerged as a powerful subset of machine learning (ML)\nand artificial intelligence (AI), outperforming traditional ML methods,\nespecially in handling unstructured and large datasets. Its impact spans across\nvarious domains, including speech recognition, healthcare, autonomous vehicles,\ncybersecurity, predictive analytics, and more. However, the complexity and\ndynamic nature of real-world problems present challenges in designing effective\ndeep learning models. Consequently, several deep learning models have been\ndeveloped to address different problems and applications. In this article, we\nconduct a comprehensive survey of various deep learning models, including\nConvolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs),\nGenerative Models, Deep Reinforcement Learning (DRL), and Deep Transfer\nLearning. We examine the structure, applications, benefits, and limitations of\neach model. Furthermore, we perform an analysis using three publicly available\ndatasets: IMDB, ARAS, and Fruit-360. We compare the performance of six renowned\ndeep learning models: CNN, Simple RNN, Long Short-Term Memory (LSTM),\nBidirectional LSTM, Gated Recurrent Unit (GRU), and Bidirectional GRU.",
                "Synthesizing a Progression of Subtasks for Block-Based Visual\n  Programming Tasks\nBlock-based visual programming environments play an increasingly important\nrole in introducing computing concepts to K-12 students. In recent years, they\nhave also gained popularity in neuro-symbolic AI, serving as a benchmark to\nevaluate general problem-solving and logical reasoning skills. The open-ended\nand conceptual nature of these visual programming tasks make them challenging,\nboth for state-of-the-art AI agents as well as for novice programmers. A\nnatural approach to providing assistance for problem-solving is breaking down a\ncomplex task into a progression of simpler subtasks; however, this is not\ntrivial given that the solution codes are typically nested and have non-linear\nexecution behavior. In this paper, we formalize the problem of synthesizing\nsuch a progression for a given reference block-based visual programming task.\nWe propose a novel synthesis algorithm that generates a progression of subtasks\nthat are high-quality, well-spaced in terms of their complexity, and solving\nthis progression leads to solving the reference task. We show the utility of\nour synthesis algorithm in improving the efficacy of AI agents (in this case,\nneural program synthesizers) for solving tasks in the Karel programming\nenvironment. Then, we conduct a user study to demonstrate that our synthesized\nprogression of subtasks can assist a novice programmer in solving tasks in the\nHour of Code: Maze Challenge by Code-dot-org.",
                "On the Value of Myopic Behavior in Policy Reuse\nLeveraging learned strategies in unfamiliar scenarios is fundamental to human\nintelligence. In reinforcement learning, rationally reusing the policies\nacquired from other tasks or human experts is critical for tackling problems\nthat are difficult to learn from scratch. In this work, we present a framework\ncalled Selective Myopic bEhavior Control~(SMEC), which results from the insight\nthat the short-term behaviors of prior policies are sharable across tasks. By\nevaluating the behaviors of prior policies via a hybrid value function\narchitecture, SMEC adaptively aggregates the sharable short-term behaviors of\nprior policies and the long-term behaviors of the task policy, leading to\ncoordinated decisions. Empirical results on a collection of manipulation and\nlocomotion tasks demonstrate that SMEC outperforms existing methods, and\nvalidate the ability of SMEC to leverage related prior policies."
            ],
            "interesting paper": 1
        },
        {
            "papers": [
                "Full High-Dimensional Intelligible Learning In 2-D Lossless\n  Visualization Space\nThis study explores a new methodology for machine learning classification\ntasks in 2-dimensional visualization space (2-D ML) using Visual knowledge\nDiscovery in lossless General Line Coordinates. It is shown that this is a full\nmachine learning approach that does not require processing n-dimensional data\nin an abstract n-dimensional space. It enables discovering n-D patterns in 2-D\nspace without loss of n-D information using graph representations of n-D data\nin 2-D. Specifically, this study shows that it can be done with static and\ndynamic In-line Based Coordinates in different modifications, which are a\ncategory of General Line Coordinates. Based on these inline coordinates,\nclassification and regression methods were developed. The viability of the\nstrategy was shown by two case studies based on benchmark datasets (Wisconsin\nBreast Cancer and Page Block Classification datasets). The characteristics of\npage block classification data led to the development of an algorithm for\nimbalanced high-resolution data with multiple classes, which exploits the\ndecision trees as a model design facilitator producing a model, which is more\ngeneral than a decision tree. This work accelerates the ongoing consolidation\nof an emerging field of full 2-D machine learning and its methodology. Within\nthis methodology the end users can discover models and justify them as\nself-service. Providing interpretable ML models is another benefit of this\napproach.",
                "Mitigating Label Biases for In-context Learning\nVarious design settings for in-context learning (ICL), such as the choice and\norder of the in-context examples, can bias a model toward a particular\nprediction without being reflective of an understanding of the task. While many\nstudies discuss these design choices, there have been few systematic\ninvestigations into categorizing them and mitigating their impact. In this\nwork, we define a typology for three types of label biases in ICL for text\nclassification: vanilla-label bias, context-label bias, and domain-label bias\n(which we conceptualize and detect for the first time).\n  Our analysis demonstrates that prior label bias calibration methods fall\nshort of addressing all three types of biases. Specifically, domain-label bias\nrestricts LLMs to random-level performance on many tasks regardless of the\nchoice of in-context examples. To mitigate the effect of these biases, we\npropose a simple bias calibration method that estimates a language model's\nlabel bias using random in-domain words from the task corpus. After controlling\nfor this estimated bias when making predictions, our novel domain-context\ncalibration significantly improves the ICL performance of GPT-J and GPT-3 on a\nwide range of tasks. The gain is substantial on tasks with large domain-label\nbias (up to 37% in Macro-F1). Furthermore, our results generalize to models\nwith different scales, pretraining methods, and manually-designed task\ninstructions, showing the prevalence of label biases in ICL.",
                "Learning to Learn from APIs: Black-Box Data-Free Meta-Learning\nData-free meta-learning (DFML) aims to enable efficient learning of new tasks\nby meta-learning from a collection of pre-trained models without access to the\ntraining data. Existing DFML work can only meta-learn from (i) white-box and\n(ii) small-scale pre-trained models (iii) with the same architecture,\nneglecting the more practical setting where the users only have inference\naccess to the APIs with arbitrary model architectures and model scale inside.\nTo solve this issue, we propose a Bi-level Data-free Meta Knowledge\nDistillation (BiDf-MKD) framework to transfer more general meta knowledge from\na collection of black-box APIs to one single meta model. Specifically, by just\nquerying APIs, we inverse each API to recover its training data via a\nzero-order gradient estimator and then perform meta-learning via a novel\nbi-level meta knowledge distillation structure, in which we design a boundary\nquery set recovery technique to recover a more informative query set near the\ndecision boundary. In addition, to encourage better generalization within the\nsetting of limited API budgets, we propose task memory replay to diversify the\nunderlying task distribution by covering more interpolated tasks. Extensive\nexperiments in various real-world scenarios show the superior performance of\nour BiDf-MKD framework.",
                "A Meta-learning Framework for Tuning Parameters of Protection Mechanisms\n  in Trustworthy Federated Learning\nTrustworthy Federated Learning (TFL) typically leverages protection\nmechanisms to guarantee privacy. However, protection mechanisms inevitably\nintroduce utility loss or efficiency reduction while protecting data privacy.\nTherefore, protection mechanisms and their parameters should be carefully\nchosen to strike an optimal tradeoff between \\textit{privacy leakage},\n\\textit{utility loss}, and \\textit{efficiency reduction}. To this end,\nfederated learning practitioners need tools to measure the three factors and\noptimize the tradeoff between them to choose the protection mechanism that is\nmost appropriate to the application at hand. Motivated by this requirement, we\npropose a framework that (1) formulates TFL as a problem of finding a\nprotection mechanism to optimize the tradeoff between privacy leakage, utility\nloss, and efficiency reduction and (2) formally defines bounded measurements of\nthe three factors. We then propose a meta-learning algorithm to approximate\nthis optimization problem and find optimal protection parameters for\nrepresentative protection mechanisms, including Randomization, Homomorphic\nEncryption, Secret Sharing, and Compression. We further design estimation\nalgorithms to quantify these found optimal protection parameters in a practical\nhorizontal federated learning setting and provide a theoretical analysis of the\nestimation error.",
                "JutePestDetect: An Intelligent Approach for Jute Pest Identification\n  Using Fine-Tuned Transfer Learning\nIn certain Asian countries, Jute is one of the primary sources of income and\nGross Domestic Product (GDP) for the agricultural sector. Like many other\ncrops, Jute is prone to pest infestations, and its identification is typically\nmade visually in countries like Bangladesh, India, Myanmar, and China. In\naddition, this method is time-consuming, challenging, and somewhat imprecise,\nwhich poses a substantial financial risk. To address this issue, the study\nproposes a high-performing and resilient transfer learning (TL) based\nJutePestDetect model to identify jute pests at the early stage. Firstly, we\nprepared jute pest dataset containing 17 classes and around 380 photos per pest\nclass, which were evaluated after manual and automatic pre-processing and\ncleaning, such as background removal and resizing. Subsequently, five prominent\npre-trained models -DenseNet201, InceptionV3, MobileNetV2, VGG19, and ResNet50\nwere selected from a previous study to design the JutePestDetect model. Each\nmodel was revised by replacing the classification layer with a global average\npooling layer and incorporating a dropout layer for regularization. To evaluate\nthe models performance, various metrics such as precision, recall, F1 score,\nROC curve, and confusion matrix were employed. These analyses provided\nadditional insights for determining the efficacy of the models. Among them, the\ncustomized regularized DenseNet201-based proposed JutePestDetect model\noutperformed the others, achieving an impressive accuracy of 99%. As a result,\nour proposed method and strategy offer an enhanced approach to pest\nidentification in the case of Jute, which can significantly benefit farmers\nworldwide.",
                "Dink-Net: Neural Clustering on Large Graphs\nDeep graph clustering, which aims to group the nodes of a graph into disjoint\nclusters with deep neural networks, has achieved promising progress in recent\nyears. However, the existing methods fail to scale to the large graph with\nmillion nodes. To solve this problem, a scalable deep graph clustering method\n(Dink-Net) is proposed with the idea of dilation and shrink. Firstly, by\ndiscriminating nodes, whether being corrupted by augmentations, representations\nare learned in a self-supervised manner. Meanwhile, the cluster centres are\ninitialized as learnable neural parameters. Subsequently, the clustering\ndistribution is optimized by minimizing the proposed cluster dilation loss and\ncluster shrink loss in an adversarial manner. By these settings, we unify the\ntwo-step clustering, i.e., representation learning and clustering optimization,\ninto an end-to-end framework, guiding the network to learn clustering-friendly\nfeatures. Besides, Dink-Net scales well to large graphs since the designed loss\nfunctions adopt the mini-batch data to optimize the clustering distribution\neven without performance drops. Both experimental results and theoretical\nanalyses demonstrate the superiority of our method. Compared to the runner-up,\nDink-Net achieves 9.62% NMI improvement on the ogbn-papers100M dataset with 111\nmillion nodes and 1.6 billion edges. The source code is released at\nhttps://github.com/yueliu1999/Dink-Net. Besides, a collection (papers, codes,\nand datasets) of deep graph clustering is shared at\nhttps://github.com/yueliu1999/Awesome-Deep-Graph-Clustering."
            ],
            "interesting paper": 2
        },
        {
            "papers": [
                "The Social Value of Dark Energy\nAstrophysics is a social enterprise exemplified here by the Dark Energy\nSurvey (DES) which completed its fieldwork in 2019 after 16 years of\npreparation and observation, while data analysis continues. Society funds\nastrophysics on a grand scale. For human capital and for governance the\ndiscipline draws on a self-governing \"republic of science\", while the funds\nwere provided by philanthropists in the past, and by governments today. The\nbenefits accrue initially to scientists themselves, in the form of a rewarding\nvocation. For the social benefit it is tempting to apply formal cost benefit\nanalysis, but that approach ignores the option value of science and imposes\nquestionable assumptions from welfare economics. Astrophysics generates some\nuseful spinoffs, offers attractive careers, appeals to the popular imagination,\nspeaks to metaphysical cravings and constitutes a good in itself. The rise of\nAI also suggests a role in exploring future habitats for intelligence and\ncognition.",
                "A machine learning approach to the prediction of heat-transfer\n  coefficients in micro-channels\nThe accurate prediction of the two-phase heat transfer coefficient (HTC) as a\nfunction of working fluids, channel geometries and process conditions is key to\nthe optimal design and operation of compact heat exchangers. Advances in\nartificial intelligence research have recently boosted the application of\nmachine learning (ML) algorithms to obtain data-driven surrogate models for the\nHTC. For most supervised learning algorithms, the task is that of a nonlinear\nregression problem. Despite the fact that these models have been proven capable\nof outperforming traditional empirical correlations, they have key limitations\nsuch as overfitting the data, the lack of uncertainty estimation, and\ninterpretability of the results. To address these limitations, in this paper,\nwe use a multi-output Gaussian process regression (GPR) to estimate the HTC in\nmicrochannels as a function of the mass flow rate, heat flux, system pressure\nand channel diameter and length. The model is trained using the Brunel\nTwo-Phase Flow database of high-fidelity experimental data. The advantages of\nGPR are data efficiency, the small number of hyperparameters to be trained\n(typically of the same order of the number of input dimensions), and the\nautomatic trade-off between data fit and model complexity guaranteed by the\nmaximization of the marginal likelihood (Bayesian approach). Our paper proposes\nresearch directions to improve the performance of the GPR-based model in\nextrapolation.",
                "Employing Explainable Artificial Intelligence (XAI) Methodologies to\n  Analyze the Correlation between Input Variables and Tensile Strength in\n  Additively Manufactured Samples\nThis research paper explores the impact of various input parameters,\nincluding Infill percentage, Layer Height, Extrusion Temperature, and Print\nSpeed, on the resulting Tensile Strength in objects produced through additive\nmanufacturing. The main objective of this study is to enhance our understanding\nof the correlation between the input parameters and Tensile Strength, as well\nas to identify the key factors influencing the performance of the additive\nmanufacturing process. To achieve this objective, we introduced the utilization\nof Explainable Artificial Intelligence (XAI) techniques for the first time,\nwhich allowed us to analyze the data and gain valuable insights into the\nsystem's behavior. Specifically, we employed SHAP (SHapley Additive\nexPlanations), a widely adopted framework for interpreting machine learning\nmodel predictions, to provide explanations for the behavior of a machine\nlearning model trained on the data. Our findings reveal that the Infill\npercentage and Extrusion Temperature have the most significant influence on\nTensile Strength, while the impact of Layer Height and Print Speed is\nrelatively minor. Furthermore, we discovered that the relationship between the\ninput parameters and Tensile Strength is highly intricate and nonlinear, making\nit difficult to accurately describe using simple linear models.",
                "Predictability and Fairness in Load Aggregation with Deadband\nVirtual power plants and load aggregation are becoming increasingly common.\nThere, one regulates the aggregate power output of an ensemble of distributed\nenergy resources (DERs). Marecek et al. [Automatica, Volume 147, January 2023,\n110743, arXiv:2110.03001] recently suggested that long-term averages of prices\nor incentives offered should exist and be independent of the initial states of\nthe operators of the DER, the aggregator, and the power grid. This can be seen\nas predictability, which underlies fairness. Unfortunately, the existence of\nsuch averages cannot be guaranteed with many traditional regulators, including\nthe proportional-integral (PI) regulator with or without deadband. Here, we\nconsider the effects of losses in the alternating current model and the\ndeadband in the controller. This yields a non-linear dynamical system (due to\nthe non-linear losses) exhibiting discontinuities (due to the deadband). We\nshow that Filippov invariant measures enable reasoning about predictability and\nfairness while considering non-linearity of the alternating-current model and\ndeadband.",
                "Re-imagining health and well-being in low resource African settings\n  using an augmented AI system and a 3D digital twin\nThis paper discusses and explores the potential and relevance of recent\ndevelopments in artificial intelligence (AI) and digital twins for health and\nwell-being in low-resource African countries. We use the case of public health\nemergency response to disease outbreaks and epidemic control. There is\npotential to take advantage of the increasing availability of data and\ndigitization to develop advanced AI methods for analysis and prediction. Using\nan AI systems perspective, we review emerging trends in AI systems and digital\ntwins and propose an initial augmented AI system architecture to illustrate how\nan AI system can work with a 3D digital twin to address public health goals. We\nhighlight scientific knowledge discovery, continual learning, pragmatic\ninteroperability, and interactive explanation and decision-making as essential\nresearch challenges for AI systems and digital twins.",
                "Key-Value Transformer\nTransformers have emerged as the prevailing standard solution for various AI\ntasks, including computer vision and natural language processing. The widely\nadopted Query, Key, and Value formulation (QKV) has played a significant role\nin this. Nevertheless, no research has examined the essentiality of these three\ncomponents for transformer performance. Therefore, we conducted an evaluation\nof the key-value formulation (KV), which generates symmetric attention maps,\nalong with an asymmetric version that incorporates a 2D positional encoding\ninto the attention matrix. Remarkably, this transformer requires fewer\nparameters and computation than the original one. Through experiments\nencompassing three task types -- synthetics (such as reversing or sorting a\nlist), vision (mnist or cifar classification), and NLP (character generation\nand translation) -- we discovered that the KV transformer occasionally\noutperforms the QKV transformer. However, it also exhibits instances of\nunderperformance compared to QKV, making it challenging to draw a definitive\nconclusion. Nonetheless, we consider the reported results to be encouraging and\nanticipate that they may pave the way for more efficient transformers in the\nfuture."
            ],
            "interesting paper": 2
        },
        {
            "papers": [
                "Learning Off-Road Terrain Traversability with Self-Supervisions Only\nEstimating the traversability of terrain should be reliable and accurate in\ndiverse conditions for autonomous driving in off-road environments. However,\nlearning-based approaches often yield unreliable results when confronted with\nunfamiliar contexts, and it is challenging to obtain manual annotations\nfrequently for new circumstances. In this paper, we introduce a method for\nlearning traversability from images that utilizes only self-supervision and no\nmanual labels, enabling it to easily learn traversability in new circumstances.\nTo this end, we first generate self-supervised traversability labels from past\ndriving trajectories by labeling regions traversed by the vehicle as highly\ntraversable. Using the self-supervised labels, we then train a neural network\nthat identifies terrains that are safe to traverse from an image using a\none-class classification algorithm. Additionally, we supplement the limitations\nof self-supervised labels by incorporating methods of self-supervised learning\nof visual representations. To conduct a comprehensive evaluation, we collect\ndata in a variety of driving environments and perceptual conditions and show\nthat our method produces reliable estimations in various environments. In\naddition, the experimental results validate that our method outperforms other\nself-supervised traversability estimation methods and achieves comparable\nperformances with supervised learning methods trained on manually labeled data.",
                "Partially Personalized Federated Learning: Breaking the Curse of Data\n  Heterogeneity\nWe present a partially personalized formulation of Federated Learning (FL)\nthat strikes a balance between the flexibility of personalization and\ncooperativeness of global training. In our framework, we split the variables\ninto global parameters, which are shared across all clients, and individual\nlocal parameters, which are kept private. We prove that under the right split\nof parameters, it is possible to find global parameters that allow each client\nto fit their data perfectly, and refer to the obtained problem as\noverpersonalized. For instance, the shared global parameters can be used to\nlearn good data representations, whereas the personalized layers are fine-tuned\nfor a specific client. Moreover, we present a simple algorithm for the\npartially personalized formulation that offers significant benefits to all\nclients. In particular, it breaks the curse of data heterogeneity in several\nsettings, such as training with local steps, asynchronous training, and\nByzantine-robust training.",
                "Task-Equivariant Graph Few-shot Learning\nAlthough Graph Neural Networks (GNNs) have been successful in node\nclassification tasks, their performance heavily relies on the availability of a\nsufficient number of labeled nodes per class. In real-world situations, not all\nclasses have many labeled nodes and there may be instances where the model\nneeds to classify new classes, making manual labeling difficult. To solve this\nproblem, it is important for GNNs to be able to classify nodes with a limited\nnumber of labeled nodes, known as few-shot node classification. Previous\nepisodic meta-learning based methods have demonstrated success in few-shot node\nclassification, but our findings suggest that optimal performance can only be\nachieved with a substantial amount of diverse training meta-tasks. To address\nthis challenge of meta-learning based few-shot learning (FSL), we propose a new\napproach, the Task-Equivariant Graph few-shot learning (TEG) framework. Our TEG\nframework enables the model to learn transferable task-adaptation strategies\nusing a limited number of training meta-tasks, allowing it to acquire\nmeta-knowledge for a wide range of meta-tasks. By incorporating equivariant\nneural networks, TEG can utilize their strong generalization abilities to learn\nhighly adaptable task-specific strategies. As a result, TEG achieves\nstate-of-the-art performance with limited training meta-tasks. Our experiments\non various benchmark datasets demonstrate TEG's superiority in terms of\naccuracy and generalization ability, even when using minimal meta-training\ndata, highlighting the effectiveness of our proposed approach in addressing the\nchallenges of meta-learning based few-shot node classification. Our code is\navailable at the following link: https://github.com/sung-won-kim/TEG",
                "Controllable Path of Destruction\nPath of Destruction (PoD) is a self-supervised method for learning iterative\ngenerators. The core idea is to produce a training set by destroying a set of\nartifacts, and for each destructive step create a training instance based on\nthe corresponding repair action. A generator trained on this dataset can then\ngenerate new artifacts by repairing from arbitrary states. The PoD method is\nvery data-efficient in terms of original training examples and well-suited to\nfunctional artifacts composed of categorical data, such as game levels and\ndiscrete 3D structures. In this paper, we extend the Path of Destruction method\nto allow designer control over aspects of the generated artifacts.\nControllability is introduced by adding conditional inputs to the state-action\npairs that make up the repair trajectories. We test the controllable PoD method\nin a 2D dungeon setting, as well as in the domain of small 3D Lego cars.",
                "Autoencoding Conditional Neural Processes for Representation Learning\nConditional neural processes (CNPs) are a flexible and efficient family of\nmodels that learn to learn a stochastic process from data. They have seen\nparticular application in contextual image completion - observing pixel values\nat some locations to predict a distribution over values at other unobserved\nlocations. However, the choice of pixels in learning CNPs is typically either\nrandom or derived from a simple statistical measure (e.g. pixel variance).\nHere, we turn the problem on its head and ask: which pixels would a CNP like to\nobserve - do they facilitate fitting better CNPs, and do such pixels tell us\nsomething meaningful about the underlying image? To this end we develop the\nPartial Pixel Space Variational Autoencoder (PPS-VAE), an amortised variational\nframework that casts CNP context as latent variables learnt simultaneously with\nthe CNP. We evaluate PPS-VAE over a number of tasks across different visual\ndata, and find that not only can it facilitate better-fit CNPs, but also that\nthe spatial arrangement and values meaningfully characterise image information\n- evaluated through the lens of classification on both within and out-of-data\ndistributions. Our model additionally allows for dynamic adaption of\ncontext-set size and the ability to scale-up to larger images, providing a\npromising avenue to explore learning meaningful and effective visual\nrepresentations.",
                "DMS: Differentiable Mean Shift for Dataset Agnostic Task Specific\n  Clustering Using Side Information\nWe present a novel approach, in which we learn to cluster data directly from\nside information, in the form of a small set of pairwise examples. Unlike\nprevious methods, with or without side information, we do not need to know the\nnumber of clusters, their centers or any kind of distance metric for\nsimilarity. Our method is able to divide the same data points in various ways\ndependant on the needs of a specific task, defined by the side information.\nContrastingly, other work generally finds only the intrinsic, most obvious,\nclusters. Inspired by the mean shift algorithm, we implement our new clustering\napproach using a custom iterative neural network to create Differentiable Mean\nShift (DMS), a state of the art, dataset agnostic, clustering method. We found\nthat it was possible to train a strong cluster definition without enforcing a\nconstraint that each cluster must be presented during training. DMS outperforms\ncurrent methods in both the intrinsic and non-intrinsic dataset tasks."
            ],
            "interesting paper": 1
        },
        {
            "papers": [
                "Towards Machine Learning and Inference for Resource-constrained MCUs\nMachine learning (ML) is moving towards edge devices. However, ML models with\nhigh computational demands and energy consumption pose challenges for ML\ninference in resource-constrained environments, such as the deep sea. To\naddress these challenges, we propose a battery-free ML inference and model\npersonalization pipeline for microcontroller units (MCUs). As an example, we\nperformed fish image recognition in the ocean. We evaluated and compared the\naccuracy, runtime, power, and energy consumption of the model before and after\noptimization. The results demonstrate that, our pipeline can achieve 97.78%\naccuracy with 483.82 KB Flash, 70.32 KB RAM, 118 ms runtime, 4.83 mW power, and\n0.57 mJ energy consumption on MCUs, reducing by 64.17%, 12.31%, 52.42%, 63.74%,\nand 82.67%, compared to the baseline. The results indicate the feasibility of\nbattery-free ML inference on MCUs.",
                "Temporally Layered Architecture for Efficient Continuous Control\nWe present a temporally layered architecture (TLA) for temporally adaptive\ncontrol with minimal energy expenditure. The TLA layers a fast and a slow\npolicy together to achieve temporal abstraction that allows each layer to focus\non a different time scale. Our design draws on the energy-saving mechanism of\nthe human brain, which executes actions at different timescales depending on\nthe environment's demands. We demonstrate that beyond energy saving, TLA\nprovides many additional advantages, including persistent exploration, fewer\nrequired decisions, reduced jerk, and increased action repetition. We evaluate\nour method on a suite of continuous control tasks and demonstrate the\nsignificant advantages of TLA over existing methods when measured over multiple\nimportant metrics. We also introduce a multi-objective score to qualitatively\nassess continuous control policies and demonstrate a significantly better score\nfor TLA. Our training algorithm uses minimal communication between the slow and\nfast layers to train both policies simultaneously, making it viable for future\napplications in distributed control.",
                "Long-term Wind Power Forecasting with Hierarchical Spatial-Temporal\n  Transformer\nWind power is attracting increasing attention around the world due to its\nrenewable, pollution-free, and other advantages. However, safely and stably\nintegrating the high permeability intermittent power energy into electric power\nsystems remains challenging. Accurate wind power forecasting (WPF) can\neffectively reduce power fluctuations in power system operations. Existing\nmethods are mainly designed for short-term predictions and lack effective\nspatial-temporal feature augmentation. In this work, we propose a novel\nend-to-end wind power forecasting model named Hierarchical Spatial-Temporal\nTransformer Network (HSTTN) to address the long-term WPF problems.\nSpecifically, we construct an hourglass-shaped encoder-decoder framework with\nskip-connections to jointly model representations aggregated in hierarchical\ntemporal scales, which benefits long-term forecasting. Based on this framework,\nwe capture the inter-scale long-range temporal dependencies and global spatial\ncorrelations with two parallel Transformer skeletons and strengthen the\nintra-scale connections with downsampling and upsampling operations. Moreover,\nthe complementary information from spatial and temporal features is fused and\npropagated in each other via Contextual Fusion Blocks (CFBs) to promote the\nprediction further. Extensive experimental results on two large-scale\nreal-world datasets demonstrate the superior performance of our HSTTN over\nexisting solutions.",
                "Perimeter Control Using Deep Reinforcement Learning: A Model-free\n  Approach towards Homogeneous Flow Rate Optimization\nPerimeter control maintains high traffic efficiency within protected regions\nby controlling transfer flows among regions to ensure that their traffic\ndensities are below critical values. Existing approaches can be categorized as\neither model-based or model-free, depending on whether they rely on network\ntransmission models (NTMs) and macroscopic fundamental diagrams (MFDs).\nAlthough model-based approaches are more data efficient and have performance\nguarantees, they are inherently prone to model bias and inaccuracy. For\nexample, NTMs often become imprecise for a large number of protected regions,\nand MFDs can exhibit scatter and hysteresis that are not captured in existing\nmodel-based works. Moreover, no existing studies have employed reinforcement\nlearning for homogeneous flow rate optimization in microscopic simulation,\nwhere spatial characteristics, vehicle-level information, and metering\nrealizations -- often overlooked in macroscopic simulations -- are taken into\naccount. To circumvent issues of model-based approaches and macroscopic\nsimulation, we propose a model-free deep reinforcement learning approach that\noptimizes the flow rate homogeneously at the perimeter at the microscopic\nlevel. Results demonstrate that our model-free reinforcement learning approach\nwithout any knowledge of NTMs or MFDs can compete and match the performance of\na model-based approach, and exhibits enhanced generalizability and scalability.",
                "Towards a Unifying Model of Rationality in Multiagent Systems\nMultiagent systems deployed in the real world need to cooperate with other\nagents (including humans) nearly as effectively as these agents cooperate with\none another. To design such AI, and provide guarantees of its effectiveness, we\nneed to clearly specify what types of agents our AI must be able to cooperate\nwith. In this work we propose a generic model of socially intelligent agents,\nwhich are individually rational learners that are also able to cooperate with\none another (in the sense that their joint behavior is Pareto efficient). We\ndefine rationality in terms of the regret incurred by each agent over its\nlifetime, and show how we can construct socially intelligent agents for\ndifferent forms of regret. We then discuss the implications of this model for\nthe development of \"robust\" MAS that can cooperate with a wide variety of\nsocially intelligent agents.",
                "What is Essential for Unseen Goal Generalization of Offline\n  Goal-conditioned RL?\nOffline goal-conditioned RL (GCRL) offers a way to train general-purpose\nagents from fully offline datasets. In addition to being conservative within\nthe dataset, the generalization ability to achieve unseen goals is another\nfundamental challenge for offline GCRL. However, to the best of our knowledge,\nthis problem has not been well studied yet. In this paper, we study\nout-of-distribution (OOD) generalization of offline GCRL both theoretically and\nempirically to identify factors that are important. In a number of experiments,\nwe observe that weighted imitation learning enjoys better generalization than\npessimism-based offline RL method. Based on this insight, we derive a theory\nfor OOD generalization, which characterizes several important design choices.\nWe then propose a new offline GCRL method, Generalizable Offline\ngoAl-condiTioned RL (GOAT), by combining the findings from our theoretical and\nempirical studies. On a new benchmark containing 9 independent identically\ndistributed (IID) tasks and 17 OOD tasks, GOAT outperforms current\nstate-of-the-art methods by a large margin."
            ],
            "interesting paper": 2
        },
        {
            "papers": [
                "A Computational Account Of Self-Supervised Visual Learning From\n  Egocentric Object Play\nResearch in child development has shown that embodied experience handling\nphysical objects contributes to many cognitive abilities, including visual\nlearning. One characteristic of such experience is that the learner sees the\nsame object from several different viewpoints. In this paper, we study how\nlearning signals that equate different viewpoints -- e.g., assigning similar\nrepresentations to different views of a single object -- can support robust\nvisual learning. We use the Toybox dataset, which contains egocentric videos of\nhumans manipulating different objects, and conduct experiments using a computer\nvision framework for self-supervised contrastive learning. We find that\nrepresentations learned by equating different physical viewpoints of an object\nbenefit downstream image classification accuracy. Further experiments show that\nthis performance improvement is robust to variations in the gaps between\nviewpoints, and that the benefits transfer to several different image\nclassification tasks.",
                "Contextual Vision Transformers for Robust Representation Learning\nWe introduce Contextual Vision Transformers (ContextViT), a method designed\nto generate robust image representations for datasets experiencing shifts in\nlatent factors across various groups. Derived from the concept of in-context\nlearning, ContextViT incorporates an additional context token to encapsulate\ngroup-specific information. This integration allows the model to adjust the\nimage representation in accordance with the group-specific context.\nSpecifically, for a given input image, ContextViT maps images with identical\ngroup membership into this context token, which is appended to the input image\ntokens. Additionally, we introduce a context inference network to predict such\ntokens on-the-fly, given a batch of samples from the group. This enables\nContextViT to adapt to new testing distributions during inference time. We\ndemonstrate the efficacy of ContextViT across a wide range of applications. In\nsupervised fine-tuning, we show that augmenting pre-trained ViTs with our\nproposed context conditioning mechanism results in consistent improvements in\nout-of-distribution generalization on iWildCam and FMoW. We also investigate\nself-supervised representation learning with ContextViT. Our experiments on the\nCamelyon17 pathology imaging benchmark and the JUMP-CP microscopy imaging\nbenchmark demonstrate that ContextViT excels in learning stable image\nfeaturizations amidst distribution shift, consistently outperforming its ViT\ncounterpart.",
                "Spotlight Attention: Robust Object-Centric Learning With a Spatial\n  Locality Prior\nThe aim of object-centric vision is to construct an explicit representation\nof the objects in a scene. This representation is obtained via a set of\ninterchangeable modules called \\emph{slots} or \\emph{object files} that compete\nfor local patches of an image. The competition has a weak inductive bias to\npreserve spatial continuity; consequently, one slot may claim patches scattered\ndiffusely throughout the image. In contrast, the inductive bias of human vision\nis strong, to the degree that attention has classically been described with a\nspotlight metaphor. We incorporate a spatial-locality prior into\nstate-of-the-art object-centric vision models and obtain significant\nimprovements in segmenting objects in both synthetic and real-world datasets.\nSimilar to human visual attention, the combination of image content and spatial\nconstraints yield robust unsupervised object-centric learning, including less\nsensitivity to model hyperparameters.",
                "DENTEX: An Abnormal Tooth Detection with Dental Enumeration and\n  Diagnosis Benchmark for Panoramic X-rays\nPanoramic X-rays are frequently used in dentistry for treatment planning, but\ntheir interpretation can be both time-consuming and prone to error. Artificial\nintelligence (AI) has the potential to aid in the analysis of these X-rays,\nthereby improving the accuracy of dental diagnoses and treatment plans.\nNevertheless, designing automated algorithms for this purpose poses significant\nchallenges, mainly due to the scarcity of annotated data and variations in\nanatomical structure. To address these issues, the Dental Enumeration and\nDiagnosis on Panoramic X-rays Challenge (DENTEX) has been organized in\nassociation with the International Conference on Medical Image Computing and\nComputer-Assisted Intervention (MICCAI) in 2023. This challenge aims to promote\nthe development of algorithms for multi-label detection of abnormal teeth,\nusing three types of hierarchically annotated data: partially annotated\nquadrant data, partially annotated quadrant-enumeration data, and fully\nannotated quadrant-enumeration-diagnosis data, inclusive of four different\ndiagnoses. In this paper, we present the results of evaluating participant\nalgorithms on the fully annotated data, additionally investigating performance\nvariation for quadrant, enumeration, and diagnosis labels in the detection of\nabnormal teeth. The provision of this annotated dataset, alongside the results\nof this challenge, may lay the groundwork for the creation of AI-powered tools\nthat can offer more precise and efficient diagnosis and treatment planning in\nthe field of dentistry. The evaluation code and datasets can be accessed at\nhttps://github.com/ibrahimethemhamamci/DENTEX",
                "FedCSD: A Federated Learning Based Approach for Code-Smell Detection\nThis paper proposes a Federated Learning Code Smell Detection (FedCSD)\napproach that allows organizations to collaboratively train federated ML models\nwhile preserving their data privacy. These assertions have been supported by\nthree experiments that have significantly leveraged three manually validated\ndatasets aimed at detecting and examining different code smell scenarios. In\nexperiment 1, which was concerned with a centralized training experiment,\ndataset two achieved the lowest accuracy (92.30%) with fewer smells, while\ndatasets one and three achieved the highest accuracy with a slight difference\n(98.90% and 99.5%, respectively). This was followed by experiment 2, which was\nconcerned with cross-evaluation, where each ML model was trained using one\ndataset, which was then evaluated over the other two datasets. Results from\nthis experiment show a significant drop in the model's accuracy (lowest\naccuracy: 63.80\\%) where fewer smells exist in the training dataset, which has\na noticeable reflection (technical debt) on the model's performance. Finally,\nthe last and third experiments evaluate our approach by splitting the dataset\ninto 10 companies. The ML model was trained on the company's site, then all\nmodel-updated weights were transferred to the server. Ultimately, an accuracy\nof 98.34% was achieved by the global model that has been trained using 10\ncompanies for 100 training rounds. The results reveal a slight difference in\nthe global model's accuracy compared to the highest accuracy of the centralized\nmodel, which can be ignored in favour of the global model's comprehensive\nknowledge, lower training cost, preservation of data privacy, and avoidance of\nthe technical debt problem.",
                "A rule-general abductive learning by rough sets\nIn real-world tasks, there is usually a large amount of unlabeled data and\nlabeled data. The task of combining the two to learn is known as\nsemi-supervised learning. Experts can use logical rules to label unlabeled\ndata, but this operation is costly. The combination of perception and reasoning\nhas a good effect in processing such semi-supervised tasks with domain\nknowledge. However, acquiring domain knowledge and the correction, reduction\nand generation of rules remain complex problems to be solved. Rough set theory\nis an important method for solving knowledge processing in information systems.\nIn this paper, we propose a rule general abductive learning by rough set\n(RS-ABL). By transforming the target concept and sub-concepts of rules into\ninformation tables, rough set theory is used to solve the acquisition of domain\nknowledge and the correction, reduction and generation of rules at a lower\ncost. This framework can also generate more extensive negative rules to enhance\nthe breadth of the knowledge base. Compared with the traditional\nsemi-supervised learning method, RS-ABL has higher accuracy in dealing with\nsemi-supervised tasks."
            ],
            "interesting paper": 2
        },
        {
            "papers": [
                "Non-convex Bayesian Learning via Stochastic Gradient Markov Chain Monte\n  Carlo\nThe rise of artificial intelligence (AI) hinges on the efficient training of\nmodern deep neural networks (DNNs) for non-convex optimization and uncertainty\nquantification, which boils down to a non-convex Bayesian learning problem. A\nstandard tool to handle the problem is Langevin Monte Carlo, which proposes to\napproximate the posterior distribution with theoretical guarantees. In this\nthesis, we start with the replica exchange Langevin Monte Carlo (also known as\nparallel tempering), which proposes appropriate swaps between exploration and\nexploitation to achieve accelerations. However, the na\\\"ive extension of swaps\nto big data problems leads to a large bias, and bias-corrected swaps are\nrequired. Such a mechanism leads to few effective swaps and insignificant\naccelerations. To alleviate this issue, we first propose a control variates\nmethod to reduce the variance of noisy energy estimators and show a potential\nto accelerate the exponential convergence. We also present the population-chain\nreplica exchange based on non-reversibility and obtain an optimal round-trip\nrate for deep learning. In the second part of the thesis, we study scalable\ndynamic importance sampling algorithms based on stochastic approximation.\nTraditional dynamic importance sampling algorithms have achieved success,\nhowever, the lack of scalability has greatly limited their extensions to big\ndata. To handle this scalability issue, we resolve the vanishing gradient\nproblem and propose two dynamic importance sampling algorithms. Theoretically,\nwe establish the stability condition for the underlying ordinary differential\nequation (ODE) system and guarantee the asymptotic convergence of the latent\nvariable to the desired fixed point. Interestingly, such a result still holds\ngiven non-convex energy landscapes.",
                "DiffLoad: Uncertainty Quantification in Electrical Load Forecasting with\n  the Diffusion Model\nElectrical load forecasting plays a crucial role in decision-making for power\nsystems, including unit commitment and economic dispatch. The integration of\nrenewable energy sources and the occurrence of external events, such as the\nCOVID-19 pandemic, have rapidly increased uncertainties in load forecasting.\nThe uncertainties in load forecasting can be divided into two types: epistemic\nuncertainty and aleatoric uncertainty. Separating these types of uncertainties\ncan help decision-makers better understand where and to what extent the\nuncertainty is, thereby enhancing their confidence in the following\ndecision-making. This paper proposes a diffusion-based Seq2Seq structure to\nestimate epistemic uncertainty and employs the robust additive Cauchy\ndistribution to estimate aleatoric uncertainty. Our method not only ensures the\naccuracy of load forecasting but also demonstrates the ability to separate the\ntwo types of uncertainties and be applicable to different levels of loads. The\nrelevant code can be found at\n\\url{https://anonymous.4open.science/r/DiffLoad-4714/}.",
                "Bigger, Better, Faster: Human-level Atari with human-level efficiency\nWe introduce a value-based RL agent, which we call BBF, that achieves\nsuper-human performance in the Atari 100K benchmark. BBF relies on scaling the\nneural networks used for value estimation, as well as a number of other design\nchoices that enable this scaling in a sample-efficient manner. We conduct\nextensive analyses of these design choices and provide insights for future\nwork. We end with a discussion about updating the goalposts for\nsample-efficient RL research on the ALE. We make our code and data publicly\navailable at\nhttps://github.com/google-research/google-research/tree/master/bigger_better_faster.",
                "Uncovering multifunctional mechano-intelligence in and through phononic\n  metastructures harnessing physical reservoir computing\nThe recent advances in autonomous systems have prompted a strong demand for\nthe next generation of adaptive structures and materials to possess more\nbuilt-in intelligence in their mechanical domain, the so-called\nmechano-intelligence (MI). Previous MI attempts mainly focused on specific\ndesigns and case studies to realize limited aspects of MI, and there is a lack\nof a systematic foundation in constructing and integrating the different\nelements of intelligence in an effective and efficient manner. Here, we propose\na new approach to create the needed foundation in realizing integrated\nmultifunctional MI via a physical reservoir computing (PRC) framework. That is,\nto concurrently embody computing power and the various elements of\nintelligence, namely perception, decision-making, and commanding, directly in\nthe mechanical domain, advancing from conventional adaptive structures that\nrely solely on add-on digital computers and massive electronics to achieve\nintelligence. As an exemplar platform, we construct a mechanically intelligent\nphononic metastructure with the integrated elements of MI by harnessing the PRC\npower hidden in their high-degree-of-freedom nonlinear dynamics. Through\nanalyses and experimental investigations, we uncover multiple adaptive\nstructural functions ranging from self-tuning wave controls to wave-based logic\ngates. This research will provide the basis for creating future new structures\nthat would greatly surpass the state of the art - such as lower power\nconsumption, more direct interactions, and much better survivability in harsh\nenvironment or under cyberattacks. Moreover, it will enable the addition of new\nfunctions and autonomy to systems without overburdening the onboard computers.",
                "The Dynamic Sensorium competition for predicting large-scale mouse\n  visual cortex activity from videos\nUnderstanding how biological visual systems process information is\nchallenging due to the complex nonlinear relationship between neuronal\nresponses and high-dimensional visual input. Artificial neural networks have\nalready improved our understanding of this system by allowing computational\nneuroscientists to create predictive models and bridge biological and machine\nvision. During the Sensorium 2022, we introduced benchmarks for vision models\nwith static input. However, animals operate and excel in dynamic environments,\nmaking it crucial to study and understand how the brain functions under these\nconditions. Moreover, many biological theories, such as predictive coding,\nsuggest that previous input is crucial for current input processing. Currently,\nthere is no standardized benchmark to identify state-of-the-art dynamic models\nof the mouse visual system. To address this gap, we propose the Sensorium 2023\nBenchmark Competition with dynamic input. It includes the collection of a new\nlarge-scale dataset from the primary visual cortex of ten mice, containing\nresponses from over 78,000 neurons to over 2 hours of dynamic stimuli per\nneuron. Participants in the main benchmark track will compete to identify the\nbest predictive models of neuronal responses for dynamic input. We will also\nhost a bonus track in which submission performance will be evaluated on\nout-of-domain input, using withheld neuronal responses to dynamic input stimuli\nwhose statistics differ from the training set. Both tracks will offer\nbehavioral data along with video stimuli. As before, we will provide code,\ntutorials, and strong pre-trained baseline models to encourage participation.\nWe hope this competition will continue to strengthen the accompanying Sensorium\nbenchmarks collection as a standard tool to measure progress in large-scale\nneural system identification models of the entire mouse visual hierarchy and\nbeyond.",
                "Catalysis distillation neural network for the few shot open catalyst\n  challenge\nThe integration of artificial intelligence and science has resulted in\nsubstantial progress in computational chemistry methods for the design and\ndiscovery of novel catalysts. Nonetheless, the challenges of electrocatalytic\nreactions and developing a large-scale language model in catalysis persist, and\nthe recent success of ChatGPT's (Chat Generative Pre-trained Transformer)\nfew-shot methods surpassing BERT (Bidirectional Encoder Representation from\nTransformers) underscores the importance of addressing limited data, expensive\ncomputations, time constraints and structure-activity relationship in research.\nHence, the development of few-shot techniques for catalysis is critical and\nessential, regardless of present and future requirements. This paper introduces\nthe Few-Shot Open Catalyst Challenge 2023, a competition aimed at advancing the\napplication of machine learning technology for predicting catalytic reactions\non catalytic surfaces, with a specific focus on dual-atom catalysts in hydrogen\nperoxide electrocatalysis. To address the challenge of limited data in\ncatalysis, we propose a machine learning approach based on MLP-Like and a\nframework called Catalysis Distillation Graph Neural Network (CDGNN). Our\nresults demonstrate that CDGNN effectively learns embeddings from catalytic\nstructures, enabling the capture of structure-adsorption relationships. This\naccomplishment has resulted in the utmost advanced and efficient determination\nof the reaction pathway for hydrogen peroxide, surpassing the current graph\nneural network approach by 16.1%.. Consequently, CDGNN presents a promising\napproach for few-shot learning in catalysis."
            ],
            "interesting paper": 2
        },
        {
            "papers": [
                "Quantifying Representation Reliability in Self-Supervised Learning\n  Models\nSelf-supervised learning models extract general-purpose representations from\ndata. Quantifying the reliability of these representations is crucial, as many\ndownstream models rely on them as input for their own tasks. To this end, we\nintroduce a formal definition of representation reliability: the representation\nfor a given test point is considered to be reliable if the downstream models\nbuilt on top of that representation can consistently generate accurate\npredictions for that test point. However, accessing downstream data to quantify\nthe representation reliability is often infeasible or restricted due to privacy\nconcerns. We propose an ensemble-based method for estimating the representation\nreliability without knowing the downstream tasks a priori. Our method is based\non the concept of neighborhood consistency across distinct pre-trained\nrepresentation spaces. The key insight is to find shared neighboring points as\nanchors to align these representation spaces before comparing them. We\ndemonstrate through comprehensive numerical experiments that our method\neffectively captures the representation reliability with a high degree of\ncorrelation, achieving robust and favorable performance compared with baseline\nmethods.",
                "Doubly Robust Self-Training\nSelf-training is an important technique for solving semi-supervised learning\nproblems. It leverages unlabeled data by generating pseudo-labels and combining\nthem with a limited labeled dataset for training. The effectiveness of\nself-training heavily relies on the accuracy of these pseudo-labels. In this\npaper, we introduce doubly robust self-training, a novel semi-supervised\nalgorithm that provably balances between two extremes. When the pseudo-labels\nare entirely incorrect, our method reduces to a training process solely using\nlabeled data. Conversely, when the pseudo-labels are completely accurate, our\nmethod transforms into a training process utilizing all pseudo-labeled data and\nlabeled data, thus increasing the effective sample size. Through empirical\nevaluations on both the ImageNet dataset for image classification and the\nnuScenes autonomous driving dataset for 3D object detection, we demonstrate the\nsuperiority of the doubly robust loss over the standard self-training baseline.",
                "SSL-CPCD: Self-supervised learning with composite pretext-class\n  discrimination for improved generalisability in endoscopic image analysis\nData-driven methods have shown tremendous progress in medical image analysis.\nIn this context, deep learning-based supervised methods are widely popular.\nHowever, they require a large amount of training data and face issues in\ngeneralisability to unseen datasets that hinder clinical translation.\nEndoscopic imaging data incorporates large inter- and intra-patient variability\nthat makes these models more challenging to learn representative features for\ndownstream tasks. Thus, despite the publicly available datasets and datasets\nthat can be generated within hospitals, most supervised models still\nunderperform. While self-supervised learning has addressed this problem to some\nextent in natural scene data, there is a considerable performance gap in the\nmedical image domain. In this paper, we propose to explore patch-level\ninstance-group discrimination and penalisation of inter-class variation using\nadditive angular margin within the cosine similarity metrics. Our novel\napproach enables models to learn to cluster similar representative patches,\nthereby improving their ability to provide better separation between different\nclasses. Our results demonstrate significant improvement on all metrics over\nthe state-of-the-art (SOTA) methods on the test set from the same and diverse\ndatasets. We evaluated our approach for classification, detection, and\nsegmentation. SSL-CPCD achieves 79.77% on Top 1 accuracy for ulcerative colitis\nclassification, 88.62% on mAP for polyp detection, and 82.32% on dice\nsimilarity coefficient for segmentation tasks are nearly over 4%, 2%, and 3%,\nrespectively, compared to the baseline architectures. We also demonstrate that\nour method generalises better than all SOTA methods to unseen datasets,\nreporting nearly 7% improvement in our generalisability assessment.",
                "MERT: Acoustic Music Understanding Model with Large-Scale\n  Self-supervised Training\nSelf-supervised learning (SSL) has recently emerged as a promising paradigm\nfor training generalisable models on large-scale data in the fields of vision,\ntext, and speech. Although SSL has been proven effective in speech and audio,\nits application to music audio has yet to be thoroughly explored. This is\npartially due to the distinctive challenges associated with modelling musical\nknowledge, particularly tonal and pitched characteristics of music. To address\nthis research gap, we propose an acoustic Music undERstanding model with\nlarge-scale self-supervised Training (MERT), which incorporates teacher models\nto provide pseudo labels in the masked language modelling (MLM) style acoustic\npre-training. In our exploration, we identified an effective combination of\nteacher models, which outperforms conventional speech and audio approaches in\nterms of performance. This combination includes an acoustic teacher based on\nResidual Vector Quantisation - Variational AutoEncoder (RVQ-VAE) and a musical\nteacher based on the Constant-Q Transform (CQT). Furthermore, we explore a wide\nrange of settings to overcome the instability in acoustic language model\npre-training, which allows our designed paradigm to scale from 95M to 330M\nparameters. Experimental results indicate that our model can generalise and\nperform well on 14 music understanding tasks and attain state-of-the-art (SOTA)\noverall scores.",
                "Feature Learning in Image Hierarchies using Functional Maximal\n  Correlation\nThis paper proposes the Hierarchical Functional Maximal Correlation Algorithm\n(HFMCA), a hierarchical methodology that characterizes dependencies across two\nhierarchical levels in multiview systems. By framing view similarities as\ndependencies and ensuring contrastivity by imposing orthonormality, HFMCA\nachieves faster convergence and increased stability in self-supervised\nlearning. HFMCA defines and measures dependencies within image hierarchies,\nfrom pixels and patches to full images. We find that the network topology for\napproximating orthonormal basis functions aligns with a vanilla CNN, enabling\nthe decomposition of density ratios between neighboring layers of feature maps.\nThis approach provides powerful interpretability, revealing the resemblance\nbetween supervision and self-supervision through the lens of internal\nrepresentations.",
                "Primal-Attention: Self-attention through Asymmetric Kernel SVD in Primal\n  Representation\nRecently, a new line of works has emerged to understand and improve\nself-attention in Transformers by treating it as a kernel machine. However,\nexisting works apply the methods for symmetric kernels to the asymmetric\nself-attention, resulting in a nontrivial gap between the analytical\nunderstanding and numerical implementation. In this paper, we provide a new\nperspective to represent and optimize self-attention through asymmetric Kernel\nSingular Value Decomposition (KSVD), which is also motivated by the low-rank\nproperty of self-attention normally observed in deep layers. Through asymmetric\nKSVD, $i$) a primal-dual representation of self-attention is formulated, where\nthe optimization objective is cast to maximize the projection variances in the\nattention outputs; $ii$) a novel attention mechanism, i.e., Primal-Attention,\nis proposed via the primal representation of KSVD, avoiding explicit\ncomputation of the kernel matrix in the dual; $iii$) with KKT conditions, we\nprove that the stationary solution to the KSVD optimization in Primal-Attention\nyields a zero-value objective. In this manner, KSVD optimization can be\nimplemented by simply minimizing a regularization loss, so that low-rank\nproperty is promoted without extra decomposition. Numerical experiments show\nstate-of-the-art performance of our Primal-Attention with improved efficiency.\nMoreover, we demonstrate that the deployed KSVD optimization regularizes\nPrimal-Attention with a sharper singular value decay than that of the canonical\nself-attention, further verifying the great potential of our method. To the\nbest of our knowledge, this is the first work that provides a primal-dual\nrepresentation for the asymmetric kernel in self-attention and successfully\napplies it to modeling and optimization."
            ],
            "interesting paper": 1
        },
        {
            "papers": [
                "BetaZero: Belief-State Planning for Long-Horizon POMDPs using Learned\n  Approximations\nReal-world planning problems, including autonomous driving and sustainable\nenergy applications like carbon storage and resource exploration, have recently\nbeen modeled as partially observable Markov decision processes (POMDPs) and\nsolved using approximate methods. To solve high-dimensional POMDPs in practice,\nstate-of-the-art methods use online planning with problem-specific heuristics\nto reduce planning horizons and make the problems tractable. Algorithms that\nlearn approximations to replace heuristics have recently found success in\nlarge-scale fully observable domains. The key insight is the combination of\nonline Monte Carlo tree search with offline neural network approximations of\nthe optimal policy and value function. In this work, we bring this insight to\npartially observable domains and propose BetaZero, a belief-state planning\nalgorithm for high-dimensional POMDPs. BetaZero learns offline approximations\nthat replace heuristics to enable online decision making in long-horizon\nproblems. We address several challenges inherent in large-scale partially\nobservable domains; namely challenges of transitioning in stochastic\nenvironments, prioritizing action branching with a limited search budget, and\nrepresenting beliefs as input to the network. To formalize the use of all\nlimited search information, we train against a novel $Q$-weighted visit counts\npolicy. We test BetaZero on various well-established POMDP benchmarks found in\nthe literature and a real-world problem of critical mineral exploration.\nExperiments show that BetaZero outperforms state-of-the-art POMDP solvers on a\nvariety of tasks.",
                "EMOTE: An Explainable architecture for Modelling the Other Through\n  Empathy\nWe can usually assume others have goals analogous to our own. This assumption\ncan also, at times, be applied to multi-agent games - e.g. Agent 1's attraction\nto green pellets is analogous to Agent 2's attraction to red pellets. This\n\"analogy\" assumption is tied closely to the cognitive process known as empathy.\nInspired by empathy, we design a simple and explainable architecture to model\nanother agent's action-value function. This involves learning an \"Imagination\nNetwork\" to transform the other agent's observed state in order to produce a\nhuman-interpretable \"empathetic state\" which, when presented to the learning\nagent, produces behaviours that mimic the other agent. Our approach is\napplicable to multi-agent scenarios consisting of a single learning agent and\nother (independent) agents acting according to fixed policies. This\narchitecture is particularly beneficial for (but not limited to) algorithms\nusing a composite value or reward function. We show our method produces better\nperformance in multi-agent games, where it robustly estimates the other's model\nin different environment configurations. Additionally, we show that the\nempathetic states are human interpretable, and thus verifiable.",
                "TorchRL: A data-driven decision-making library for PyTorch\nPyTorch has ascended as a premier machine learning framework, yet it lacks a\nnative and comprehensive library for decision and control tasks suitable for\nlarge development teams dealing with complex real-world data and environments.\nTo address this issue, we propose TorchRL, a generalistic control library for\nPyTorch that provides well-integrated, yet standalone components. We introduce\na new and flexible PyTorch primitive, the TensorDict, which facilitates\nstreamlined algorithm development across the many branches of Reinforcement\nLearning (RL) and control. We provide a detailed description of the building\nblocks and an extensive overview of the library across domains and tasks.\nFinally, we experimentally demonstrate its reliability and flexibility and show\ncomparative benchmarks to demonstrate its computational efficiency. TorchRL\nfosters long-term support and is publicly available on GitHub for greater\nreproducibility and collaboration within the research community. The code is\nopen-sourced on GitHub.",
                "Adam Accumulation to Reduce Memory Footprints of both Activations and\n  Gradients for Large-scale DNN Training\nRunning out of GPU memory has become a main bottleneck for large-scale DNN\ntraining. How to reduce the memory footprint during training has received\nintensive research attention. We find that previous gradient accumulation\nreduces activation memory but fails to be compatible with gradient memory\nreduction due to a contradiction between preserving gradients and releasing\ngradients. To address this issue, we propose a novel optimizer accumulation\nmethod for Adam, named Adam Accumulation (AdamA), which enables reducing both\nactivation and gradient memory. Specifically, AdamA directly integrates\ngradients into optimizer states and accumulates optimizer states over\nmicro-batches, so that gradients can be released immediately after use. We\nmathematically and experimentally demonstrate AdamA yields the same convergence\nproperties as Adam. Evaluated on transformer-based models, AdamA achieves up to\n23% memory reduction compared to gradient accumulation with less than 2%\ndegradation in training throughput. Notably, AdamA can work together with\nmemory reduction methods for optimizer states to fit 1.26x~3.14x larger models\nover PyTorch and DeepSpeed baseline on GPUs with different memory capacities.",
                "Power Control with QoS Guarantees: A Differentiable Projection-based\n  Unsupervised Learning Framework\nDeep neural networks (DNNs) are emerging as a potential solution to solve\nNP-hard wireless resource allocation problems. However, in the presence of\nintricate constraints, e.g., users' quality-of-service (QoS) constraints,\nguaranteeing constraint satisfaction becomes a fundamental challenge. In this\npaper, we propose a novel unsupervised learning framework to solve the\nclassical power control problem in a multi-user interference channel, where the\nobjective is to maximize the network sumrate under users' minimum data rate or\nQoS requirements and power budget constraints. Utilizing a differentiable\nprojection function, two novel deep learning (DL) solutions are pursued. The\nfirst is called Deep Implicit Projection Network (DIPNet), and the second is\ncalled Deep Explicit Projection Network (DEPNet). DIPNet utilizes a\ndifferentiable convex optimization layer to implicitly define a projection\nfunction. On the other hand, DEPNet uses an explicitly-defined projection\nfunction, which has an iterative nature and relies on a differentiable\ncorrection process. DIPNet requires convex constraints; whereas, the DEPNet\ndoes not require convexity and has a reduced computational complexity. To\nenhance the sum-rate performance of the proposed models even further,\nFrank-Wolfe algorithm (FW) has been applied to the output of the proposed\nmodels. Extensive simulations depict that the proposed DNN solutions not only\nimprove the achievable data rate but also achieve zero constraint violation\nprobability, compared to the existing DNNs. The proposed solutions outperform\nthe classic optimization methods in terms of computation time complexity.",
                "Efficient Diffusion Policies for Offline Reinforcement Learning\nOffline reinforcement learning (RL) aims to learn optimal policies from\noffline datasets, where the parameterization of policies is crucial but often\noverlooked. Recently, Diffsuion-QL significantly boosts the performance of\noffline RL by representing a policy with a diffusion model, whose success\nrelies on a parametrized Markov Chain with hundreds of steps for sampling.\nHowever, Diffusion-QL suffers from two critical limitations. 1) It is\ncomputationally inefficient to forward and backward through the whole Markov\nchain during training. 2) It is incompatible with maximum likelihood-based RL\nalgorithms (e.g., policy gradient methods) as the likelihood of diffusion\nmodels is intractable. Therefore, we propose efficient diffusion policy (EDP)\nto overcome these two challenges. EDP approximately constructs actions from\ncorrupted ones at training to avoid running the sampling chain. We conduct\nextensive experiments on the D4RL benchmark. The results show that EDP can\nreduce the diffusion policy training time from 5 days to 5 hours on\ngym-locomotion tasks. Moreover, we show that EDP is compatible with various\noffline RL algorithms (TD3, CRR, and IQL) and achieves new state-of-the-art on\nD4RL by large margins over previous methods. Our code is available at\nhttps://github.com/sail-sg/edp."
            ],
            "interesting paper": 1
        },
        {
            "papers": [
                "Scaling Up Semi-supervised Learning with Unconstrained Unlabelled Data\nWe propose UnMixMatch, a semi-supervised learning framework which can learn\neffective representations from unconstrained unlabelled data in order to scale\nup performance. Most existing semi-supervised methods rely on the assumption\nthat labelled and unlabelled samples are drawn from the same distribution,\nwhich limits the potential for improvement through the use of free-living\nunlabeled data. Consequently, the generalizability and scalability of\nsemi-supervised learning are often hindered by this assumption. Our method aims\nto overcome these constraints and effectively utilize unconstrained unlabelled\ndata in semi-supervised learning. UnMixMatch consists of three main components:\na supervised learner with hard augmentations that provides strong\nregularization, a contrastive consistency regularizer to learn underlying\nrepresentations from the unlabelled data, and a self-supervised loss to enhance\nthe representations that are learnt from the unlabelled data. We perform\nextensive experiments on 4 commonly used datasets and demonstrate superior\nperformance over existing semi-supervised methods with a performance boost of\n4.79%. Extensive ablation and sensitivity studies show the effectiveness and\nimpact of each of the proposed components of our method.",
                "Efficient Failure Pattern Identification of Predictive Algorithms\nGiven a (machine learning) classifier and a collection of unlabeled data, how\ncan we efficiently identify misclassification patterns presented in this\ndataset? To address this problem, we propose a human-machine collaborative\nframework that consists of a team of human annotators and a sequential\nrecommendation algorithm. The recommendation algorithm is conceptualized as a\nstochastic sampler that, in each round, queries the annotators a subset of\nsamples for their true labels and obtains the feedback information on whether\nthe samples are misclassified. The sampling mechanism needs to balance between\ndiscovering new patterns of misclassification (exploration) and confirming the\npotential patterns of classification (exploitation). We construct a\ndeterminantal point process, whose intensity balances the\nexploration-exploitation trade-off through the weighted update of the posterior\nat each round to form the generator of the stochastic sampler. The numerical\nresults empirically demonstrate the competitive performance of our framework on\nmultiple datasets at various signal-to-noise ratios.",
                "Rotating Features for Object Discovery\nThe binding problem in human cognition, concerning how the brain represents\nand connects objects within a fixed network of neural connections, remains a\nsubject of intense debate. Most machine learning efforts addressing this issue\nin an unsupervised setting have focused on slot-based methods, which may be\nlimiting due to their discrete nature and difficulty to express uncertainty.\nRecently, the Complex AutoEncoder was proposed as an alternative that learns\ncontinuous and distributed object-centric representations. However, it is only\napplicable to simple toy data. In this paper, we present Rotating Features, a\ngeneralization of complex-valued features to higher dimensions, and a new\nevaluation procedure for extracting objects from distributed representations.\nAdditionally, we show the applicability of our approach to pre-trained\nfeatures. Together, these advancements enable us to scale distributed\nobject-centric representations from simple toy to real-world data. We believe\nthis work advances a new paradigm for addressing the binding problem in machine\nlearning and has the potential to inspire further innovation in the field.",
                "Federated Domain Generalization: A Survey\nMachine learning typically relies on the assumption that training and testing\ndistributions are identical and that data is centrally stored for training and\ntesting. However, in real-world scenarios, distributions may differ\nsignificantly and data is often distributed across different devices,\norganizations, or edge nodes. Consequently, it is imperative to develop models\nthat can effectively generalize to unseen distributions where data is\ndistributed across different domains. In response to this challenge, there has\nbeen a surge of interest in federated domain generalization (FDG) in recent\nyears. FDG combines the strengths of federated learning (FL) and domain\ngeneralization (DG) techniques to enable multiple source domains to\ncollaboratively learn a model capable of directly generalizing to unseen\ndomains while preserving data privacy. However, generalizing the federated\nmodel under domain shifts is a technically challenging problem that has\nreceived scant attention in the research area so far. This paper presents the\nfirst survey of recent advances in this area. Initially, we discuss the\ndevelopment process from traditional machine learning to domain adaptation and\ndomain generalization, leading to FDG as well as provide the corresponding\nformal definition. Then, we categorize recent methodologies into four classes:\nfederated domain alignment, data manipulation, learning strategies, and\naggregation optimization, and present suitable algorithms in detail for each\ncategory. Next, we introduce commonly used datasets, applications, evaluations,\nand benchmarks. Finally, we conclude this survey by providing some potential\nresearch topics for the future.",
                "Learning Causally Disentangled Representations via the Principle of\n  Independent Causal Mechanisms\nLearning disentangled causal representations is a challenging problem that\nhas gained significant attention recently due to its implications for\nextracting meaningful information for downstream tasks. In this work, we define\na new notion of causal disentanglement from the perspective of independent\ncausal mechanisms. We propose ICM-VAE, a framework for learning causally\ndisentangled representations supervised by causally related observed labels. We\nmodel causal mechanisms using nonlinear learnable flow-based diffeomorphic\nfunctions to map noise variables to latent causal variables. Further, to\npromote the disentanglement of causal factors, we propose a causal\ndisentanglement prior learned from auxiliary labels and the latent causal\nstructure. We theoretically show the identifiability of causal factors and\nmechanisms up to permutation and elementwise reparameterization. We empirically\ndemonstrate that our framework induces highly disentangled causal factors,\nimproves interventional robustness, and is compatible with counterfactual\ngeneration.",
                "Joint Learning of Label and Environment Causal Independence for Graph\n  Out-of-Distribution Generalization\nWe tackle the problem of graph out-of-distribution (OOD) generalization.\nExisting graph OOD algorithms either rely on restricted assumptions or fail to\nexploit environment information in training data. In this work, we propose to\nsimultaneously incorporate label and environment causal independence (LECI) to\nfully make use of label and environment information, thereby addressing the\nchallenges faced by prior methods on identifying causal and invariant\nsubgraphs. We further develop an adversarial training strategy to jointly\noptimize these two properties for causal subgraph discovery with theoretical\nguarantees. Extensive experiments and analysis show that LECI significantly\noutperforms prior methods on both synthetic and real-world datasets,\nestablishing LECI as a practical and effective solution for graph OOD\ngeneralization.\n  Our code is available at https://github.com/divelab/LECI."
            ],
            "interesting paper": 1
        },
        {
            "papers": [
                "AI Liability Insurance With an Example in AI-Powered E-diagnosis System\nArtificial Intelligence (AI) has received an increasing amount of attention\nin multiple areas. The uncertainties and risks in AI-powered systems have\ncreated reluctance in their wild adoption. As an economic solution to\ncompensate for potential damages, AI liability insurance is a promising market\nto enhance the integration of AI into daily life. In this work, we use an\nAI-powered E-diagnosis system as an example to study AI liability insurance. We\nprovide a quantitative risk assessment model with evidence-based numerical\nanalysis. We discuss the insurability criteria for AI technologies and suggest\nnecessary adjustments to accommodate the features of AI products. We show that\nAI liability insurance can act as a regulatory mechanism to incentivize\ncompliant behaviors and serve as a certificate of high-quality AI systems.\nFurthermore, we suggest premium adjustment to reflect the dynamic evolution of\nthe inherent uncertainty in AI. Moral hazard problems are discussed and\nsuggestions for AI liability insurance are provided.",
                "AI and the creative realm: A short review of current and future\n  applications\nThis study explores the concept of creativity and artificial intelligence\n(AI) and their recent integration. While AI has traditionally been perceived as\nincapable of generating new ideas or creating art, the development of more\nsophisticated AI models and the proliferation of human-computer interaction\ntools have opened up new possibilities for AI in artistic creation. This study\ninvestigates the various applications of AI in a creative context,\ndifferentiating between the type of art, language, and algorithms used. It also\nconsiders the philosophical implications of AI and creativity, questioning\nwhether consciousness can be researched in machines and AI's potential\ninterests and decision-making capabilities. Overall, we aim to stimulate a\nreflection on AI's use and ethical implications in creative contexts.",
                "Deep Q-Learning versus Proximal Policy Optimization: Performance\n  Comparison in a Material Sorting Task\nThis paper presents a comparison between two well-known deep Reinforcement\nLearning (RL) algorithms: Deep Q-Learning (DQN) and Proximal Policy\nOptimization (PPO) in a simulated production system. We utilize a Petri Net\n(PN)-based simulation environment, which was previously proposed in related\nwork. The performance of the two algorithms is compared based on several\nevaluation metrics, including average percentage of correctly assembled and\nsorted products, average episode length, and percentage of successful episodes.\nThe results show that PPO outperforms DQN in terms of all evaluation metrics.\nThe study highlights the advantages of policy-based algorithms in problems with\nhigh-dimensional state and action spaces. The study contributes to the field of\ndeep RL in context of production systems by providing insights into the\neffectiveness of different algorithms and their suitability for different\ntasks.",
                "Integrated Sensing-Communication-Computation for Edge Artificial\n  Intelligence\nEdge artificial intelligence (AI) has been a promising solution towards 6G to\nempower a series of advanced techniques such as digital twins, holographic\nprojection, semantic communications, and auto-driving, for achieving\nintelligence of everything. The performance of edge AI tasks, including edge\nlearning and edge AI inference, depends on the quality of three highly coupled\nprocesses, i.e., sensing for data acquisition, computation for information\nextraction, and communication for information transmission. However, these\nthree modules need to compete for network resources for enhancing their own\nquality-of-services. To this end, integrated sensing-communication-computation\n(ISCC) is of paramount significance for improving resource utilization as well\nas achieving the customized goals of edge AI tasks. By investigating the\ninterplay among the three modules, this article presents various kinds of ISCC\nschemes for federated edge learning tasks and edge AI inference tasks in both\napplication and physical layers.",
                "An Architecture for Deploying Reinforcement Learning in Industrial\n  Environments\nIndustry 4.0 is driven by demands like shorter time-to-market, mass\ncustomization of products, and batch size one production. Reinforcement\nLearning (RL), a machine learning paradigm shown to possess a great potential\nin improving and surpassing human level performance in numerous complex tasks,\nallows coping with the mentioned demands. In this paper, we present an OPC UA\nbased Operational Technology (OT)-aware RL architecture, which extends the\nstandard RL setting, combining it with the setting of digital twins. Moreover,\nwe define an OPC UA information model allowing for a generalized plug-and-play\nlike approach for exchanging the RL agent used. In conclusion, we demonstrate\nand evaluate the architecture, by creating a proof of concept. By means of\nsolving a toy example, we show that this architecture can be used to determine\nthe optimal policy using a real control system.",
                "An Overview on Generative AI at Scale with Edge-Cloud Computing\nAs a specific category of artificial intelligence (AI), generative artificial\nintelligence (GenAI) generates new content that resembles what is created by\nhumans. The rapid development of GenAI systems has created a huge amount of new\ndata on the Internet, posing new challenges to current computing and\ncommunication frameworks. Currently, GenAI services rely on the traditional\ncloud computing framework due to the need for large computation resources.\nHowever, such services will encounter high latency because of data transmission\nand a high volume of requests. On the other hand, edge-cloud computing can\nprovide adequate computation power and low latency at the same time through the\ncollaboration between edges and the cloud. Thus, it is attractive to build\nGenAI systems at scale by leveraging the edge-cloud computing paradigm. In this\noverview paper, we review recent developments in GenAI and edge-cloud\ncomputing, respectively. Then, we use two exemplary GenAI applications to\ndiscuss technical challenges in scaling up their solutions using edge-cloud\ncollaborative systems. Finally, we list design considerations for training and\ndeploying GenAI systems at scale and point out future research directions."
            ],
            "interesting paper": 3
        },
        {
            "papers": [
                "The Information Pathways Hypothesis: Transformers are Dynamic\n  Self-Ensembles\nTransformers use the dense self-attention mechanism which gives a lot of\nflexibility for long-range connectivity. Over multiple layers of a deep\ntransformer, the number of possible connectivity patterns increases\nexponentially. However, very few of these contribute to the performance of the\nnetwork, and even fewer are essential. We hypothesize that there are sparsely\nconnected sub-networks within a transformer, called information pathways which\ncan be trained independently. However, the dynamic (i.e., input-dependent)\nnature of these pathways makes it difficult to prune dense self-attention\nduring training. But the overall distribution of these pathways is often\npredictable. We take advantage of this fact to propose Stochastically\nSubsampled self-Attention (SSA) - a general-purpose training strategy for\ntransformers that can reduce both the memory and computational cost of\nself-attention by 4 to 8 times during training while also serving as a\nregularization method - improving generalization over dense training. We show\nthat an ensemble of sub-models can be formed from the subsampled pathways\nwithin a network, which can achieve better performance than its densely\nattended counterpart. We perform experiments on a variety of NLP, computer\nvision and graph learning tasks in both generative and discriminative settings\nto provide empirical evidence for our claims and show the effectiveness of the\nproposed method.",
                "HomE: Homography-Equivariant Video Representation Learning\nRecent advances in self-supervised representation learning have enabled more\nefficient and robust model performance without relying on extensive labeled\ndata. However, most works are still focused on images, with few working on\nvideos and even fewer on multi-view videos, where more powerful inductive\nbiases can be leveraged for self-supervision. In this work, we propose a novel\nmethod for representation learning of multi-view videos, where we explicitly\nmodel the representation space to maintain Homography Equivariance (HomE). Our\nmethod learns an implicit mapping between different views, culminating in a\nrepresentation space that maintains the homography relationship between\nneighboring views. We evaluate our HomE representation via action recognition\nand pedestrian intent prediction as downstream tasks. On action classification,\nour method obtains 96.4% 3-fold accuracy on the UCF101 dataset, better than\nmost state-of-the-art self-supervised learning methods. Similarly, on the STIP\ndataset, we outperform the state-of-the-art by 6% for pedestrian intent\nprediction one second into the future while also obtaining an accuracy of 91.2%\nfor pedestrian action (cross vs. not-cross) classification. Code is available\nat https://github.com/anirudhs123/HomE.",
                "Affinity Clustering Framework for Data Debiasing Using Pairwise\n  Distribution Discrepancy\nGroup imbalance, resulting from inadequate or unrepresentative data\ncollection methods, is a primary cause of representation bias in datasets.\nRepresentation bias can exist with respect to different groups of one or more\nprotected attributes and might lead to prejudicial and discriminatory outcomes\ntoward certain groups of individuals; in cases where a learning model is\ntrained on such biased data. This paper presents MASC, a data augmentation\napproach that leverages affinity clustering to balance the representation of\nnon-protected and protected groups of a target dataset by utilizing instances\nof the same protected attributes from similar datasets that are categorized in\nthe same cluster as the target dataset by sharing instances of the protected\nattribute. The proposed method involves constructing an affinity matrix by\nquantifying distribution discrepancies between dataset pairs and transforming\nthem into a symmetric pairwise similarity matrix. A non-parametric spectral\nclustering is then applied to this affinity matrix, automatically categorizing\nthe datasets into an optimal number of clusters. We perform a step-by-step\nexperiment as a demo of our method to show the procedure of the proposed data\naugmentation method and evaluate and discuss its performance. A comparison with\nother data augmentation methods, both pre- and post-augmentation, is conducted,\nalong with a model evaluation analysis of each method. Our method can handle\nnon-binary protected attributes so, in our experiments, bias is measured in a\nnon-binary protected attribute setup w.r.t. racial groups distribution for two\nseparate minority groups in comparison with the majority group before and after\ndebiasing. Empirical results imply that our method of augmenting dataset biases\nusing real (genuine) data from similar contexts can effectively debias the\ntarget datasets comparably to existing data augmentation strategies.",
                "Comparing a composite model versus chained models to locate a nearest\n  visual object\nExtracting information from geographic images and text is crucial for\nautonomous vehicles to determine in advance the best cell stations to connect\nto along their future path. Multiple artificial neural network models can\naddress this challenge; however, there is no definitive guidance on the\nselection of an appropriate model for such use cases. Therefore, we\nexperimented two architectures to solve such a task: a first architecture with\nchained models where each model in the chain addresses a sub-task of the task;\nand a second architecture with a single model that addresses the whole task.\nOur results showed that these two architectures achieved the same level\nperformance with a root mean square error (RMSE) of 0.055 and 0.056; The\nfindings further revealed that when the task can be decomposed into sub-tasks,\nthe chain architecture exhibits a twelve-fold increase in training speed\ncompared to the composite model. Nevertheless, the composite model\nsignificantly alleviates the burden of data labeling.",
                "Simple Data Augmentation Techniques for Chinese Disease Normalization\nDisease name normalization is an important task in the medical domain. It\nclassifies disease names written in various formats into standardized names,\nserving as a fundamental component in smart healthcare systems for various\ndisease-related functions. Nevertheless, the most significant obstacle to\nexisting disease name normalization systems is the severe shortage of training\ndata. Consequently, we present a novel data augmentation approach that includes\na series of data augmentation techniques and some supporting modules to help\nmitigate the problem. Our proposed methods rely on the Structural Invariance\nproperty of disease names and the Hierarchy property of the disease\nclassification system. The goal is to equip the models with extensive\nunderstanding of the disease names and the hierarchical structure of the\ndisease name classification system. Through extensive experimentation, we\nillustrate that our proposed approach exhibits significant performance\nimprovements across various baseline models and training objectives,\nparticularly in scenarios with limited training data.",
                "Unifying (Machine) Vision via Counterfactual World Modeling\nLeading approaches in machine vision employ different architectures for\ndifferent tasks, trained on costly task-specific labeled datasets. This\ncomplexity has held back progress in areas, such as robotics, where robust\ntask-general perception remains a bottleneck. In contrast, \"foundation models\"\nof natural language have shown how large pre-trained neural networks can\nprovide zero-shot solutions to a broad spectrum of apparently distinct tasks.\nHere we introduce Counterfactual World Modeling (CWM), a framework for\nconstructing a visual foundation model: a unified, unsupervised network that\ncan be prompted to perform a wide variety of visual computations. CWM has two\nkey components, which resolve the core issues that have hindered application of\nthe foundation model concept to vision. The first is structured masking, a\ngeneralization of masked prediction methods that encourages a prediction model\nto capture the low-dimensional structure in visual data. The model thereby\nfactors the key physical components of a scene and exposes an interface to them\nvia small sets of visual tokens. This in turn enables CWM's second main idea --\ncounterfactual prompting -- the observation that many apparently distinct\nvisual representations can be computed, in a zero-shot manner, by comparing the\nprediction model's output on real inputs versus slightly modified\n(\"counterfactual\") inputs. We show that CWM generates high-quality readouts on\nreal-world images and videos for a diversity of tasks, including estimation of\nkeypoints, optical flow, occlusions, object segments, and relative depth. Taken\ntogether, our results show that CWM is a promising path to unifying the\nmanifold strands of machine vision in a conceptually simple foundation."
            ],
            "interesting paper": 2
        },
        {
            "papers": [
                "A Novel Correlation-optimized Deep Learning Method for Wind Speed\n  Forecast\nThe increasing installation rate of wind power poses great challenges to the\nglobal power system. In order to ensure the reliable operation of the power\nsystem, it is necessary to accurately forecast the wind speed and power of the\nwind turbines. At present, deep learning is progressively applied to the wind\nspeed prediction. Nevertheless, the recent deep learning methods still reflect\nthe embarrassment for practical applications due to model interpretability and\nhardware limitation. To this end, a novel deep knowledge-based learning method\nis proposed in this paper. The proposed method hybridizes pre-training method\nand auto-encoder structure to improve data representation and modeling of the\ndeep knowledge-based learning framework. In order to form knowledge and\ncorresponding absorbers, the original data is preprocessed by an optimization\nmodel based on correlation to construct multi-layer networks (knowledge) which\nare absorbed by sequence to sequence (Seq2Seq) models. Specifically, new\ncognition and memory units (CMU) are designed to reinforce traditional deep\nlearning framework. Finally, the effectiveness of the proposed method is\nverified by three wind prediction cases from a wind farm in Liaoning, China.\nExperimental results show that the proposed method increases the stability and\ntraining efficiency compared to the traditional LSTM method and LSTM/GRU-based\nSeq2Seq method for applications of wind speed forecasting.",
                "Comparing a composite model versus chained models to locate a nearest\n  visual object\nExtracting information from geographic images and text is crucial for\nautonomous vehicles to determine in advance the best cell stations to connect\nto along their future path. Multiple artificial neural network models can\naddress this challenge; however, there is no definitive guidance on the\nselection of an appropriate model for such use cases. Therefore, we\nexperimented two architectures to solve such a task: a first architecture with\nchained models where each model in the chain addresses a sub-task of the task;\nand a second architecture with a single model that addresses the whole task.\nOur results showed that these two architectures achieved the same level\nperformance with a root mean square error (RMSE) of 0.055 and 0.056; The\nfindings further revealed that when the task can be decomposed into sub-tasks,\nthe chain architecture exhibits a twelve-fold increase in training speed\ncompared to the composite model. Nevertheless, the composite model\nsignificantly alleviates the burden of data labeling.",
                "Accelerating science with human-aware artificial intelligence\nArtificial intelligence (AI) models trained on published scientific findings\nhave been used to invent valuable materials and targeted therapies, but they\ntypically ignore the human scientists who continually alter the landscape of\ndiscovery. Here we show that incorporating the distribution of human expertise\nby training unsupervised models on simulated inferences cognitively accessible\nto experts dramatically improves (up to 400%) AI prediction of future\ndiscoveries beyond those focused on research content alone, especially when\nrelevant literature is sparse. These models succeed by predicting human\npredictions and the scientists who will make them. By tuning human-aware AI to\navoid the crowd, we can generate scientifically promising \"alien\" hypotheses\nunlikely to be imagined or pursued without intervention until the distant\nfuture, which hold promise to punctuate scientific advance beyond questions\ncurrently pursued. Accelerating human discovery or probing its blind spots,\nhuman-aware AI enables us to move toward and beyond the contemporary scientific\nfrontier.",
                "The Information Pathways Hypothesis: Transformers are Dynamic\n  Self-Ensembles\nTransformers use the dense self-attention mechanism which gives a lot of\nflexibility for long-range connectivity. Over multiple layers of a deep\ntransformer, the number of possible connectivity patterns increases\nexponentially. However, very few of these contribute to the performance of the\nnetwork, and even fewer are essential. We hypothesize that there are sparsely\nconnected sub-networks within a transformer, called information pathways which\ncan be trained independently. However, the dynamic (i.e., input-dependent)\nnature of these pathways makes it difficult to prune dense self-attention\nduring training. But the overall distribution of these pathways is often\npredictable. We take advantage of this fact to propose Stochastically\nSubsampled self-Attention (SSA) - a general-purpose training strategy for\ntransformers that can reduce both the memory and computational cost of\nself-attention by 4 to 8 times during training while also serving as a\nregularization method - improving generalization over dense training. We show\nthat an ensemble of sub-models can be formed from the subsampled pathways\nwithin a network, which can achieve better performance than its densely\nattended counterpart. We perform experiments on a variety of NLP, computer\nvision and graph learning tasks in both generative and discriminative settings\nto provide empirical evidence for our claims and show the effectiveness of the\nproposed method.",
                "Enough With \"Human-AI Collaboration\"\nDescribing our interaction with Artificial Intelligence (AI) systems as\n'collaboration' is well-intentioned, but flawed. Not only is it misleading, but\nit also takes away the credit of AI 'labour' from the humans behind it, and\nerases and obscures an often exploitative arrangement between AI producers and\nconsumers. The AI 'collaboration' metaphor is merely the latest episode in a\nlong history of labour appropriation and credit reassignment that\ndisenfranchises labourers in the Global South. I propose that viewing AI as a\ntool or an instrument, rather than a collaborator, is more accurate, and\nultimately fairer.",
                "DataAI-6G: A System Parameters Configurable Channel Dataset for AI-6G\n  Research\nWith the acceleration of the commercialization of fifth generation (5G)\nmobile communication technology and the research for 6G communication systems,\nthe communication system has the characteristics of high frequency, multi-band,\nhigh speed movement of users and large antenna array. These bring many\ndifficulties to obtain accurate channel state information (CSI), which makes\nthe performance of traditional communication methods be greatly restricted.\nTherefore, there has been a lot of interest in using artificial intelligence\n(AI) instead of traditional methods to improve performance. A common and\naccurate dataset is essential for the research of AI communication. However,\nthe common datasets nowadays still lack some important features, such as mobile\nfeatures, spatial non-stationary features etc. To address these issues, we give\na dataset for future 6G communication. In this dataset, we address these issues\nwith specific simulation methods and accompanying code processing."
            ],
            "interesting paper": 1
        }
    ],
    "Drew Lopez": [
        {
            "papers": [
                "Towards a Capability Assessment Model for the Comprehension and Adoption\n  of AI in Organisations\nThe comprehension and adoption of Artificial Intelligence (AI) are beset with\npractical and ethical problems. This article presents a 5-level AI Capability\nAssessment Model (AI-CAM) and a related AI Capabilities Matrix (AI-CM) to\nassist practitioners in AI comprehension and adoption. These practical tools\nwere developed with business executives, technologists, and other\norganisational stakeholders in mind. They are founded on a comprehensive\nconception of AI compared to those in other AI adoption models and are also\nopen-source artefacts. Thus, the AI-CAM and AI-CM present an accessible\nresource to help inform organisational decision-makers on the capability\nrequirements for (1) AI-based data analytics use cases based on machine\nlearning technologies; (2) Knowledge representation to engineer and represent\ndata, information and knowledge using semantic technologies; and (3) AI-based\nsolutions that seek to emulate human reasoning and decision-making. The AI-CAM\ncovers the core capability dimensions (business, data, technology,\norganisation, AI skills, risks, and ethical considerations) required at the\nfive capability maturity levels to achieve optimal use of AI in organisations.",
                "Trends and Challenges Towards an Effective Data-Driven Decision Making\n  in UK SMEs: Case Studies and Lessons Learnt from the Analysis of 85 SMEs\nThe adoption of data science brings vast benefits to Small and Medium-sized\nEnterprises (SMEs) including business productivity, economic growth, innovation\nand jobs creation. Data Science can support SMEs to optimise production\nprocesses, anticipate customers' needs, predict machinery failures and deliver\nefficient smart services. Businesses can also harness the power of Artificial\nIntelligence (AI) and Big Data and the smart use of digital technologies to\nenhance productivity and performance, paving the way for innovation. However,\nintegrating data science decisions into an SME requires both skills and IT\ninvestments. In most cases, such expenses are beyond the means of SMEs due to\nlimited resources and restricted access to financing. This paper presents\ntrends and challenges towards an effective data-driven decision making for\norganisations based on a case study of 85 SMEs, mostly from the West Midlands\nregion of England. The work is supported as part of a 3 years ERDF (European\nRegional Development Funded project) in the areas of big data management,\nanalytics and business intelligence. We present two case studies that\ndemonstrates the potential of Digitisation, AI and Machine Learning and use\nthese as examples to unveil challenges and showcase the wealth of current\navailable opportunities for SMEs.",
                "Deep Learning and Ethics\nThis article appears as chapter 21 of Prince (2023, Understanding Deep\nLearning); a complete draft of the textbook is available here:\nhttp://udlbook.com. This chapter considers potential harms arising from the\ndesign and use of AI systems. These include algorithmic bias, lack of\nexplainability, data privacy violations, militarization, fraud, and\nenvironmental concerns. The aim is not to provide advice on being more ethical.\nInstead, the goal is to express ideas and start conversations in key areas that\nhave received attention in philosophy, political science, and the broader\nsocial sciences.",
                "Symbolic Regression via Control Variable Genetic Programming\nLearning symbolic expressions directly from experiment data is a vital step\nin AI-driven scientific discovery. Nevertheless, state-of-the-art approaches\nare limited to learning simple expressions. Regressing expressions involving\nmany independent variables still remain out of reach. Motivated by the control\nvariable experiments widely utilized in science, we propose Control Variable\nGenetic Programming (CVGP) for symbolic regression over many independent\nvariables. CVGP expedites symbolic expression discovery via customized\nexperiment design, rather than learning from a fixed dataset collected a\npriori. CVGP starts by fitting simple expressions involving a small set of\nindependent variables using genetic programming, under controlled experiments\nwhere other variables are held as constants. It then extends expressions\nlearned in previous generations by adding new independent variables, using new\ncontrol variable experiments in which these variables are allowed to vary.\nTheoretically, we show CVGP as an incremental building approach can yield an\nexponential reduction in the search space when learning a class of expressions.\nExperimentally, CVGP outperforms several baselines in learning symbolic\nexpressions involving multiple independent variables.",
                "Science in the Era of ChatGPT, Large Language Models and Generative AI:\n  Challenges for Research Ethics and How to Respond\nLarge language models of artificial intelligence (AI), such as ChatGPT, find\nremarkable but controversial applicability in science and research. This paper\nreviews epistemological challenges, ethical and integrity risks in science\nconduct in the advent of generative AI. This is with the aim to lay new timely\nfoundations for a high-quality research ethics review. The role of AI language\nmodels as a research instrument and subject is scrutinized along with ethical\nimplications for scientists, participants and reviewers. New emerging practices\nfor research ethics review are discussed, concluding with ten recommendations\nthat shape a response for a more responsible research conduct in the era of AI.",
                "Waiting, Banning, and Embracing: An Empirical Analysis of Adapting\n  Policies for Generative AI in Higher Education\nGenerative AI tools such as ChatGPT have recently gained significant\nattention in higher education. This study aims to understand how universities\nestablish policies regarding the use of AI tools and explore the factors that\ninfluence their decisions. Our study examines ChatGPT policies implemented at\nuniversities around the world, including their existence, content, and issuance\ndates. Specifically, we analyzed the top 500 universities according to the 2022\nQS World University Rankings. Our findings indicate that there is significant\nvariation in university policies. Less than one-third of the universities\nincluded in the study had implemented ChatGPT policies. Of the universities\nwith ChatGPT policies, approximately 67 percent embraced ChatGPT in teaching\nand learning, more than twice the number of universities that banned it. The\nmajority of the universities that ban the use of ChatGPT in assessments allow\nindividual instructors to deviate from this restrictive policy. Our empirical\nanalysis identifies several factors that are significantly and positively\ncorrelated with a university's likelihood of having a ChatGPT policy, including\nthe university's academic reputation score, being in an English-speaking\ncountry, and the general public attitudes toward ChatGPT. In addition, we found\nthat a university's likelihood of having a ban policy is positively associated\nwith faculty student ratio, citations, and the English-speaking country dummy,\nwhile negatively associated with the number of peer universities within the\nsame country that have banned ChatGPT. We discuss the challenges faced by\nuniversities based our empirical findings."
            ],
            "interesting paper": 4
        },
        {
            "papers": [
                "Attention Paper: How Generative AI Reshapes Digital Shadow Industry?\nThe rapid development of digital economy has led to the emergence of various\nblack and shadow internet industries, which pose potential risks that can be\nidentified and managed through digital risk management (DRM) that uses\ndifferent techniques such as machine learning and deep learning. The evolution\nof DRM architecture has been driven by changes in data forms. However, the\ndevelopment of AI-generated content (AIGC) technology, such as ChatGPT and\nStable Diffusion, has given black and shadow industries powerful tools to\npersonalize data and generate realistic images and conversations for fraudulent\nactivities. This poses a challenge for DRM systems to control risks from the\nsource of data generation and to respond quickly to the fast-changing risk\nenvironment. This paper aims to provide a technical analysis of the challenges\nand opportunities of AIGC from upstream, midstream, and downstream paths of\nblack/shadow industries and suggest future directions for improving existing\nrisk control systems. The paper will explore the new black and shadow\ntechniques triggered by generative AI technology and provide insights for\nbuilding the next-generation DRM system.",
                "Alert of the Second Decision-maker: An Introduction to Human-AI Conflict\nThe collaboration between humans and artificial intelligence (AI) is a\nsignificant feature in this digital age. However, humans and AI may have\nobservation, interpretation, and action conflicts when working synchronously.\nThis phenomenon is often masked by faults and, unfortunately, overlooked. This\npaper systematically introduces the human-AI conflict concept, causes,\nmeasurement methods, and risk assessment. The results highlight that there is a\npotential second decision-maker besides the human, which is the AI; the\nhuman-AI conflict is a unique and emerging risk in digitalized process systems;\nand this is an interdisciplinary field that needs to be distinguished from\ntraditional fault and failure analysis; the conflict risk is significant and\ncannot be ignored.",
                "Generating Synergistic Formulaic Alpha Collections via Reinforcement\n  Learning\nIn the field of quantitative trading, it is common practice to transform raw\nhistorical stock data into indicative signals for the market trend. Such\nsignals are called alpha factors. Alphas in formula forms are more\ninterpretable and thus favored by practitioners concerned with risk. In\npractice, a set of formulaic alphas is often used together for better modeling\nprecision, so we need to find synergistic formulaic alpha sets that work well\ntogether. However, most traditional alpha generators mine alphas one by one\nseparately, overlooking the fact that the alphas would be combined later. In\nthis paper, we propose a new alpha-mining framework that prioritizes mining a\nsynergistic set of alphas, i.e., it directly uses the performance of the\ndownstream combination model to optimize the alpha generator. Our framework\nalso leverages the strong exploratory capabilities of reinforcement\nlearning~(RL) to better explore the vast search space of formulaic alphas. The\ncontribution to the combination models' performance is assigned to be the\nreturn used in the RL process, driving the alpha generator to find better\nalphas that improve upon the current set. Experimental evaluations on\nreal-world stock market data demonstrate both the effectiveness and the\nefficiency of our framework for stock trend forecasting. The investment\nsimulation results show that our framework is able to achieve higher returns\ncompared to previous approaches.",
                "Applying Interdisciplinary Frameworks to Understand Algorithmic\n  Decision-Making\nWe argue that explanations for \"algorithmic decision-making\" (ADM) systems\ncan profit by adopting practices that are already used in the learning\nsciences. We shortly introduce the importance of explaining ADM systems, give a\nbrief overview of approaches drawing from other disciplines to improve\nexplanations, and present the results of our qualitative task-based study\nincorporating the \"six facets of understanding\" framework. We close with\nquestions guiding the discussion of how future studies can leverage an\ninterdisciplinary approach.",
                "AI Techniques in the Microservices Life-Cycle: A Survey\nMicroservices is a popular architectural style for the development of\ndistributed software, with an emphasis on modularity, scalability, and\nflexibility. Indeed, in microservice systems, functionalities are provided by\nloosely coupled, small services, each focusing on a specific business\ncapability. Building a system according to the microservices architectural\nstyle brings a number of challenges, mainly related to how the different\nmicroservices are deployed and coordinated and how they interact. In this\npaper, we provide a survey about how techniques in the area of Artificial\nIntelligence have been used to tackle these challenges.",
                "Enhancing Human Capabilities through Symbiotic Artificial Intelligence\n  with Shared Sensory Experiences\nThe merging of human intelligence and artificial intelligence has long been a\nsubject of interest in both science fiction and academia. In this paper, we\nintroduce a novel concept in Human-AI interaction called Symbiotic Artificial\nIntelligence with Shared Sensory Experiences (SAISSE), which aims to establish\na mutually beneficial relationship between AI systems and human users through\nshared sensory experiences. By integrating multiple sensory input channels and\nprocessing human experiences, SAISSE fosters a strong human-AI bond, enabling\nAI systems to learn from and adapt to individual users, providing personalized\nsupport, assistance, and enhancement. Furthermore, we discuss the incorporation\nof memory storage units for long-term growth and development of both the AI\nsystem and its human user. As we address user privacy and ethical guidelines\nfor responsible AI-human symbiosis, we also explore potential biases and\ninequalities in AI-human symbiosis and propose strategies to mitigate these\nchallenges. Our research aims to provide a comprehensive understanding of the\nSAISSE concept and its potential to effectively support and enhance individual\nhuman users through symbiotic AI systems. This position article aims at\ndiscussing poteintial AI-human interaction related topics within the scientific\ncommunity, rather than providing experimental or theoretical results."
            ],
            "interesting paper": 3
        },
        {
            "papers": [
                "Frontier AI developers need an internal audit function\nThis article argues that frontier artificial intelligence (AI) developers\nneed an internal audit function. First, it describes the role of internal audit\nin corporate governance: internal audit evaluates the adequacy and\neffectiveness of a company's risk management, control, and governance\nprocesses. It is organizationally independent from senior management and\nreports directly to the board of directors, typically its audit committee. In\nthe IIA's Three Lines Model, internal audit serves as the third line and is\nresponsible for providing assurance to the board, while the Combined Assurance\nFramework highlights the need to coordinate the activities of internal and\nexternal assurance providers. Next, the article provides an overview of key\ngovernance challenges in frontier AI development: dangerous capabilities can\narise unpredictably and undetected; it is difficult to prevent a deployed model\nfrom causing harm; frontier models can proliferate rapidly; it is inherently\ndifficult to assess frontier AI risks; and frontier AI developers do not seem\nto follow best practices in risk governance. Finally, the article discusses how\nan internal audit function could address some of these challenges: internal\naudit could identify ineffective risk management practices; it could ensure\nthat the board of directors has a more accurate understanding of the current\nlevel of risk and the adequacy of the developer's risk management practices;\nand it could serve as a contact point for whistleblowers. In light of rapid\nprogress in AI research and development, frontier AI developers need to\nstrengthen their risk governance. Instead of reinventing the wheel, they should\nfollow existing best practices. While this might not be sufficient, they should\nnot skip this obvious first step.",
                "How Do UX Practitioners Communicate AI as a Design Material? Artifacts,\n  Conceptions, and Propositions\nUX practitioners (UXPs) face novel challenges when working with and\ncommunicating artificial intelligence (AI) as a design material. We explore how\nUXPs communicate AI concepts when given hands-on experience training and\nexperimenting with AI models. To do so, we conducted a task-based design study\nwith 27 UXPs in which they prototyped and created a design presentation for a\nAI-enabled interface while having access to a simple AI model training tool.\nThrough analyzing UXPs' design presentations and post-activity interviews, we\nfound that although UXPs struggled to clearly communicate some AI concepts,\ntinkering with AI broadened common ground when communicating with technical\nstakeholders. UXPs also identified key risks and benefits of AI in their\ndesigns, and proposed concrete next steps for both UX and AI work. We conclude\nwith a sensitizing concept and recommendations for design and AI tools to\nenhance multi-stakeholder communication and collaboration when crafting\nhuman-centered AI experiences.",
                "A Framework for Incentivized Collaborative Learning\nCollaborations among various entities, such as companies, research labs, AI\nagents, and edge devices, have become increasingly crucial for achieving\nmachine learning tasks that cannot be accomplished by a single entity alone.\nThis is likely due to factors such as security constraints, privacy concerns,\nand limitations in computation resources. As a result, collaborative learning\n(CL) research has been gaining momentum. However, a significant challenge in\npractical applications of CL is how to effectively incentivize multiple\nentities to collaborate before any collaboration occurs. In this study, we\npropose ICL, a general framework for incentivized collaborative learning, and\nprovide insights into the critical issue of when and why incentives can improve\ncollaboration performance. Furthermore, we show the broad applicability of ICL\nto specific cases in federated learning, assisted learning, and multi-armed\nbandit with both theory and experimental results.",
                "Learning Interpretable Models of Aircraft Handling Behaviour by\n  Reinforcement Learning from Human Feedback\nWe propose a method to capture the handling abilities of fast jet pilots in a\nsoftware model via reinforcement learning (RL) from human preference feedback.\nWe use pairwise preferences over simulated flight trajectories to learn an\ninterpretable rule-based model called a reward tree, which enables the\nautomated scoring of trajectories alongside an explanatory rationale. We train\nan RL agent to execute high-quality handling behaviour by using the reward tree\nas the objective, and thereby generate data for iterative preference collection\nand further refinement of both tree and agent. Experiments with synthetic\npreferences show reward trees to be competitive with uninterpretable neural\nnetwork reward models on quantitative and qualitative evaluations.",
                "Towards Cognitive Bots: Architectural Research Challenges\nSoftware bots operating in multiple virtual digital platforms must understand\nthe platforms' affordances and behave like human users. Platform affordances or\nfeatures differ from one application platform to another or through a life\ncycle, requiring such bots to be adaptable. Moreover, bots in such platforms\ncould cooperate with humans or other software agents for work or to learn\nspecific behavior patterns. However, present-day bots, particularly chatbots,\nother than language processing and prediction, are far from reaching a human\nuser's behavior level within complex business information systems. They lack\nthe cognitive capabilities to sense and act in such virtual environments,\nrendering their development a challenge to artificial general intelligence\nresearch. In this study, we problematize and investigate assumptions in\nconceptualizing software bot architecture by directing attention to significant\narchitectural research challenges in developing cognitive bots endowed with\ncomplex behavior for operation on information systems. As an outlook, we\npropose alternate architectural assumptions to consider in future bot design\nand bot development frameworks.",
                "Moral Machine or Tyranny of the Majority?\nWith Artificial Intelligence systems increasingly applied in consequential\ndomains, researchers have begun to ask how these systems ought to act in\nethically charged situations where even humans lack consensus. In the Moral\nMachine project, researchers crowdsourced answers to \"Trolley Problems\"\nconcerning autonomous vehicles. Subsequently, Noothigattu et al. (2018)\nproposed inferring linear functions that approximate each individual's\npreferences and aggregating these linear models by averaging parameters across\nthe population. In this paper, we examine this averaging mechanism, focusing on\nfairness concerns in the presence of strategic effects. We investigate a simple\nsetting where the population consists of two groups, with the minority\nconstituting an {\\alpha} < 0.5 share of the population. To simplify the\nanalysis, we consider the extreme case in which within-group preferences are\nhomogeneous. Focusing on the fraction of contested cases where the minority\ngroup prevails, we make the following observations: (a) even when all parties\nreport their preferences truthfully, the fraction of disputes where the\nminority prevails is less than proportionate in {\\alpha}; (b) the degree of\nsub-proportionality grows more severe as the level of disagreement between the\ngroups increases; (c) when parties report preferences strategically, pure\nstrategy equilibria do not always exist; and (d) whenever a pure strategy\nequilibrium exists, the majority group prevails 100% of the time. These\nfindings raise concerns about stability and fairness of preference vector\naveraging as a mechanism for aggregating diverging voices. Finally, we discuss\nalternatives, including randomized dictatorship and median-based mechanisms."
            ],
            "interesting paper": 3
        },
        {
            "papers": [
                "Incentivizing honest performative predictions with proper scoring rules\nProper scoring rules incentivize experts to accurately report beliefs,\nassuming predictions cannot influence outcomes. We relax this assumption and\ninvestigate incentives when predictions are performative, i.e., when they can\ninfluence the outcome of the prediction, such as when making public predictions\nabout the stock market. We say a prediction is a fixed point if it accurately\nreflects the expert's beliefs after that prediction has been made. We show that\nin this setting, reports maximizing expected score generally do not reflect an\nexpert's beliefs, and we give bounds on the inaccuracy of such reports. We show\nthat, for binary predictions, if the influence of the expert's prediction on\noutcomes is bounded, it is possible to define scoring rules under which optimal\nreports are arbitrarily close to fixed points. However, this is impossible for\npredictions over more than two outcomes. We also perform numerical simulations\nin a toy setting, showing that our bounds are tight in some situations and that\nprediction error is often substantial (greater than 5-10%). Lastly, we discuss\nalternative notions of optimality, including performative stability, and show\nthat they incentivize reporting fixed points.",
                "Optimization's Neglected Normative Commitments\nOptimization is offered as an objective approach to resolving complex,\nreal-world decisions involving uncertainty and conflicting interests. It drives\nbusiness strategies as well as public policies and, increasingly, lies at the\nheart of sophisticated machine learning systems. A paradigm used to approach\npotentially high-stakes decisions, optimization relies on abstracting the real\nworld to a set of decision(s), objective(s) and constraint(s). Drawing from the\nmodeling process and a range of actual cases, this paper describes the\nnormative choices and assumptions that are necessarily part of using\noptimization. It then identifies six emergent problems that may be neglected:\n1) Misspecified values can yield optimizations that omit certain imperatives\naltogether or incorporate them incorrectly as a constraint or as part of the\nobjective, 2) Problematic decision boundaries can lead to faulty modularity\nassumptions and feedback loops, 3) Failing to account for multiple agents'\ndivergent goals and decisions can lead to policies that serve only certain\nnarrow interests, 4) Mislabeling and mismeasurement can introduce bias and\nimprecision, 5) Faulty use of relaxation and approximation methods,\nunaccompanied by formal characterizations and guarantees, can severely impede\napplicability, and 6) Treating optimization as a justification for action,\nwithout specifying the necessary contextual information, can lead to ethically\ndubious or faulty decisions. Suggestions are given to further understand and\ncurb the harms that can arise when optimization is used wrongfully.",
                "AI Coach Assist: An Automated Approach for Call Recommendation in\n  Contact Centers for Agent Coaching\nIn recent years, the utilization of Artificial Intelligence (AI) in the\ncontact center industry is on the rise. One area where AI can have a\nsignificant impact is in the coaching of contact center agents. By analyzing\ncall transcripts using Natural Language Processing (NLP) techniques, it would\nbe possible to quickly determine which calls are most relevant for coaching\npurposes. In this paper, we present AI Coach Assist, which leverages the\npre-trained transformer-based language models to determine whether a given call\nis coachable or not based on the quality assurance (QA) questions asked by the\ncontact center managers or supervisors. The system was trained and evaluated on\na large dataset collected from real-world contact centers and provides an\neffective way to recommend calls to the contact center managers that are more\nlikely to contain coachable moments. Our experimental findings demonstrate the\npotential of AI Coach Assist to improve the coaching process, resulting in\nenhancing the performance of contact center agents.",
                "A Comprehensive Overview and Comparative Analysis on Deep Learning\n  Models: CNN, RNN, LSTM, GRU\nDeep learning (DL) has emerged as a powerful subset of machine learning (ML)\nand artificial intelligence (AI), outperforming traditional ML methods,\nespecially in handling unstructured and large datasets. Its impact spans across\nvarious domains, including speech recognition, healthcare, autonomous vehicles,\ncybersecurity, predictive analytics, and more. However, the complexity and\ndynamic nature of real-world problems present challenges in designing effective\ndeep learning models. Consequently, several deep learning models have been\ndeveloped to address different problems and applications. In this article, we\nconduct a comprehensive survey of various deep learning models, including\nConvolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs),\nGenerative Models, Deep Reinforcement Learning (DRL), and Deep Transfer\nLearning. We examine the structure, applications, benefits, and limitations of\neach model. Furthermore, we perform an analysis using three publicly available\ndatasets: IMDB, ARAS, and Fruit-360. We compare the performance of six renowned\ndeep learning models: CNN, Simple RNN, Long Short-Term Memory (LSTM),\nBidirectional LSTM, Gated Recurrent Unit (GRU), and Bidirectional GRU.",
                "Counterfactual Formulation of Patient-Specific Root Causes of Disease\nRoot causes of disease intuitively correspond to root vertices that increase\nthe likelihood of a diagnosis. This description of a root cause nevertheless\nlacks the rigorous mathematical formulation needed for the development of\ncomputer algorithms designed to automatically detect root causes from data.\nPrior work defined patient-specific root causes of disease using an\ninterventionalist account that only climbs to the second rung of Pearl's Ladder\nof Causation. In this theoretical piece, we climb to the third rung by\nproposing a counterfactual definition matching clinical intuition based on\nfixed factual data alone. We then show how to assign a root causal contribution\nscore to each variable using Shapley values from explainable artificial\nintelligence. The proposed counterfactual formulation of patient-specific root\ncauses of disease accounts for noisy labels, adapts to disease prevalence and\nadmits fast computation without the need for counterfactual simulation.",
                "Backdooring Neural Code Search\nReusing off-the-shelf code snippets from online repositories is a common\npractice, which significantly enhances the productivity of software developers.\nTo find desired code snippets, developers resort to code search engines through\nnatural language queries. Neural code search models are hence behind many such\nengines. These models are based on deep learning and gain substantial attention\ndue to their impressive performance. However, the security aspect of these\nmodels is rarely studied. Particularly, an adversary can inject a backdoor in\nneural code search models, which return buggy or even vulnerable code with\nsecurity/privacy issues. This may impact the downstream software (e.g., stock\ntrading systems and autonomous driving) and cause financial loss and/or\nlife-threatening incidents. In this paper, we demonstrate such attacks are\nfeasible and can be quite stealthy. By simply modifying one variable/function\nname, the attacker can make buggy/vulnerable code rank in the top 11%. Our\nattack BADCODE features a special trigger generation and injection procedure,\nmaking the attack more effective and stealthy. The evaluation is conducted on\ntwo neural code search models and the results show our attack outperforms\nbaselines by 60%. Our user study demonstrates that our attack is more stealthy\nthan the baseline by two times based on the F1 score."
            ],
            "interesting paper": 6
        },
        {
            "papers": [
                "The Digital Divide in Process Safety: Quantitative Risk Analysis of\n  Human-AI Collaboration\nDigital technologies have dramatically accelerated the digital transformation\nin process industries, boosted new industrial applications, upgraded the\nproduction system, and enhanced operational efficiency. In contrast, the\nchallenges and gaps between human and artificial intelligence (AI) have become\nmore and more prominent, whereas the digital divide in process safety is\naggregating. The study attempts to address the following questions: (i)What is\nAI in the process safety context? (ii)What is the difference between AI and\nhumans in process safety? (iii)How do AI and humans collaborate in process\nsafety? (iv)What are the challenges and gaps in human-AI collaboration? (v)How\nto quantify the risk of human-AI collaboration in process safety? Qualitative\nrisk analysis based on brainstorming and literature review, and quantitative\nrisk analysis based on layer of protection analysis (LOPA) and Bayesian network\n(BN), were applied to explore and model. The importance of human reliability\nshould be stressed in the digital age, not usually to increase the reliability\nof AI, and human-centered AI design in process safety needs to be propagated.",
                "ChatGPT Informed Graph Neural Network for Stock Movement Prediction\nChatGPT has demonstrated remarkable capabilities across various natural\nlanguage processing (NLP) tasks. However, its potential for inferring dynamic\nnetwork structures from temporal textual data, specifically financial news,\nremains an unexplored frontier. In this research, we introduce a novel\nframework that leverages ChatGPT's graph inference capabilities to enhance\nGraph Neural Networks (GNN). Our framework adeptly extracts evolving network\nstructures from textual data, and incorporates these networks into graph neural\nnetworks for subsequent predictive tasks. The experimental results from stock\nmovement forecasting indicate our model has consistently outperformed the\nstate-of-the-art Deep Learning-based benchmarks. Furthermore, the portfolios\nconstructed based on our model's outputs demonstrate higher annualized\ncumulative returns, alongside reduced volatility and maximum drawdown. This\nsuperior performance highlights the potential of ChatGPT for text-based network\ninferences and underscores its promising implications for the financial sector.",
                "The Social Value of Dark Energy\nAstrophysics is a social enterprise exemplified here by the Dark Energy\nSurvey (DES) which completed its fieldwork in 2019 after 16 years of\npreparation and observation, while data analysis continues. Society funds\nastrophysics on a grand scale. For human capital and for governance the\ndiscipline draws on a self-governing \"republic of science\", while the funds\nwere provided by philanthropists in the past, and by governments today. The\nbenefits accrue initially to scientists themselves, in the form of a rewarding\nvocation. For the social benefit it is tempting to apply formal cost benefit\nanalysis, but that approach ignores the option value of science and imposes\nquestionable assumptions from welfare economics. Astrophysics generates some\nuseful spinoffs, offers attractive careers, appeals to the popular imagination,\nspeaks to metaphysical cravings and constitutes a good in itself. The rise of\nAI also suggests a role in exploring future habitats for intelligence and\ncognition.",
                "Employing Explainable Artificial Intelligence (XAI) Methodologies to\n  Analyze the Correlation between Input Variables and Tensile Strength in\n  Additively Manufactured Samples\nThis research paper explores the impact of various input parameters,\nincluding Infill percentage, Layer Height, Extrusion Temperature, and Print\nSpeed, on the resulting Tensile Strength in objects produced through additive\nmanufacturing. The main objective of this study is to enhance our understanding\nof the correlation between the input parameters and Tensile Strength, as well\nas to identify the key factors influencing the performance of the additive\nmanufacturing process. To achieve this objective, we introduced the utilization\nof Explainable Artificial Intelligence (XAI) techniques for the first time,\nwhich allowed us to analyze the data and gain valuable insights into the\nsystem's behavior. Specifically, we employed SHAP (SHapley Additive\nexPlanations), a widely adopted framework for interpreting machine learning\nmodel predictions, to provide explanations for the behavior of a machine\nlearning model trained on the data. Our findings reveal that the Infill\npercentage and Extrusion Temperature have the most significant influence on\nTensile Strength, while the impact of Layer Height and Print Speed is\nrelatively minor. Furthermore, we discovered that the relationship between the\ninput parameters and Tensile Strength is highly intricate and nonlinear, making\nit difficult to accurately describe using simple linear models.",
                "AI Audit: A Card Game to Reflect on Everyday AI Systems\nAn essential element of K-12 AI literacy is educating learners about the\nethical and societal implications of AI systems. Previous work in AI ethics\nliteracy have developed curriculum and classroom activities that engage\nlearners in reflecting on the ethical implications of AI systems and developing\nresponsible AI. There is little work in using game-based learning methods in AI\nliteracy. Games are known to be compelling media to teach children about\ncomplex STEM concepts. In this work, we developed a competitive card game for\nmiddle and high school students called \"AI Audit\" where they play as AI\nstart-up founders building novel AI-powered technology. Players can challenge\nother players with potential harms of their technology or defend their own\nbusinesses by features that mitigate these harms. The game mechanics reward\nsystems that are ethically developed or that take steps to mitigate potential\nharms. In this paper, we present the game design, teacher resources for\nclassroom deployment and early playtesting results. We discuss our reflections\nabout using games as teaching tools for AI literacy in K-12 classrooms.",
                "Safety of autonomous vehicles: A survey on Model-based vs. AI-based\n  approaches\nThe growing advancements in Autonomous Vehicles (AVs) have emphasized the\ncritical need to prioritize the absolute safety of AV maneuvers, especially in\ndynamic and unpredictable environments or situations. This objective becomes\neven more challenging due to the uniqueness of every traffic\nsituation/condition. To cope with all these very constrained and complex\nconfigurations, AVs must have appropriate control architectures with reliable\nand real-time Risk Assessment and Management Strategies (RAMS). These targeted\nRAMS must lead to reduce drastically the navigation risks. However, the lack of\nsafety guarantees proves, which is one of the key challenges to be addressed,\nlimit drastically the ambition to introduce more broadly AVs on our roads and\nrestrict the use of AVs to very limited use cases. Therefore, the focus and the\nambition of this paper is to survey research on autonomous vehicles while\nfocusing on the important topic of safety guarantee of AVs. For this purpose,\nit is proposed to review research on relevant methods and concepts defining an\noverall control architecture for AVs, with an emphasis on the safety assessment\nand decision-making systems composing these architectures. Moreover, it is\nintended through this reviewing process to highlight researches that use either\nmodel-based methods or AI-based approaches. This is performed while emphasizing\nthe strengths and weaknesses of each methodology and investigating the research\nthat proposes a comprehensive multi-modal design that combines model-based and\nAI approaches. This paper ends with discussions on the methods used to\nguarantee the safety of AVs namely: safety verification techniques and the\nstandardization/generalization of safety frameworks."
            ],
            "interesting paper": 2
        },
        {
            "papers": [
                "RE-centric Recommendations for the Development of Trustworthy(er)\n  Autonomous Systems\nComplying with the EU AI Act (AIA) guidelines while developing and\nimplementing AI systems will soon be mandatory within the EU. However,\npractitioners lack actionable instructions to operationalise ethics during AI\nsystems development. A literature review of different ethical guidelines\nrevealed inconsistencies in the principles addressed and the terminology used\nto describe them. Furthermore, requirements engineering (RE), which is\nidentified to foster trustworthiness in the AI development process from the\nearly stages was observed to be absent in a lot of frameworks that support the\ndevelopment of ethical and trustworthy AI. This incongruous phrasing combined\nwith a lack of concrete development practices makes trustworthy AI development\nharder. To address this concern, we formulated a comparison table for the\nterminology used and the coverage of the ethical AI principles in major ethical\nAI guidelines. We then examined the applicability of ethical AI development\nframeworks for performing effective RE during the development of trustworthy AI\nsystems. A tertiary review and meta-analysis of literature discussing ethical\nAI frameworks revealed their limitations when developing trustworthy AI. Based\non our findings, we propose recommendations to address such limitations during\nthe development of trustworthy AI.",
                "Shapley Based Residual Decomposition for Instance Analysis\nIn this paper, we introduce the idea of decomposing the residuals of\nregression with respect to the data instances instead of features. This allows\nus to determine the effects of each individual instance on the model and each\nother, and in doing so makes for a model-agnostic method of identifying\ninstances of interest. In doing so, we can also determine the appropriateness\nof the model and data in the wider context of a given study. The paper focuses\non the possible applications that such a framework brings to the relatively\nunexplored field of instance analysis in the context of Explainable AI tasks.",
                "Towards a Unifying Model of Rationality in Multiagent Systems\nMultiagent systems deployed in the real world need to cooperate with other\nagents (including humans) nearly as effectively as these agents cooperate with\none another. To design such AI, and provide guarantees of its effectiveness, we\nneed to clearly specify what types of agents our AI must be able to cooperate\nwith. In this work we propose a generic model of socially intelligent agents,\nwhich are individually rational learners that are also able to cooperate with\none another (in the sense that their joint behavior is Pareto efficient). We\ndefine rationality in terms of the regret incurred by each agent over its\nlifetime, and show how we can construct socially intelligent agents for\ndifferent forms of regret. We then discuss the implications of this model for\nthe development of \"robust\" MAS that can cooperate with a wide variety of\nsocially intelligent agents.",
                "Doing the right thing for the right reason: Evaluating artificial moral\n  cognition by probing cost insensitivity\nIs it possible to evaluate the moral cognition of complex artificial agents?\nIn this work, we take a look at one aspect of morality: `doing the right thing\nfor the right reasons.' We propose a behavior-based analysis of artificial\nmoral cognition which could also be applied to humans to facilitate\nlike-for-like comparison. Morally-motivated behavior should persist despite\nmounting cost; by measuring an agent's sensitivity to this cost, we gain deeper\ninsight into underlying motivations. We apply this evaluation to a particular\nset of deep reinforcement learning agents, trained by memory-based\nmeta-reinforcement learning. Our results indicate that agents trained with a\nreward function that includes other-regarding preferences perform helping\nbehavior in a way that is less sensitive to increasing cost than agents trained\nwith more self-interested preferences.",
                "Generalized Disparate Impact for Configurable Fairness Solutions in ML\nWe make two contributions in the field of AI fairness over continuous\nprotected attributes. First, we show that the Hirschfeld-Gebelein-Renyi (HGR)\nindicator (the only one currently available for such a case) is valuable but\nsubject to a few crucial limitations regarding semantics, interpretability, and\nrobustness. Second, we introduce a family of indicators that are: 1)\ncomplementary to HGR in terms of semantics; 2) fully interpretable and\ntransparent; 3) robust over finite samples; 4) configurable to suit specific\napplications. Our approach also allows us to define fine-grained constraints to\npermit certain types of dependence and forbid others selectively. By expanding\nthe available options for continuous protected attributes, our approach\nrepresents a significant contribution to the area of fair artificial\nintelligence.",
                "Steering control of payoff-maximizing players in adaptive learning\n  dynamics\nEvolutionary game theory provides a mathematical foundation for\ncross-disciplinary fertilization, especially for integrating ideas from\nartificial intelligence and game theory. Such integration offers a transparent\nand rigorous approach to complex decision-making problems in a variety of\nimportant contexts, ranging from evolutionary computation to machine behavior.\nDespite the astronomically huge individual behavioral strategy space for\ninteractions in the iterated Prisoner's Dilemma (IPD) games, the so-called\nZero-Determinant (ZD) strategies is a set of rather simple memory-one\nstrategies yet can unilaterally set a linear payoff relationship between\nthemselves and their opponent. Although the witting of ZD strategies gives\nplayers an upper hand in the IPD games, we find and characterize unbending\nstrategies that can force ZD players to be fair in their own interest.\nMoreover, our analysis reveals the ubiquity of unbending properties in common\nIPD strategies which are previously overlooked. In this work, we demonstrate\nthe important steering role of unbending strategies in fostering fairness and\ncooperation in pairwise interactions. Our results will help bring a new\nperspective by means of combining game theory and multi-agent learning systems\nfor optimizing winning strategies that are robust to noises, errors, and\ndeceptions in non-zero-sum games."
            ],
            "interesting paper": 2
        },
        {
            "papers": [
                "Intent-aligned AI systems deplete human agency: the need for agency\n  foundations research in AI safety\nThe rapid advancement of artificial intelligence (AI) systems suggests that\nartificial general intelligence (AGI) systems may soon arrive. Many researchers\nare concerned that AIs and AGIs will harm humans via intentional misuse\n(AI-misuse) or through accidents (AI-accidents). In respect of AI-accidents,\nthere is an increasing effort focused on developing algorithms and paradigms\nthat ensure AI systems are aligned to what humans intend, e.g. AI systems that\nyield actions or recommendations that humans might judge as consistent with\ntheir intentions and goals. Here we argue that alignment to human intent is\ninsufficient for safe AI systems and that preservation of long-term agency of\nhumans may be a more robust standard, and one that needs to be separated\nexplicitly and a priori during optimization. We argue that AI systems can\nreshape human intention and discuss the lack of biological and psychological\nmechanisms that protect humans from loss of agency. We provide the first formal\ndefinition of agency-preserving AI-human interactions which focuses on\nforward-looking agency evaluations and argue that AI systems - not humans -\nmust be increasingly tasked with making these evaluations. We show how agency\nloss can occur in simple environments containing embedded agents that use\ntemporal-difference learning to make action recommendations. Finally, we\npropose a new area of research called \"agency foundations\" and pose four\ninitial topics designed to improve our understanding of agency in AI-human\ninteractions: benevolent game theory, algorithmic foundations of human rights,\nmechanistic interpretability of agency representation in neural-networks and\nreinforcement learning from internal states.",
                "Disentangling and Operationalizing AI Fairness at LinkedIn\nOperationalizing AI fairness at LinkedIn's scale is challenging not only\nbecause there are multiple mutually incompatible definitions of fairness but\nalso because determining what is fair depends on the specifics and context of\nthe product where AI is deployed. Moreover, AI practitioners need clarity on\nwhat fairness expectations need to be addressed at the AI level. In this paper,\nwe present the evolving AI fairness framework used at LinkedIn to address these\nthree challenges. The framework disentangles AI fairness by separating out\nequal treatment and equitable product expectations. Rather than imposing a\ntrade-off between these two commonly opposing interpretations of fairness, the\nframework provides clear guidelines for operationalizing equal AI treatment\ncomplemented with a product equity strategy. This paper focuses on the equal AI\ntreatment component of LinkedIn's AI fairness framework, shares the principles\nthat support it, and illustrates their application through a case study. We\nhope this paper will encourage other big tech companies to join us in sharing\ntheir approach to operationalizing AI fairness at scale, so that together we\ncan keep advancing this constantly evolving field.",
                "Bigger, Better, Faster: Human-level Atari with human-level efficiency\nWe introduce a value-based RL agent, which we call BBF, that achieves\nsuper-human performance in the Atari 100K benchmark. BBF relies on scaling the\nneural networks used for value estimation, as well as a number of other design\nchoices that enable this scaling in a sample-efficient manner. We conduct\nextensive analyses of these design choices and provide insights for future\nwork. We end with a discussion about updating the goalposts for\nsample-efficient RL research on the ALE. We make our code and data publicly\navailable at\nhttps://github.com/google-research/google-research/tree/master/bigger_better_faster.",
                "Evaluating GPT's Programming Capability through CodeWars' Katas\nIn the burgeoning field of artificial intelligence (AI), understanding the\ncapabilities and limitations of programming-oriented models is crucial. This\npaper presents a novel evaluation of the programming proficiency of Generative\nPretrained Transformer (GPT) models, specifically GPT-3.5 and GPT-4, against\ncoding problems of varying difficulty levels drawn from Codewars. The\nexperiments reveal a distinct boundary at the 3kyu level, beyond which these\nGPT models struggle to provide solutions. These findings led to the proposal of\na measure for coding problem complexity that incorporates both problem\ndifficulty and the time required for solution. The research emphasizes the need\nfor validation and creative thinking capabilities in AI models to better\nemulate human problem-solving techniques. Future work aims to refine this\nproposed complexity measure, enhance AI models with these suggested\ncapabilities, and develop an objective measure for programming problem\ndifficulty. The results of this research offer invaluable insights for\nimproving AI programming capabilities and advancing the frontier of AI\nproblem-solving abilities.",
                "Using Data Analytics to Derive Business Intelligence: A Case Study\nThe data revolution experienced in recent times has thrown up new challenges\nand opportunities for businesses of all sizes in diverse industries. Big data\nanalytics is already at the forefront of innovations to help make meaningful\nbusiness decisions from the abundance of raw data available today. Business\nintelligence and analytics has become a huge trend in todays IT world as\ncompanies of all sizes are looking to improve their business processes and\nscale up using data driven solutions. This paper aims to demonstrate the data\nanalytical process of deriving business intelligence via the historical data of\na fictional bike share company seeking to find innovative ways to convert their\ncasual riders to annual paying registered members. The dataset used is freely\navailable as Chicago Divvy Bicycle Sharing Data on Kaggle. The authors used the\nRTidyverse library in RStudio to analyse the data and followed the six data\nanalysis steps of ask, prepare, process, analyse, share, and act to recommend\nsome actionable approaches the company could adopt to convert casual riders to\npaying annual members. The findings from this research serve as a valuable case\nexample, of a real world deployment of BIA technologies in the industry, and a\ndemonstration of the data analysis cycle for data practitioners, researchers,\nand other potential users.",
                "Data and Knowledge for Overtaking Scenarios in Autonomous Driving\nAutonomous driving has become one of the most popular research topics within\nArtificial Intelligence. An autonomous vehicle is understood as a system that\ncombines perception, decision-making, planning, and control. All of those tasks\nrequire that the vehicle collects surrounding data in order to make a good\ndecision and action. In particular, the overtaking maneuver is one of the most\ncritical actions of driving. The process involves lane changes, acceleration\nand deceleration actions, and estimation of the speed and distance of the\nvehicle in front or in the lane in which it is moving. Despite the amount of\nwork available in the literature, just a few handle overtaking maneuvers and,\nbecause overtaking can be risky, no real-world dataset is available. This work\ncontributes in this area by presenting a new synthetic dataset whose focus is\nthe overtaking maneuver. We start by performing a thorough review of the state\nof the art in autonomous driving and then explore the main datasets found in\nthe literature (public and private, synthetic and real), highlighting their\nlimitations, and suggesting a new set of features whose focus is the overtaking\nmaneuver."
            ],
            "interesting paper": 2
        },
        {
            "papers": [
                "Improved Financial Forecasting via Quantum Machine Learning\nQuantum algorithms have the potential to enhance machine learning across a\nvariety of domains and applications. In this work, we show how quantum machine\nlearning can be used to improve financial forecasting. First, we use classical\nand quantum Determinantal Point Processes to enhance Random Forest models for\nchurn prediction, improving precision by almost 6%. Second, we design quantum\nneural network architectures with orthogonal and compound layers for credit\nrisk assessment, which match classical performance with significantly fewer\nparameters. Our results demonstrate that leveraging quantum ideas can\neffectively enhance the performance of machine learning, both today as\nquantum-inspired classical ML solutions, and even more in the future, with the\nadvent of better quantum hardware.",
                "Survey of Trustworthy AI: A Meta Decision of AI\nWhen making strategic decisions, we are often confronted with overwhelming\ninformation to process. The situation can be further complicated when some\npieces of evidence are contradicted each other or paradoxical. The challenge\nthen becomes how to determine which information is useful and which ones should\nbe eliminated. This process is known as meta-decision. Likewise, when it comes\nto using Artificial Intelligence (AI) systems for strategic decision-making,\nplacing trust in the AI itself becomes a meta-decision, given that many AI\nsystems are viewed as opaque \"black boxes\" that process large amounts of data.\nTrusting an opaque system involves deciding on the level of Trustworthy AI\n(TAI). We propose a new approach to address this issue by introducing a novel\ntaxonomy or framework of TAI, which encompasses three crucial domains:\narticulate, authentic, and basic for different levels of trust. To underpin\nthese domains, we create ten dimensions to measure trust:\nexplainability/transparency, fairness/diversity, generalizability, privacy,\ndata governance, safety/robustness, accountability, reproducibility,\nreliability, and sustainability. We aim to use this taxonomy to conduct a\ncomprehensive survey and explore different TAI approaches from a strategic\ndecision-making perspective.",
                "Credit Card Fraud Detection Using Asexual Reproduction Optimization\nAs the number of credit card users has increased, detecting fraud in this\ndomain has become a vital issue. Previous literature has applied various\nsupervised and unsupervised machine learning methods to find an effective fraud\ndetection system. However, some of these methods require an enormous amount of\ntime to achieve reasonable accuracy. In this paper, an Asexual Reproduction\nOptimization (ARO) approach was employed, which is a supervised method to\ndetect credit card fraud. ARO refers to a kind of production in which one\nparent produces some offspring. By applying this method and sampling just from\nthe majority class, the effectiveness of the classification is increased. A\ncomparison to Artificial Immune Systems (AIS), which is one of the best methods\nimplemented on current datasets, has shown that the proposed method is able to\nremarkably reduce the required training time and at the same time increase the\nrecall that is important in fraud detection problems. The obtained results show\nthat ARO achieves the best cost in a short time, and consequently, it can be\nconsidered a real-time fraud detection system.",
                "Citizen Perspectives on Necessary Safeguards to the Use of AI by Law\n  Enforcement Agencies\nIn the light of modern technological advances, Artificial Intelligence (AI)\nis relied upon to enhance performance, increase efficiency, and maximize gains.\nFor Law Enforcement Agencies (LEAs), it can prove valuable in optimizing\nevidence analysis and establishing proactive prevention measures. Nevertheless,\ncitizens raise legitimate concerns around privacy invasions, biases,\ninequalities, and inaccurate decisions. This study explores the views of 111\ncitizens towards AI use by police through interviews, and integrates societal\nconcerns along with propositions of safeguards from negative effects of AI use\nby LEAs in the context of cybercrime and terrorism.",
                "Responsible Design Patterns for Machine Learning Pipelines\nIntegrating ethical practices into the AI development process for artificial\nintelligence (AI) is essential to ensure safe, fair, and responsible operation.\nAI ethics involves applying ethical principles to the entire life cycle of AI\nsystems. This is essential to mitigate potential risks and harms associated\nwith AI, such as algorithm biases. To achieve this goal, responsible design\npatterns (RDPs) are critical for Machine Learning (ML) pipelines to guarantee\nethical and fair outcomes. In this paper, we propose a comprehensive framework\nincorporating RDPs into ML pipelines to mitigate risks and ensure the ethical\ndevelopment of AI systems. Our framework comprises new responsible AI design\npatterns for ML pipelines identified through a survey of AI ethics and data\nmanagement experts and validated through real-world scenarios with expert\nfeedback. The framework guides AI developers, data scientists, and\npolicy-makers to implement ethical practices in AI development and deploy\nresponsible AI systems in production.",
                "Human Control: Definitions and Algorithms\nHow can humans stay in control of advanced artificial intelligence systems?\nOne proposal is corrigibility, which requires the agent to follow the\ninstructions of a human overseer, without inappropriately influencing them. In\nthis paper, we formally define a variant of corrigibility called shutdown\ninstructability, and show that it implies appropriate shutdown behavior,\nretention of human autonomy, and avoidance of user harm. We also analyse the\nrelated concepts of non-obstruction and shutdown alignment, three previously\nproposed algorithms for human control, and one new algorithm."
            ],
            "interesting paper": 1
        },
        {
            "papers": [
                "AI Liability Insurance With an Example in AI-Powered E-diagnosis System\nArtificial Intelligence (AI) has received an increasing amount of attention\nin multiple areas. The uncertainties and risks in AI-powered systems have\ncreated reluctance in their wild adoption. As an economic solution to\ncompensate for potential damages, AI liability insurance is a promising market\nto enhance the integration of AI into daily life. In this work, we use an\nAI-powered E-diagnosis system as an example to study AI liability insurance. We\nprovide a quantitative risk assessment model with evidence-based numerical\nanalysis. We discuss the insurability criteria for AI technologies and suggest\nnecessary adjustments to accommodate the features of AI products. We show that\nAI liability insurance can act as a regulatory mechanism to incentivize\ncompliant behaviors and serve as a certificate of high-quality AI systems.\nFurthermore, we suggest premium adjustment to reflect the dynamic evolution of\nthe inherent uncertainty in AI. Moral hazard problems are discussed and\nsuggestions for AI liability insurance are provided.",
                "AI and the creative realm: A short review of current and future\n  applications\nThis study explores the concept of creativity and artificial intelligence\n(AI) and their recent integration. While AI has traditionally been perceived as\nincapable of generating new ideas or creating art, the development of more\nsophisticated AI models and the proliferation of human-computer interaction\ntools have opened up new possibilities for AI in artistic creation. This study\ninvestigates the various applications of AI in a creative context,\ndifferentiating between the type of art, language, and algorithms used. It also\nconsiders the philosophical implications of AI and creativity, questioning\nwhether consciousness can be researched in machines and AI's potential\ninterests and decision-making capabilities. Overall, we aim to stimulate a\nreflection on AI's use and ethical implications in creative contexts.",
                "The ethical ambiguity of AI data enrichment: Measuring gaps in research\n  ethics norms and practices\nThe technical progression of artificial intelligence (AI) research has been\nbuilt on breakthroughs in fields such as computer science, statistics, and\nmathematics. However, in the past decade AI researchers have increasingly\nlooked to the social sciences, turning to human interactions to solve the\nchallenges of model development. Paying crowdsourcing workers to generate or\ncurate data, or data enrichment, has become indispensable for many areas of AI\nresearch, from natural language processing to reinforcement learning from human\nfeedback (RLHF). Other fields that routinely interact with crowdsourcing\nworkers, such as Psychology, have developed common governance requirements and\nnorms to ensure research is undertaken ethically. This study explores how, and\nto what extent, comparable research ethics requirements and norms have\ndeveloped for AI research and data enrichment. We focus on the approach taken\nby two leading conferences: ICLR and NeurIPS, and journal publisher Springer.\nIn a longitudinal study of accepted papers, and via a comparison with\nPsychology and CHI papers, this work finds that leading AI venues have begun to\nestablish protocols for human data collection, but these are are inconsistently\nfollowed by authors. Whilst Psychology papers engaging with crowdsourcing\nworkers frequently disclose ethics reviews, payment data, demographic data and\nother information, similar disclosures are far less common in leading AI venues\ndespite similar guidance. The work concludes with hypotheses to explain these\ngaps in research ethics practices and considerations for its implications.",
                "What model does MuZero learn?\nModel-based reinforcement learning (MBRL) has drawn considerable interest in\nrecent years, given its promise to improve sample efficiency. Moreover, when\nusing deep-learned models, it is possible to learn compact and generalizable\nmodels from data. In this work, we study MuZero, a state-of-the-art deep\nmodel-based reinforcement learning algorithm that distinguishes itself from\nexisting algorithms by learning a value-equivalent model. Despite MuZero's\nsuccess and impact in the field of MBRL, existing literature has not thoroughly\naddressed why MuZero performs so well in practice. Specifically, there is a\nlack of in-depth investigation into the value-equivalent model learned by\nMuZero and its effectiveness in model-based credit assignment and policy\nimprovement, which is vital for achieving sample efficiency in MBRL. To fill\nthis gap, we explore two fundamental questions through our empirical analysis:\n1) to what extent does MuZero achieve its learning objective of a\nvalue-equivalent model, and 2) how useful are these models for policy\nimprovement? Our findings reveal that MuZero's model struggles to generalize\nwhen evaluating unseen policies, which limits its capacity for additional\npolicy improvement. However, MuZero's incorporation of the policy prior in MCTS\nalleviates this problem, which biases the search towards actions where the\nmodel is more accurate.",
                "Interpretable and Explainable Logical Policies via Neurally Guided\n  Symbolic Abstraction\nThe limited priors required by neural networks make them the dominating\nchoice to encode and learn policies using reinforcement learning (RL). However,\nthey are also black-boxes, making it hard to understand the agent's behaviour,\nespecially when working on the image level. Therefore, neuro-symbolic RL aims\nat creating policies that are interpretable in the first place. Unfortunately,\ninterpretability is not explainability. To achieve both, we introduce Neurally\ngUided Differentiable loGic policiEs (NUDGE). NUDGE exploits trained neural\nnetwork-based agents to guide the search of candidate-weighted logic rules,\nthen uses differentiable logic to train the logic agents. Our experimental\nevaluation demonstrates that NUDGE agents can induce interpretable and\nexplainable policies while outperforming purely neural ones and showing good\nflexibility to environments of different initial states and problem sizes.",
                "Theoretical Behavior of XAI Methods in the Presence of Suppressor\n  Variables\nIn recent years, the community of 'explainable artificial intelligence' (XAI)\nhas created a vast body of methods to bridge a perceived gap between model\n'complexity' and 'interpretability'. However, a concrete problem to be solved\nby XAI methods has not yet been formally stated. As a result, XAI methods are\nlacking theoretical and empirical evidence for the 'correctness' of their\nexplanations, limiting their potential use for quality-control and transparency\npurposes. At the same time, Haufe et al. (2014) showed, using simple toy\nexamples, that even standard interpretations of linear models can be highly\nmisleading. Specifically, high importance may be attributed to so-called\nsuppressor variables lacking any statistical relation to the prediction target.\nThis behavior has been confirmed empirically for a large array of XAI methods\nin Wilming et al. (2022). Here, we go one step further by deriving analytical\nexpressions for the behavior of a variety of popular XAI methods on a simple\ntwo-dimensional binary classification problem involving Gaussian\nclass-conditional distributions. We show that the majority of the studied\napproaches will attribute non-zero importance to a non-class-related suppressor\nfeature in the presence of correlated noise. This poses important limitations\non the interpretations and conclusions that the outputs of these XAI methods\ncan afford."
            ],
            "interesting paper": 1
        },
        {
            "papers": [
                "Enough With \"Human-AI Collaboration\"\nDescribing our interaction with Artificial Intelligence (AI) systems as\n'collaboration' is well-intentioned, but flawed. Not only is it misleading, but\nit also takes away the credit of AI 'labour' from the humans behind it, and\nerases and obscures an often exploitative arrangement between AI producers and\nconsumers. The AI 'collaboration' metaphor is merely the latest episode in a\nlong history of labour appropriation and credit reassignment that\ndisenfranchises labourers in the Global South. I propose that viewing AI as a\ntool or an instrument, rather than a collaborator, is more accurate, and\nultimately fairer.",
                "XAI Renaissance: Redefining Interpretability in Medical Diagnostic\n  Models\nAs machine learning models become increasingly prevalent in medical\ndiagnostics, the need for interpretability and transparency becomes paramount.\nThe XAI Renaissance signifies a significant shift in the field, aiming to\nredefine the interpretability of medical diagnostic models. This paper explores\nthe innovative approaches and methodologies within the realm of Explainable AI\n(XAI) that are revolutionizing the interpretability of medical diagnostic\nmodels. By shedding light on the underlying decision-making process, XAI\ntechniques empower healthcare professionals to understand, trust, and\neffectively utilize these models for accurate and reliable medical diagnoses.\nThis review highlights the key advancements in XAI for medical diagnostics and\ntheir potential to transform the healthcare landscape, ultimately improving\npatient outcomes and fostering trust in AI-driven diagnostic systems.",
                "Accelerating science with human-aware artificial intelligence\nArtificial intelligence (AI) models trained on published scientific findings\nhave been used to invent valuable materials and targeted therapies, but they\ntypically ignore the human scientists who continually alter the landscape of\ndiscovery. Here we show that incorporating the distribution of human expertise\nby training unsupervised models on simulated inferences cognitively accessible\nto experts dramatically improves (up to 400%) AI prediction of future\ndiscoveries beyond those focused on research content alone, especially when\nrelevant literature is sparse. These models succeed by predicting human\npredictions and the scientists who will make them. By tuning human-aware AI to\navoid the crowd, we can generate scientifically promising \"alien\" hypotheses\nunlikely to be imagined or pursued without intervention until the distant\nfuture, which hold promise to punctuate scientific advance beyond questions\ncurrently pursued. Accelerating human discovery or probing its blind spots,\nhuman-aware AI enables us to move toward and beyond the contemporary scientific\nfrontier.",
                "SourceP: Detecting Ponzi Schemes on Ethereum with Source Code\nAs blockchain technology becomes more and more popular, a typical financial\nscam, the Ponzi scheme, has also emerged in the blockchain platform Ethereum.\nThis Ponzi scheme deployed through smart contracts, also known as the smart\nPonzi scheme, has caused a lot of economic losses and negative impacts.\nExisting methods for detecting smart Ponzi schemes on Ethereum mainly rely on\nbytecode features, opcode features, account features, and transaction behavior\nfeatures of smart contracts, which are unable to truly characterize the\nbehavioral features of Ponzi schemes, and thus generally perform poorly in\nterms of detection accuracy and false alarm rates. In this paper, we propose\nSourceP, a method to detect smart Ponzi schemes on the Ethereum platform using\npre-trained models and data flow, which only requires using the source code of\nsmart contracts as features. SourceP reduces the difficulty of data acquisition\nand feature extraction of existing detection methods. Specifically, we first\nconvert the source code of a smart contract into a data flow graph and then\nintroduce a pre-trained model based on learning code representations to build a\nclassification model to identify Ponzi schemes in smart contracts. The\nexperimental results show that SourceP achieves 87.2% recall and 90.7% F-score\nfor detecting smart Ponzi schemes within Ethereum's smart contract dataset,\noutperforming state-of-the-art methods in terms of performance and\nsustainability. We also demonstrate through additional experiments that\npre-trained models and data flow play an important contribution to SourceP, as\nwell as proving that SourceP has a good generalization ability.",
                "No Bidding, No Regret: Pairwise-Feedback Mechanisms for Digital Goods\n  and Data Auctions\nThe growing demand for data and AI-generated digital goods, such as\npersonalized written content and artwork, necessitates effective pricing and\nfeedback mechanisms that account for uncertain utility and costly production.\nMotivated by these developments, this study presents a novel mechanism design\naddressing a general repeated-auction setting where the utility derived from a\nsold good is revealed post-sale. The mechanism's novelty lies in using pairwise\ncomparisons for eliciting information from the bidder, arguably easier for\nhumans than assigning a numerical value. Our mechanism chooses allocations\nusing an epsilon-greedy strategy and relies on pairwise comparisons between\nrealized utility from allocated goods and an arbitrary value, avoiding the\nlearning-to-bid problem explored in previous work. We prove this mechanism to\nbe asymptotically truthful, individually rational, and welfare and revenue\nmaximizing. The mechanism's relevance is broad, applying to any setting with\nmade-to-order goods of variable quality. Experimental results on multi-label\ntoxicity annotation data, an example of negative utilities, highlight how our\nproposed mechanism could enhance social welfare in data auctions. Overall, our\nfocus on human factors contributes to the development of more human-aware and\nefficient mechanism design.",
                "AI Transparency in the Age of LLMs: A Human-Centered Research Roadmap\nThe rise of powerful large language models (LLMs) brings about tremendous\nopportunities for innovation but also looming risks for individuals and society\nat large. We have reached a pivotal moment for ensuring that LLMs and\nLLM-infused applications are developed and deployed responsibly. However, a\ncentral pillar of responsible AI -- transparency -- is largely missing from the\ncurrent discourse around LLMs. It is paramount to pursue new approaches to\nprovide transparency for LLMs, and years of research at the intersection of AI\nand human-computer interaction (HCI) highlight that we must do so with a\nhuman-centered perspective: Transparency is fundamentally about supporting\nappropriate human understanding, and this understanding is sought by different\nstakeholders with different goals in different contexts. In this new era of\nLLMs, we must develop and design approaches to transparency by considering the\nneeds of stakeholders in the emerging LLM ecosystem, the novel types of\nLLM-infused applications being built, and the new usage patterns and challenges\naround LLMs, all while building on lessons learned about how people process,\ninteract with, and make use of information. We reflect on the unique challenges\nthat arise in providing transparency for LLMs, along with lessons learned from\nHCI and responsible AI research that has taken a human-centered perspective on\nAI transparency. We then lay out four common approaches that the community has\ntaken to achieve transparency -- model reporting, publishing evaluation\nresults, providing explanations, and communicating uncertainty -- and call out\nopen questions around how these approaches may or may not be applied to LLMs.\nWe hope this provides a starting point for discussion and a useful roadmap for\nfuture research."
            ],
            "interesting paper": 4
        }
    ],
    "Riley Lopez": [
        {
            "papers": [
                "Deep Learning and Ethics\nThis article appears as chapter 21 of Prince (2023, Understanding Deep\nLearning); a complete draft of the textbook is available here:\nhttp://udlbook.com. This chapter considers potential harms arising from the\ndesign and use of AI systems. These include algorithmic bias, lack of\nexplainability, data privacy violations, militarization, fraud, and\nenvironmental concerns. The aim is not to provide advice on being more ethical.\nInstead, the goal is to express ideas and start conversations in key areas that\nhave received attention in philosophy, political science, and the broader\nsocial sciences.",
                "Science in the Era of ChatGPT, Large Language Models and Generative AI:\n  Challenges for Research Ethics and How to Respond\nLarge language models of artificial intelligence (AI), such as ChatGPT, find\nremarkable but controversial applicability in science and research. This paper\nreviews epistemological challenges, ethical and integrity risks in science\nconduct in the advent of generative AI. This is with the aim to lay new timely\nfoundations for a high-quality research ethics review. The role of AI language\nmodels as a research instrument and subject is scrutinized along with ethical\nimplications for scientists, participants and reviewers. New emerging practices\nfor research ethics review are discussed, concluding with ten recommendations\nthat shape a response for a more responsible research conduct in the era of AI.",
                "Model evaluation for extreme risks\nCurrent approaches to building general-purpose AI systems tend to produce\nsystems with both beneficial and harmful capabilities. Further progress in AI\ndevelopment could lead to capabilities that pose extreme risks, such as\noffensive cyber capabilities or strong manipulation skills. We explain why\nmodel evaluation is critical for addressing extreme risks. Developers must be\nable to identify dangerous capabilities (through \"dangerous capability\nevaluations\") and the propensity of models to apply their capabilities for harm\n(through \"alignment evaluations\"). These evaluations will become critical for\nkeeping policymakers and other stakeholders informed, and for making\nresponsible decisions about model training, deployment, and security.",
                "Towards a Capability Assessment Model for the Comprehension and Adoption\n  of AI in Organisations\nThe comprehension and adoption of Artificial Intelligence (AI) are beset with\npractical and ethical problems. This article presents a 5-level AI Capability\nAssessment Model (AI-CAM) and a related AI Capabilities Matrix (AI-CM) to\nassist practitioners in AI comprehension and adoption. These practical tools\nwere developed with business executives, technologists, and other\norganisational stakeholders in mind. They are founded on a comprehensive\nconception of AI compared to those in other AI adoption models and are also\nopen-source artefacts. Thus, the AI-CAM and AI-CM present an accessible\nresource to help inform organisational decision-makers on the capability\nrequirements for (1) AI-based data analytics use cases based on machine\nlearning technologies; (2) Knowledge representation to engineer and represent\ndata, information and knowledge using semantic technologies; and (3) AI-based\nsolutions that seek to emulate human reasoning and decision-making. The AI-CAM\ncovers the core capability dimensions (business, data, technology,\norganisation, AI skills, risks, and ethical considerations) required at the\nfive capability maturity levels to achieve optimal use of AI in organisations.",
                "An Experimental Investigation into the Evaluation of Explainability\n  Methods\nEXplainable Artificial Intelligence (XAI) aims to help users to grasp the\nreasoning behind the predictions of an Artificial Intelligence (AI) system.\nMany XAI approaches have emerged in recent years. Consequently, a subfield\nrelated to the evaluation of XAI methods has gained considerable attention,\nwith the aim to determine which methods provide the best explanation using\nvarious approaches and criteria. However, the literature lacks a comparison of\nthe evaluation metrics themselves, that one can use to evaluate XAI methods.\nThis work aims to fill this gap by comparing 14 different metrics when applied\nto nine state-of-the-art XAI methods and three dummy methods (e.g., random\nsaliency maps) used as references. Experimental results show which of these\nmetrics produces highly correlated results, indicating potential redundancy. We\nalso demonstrate the significant impact of varying the baseline hyperparameter\non the evaluation metric values. Finally, we use dummy methods to assess the\nreliability of metrics in terms of ranking, pointing out their limitations.",
                "Asking Before Acting: Gather Information in Embodied Decision Making\n  with Language Models\nWith strong capabilities of reasoning and a broad understanding of the world,\nLarge Language Models (LLMs) have demonstrated immense potential in building\nversatile embodied decision-making agents capable of executing a wide array of\ntasks. Nevertheless, when deployed in unfamiliar environments, we show that LLM\nagents encounter challenges in efficiently gathering essential information,\nleading to suboptimal performance. Conversely, human individuals often seek\nadditional information from their peers prior to taking action, harnessing\nexternal knowledge to avoid unnecessary trial and error. Drawing inspiration\nfrom this behavior, we propose \\textit{Asking Before Acting} (ABA), a method\nthat empowers the agent to proactively inquire with external sources for\npertinent information using natural language during their interactions within\nthe environment. In this way, the agent is able to enhance its efficiency and\nperformance by circumventing potentially laborious steps and combating the\ndifficulties associated with exploration in unfamiliar environments and\nvagueness of the instructions. We conduct extensive experiments involving a\nspectrum of environments including text-based household everyday tasks, robot\narm manipulation tasks, and real world open domain image based embodied tasks.\nThe experiments involve various models from Vicuna to GPT-4. The results\ndemonstrate that, even with modest prompts modifications, ABA exhibits\nsubstantial advantages on both performance and efficiency over baseline LLM\nagents. Further finetuning ABA with reformulated metadata (ABA-FT) faciliates\nlearning the rationale for asking and allows for additional enhancements\nespecially in tasks that baselines struggle to solve."
            ],
            "interesting paper": 2
        },
        {
            "papers": [
                "Enhancing Human Capabilities through Symbiotic Artificial Intelligence\n  with Shared Sensory Experiences\nThe merging of human intelligence and artificial intelligence has long been a\nsubject of interest in both science fiction and academia. In this paper, we\nintroduce a novel concept in Human-AI interaction called Symbiotic Artificial\nIntelligence with Shared Sensory Experiences (SAISSE), which aims to establish\na mutually beneficial relationship between AI systems and human users through\nshared sensory experiences. By integrating multiple sensory input channels and\nprocessing human experiences, SAISSE fosters a strong human-AI bond, enabling\nAI systems to learn from and adapt to individual users, providing personalized\nsupport, assistance, and enhancement. Furthermore, we discuss the incorporation\nof memory storage units for long-term growth and development of both the AI\nsystem and its human user. As we address user privacy and ethical guidelines\nfor responsible AI-human symbiosis, we also explore potential biases and\ninequalities in AI-human symbiosis and propose strategies to mitigate these\nchallenges. Our research aims to provide a comprehensive understanding of the\nSAISSE concept and its potential to effectively support and enhance individual\nhuman users through symbiotic AI systems. This position article aims at\ndiscussing poteintial AI-human interaction related topics within the scientific\ncommunity, rather than providing experimental or theoretical results.",
                "Alert of the Second Decision-maker: An Introduction to Human-AI Conflict\nThe collaboration between humans and artificial intelligence (AI) is a\nsignificant feature in this digital age. However, humans and AI may have\nobservation, interpretation, and action conflicts when working synchronously.\nThis phenomenon is often masked by faults and, unfortunately, overlooked. This\npaper systematically introduces the human-AI conflict concept, causes,\nmeasurement methods, and risk assessment. The results highlight that there is a\npotential second decision-maker besides the human, which is the AI; the\nhuman-AI conflict is a unique and emerging risk in digitalized process systems;\nand this is an interdisciplinary field that needs to be distinguished from\ntraditional fault and failure analysis; the conflict risk is significant and\ncannot be ignored.",
                "Applying Interdisciplinary Frameworks to Understand Algorithmic\n  Decision-Making\nWe argue that explanations for \"algorithmic decision-making\" (ADM) systems\ncan profit by adopting practices that are already used in the learning\nsciences. We shortly introduce the importance of explaining ADM systems, give a\nbrief overview of approaches drawing from other disciplines to improve\nexplanations, and present the results of our qualitative task-based study\nincorporating the \"six facets of understanding\" framework. We close with\nquestions guiding the discussion of how future studies can leverage an\ninterdisciplinary approach.",
                "Attention Paper: How Generative AI Reshapes Digital Shadow Industry?\nThe rapid development of digital economy has led to the emergence of various\nblack and shadow internet industries, which pose potential risks that can be\nidentified and managed through digital risk management (DRM) that uses\ndifferent techniques such as machine learning and deep learning. The evolution\nof DRM architecture has been driven by changes in data forms. However, the\ndevelopment of AI-generated content (AIGC) technology, such as ChatGPT and\nStable Diffusion, has given black and shadow industries powerful tools to\npersonalize data and generate realistic images and conversations for fraudulent\nactivities. This poses a challenge for DRM systems to control risks from the\nsource of data generation and to respond quickly to the fast-changing risk\nenvironment. This paper aims to provide a technical analysis of the challenges\nand opportunities of AIGC from upstream, midstream, and downstream paths of\nblack/shadow industries and suggest future directions for improving existing\nrisk control systems. The paper will explore the new black and shadow\ntechniques triggered by generative AI technology and provide insights for\nbuilding the next-generation DRM system.",
                "Quantifying the Intrinsic Usefulness of Attributional Explanations for\n  Graph Neural Networks with Artificial Simulatability Studies\nDespite the increasing relevance of explainable AI, assessing the quality of\nexplanations remains a challenging issue. Due to the high costs associated with\nhuman-subject experiments, various proxy metrics are often used to\napproximately quantify explanation quality. Generally, one possible\ninterpretation of the quality of an explanation is its inherent value for\nteaching a related concept to a student. In this work, we extend artificial\nsimulatability studies to the domain of graph neural networks. Instead of\ncostly human trials, we use explanation-supervisable graph neural networks to\nperform simulatability studies to quantify the inherent usefulness of\nattributional graph explanations. We perform an extensive ablation study to\ninvestigate the conditions under which the proposed analyses are most\nmeaningful. We additionally validate our methods applicability on real-world\ngraph classification and regression datasets. We find that relevant\nexplanations can significantly boost the sample efficiency of graph neural\nnetworks and analyze the robustness towards noise and bias in the explanations.\nWe believe that the notion of usefulness obtained from our proposed\nsimulatability analysis provides a dimension of explanation quality that is\nlargely orthogonal to the common practice of faithfulness and has great\npotential to expand the toolbox of explanation quality assessments,\nspecifically for graph explanations.",
                "A Survey on ChatGPT: AI-Generated Contents, Challenges, and Solutions\nWith the widespread use of large artificial intelligence (AI) models such as\nChatGPT, AI-generated content (AIGC) has garnered increasing attention and is\nleading a paradigm shift in content creation and knowledge representation. AIGC\nuses generative large AI algorithms to assist or replace humans in creating\nmassive, high-quality, and human-like content at a faster pace and lower cost,\nbased on user-provided prompts. Despite the recent significant progress in\nAIGC, security, privacy, ethical, and legal challenges still need to be\naddressed. This paper presents an in-depth survey of working principles,\nsecurity and privacy threats, state-of-the-art solutions, and future challenges\nof the AIGC paradigm. Specifically, we first explore the enabling technologies,\ngeneral architecture of AIGC, and discuss its working modes and key\ncharacteristics. Then, we investigate the taxonomy of security and privacy\nthreats to AIGC and highlight the ethical and societal implications of GPT and\nAIGC technologies. Furthermore, we review the state-of-the-art AIGC\nwatermarking approaches for regulatable AIGC paradigms regarding the AIGC model\nand its produced content. Finally, we identify future challenges and open\nresearch directions related to AIGC."
            ],
            "interesting paper": 1
        },
        {
            "papers": [
                "Frontier AI developers need an internal audit function\nThis article argues that frontier artificial intelligence (AI) developers\nneed an internal audit function. First, it describes the role of internal audit\nin corporate governance: internal audit evaluates the adequacy and\neffectiveness of a company's risk management, control, and governance\nprocesses. It is organizationally independent from senior management and\nreports directly to the board of directors, typically its audit committee. In\nthe IIA's Three Lines Model, internal audit serves as the third line and is\nresponsible for providing assurance to the board, while the Combined Assurance\nFramework highlights the need to coordinate the activities of internal and\nexternal assurance providers. Next, the article provides an overview of key\ngovernance challenges in frontier AI development: dangerous capabilities can\narise unpredictably and undetected; it is difficult to prevent a deployed model\nfrom causing harm; frontier models can proliferate rapidly; it is inherently\ndifficult to assess frontier AI risks; and frontier AI developers do not seem\nto follow best practices in risk governance. Finally, the article discusses how\nan internal audit function could address some of these challenges: internal\naudit could identify ineffective risk management practices; it could ensure\nthat the board of directors has a more accurate understanding of the current\nlevel of risk and the adequacy of the developer's risk management practices;\nand it could serve as a contact point for whistleblowers. In light of rapid\nprogress in AI research and development, frontier AI developers need to\nstrengthen their risk governance. Instead of reinventing the wheel, they should\nfollow existing best practices. While this might not be sufficient, they should\nnot skip this obvious first step.",
                "Moral Machine or Tyranny of the Majority?\nWith Artificial Intelligence systems increasingly applied in consequential\ndomains, researchers have begun to ask how these systems ought to act in\nethically charged situations where even humans lack consensus. In the Moral\nMachine project, researchers crowdsourced answers to \"Trolley Problems\"\nconcerning autonomous vehicles. Subsequently, Noothigattu et al. (2018)\nproposed inferring linear functions that approximate each individual's\npreferences and aggregating these linear models by averaging parameters across\nthe population. In this paper, we examine this averaging mechanism, focusing on\nfairness concerns in the presence of strategic effects. We investigate a simple\nsetting where the population consists of two groups, with the minority\nconstituting an {\\alpha} < 0.5 share of the population. To simplify the\nanalysis, we consider the extreme case in which within-group preferences are\nhomogeneous. Focusing on the fraction of contested cases where the minority\ngroup prevails, we make the following observations: (a) even when all parties\nreport their preferences truthfully, the fraction of disputes where the\nminority prevails is less than proportionate in {\\alpha}; (b) the degree of\nsub-proportionality grows more severe as the level of disagreement between the\ngroups increases; (c) when parties report preferences strategically, pure\nstrategy equilibria do not always exist; and (d) whenever a pure strategy\nequilibrium exists, the majority group prevails 100% of the time. These\nfindings raise concerns about stability and fairness of preference vector\naveraging as a mechanism for aggregating diverging voices. Finally, we discuss\nalternatives, including randomized dictatorship and median-based mechanisms.",
                "How Do UX Practitioners Communicate AI as a Design Material? Artifacts,\n  Conceptions, and Propositions\nUX practitioners (UXPs) face novel challenges when working with and\ncommunicating artificial intelligence (AI) as a design material. We explore how\nUXPs communicate AI concepts when given hands-on experience training and\nexperimenting with AI models. To do so, we conducted a task-based design study\nwith 27 UXPs in which they prototyped and created a design presentation for a\nAI-enabled interface while having access to a simple AI model training tool.\nThrough analyzing UXPs' design presentations and post-activity interviews, we\nfound that although UXPs struggled to clearly communicate some AI concepts,\ntinkering with AI broadened common ground when communicating with technical\nstakeholders. UXPs also identified key risks and benefits of AI in their\ndesigns, and proposed concrete next steps for both UX and AI work. We conclude\nwith a sensitizing concept and recommendations for design and AI tools to\nenhance multi-stakeholder communication and collaboration when crafting\nhuman-centered AI experiences.",
                "Towards Cognitive Bots: Architectural Research Challenges\nSoftware bots operating in multiple virtual digital platforms must understand\nthe platforms' affordances and behave like human users. Platform affordances or\nfeatures differ from one application platform to another or through a life\ncycle, requiring such bots to be adaptable. Moreover, bots in such platforms\ncould cooperate with humans or other software agents for work or to learn\nspecific behavior patterns. However, present-day bots, particularly chatbots,\nother than language processing and prediction, are far from reaching a human\nuser's behavior level within complex business information systems. They lack\nthe cognitive capabilities to sense and act in such virtual environments,\nrendering their development a challenge to artificial general intelligence\nresearch. In this study, we problematize and investigate assumptions in\nconceptualizing software bot architecture by directing attention to significant\narchitectural research challenges in developing cognitive bots endowed with\ncomplex behavior for operation on information systems. As an outlook, we\npropose alternate architectural assumptions to consider in future bot design\nand bot development frameworks.",
                "Mindstorms in Natural Language-Based Societies of Mind\nBoth Minsky's \"society of mind\" and Schmidhuber's \"learning to think\" inspire\ndiverse societies of large multimodal neural networks (NNs) that solve problems\nby interviewing each other in a \"mindstorm.\" Recent implementations of NN-based\nsocieties of minds consist of large language models (LLMs) and other NN-based\nexperts communicating through a natural language interface. In doing so, they\novercome the limitations of single LLMs, improving multimodal zero-shot\nreasoning. In these natural language-based societies of mind (NLSOMs), new\nagents -- all communicating through the same universal symbolic language -- are\neasily added in a modular fashion. To demonstrate the power of NLSOMs, we\nassemble and experiment with several of them (having up to 129 members),\nleveraging mindstorms in them to solve some practical AI tasks: visual question\nanswering, image captioning, text-to-image synthesis, 3D generation, egocentric\nretrieval, embodied AI, and general language-based task solving. We view this\nas a starting point towards much larger NLSOMs with billions of agents-some of\nwhich may be humans. And with this emergence of great societies of\nheterogeneous minds, many new research questions have suddenly become paramount\nto the future of artificial intelligence. What should be the social structure\nof an NLSOM? What would be the (dis)advantages of having a monarchical rather\nthan a democratic structure? How can principles of NN economies be used to\nmaximize the total reward of a reinforcement learning NLSOM? In this work, we\nidentify, discuss, and try to answer some of these questions.",
                "Training Socially Aligned Language Models on Simulated Social\n  Interactions\nSocial alignment in AI systems aims to ensure that these models behave\naccording to established societal values. However, unlike humans, who derive\nconsensus on value judgments through social interaction, current language\nmodels (LMs) are trained to rigidly replicate their training corpus in\nisolation, leading to subpar generalization in unfamiliar scenarios and\nvulnerability to adversarial attacks. This work presents a novel training\nparadigm that permits LMs to learn from simulated social interactions. In\ncomparison to existing methodologies, our approach is considerably more\nscalable and efficient, demonstrating superior performance in alignment\nbenchmarks and human evaluations. This paradigm shift in the training of LMs\nbrings us a step closer to developing AI systems that can robustly and\naccurately reflect societal norms and values."
            ],
            "interesting paper": 2
        },
        {
            "papers": [
                "Optimization's Neglected Normative Commitments\nOptimization is offered as an objective approach to resolving complex,\nreal-world decisions involving uncertainty and conflicting interests. It drives\nbusiness strategies as well as public policies and, increasingly, lies at the\nheart of sophisticated machine learning systems. A paradigm used to approach\npotentially high-stakes decisions, optimization relies on abstracting the real\nworld to a set of decision(s), objective(s) and constraint(s). Drawing from the\nmodeling process and a range of actual cases, this paper describes the\nnormative choices and assumptions that are necessarily part of using\noptimization. It then identifies six emergent problems that may be neglected:\n1) Misspecified values can yield optimizations that omit certain imperatives\naltogether or incorporate them incorrectly as a constraint or as part of the\nobjective, 2) Problematic decision boundaries can lead to faulty modularity\nassumptions and feedback loops, 3) Failing to account for multiple agents'\ndivergent goals and decisions can lead to policies that serve only certain\nnarrow interests, 4) Mislabeling and mismeasurement can introduce bias and\nimprecision, 5) Faulty use of relaxation and approximation methods,\nunaccompanied by formal characterizations and guarantees, can severely impede\napplicability, and 6) Treating optimization as a justification for action,\nwithout specifying the necessary contextual information, can lead to ethically\ndubious or faulty decisions. Suggestions are given to further understand and\ncurb the harms that can arise when optimization is used wrongfully.",
                "AI Coach Assist: An Automated Approach for Call Recommendation in\n  Contact Centers for Agent Coaching\nIn recent years, the utilization of Artificial Intelligence (AI) in the\ncontact center industry is on the rise. One area where AI can have a\nsignificant impact is in the coaching of contact center agents. By analyzing\ncall transcripts using Natural Language Processing (NLP) techniques, it would\nbe possible to quickly determine which calls are most relevant for coaching\npurposes. In this paper, we present AI Coach Assist, which leverages the\npre-trained transformer-based language models to determine whether a given call\nis coachable or not based on the quality assurance (QA) questions asked by the\ncontact center managers or supervisors. The system was trained and evaluated on\na large dataset collected from real-world contact centers and provides an\neffective way to recommend calls to the contact center managers that are more\nlikely to contain coachable moments. Our experimental findings demonstrate the\npotential of AI Coach Assist to improve the coaching process, resulting in\nenhancing the performance of contact center agents.",
                "Incentivizing honest performative predictions with proper scoring rules\nProper scoring rules incentivize experts to accurately report beliefs,\nassuming predictions cannot influence outcomes. We relax this assumption and\ninvestigate incentives when predictions are performative, i.e., when they can\ninfluence the outcome of the prediction, such as when making public predictions\nabout the stock market. We say a prediction is a fixed point if it accurately\nreflects the expert's beliefs after that prediction has been made. We show that\nin this setting, reports maximizing expected score generally do not reflect an\nexpert's beliefs, and we give bounds on the inaccuracy of such reports. We show\nthat, for binary predictions, if the influence of the expert's prediction on\noutcomes is bounded, it is possible to define scoring rules under which optimal\nreports are arbitrarily close to fixed points. However, this is impossible for\npredictions over more than two outcomes. We also perform numerical simulations\nin a toy setting, showing that our bounds are tight in some situations and that\nprediction error is often substantial (greater than 5-10%). Lastly, we discuss\nalternative notions of optimality, including performative stability, and show\nthat they incentivize reporting fixed points.",
                "On the Value of Myopic Behavior in Policy Reuse\nLeveraging learned strategies in unfamiliar scenarios is fundamental to human\nintelligence. In reinforcement learning, rationally reusing the policies\nacquired from other tasks or human experts is critical for tackling problems\nthat are difficult to learn from scratch. In this work, we present a framework\ncalled Selective Myopic bEhavior Control~(SMEC), which results from the insight\nthat the short-term behaviors of prior policies are sharable across tasks. By\nevaluating the behaviors of prior policies via a hybrid value function\narchitecture, SMEC adaptively aggregates the sharable short-term behaviors of\nprior policies and the long-term behaviors of the task policy, leading to\ncoordinated decisions. Empirical results on a collection of manipulation and\nlocomotion tasks demonstrate that SMEC outperforms existing methods, and\nvalidate the ability of SMEC to leverage related prior policies.",
                "Backdooring Neural Code Search\nReusing off-the-shelf code snippets from online repositories is a common\npractice, which significantly enhances the productivity of software developers.\nTo find desired code snippets, developers resort to code search engines through\nnatural language queries. Neural code search models are hence behind many such\nengines. These models are based on deep learning and gain substantial attention\ndue to their impressive performance. However, the security aspect of these\nmodels is rarely studied. Particularly, an adversary can inject a backdoor in\nneural code search models, which return buggy or even vulnerable code with\nsecurity/privacy issues. This may impact the downstream software (e.g., stock\ntrading systems and autonomous driving) and cause financial loss and/or\nlife-threatening incidents. In this paper, we demonstrate such attacks are\nfeasible and can be quite stealthy. By simply modifying one variable/function\nname, the attacker can make buggy/vulnerable code rank in the top 11%. Our\nattack BADCODE features a special trigger generation and injection procedure,\nmaking the attack more effective and stealthy. The evaluation is conducted on\ntwo neural code search models and the results show our attack outperforms\nbaselines by 60%. Our user study demonstrates that our attack is more stealthy\nthan the baseline by two times based on the F1 score.",
                "Integrating Action Knowledge and LLMs for Task Planning and Situation\n  Handling in Open Worlds\nTask planning systems have been developed to help robots use human knowledge\n(about actions) to complete long-horizon tasks. Most of them have been\ndeveloped for \"closed worlds\" while assuming the robot is provided with\ncomplete world knowledge. However, the real world is generally open, and the\nrobots frequently encounter unforeseen situations that can potentially break\nthe planner's completeness. Could we leverage the recent advances on\npre-trained Large Language Models (LLMs) to enable classical planning systems\nto deal with novel situations?\n  This paper introduces a novel framework, called COWP, for open-world task\nplanning and situation handling. COWP dynamically augments the robot's action\nknowledge, including the preconditions and effects of actions, with\ntask-oriented commonsense knowledge. COWP embraces the openness from LLMs, and\nis grounded to specific domains via action knowledge. For systematic\nevaluations, we collected a dataset that includes 1,085 execution-time\nsituations. Each situation corresponds to a state instance wherein a robot is\npotentially unable to complete a task using a solution that normally works.\nExperimental results show that our approach outperforms competitive baselines\nfrom the literature in the success rate of service tasks. Additionally, we have\ndemonstrated COWP using a mobile manipulator. Supplementary materials are\navailable at: https://cowplanning.github.io/"
            ],
            "interesting paper": 1
        },
        {
            "papers": [
                "AI Audit: A Card Game to Reflect on Everyday AI Systems\nAn essential element of K-12 AI literacy is educating learners about the\nethical and societal implications of AI systems. Previous work in AI ethics\nliteracy have developed curriculum and classroom activities that engage\nlearners in reflecting on the ethical implications of AI systems and developing\nresponsible AI. There is little work in using game-based learning methods in AI\nliteracy. Games are known to be compelling media to teach children about\ncomplex STEM concepts. In this work, we developed a competitive card game for\nmiddle and high school students called \"AI Audit\" where they play as AI\nstart-up founders building novel AI-powered technology. Players can challenge\nother players with potential harms of their technology or defend their own\nbusinesses by features that mitigate these harms. The game mechanics reward\nsystems that are ethically developed or that take steps to mitigate potential\nharms. In this paper, we present the game design, teacher resources for\nclassroom deployment and early playtesting results. We discuss our reflections\nabout using games as teaching tools for AI literacy in K-12 classrooms.",
                "Voluminous yet Vacuous? Semantic Capital in an Age of Large Language\n  Models\nLarge Language Models (LLMs) have emerged as transformative forces in the\nrealm of natural language processing, wielding the power to generate human-like\ntext. However, despite their potential for content creation, they carry the\nrisk of eroding our Semantic Capital (SC) - the collective knowledge within our\ndigital ecosystem - thereby posing diverse social epistemic challenges. This\npaper explores the evolution, capabilities, and limitations of these models,\nwhile highlighting ethical concerns they raise. The study contribution is\ntwo-fold: first, it is acknowledged that, withstanding the challenges of\ntracking and controlling LLM impacts, it is necessary to reconsider our\ninteraction with these AI technologies and the narratives that form public\nperception of them. It is argued that before achieving this goal, it is\nessential to confront a potential deontological tipping point in an increasing\nAI-driven infosphere. This goes beyond just adhering to AI ethical norms or\nregulations and requires understanding the spectrum of social epistemic risks\nLLMs might bring to our collective SC. Secondly, building on Luciano Floridi's\ntaxonomy for SC risks, those are mapped within the functionality and\nconstraints of LLMs. By this outlook, we aim to protect and enrich our SC while\nfostering a collaborative environment between humans and AI that augments human\nintelligence rather than replacing it.",
                "Re-imagining health and well-being in low resource African settings\n  using an augmented AI system and a 3D digital twin\nThis paper discusses and explores the potential and relevance of recent\ndevelopments in artificial intelligence (AI) and digital twins for health and\nwell-being in low-resource African countries. We use the case of public health\nemergency response to disease outbreaks and epidemic control. There is\npotential to take advantage of the increasing availability of data and\ndigitization to develop advanced AI methods for analysis and prediction. Using\nan AI systems perspective, we review emerging trends in AI systems and digital\ntwins and propose an initial augmented AI system architecture to illustrate how\nan AI system can work with a 3D digital twin to address public health goals. We\nhighlight scientific knowledge discovery, continual learning, pragmatic\ninteroperability, and interactive explanation and decision-making as essential\nresearch challenges for AI systems and digital twins.",
                "The Digital Divide in Process Safety: Quantitative Risk Analysis of\n  Human-AI Collaboration\nDigital technologies have dramatically accelerated the digital transformation\nin process industries, boosted new industrial applications, upgraded the\nproduction system, and enhanced operational efficiency. In contrast, the\nchallenges and gaps between human and artificial intelligence (AI) have become\nmore and more prominent, whereas the digital divide in process safety is\naggregating. The study attempts to address the following questions: (i)What is\nAI in the process safety context? (ii)What is the difference between AI and\nhumans in process safety? (iii)How do AI and humans collaborate in process\nsafety? (iv)What are the challenges and gaps in human-AI collaboration? (v)How\nto quantify the risk of human-AI collaboration in process safety? Qualitative\nrisk analysis based on brainstorming and literature review, and quantitative\nrisk analysis based on layer of protection analysis (LOPA) and Bayesian network\n(BN), were applied to explore and model. The importance of human reliability\nshould be stressed in the digital age, not usually to increase the reliability\nof AI, and human-centered AI design in process safety needs to be propagated.",
                "The Social Value of Dark Energy\nAstrophysics is a social enterprise exemplified here by the Dark Energy\nSurvey (DES) which completed its fieldwork in 2019 after 16 years of\npreparation and observation, while data analysis continues. Society funds\nastrophysics on a grand scale. For human capital and for governance the\ndiscipline draws on a self-governing \"republic of science\", while the funds\nwere provided by philanthropists in the past, and by governments today. The\nbenefits accrue initially to scientists themselves, in the form of a rewarding\nvocation. For the social benefit it is tempting to apply formal cost benefit\nanalysis, but that approach ignores the option value of science and imposes\nquestionable assumptions from welfare economics. Astrophysics generates some\nuseful spinoffs, offers attractive careers, appeals to the popular imagination,\nspeaks to metaphysical cravings and constitutes a good in itself. The rise of\nAI also suggests a role in exploring future habitats for intelligence and\ncognition.",
                "Taming AI Bots: Controllability of Neural States in Large Language\n  Models\nWe tackle the question of whether an agent can, by suitable choice of\nprompts, control an AI bot to any state. To that end, we first introduce a\nformal definition of ``meaning'' that is amenable to analysis. Then, we\ncharacterize ``meaningful data'' on which large language models (LLMs) are\nostensibly trained, and ``well-trained LLMs'' through conditions that are\nlargely met by today's LLMs. While a well-trained LLM constructs an embedding\nspace of meanings that is Euclidean, meanings themselves do not form a vector\n(linear) subspace, but rather a quotient space within. We then characterize the\nsubset of meanings that can be reached by the state of the LLMs for some input\nprompt, and show that a well-trained bot can reach any meaning albeit with\nsmall probability. We then introduce a stronger notion of controllability as\n{\\em almost certain reachability}, and show that, when restricted to the space\nof meanings, an AI bot is controllable. We do so after introducing a functional\ncharacterization of attentive AI bots, and finally derive necessary and\nsufficient conditions for controllability. The fact that AI bots are\ncontrollable means that an adversary could steer them towards any state.\nHowever, the sampling process can be designed to counteract adverse actions and\navoid reaching undesirable regions of state space before their boundary is\ncrossed."
            ],
            "interesting paper": 2
        },
        {
            "papers": [
                "RE-centric Recommendations for the Development of Trustworthy(er)\n  Autonomous Systems\nComplying with the EU AI Act (AIA) guidelines while developing and\nimplementing AI systems will soon be mandatory within the EU. However,\npractitioners lack actionable instructions to operationalise ethics during AI\nsystems development. A literature review of different ethical guidelines\nrevealed inconsistencies in the principles addressed and the terminology used\nto describe them. Furthermore, requirements engineering (RE), which is\nidentified to foster trustworthiness in the AI development process from the\nearly stages was observed to be absent in a lot of frameworks that support the\ndevelopment of ethical and trustworthy AI. This incongruous phrasing combined\nwith a lack of concrete development practices makes trustworthy AI development\nharder. To address this concern, we formulated a comparison table for the\nterminology used and the coverage of the ethical AI principles in major ethical\nAI guidelines. We then examined the applicability of ethical AI development\nframeworks for performing effective RE during the development of trustworthy AI\nsystems. A tertiary review and meta-analysis of literature discussing ethical\nAI frameworks revealed their limitations when developing trustworthy AI. Based\non our findings, we propose recommendations to address such limitations during\nthe development of trustworthy AI.",
                "Doing the right thing for the right reason: Evaluating artificial moral\n  cognition by probing cost insensitivity\nIs it possible to evaluate the moral cognition of complex artificial agents?\nIn this work, we take a look at one aspect of morality: `doing the right thing\nfor the right reasons.' We propose a behavior-based analysis of artificial\nmoral cognition which could also be applied to humans to facilitate\nlike-for-like comparison. Morally-motivated behavior should persist despite\nmounting cost; by measuring an agent's sensitivity to this cost, we gain deeper\ninsight into underlying motivations. We apply this evaluation to a particular\nset of deep reinforcement learning agents, trained by memory-based\nmeta-reinforcement learning. Our results indicate that agents trained with a\nreward function that includes other-regarding preferences perform helping\nbehavior in a way that is less sensitive to increasing cost than agents trained\nwith more self-interested preferences.",
                "Generalized Disparate Impact for Configurable Fairness Solutions in ML\nWe make two contributions in the field of AI fairness over continuous\nprotected attributes. First, we show that the Hirschfeld-Gebelein-Renyi (HGR)\nindicator (the only one currently available for such a case) is valuable but\nsubject to a few crucial limitations regarding semantics, interpretability, and\nrobustness. Second, we introduce a family of indicators that are: 1)\ncomplementary to HGR in terms of semantics; 2) fully interpretable and\ntransparent; 3) robust over finite samples; 4) configurable to suit specific\napplications. Our approach also allows us to define fine-grained constraints to\npermit certain types of dependence and forbid others selectively. By expanding\nthe available options for continuous protected attributes, our approach\nrepresents a significant contribution to the area of fair artificial\nintelligence.",
                "Towards a Unifying Model of Rationality in Multiagent Systems\nMultiagent systems deployed in the real world need to cooperate with other\nagents (including humans) nearly as effectively as these agents cooperate with\none another. To design such AI, and provide guarantees of its effectiveness, we\nneed to clearly specify what types of agents our AI must be able to cooperate\nwith. In this work we propose a generic model of socially intelligent agents,\nwhich are individually rational learners that are also able to cooperate with\none another (in the sense that their joint behavior is Pareto efficient). We\ndefine rationality in terms of the regret incurred by each agent over its\nlifetime, and show how we can construct socially intelligent agents for\ndifferent forms of regret. We then discuss the implications of this model for\nthe development of \"robust\" MAS that can cooperate with a wide variety of\nsocially intelligent agents.",
                "Perceived Trustworthiness of Natural Language Generators\nNatural Language Generation tools, such as chatbots that can generate\nhuman-like conversational text, are becoming more common both for personal and\nprofessional use. However, there are concerns about their trustworthiness and\nethical implications. The paper addresses the problem of understanding how\ndifferent users (e.g., linguists, engineers) perceive and adopt these tools and\ntheir perception of machine-generated text quality. It also discusses the\nperceived advantages and limitations of Natural Language Generation tools, as\nwell as users' beliefs on governance strategies. The main findings of this\nstudy include the impact of users' field and level of expertise on the\nperceived trust and adoption of Natural Language Generation tools, the users'\nassessment of the accuracy, fluency, and potential biases of machine-generated\ntext in comparison to human-written text, and an analysis of the advantages and\nethical risks associated with these tools as identified by the participants.\nMoreover, this paper discusses the potential implications of these findings for\nenhancing the AI development process. The paper sheds light on how different\nuser characteristics shape their beliefs on the quality and overall\ntrustworthiness of machine-generated text. Furthermore, it examines the\nbenefits and risks of these tools from the perspectives of different users.",
                "Game of Tones: Faculty detection of GPT-4 generated content in\n  university assessments\nThis study explores the robustness of university assessments against the use\nof Open AI's Generative Pre-Trained Transformer 4 (GPT-4) generated content and\nevaluates the ability of academic staff to detect its use when supported by the\nTurnitin Artificial Intelligence (AI) detection tool. The research involved\ntwenty-two GPT-4 generated submissions being created and included in the\nassessment process to be marked by fifteen different faculty members. The study\nreveals that although the detection tool identified 91% of the experimental\nsubmissions as containing some AI-generated content, the total detected content\nwas only 54.8%. This suggests that the use of adversarial techniques regarding\nprompt engineering is an effective method in evading AI detection tools and\nhighlights that improvements to AI detection software are needed. Using the\nTurnitin AI detect tool, faculty reported 54.5% of the experimental submissions\nto the academic misconduct process, suggesting the need for increased awareness\nand training into these tools. Genuine submissions received a mean score of\n54.4, whereas AI-generated content scored 52.3, indicating the comparable\nperformance of GPT-4 in real-life situations. Recommendations include adjusting\nassessment strategies to make them more resistant to the use of AI tools, using\nAI-inclusive assessment where possible, and providing comprehensive training\nprograms for faculty and students. This research contributes to understanding\nthe relationship between AI-generated content and academic assessment, urging\nfurther investigation to preserve academic integrity."
            ],
            "interesting paper": 2
        },
        {
            "papers": [
                "Intent-aligned AI systems deplete human agency: the need for agency\n  foundations research in AI safety\nThe rapid advancement of artificial intelligence (AI) systems suggests that\nartificial general intelligence (AGI) systems may soon arrive. Many researchers\nare concerned that AIs and AGIs will harm humans via intentional misuse\n(AI-misuse) or through accidents (AI-accidents). In respect of AI-accidents,\nthere is an increasing effort focused on developing algorithms and paradigms\nthat ensure AI systems are aligned to what humans intend, e.g. AI systems that\nyield actions or recommendations that humans might judge as consistent with\ntheir intentions and goals. Here we argue that alignment to human intent is\ninsufficient for safe AI systems and that preservation of long-term agency of\nhumans may be a more robust standard, and one that needs to be separated\nexplicitly and a priori during optimization. We argue that AI systems can\nreshape human intention and discuss the lack of biological and psychological\nmechanisms that protect humans from loss of agency. We provide the first formal\ndefinition of agency-preserving AI-human interactions which focuses on\nforward-looking agency evaluations and argue that AI systems - not humans -\nmust be increasingly tasked with making these evaluations. We show how agency\nloss can occur in simple environments containing embedded agents that use\ntemporal-difference learning to make action recommendations. Finally, we\npropose a new area of research called \"agency foundations\" and pose four\ninitial topics designed to improve our understanding of agency in AI-human\ninteractions: benevolent game theory, algorithmic foundations of human rights,\nmechanistic interpretability of agency representation in neural-networks and\nreinforcement learning from internal states.",
                "Disentangling and Operationalizing AI Fairness at LinkedIn\nOperationalizing AI fairness at LinkedIn's scale is challenging not only\nbecause there are multiple mutually incompatible definitions of fairness but\nalso because determining what is fair depends on the specifics and context of\nthe product where AI is deployed. Moreover, AI practitioners need clarity on\nwhat fairness expectations need to be addressed at the AI level. In this paper,\nwe present the evolving AI fairness framework used at LinkedIn to address these\nthree challenges. The framework disentangles AI fairness by separating out\nequal treatment and equitable product expectations. Rather than imposing a\ntrade-off between these two commonly opposing interpretations of fairness, the\nframework provides clear guidelines for operationalizing equal AI treatment\ncomplemented with a product equity strategy. This paper focuses on the equal AI\ntreatment component of LinkedIn's AI fairness framework, shares the principles\nthat support it, and illustrates their application through a case study. We\nhope this paper will encourage other big tech companies to join us in sharing\ntheir approach to operationalizing AI fairness at scale, so that together we\ncan keep advancing this constantly evolving field.",
                "Towards Explainable and Language-Agnostic LLMs: Symbolic Reverse\n  Engineering of Language at Scale\nLarge language models (LLMs) have achieved a milestone that undenia-bly\nchanged many held beliefs in artificial intelligence (AI). However, there\nremains many limitations of these LLMs when it comes to true language\nunderstanding, limitations that are a byproduct of the under-lying architecture\nof deep neural networks. Moreover, and due to their subsymbolic nature,\nwhatever knowledge these models acquire about how language works will always be\nburied in billions of microfeatures (weights), none of which is meaningful on\nits own, making such models hopelessly unexplainable. To address these\nlimitations, we suggest com-bining the strength of symbolic representations\nwith what we believe to be the key to the success of LLMs, namely a successful\nbottom-up re-verse engineering of language at scale. As such we argue for a\nbottom-up reverse engineering of language in a symbolic setting. Hints on what\nthis project amounts to have been suggested by several authors, and we discuss\nin some detail here how this project could be accomplished.",
                "Evaluating GPT's Programming Capability through CodeWars' Katas\nIn the burgeoning field of artificial intelligence (AI), understanding the\ncapabilities and limitations of programming-oriented models is crucial. This\npaper presents a novel evaluation of the programming proficiency of Generative\nPretrained Transformer (GPT) models, specifically GPT-3.5 and GPT-4, against\ncoding problems of varying difficulty levels drawn from Codewars. The\nexperiments reveal a distinct boundary at the 3kyu level, beyond which these\nGPT models struggle to provide solutions. These findings led to the proposal of\na measure for coding problem complexity that incorporates both problem\ndifficulty and the time required for solution. The research emphasizes the need\nfor validation and creative thinking capabilities in AI models to better\nemulate human problem-solving techniques. Future work aims to refine this\nproposed complexity measure, enhance AI models with these suggested\ncapabilities, and develop an objective measure for programming problem\ndifficulty. The results of this research offer invaluable insights for\nimproving AI programming capabilities and advancing the frontier of AI\nproblem-solving abilities.",
                "Building Extractive Question Answering System to Support Human-AI Health\n  Coaching Model for Sleep Domain\nNon-communicable diseases (NCDs) are a leading cause of global deaths,\nnecessitating a focus on primary prevention and lifestyle behavior change.\nHealth coaching, coupled with Question Answering (QA) systems, has the\npotential to transform preventive healthcare. This paper presents a\nhuman-Artificial Intelligence (AI) health coaching model incorporating a\ndomain-specific extractive QA system. A sleep-focused dataset, SleepQA, was\nmanually assembled and used to fine-tune domain-specific BERT models. The QA\nsystem was evaluated using automatic and human methods. A data-centric\nframework enhanced the system's performance by improving passage retrieval and\nquestion reformulation. Although the system did not outperform the baseline in\nautomatic evaluation, it excelled in the human evaluation of real-world\nquestions. Integration into a Human-AI health coaching model was tested in a\npilot Randomized Controlled Trial (RCT).",
                "Seeing Seeds Beyond Weeds: Green Teaming Generative AI for Beneficial\n  Uses\nLarge generative AI models (GMs) like GPT and DALL-E are trained to generate\ncontent for general, wide-ranging purposes. GM content filters are generalized\nto filter out content which has a risk of harm in many cases, e.g., hate\nspeech. However, prohibited content is not always harmful -- there are\ninstances where generating prohibited content can be beneficial. So, when GMs\nfilter out content, they preclude beneficial use cases along with harmful ones.\nWhich use cases are precluded reflects the values embedded in GM content\nfiltering. Recent work on red teaming proposes methods to bypass GM content\nfilters to generate harmful content. We coin the term green teaming to describe\nmethods of bypassing GM content filters to design for beneficial use cases. We\nshowcase green teaming by: 1) Using ChatGPT as a virtual patient to simulate a\nperson experiencing suicidal ideation, for suicide support training; 2) Using\nCodex to intentionally generate buggy solutions to train students on debugging;\nand 3) Examining an Instagram page using Midjourney to generate images of\nanti-LGBTQ+ politicians in drag. Finally, we discuss how our use cases\ndemonstrate green teaming as both a practical design method and a mode of\ncritique, which problematizes and subverts current understandings of harms and\nvalues in generative AI."
            ],
            "interesting paper": 1
        },
        {
            "papers": [
                "Responsible Design Patterns for Machine Learning Pipelines\nIntegrating ethical practices into the AI development process for artificial\nintelligence (AI) is essential to ensure safe, fair, and responsible operation.\nAI ethics involves applying ethical principles to the entire life cycle of AI\nsystems. This is essential to mitigate potential risks and harms associated\nwith AI, such as algorithm biases. To achieve this goal, responsible design\npatterns (RDPs) are critical for Machine Learning (ML) pipelines to guarantee\nethical and fair outcomes. In this paper, we propose a comprehensive framework\nincorporating RDPs into ML pipelines to mitigate risks and ensure the ethical\ndevelopment of AI systems. Our framework comprises new responsible AI design\npatterns for ML pipelines identified through a survey of AI ethics and data\nmanagement experts and validated through real-world scenarios with expert\nfeedback. The framework guides AI developers, data scientists, and\npolicy-makers to implement ethical practices in AI development and deploy\nresponsible AI systems in production.",
                "Citizen Perspectives on Necessary Safeguards to the Use of AI by Law\n  Enforcement Agencies\nIn the light of modern technological advances, Artificial Intelligence (AI)\nis relied upon to enhance performance, increase efficiency, and maximize gains.\nFor Law Enforcement Agencies (LEAs), it can prove valuable in optimizing\nevidence analysis and establishing proactive prevention measures. Nevertheless,\ncitizens raise legitimate concerns around privacy invasions, biases,\ninequalities, and inaccurate decisions. This study explores the views of 111\ncitizens towards AI use by police through interviews, and integrates societal\nconcerns along with propositions of safeguards from negative effects of AI use\nby LEAs in the context of cybercrime and terrorism.",
                "AI Imagery and the Overton Window\nAI-based text-to-image generation has undergone a significant leap in the\nproduction of visually comprehensive and aesthetic imagery over the past year,\nto the point where differentiating between a man-made piece of art and an\nAI-generated image is becoming more difficult. Generative Models such as Stable\nDiffusion, Midjourney and others are expected to affect several major\nindustries in technological and ethical aspects. Striking the balance between\nraising human standard of life and work vs exploiting one group of people to\nenrich another is a complex and crucial part of the discussion. Due to the\nrapid growth of this technology, the way in which its models operate, and gray\narea legalities, visual and artistic domains - including the video game\nindustry, are at risk of being taken over from creators by AI infrastructure\nowners. This paper is a literature review examining the concerns facing both AI\ndevelopers and users today, including identity theft, data laundering and more.\nIt discusses legalization challenges and ethical concerns, and concludes with\nhow AI generative models can be tremendously useful in streamlining the process\nof visual creativity in both static and interactive media given proper\nregulation.\n  Keywords: AI text-to-image generation, Midjourney, Stable Diffusion, AI\nEthics, Game Design, Digital Art, Data Laundering",
                "From Human-Centered to Social-Centered Artificial Intelligence:\n  Assessing ChatGPT's Impact through Disruptive Events\nLarge language models (LLMs) and dialogue agents have existed for years, but\nthe release of recent GPT models has been a watershed moment for artificial\nintelligence (AI) research and society at large. Immediately recognized for its\ngenerative capabilities and versatility, ChatGPT's impressive proficiency\nacross technical and creative domains led to its widespread adoption. While\nsociety grapples with the emerging cultural impacts of ChatGPT, critiques of\nChatGPT's impact within the machine learning community have coalesced around\nits performance or other conventional Responsible AI evaluations relating to\nbias, toxicity, and 'hallucination.' We argue that these latter critiques draw\nheavily on a particular conceptualization of the 'human-centered' framework,\nwhich tends to cast atomized individuals as the key recipients of both the\nbenefits and detriments of technology. In this article, we direct attention to\nanother dimension of LLMs and dialogue agents' impact: their effect on social\ngroups, institutions, and accompanying norms and practices. By illustrating\nChatGPT's social impact through three disruptive events, we challenge\nindividualistic approaches in AI development and contribute to ongoing debates\naround the ethical and responsible implementation of AI systems. We hope this\neffort will call attention to more comprehensive and longitudinal evaluation\ntools and compel technologists to go beyond human-centered thinking and ground\ntheir efforts through social-centered AI.",
                "Human Control: Definitions and Algorithms\nHow can humans stay in control of advanced artificial intelligence systems?\nOne proposal is corrigibility, which requires the agent to follow the\ninstructions of a human overseer, without inappropriately influencing them. In\nthis paper, we formally define a variant of corrigibility called shutdown\ninstructability, and show that it implies appropriate shutdown behavior,\nretention of human autonomy, and avoidance of user harm. We also analyse the\nrelated concepts of non-obstruction and shutdown alignment, three previously\nproposed algorithms for human control, and one new algorithm.",
                "EMOTE: An Explainable architecture for Modelling the Other Through\n  Empathy\nWe can usually assume others have goals analogous to our own. This assumption\ncan also, at times, be applied to multi-agent games - e.g. Agent 1's attraction\nto green pellets is analogous to Agent 2's attraction to red pellets. This\n\"analogy\" assumption is tied closely to the cognitive process known as empathy.\nInspired by empathy, we design a simple and explainable architecture to model\nanother agent's action-value function. This involves learning an \"Imagination\nNetwork\" to transform the other agent's observed state in order to produce a\nhuman-interpretable \"empathetic state\" which, when presented to the learning\nagent, produces behaviours that mimic the other agent. Our approach is\napplicable to multi-agent scenarios consisting of a single learning agent and\nother (independent) agents acting according to fixed policies. This\narchitecture is particularly beneficial for (but not limited to) algorithms\nusing a composite value or reward function. We show our method produces better\nperformance in multi-agent games, where it robustly estimates the other's model\nin different environment configurations. Additionally, we show that the\nempathetic states are human interpretable, and thus verifiable."
            ],
            "interesting paper": 1
        },
        {
            "papers": [
                "The ethical ambiguity of AI data enrichment: Measuring gaps in research\n  ethics norms and practices\nThe technical progression of artificial intelligence (AI) research has been\nbuilt on breakthroughs in fields such as computer science, statistics, and\nmathematics. However, in the past decade AI researchers have increasingly\nlooked to the social sciences, turning to human interactions to solve the\nchallenges of model development. Paying crowdsourcing workers to generate or\ncurate data, or data enrichment, has become indispensable for many areas of AI\nresearch, from natural language processing to reinforcement learning from human\nfeedback (RLHF). Other fields that routinely interact with crowdsourcing\nworkers, such as Psychology, have developed common governance requirements and\nnorms to ensure research is undertaken ethically. This study explores how, and\nto what extent, comparable research ethics requirements and norms have\ndeveloped for AI research and data enrichment. We focus on the approach taken\nby two leading conferences: ICLR and NeurIPS, and journal publisher Springer.\nIn a longitudinal study of accepted papers, and via a comparison with\nPsychology and CHI papers, this work finds that leading AI venues have begun to\nestablish protocols for human data collection, but these are are inconsistently\nfollowed by authors. Whilst Psychology papers engaging with crowdsourcing\nworkers frequently disclose ethics reviews, payment data, demographic data and\nother information, similar disclosures are far less common in leading AI venues\ndespite similar guidance. The work concludes with hypotheses to explain these\ngaps in research ethics practices and considerations for its implications.",
                "AI Liability Insurance With an Example in AI-Powered E-diagnosis System\nArtificial Intelligence (AI) has received an increasing amount of attention\nin multiple areas. The uncertainties and risks in AI-powered systems have\ncreated reluctance in their wild adoption. As an economic solution to\ncompensate for potential damages, AI liability insurance is a promising market\nto enhance the integration of AI into daily life. In this work, we use an\nAI-powered E-diagnosis system as an example to study AI liability insurance. We\nprovide a quantitative risk assessment model with evidence-based numerical\nanalysis. We discuss the insurability criteria for AI technologies and suggest\nnecessary adjustments to accommodate the features of AI products. We show that\nAI liability insurance can act as a regulatory mechanism to incentivize\ncompliant behaviors and serve as a certificate of high-quality AI systems.\nFurthermore, we suggest premium adjustment to reflect the dynamic evolution of\nthe inherent uncertainty in AI. Moral hazard problems are discussed and\nsuggestions for AI liability insurance are provided.",
                "AI and the creative realm: A short review of current and future\n  applications\nThis study explores the concept of creativity and artificial intelligence\n(AI) and their recent integration. While AI has traditionally been perceived as\nincapable of generating new ideas or creating art, the development of more\nsophisticated AI models and the proliferation of human-computer interaction\ntools have opened up new possibilities for AI in artistic creation. This study\ninvestigates the various applications of AI in a creative context,\ndifferentiating between the type of art, language, and algorithms used. It also\nconsiders the philosophical implications of AI and creativity, questioning\nwhether consciousness can be researched in machines and AI's potential\ninterests and decision-making capabilities. Overall, we aim to stimulate a\nreflection on AI's use and ethical implications in creative contexts.",
                "Navigating Fairness in Radiology AI: Concepts, Consequences,and Crucial\n  Considerations\nArtificial Intelligence (AI) has significantly revolutionized radiology,\npromising improved patient outcomes and streamlined processes. However, it's\ncritical to ensure the fairness of AI models to prevent stealthy bias and\ndisparities from leading to unequal outcomes. This review discusses the concept\nof fairness in AI, focusing on bias auditing using the Aequitas toolkit, and\nits real-world implications in radiology, particularly in disease screening\nscenarios. Aequitas, an open-source bias audit toolkit, scrutinizes AI models'\ndecisions, identifying hidden biases that may result in disparities across\ndifferent demographic groups and imaging equipment brands. This toolkit\noperates on statistical theories, analyzing a large dataset to reveal a model's\nfairness. It excels in its versatility to handle various variables\nsimultaneously, especially in a field as diverse as radiology. The review\nexplicates essential fairness metrics: Equal and Proportional Parity, False\nPositive Rate Parity, False Discovery Rate Parity, False Negative Rate Parity,\nand False Omission Rate Parity. Each metric serves unique purposes and offers\ndifferent insights. We present hypothetical scenarios to demonstrate their\nrelevance in disease screening settings, and how disparities can lead to\nsignificant real-world impacts.",
                "Interpretable and Explainable Logical Policies via Neurally Guided\n  Symbolic Abstraction\nThe limited priors required by neural networks make them the dominating\nchoice to encode and learn policies using reinforcement learning (RL). However,\nthey are also black-boxes, making it hard to understand the agent's behaviour,\nespecially when working on the image level. Therefore, neuro-symbolic RL aims\nat creating policies that are interpretable in the first place. Unfortunately,\ninterpretability is not explainability. To achieve both, we introduce Neurally\ngUided Differentiable loGic policiEs (NUDGE). NUDGE exploits trained neural\nnetwork-based agents to guide the search of candidate-weighted logic rules,\nthen uses differentiable logic to train the logic agents. Our experimental\nevaluation demonstrates that NUDGE agents can induce interpretable and\nexplainable policies while outperforming purely neural ones and showing good\nflexibility to environments of different initial states and problem sizes.",
                "ExTRUST: Reducing Exploit Stockpiles with a Privacy-Preserving Depletion\n  System for Inter-State Relationships\nCyberspace is a fragile construct threatened by malicious cyber operations of\ndifferent actors, with vulnerabilities in IT hardware and software forming the\nbasis for such activities, thus also posing a threat to global IT security.\nAdvancements in the field of artificial intelligence accelerate this\ndevelopment, either with artificial intelligence enabled cyber weapons,\nautomated cyber defense measures, or artificial intelligence-based threat and\nvulnerability detection. Especially state actors, with their long-term\nstrategic security interests, often stockpile such knowledge of vulnerabilities\nand exploits to enable their military or intelligence service cyberspace\noperations. While treaties and regulations to limit these developments and to\nenhance global IT security by disclosing vulnerabilities are currently being\ndiscussed on the international level, these efforts are hindered by state\nconcerns about the disclosure of unique knowledge and about giving up tactical\nadvantages. This leads to a situation where multiple states are likely to\nstockpile at least some identical exploits, with technical measures to enable a\ndepletion process for these stockpiles that preserve state secrecy interests\nand consider the special constraints of interacting states as well as the\nrequirements within such environments being non-existent. This paper proposes\nsuch a privacy-preserving approach that allows multiple state parties to\nprivately compare their stock of vulnerabilities and exploits to check for\nitems that occur in multiple stockpiles without revealing them so that their\ndisclosure can be considered. We call our system ExTRUST and show that it is\nscalable and can withstand several attack scenarios. Beyond the\nintergovernmental setting, ExTRUST can also be used for other zero-trust use\ncases, such as bug-bounty programs."
            ],
            "interesting paper": 1
        },
        {
            "papers": [
                "Enough With \"Human-AI Collaboration\"\nDescribing our interaction with Artificial Intelligence (AI) systems as\n'collaboration' is well-intentioned, but flawed. Not only is it misleading, but\nit also takes away the credit of AI 'labour' from the humans behind it, and\nerases and obscures an often exploitative arrangement between AI producers and\nconsumers. The AI 'collaboration' metaphor is merely the latest episode in a\nlong history of labour appropriation and credit reassignment that\ndisenfranchises labourers in the Global South. I propose that viewing AI as a\ntool or an instrument, rather than a collaborator, is more accurate, and\nultimately fairer.",
                "XAI Renaissance: Redefining Interpretability in Medical Diagnostic\n  Models\nAs machine learning models become increasingly prevalent in medical\ndiagnostics, the need for interpretability and transparency becomes paramount.\nThe XAI Renaissance signifies a significant shift in the field, aiming to\nredefine the interpretability of medical diagnostic models. This paper explores\nthe innovative approaches and methodologies within the realm of Explainable AI\n(XAI) that are revolutionizing the interpretability of medical diagnostic\nmodels. By shedding light on the underlying decision-making process, XAI\ntechniques empower healthcare professionals to understand, trust, and\neffectively utilize these models for accurate and reliable medical diagnoses.\nThis review highlights the key advancements in XAI for medical diagnostics and\ntheir potential to transform the healthcare landscape, ultimately improving\npatient outcomes and fostering trust in AI-driven diagnostic systems.",
                "Accelerating science with human-aware artificial intelligence\nArtificial intelligence (AI) models trained on published scientific findings\nhave been used to invent valuable materials and targeted therapies, but they\ntypically ignore the human scientists who continually alter the landscape of\ndiscovery. Here we show that incorporating the distribution of human expertise\nby training unsupervised models on simulated inferences cognitively accessible\nto experts dramatically improves (up to 400%) AI prediction of future\ndiscoveries beyond those focused on research content alone, especially when\nrelevant literature is sparse. These models succeed by predicting human\npredictions and the scientists who will make them. By tuning human-aware AI to\navoid the crowd, we can generate scientifically promising \"alien\" hypotheses\nunlikely to be imagined or pursued without intervention until the distant\nfuture, which hold promise to punctuate scientific advance beyond questions\ncurrently pursued. Accelerating human discovery or probing its blind spots,\nhuman-aware AI enables us to move toward and beyond the contemporary scientific\nfrontier.",
                "AI Transparency in the Age of LLMs: A Human-Centered Research Roadmap\nThe rise of powerful large language models (LLMs) brings about tremendous\nopportunities for innovation but also looming risks for individuals and society\nat large. We have reached a pivotal moment for ensuring that LLMs and\nLLM-infused applications are developed and deployed responsibly. However, a\ncentral pillar of responsible AI -- transparency -- is largely missing from the\ncurrent discourse around LLMs. It is paramount to pursue new approaches to\nprovide transparency for LLMs, and years of research at the intersection of AI\nand human-computer interaction (HCI) highlight that we must do so with a\nhuman-centered perspective: Transparency is fundamentally about supporting\nappropriate human understanding, and this understanding is sought by different\nstakeholders with different goals in different contexts. In this new era of\nLLMs, we must develop and design approaches to transparency by considering the\nneeds of stakeholders in the emerging LLM ecosystem, the novel types of\nLLM-infused applications being built, and the new usage patterns and challenges\naround LLMs, all while building on lessons learned about how people process,\ninteract with, and make use of information. We reflect on the unique challenges\nthat arise in providing transparency for LLMs, along with lessons learned from\nHCI and responsible AI research that has taken a human-centered perspective on\nAI transparency. We then lay out four common approaches that the community has\ntaken to achieve transparency -- model reporting, publishing evaluation\nresults, providing explanations, and communicating uncertainty -- and call out\nopen questions around how these approaches may or may not be applied to LLMs.\nWe hope this provides a starting point for discussion and a useful roadmap for\nfuture research.",
                "OMNI: Open-endedness via Models of human Notions of Interestingness\nOpen-ended algorithms aim to learn new, interesting behaviors forever. That\nrequires a vast environment search space, but there are thus infinitely many\npossible tasks. Even after filtering for tasks the current agent can learn\n(i.e., learning progress), countless learnable yet uninteresting tasks remain\n(e.g., minor variations of previously learned tasks). An Achilles Heel of\nopen-endedness research is the inability to quantify (and thus prioritize)\ntasks that are not just learnable, but also $\\textit{interesting}$ (e.g.,\nworthwhile and novel). We propose solving this problem by\n$\\textit{Open-endedness via Models of human Notions of Interestingness}$\n(OMNI). The insight is that we can utilize foundation models (FMs) as a model\nof interestingness (MoI), because they $\\textit{already}$ internalize human\nconcepts of interestingness from training on vast amounts of human-generated\ndata, where humans naturally write about what they find interesting or boring.\nWe show that FM-based MoIs improve open-ended learning by focusing on tasks\nthat are both learnable $\\textit{and interesting}$, outperforming baselines\nbased on uniform task sampling or learning progress alone. This approach has\nthe potential to dramatically advance the ability to intelligently select which\ntasks to focus on next (i.e., auto-curricula), and could be seen as AI\nselecting its own next task to learn, facilitating self-improving AI and\nAI-Generating Algorithms. Project website at https://www.jennyzhangzt.com/omni/",
                "Milestones in Autonomous Driving and Intelligent Vehicles Part II:\n  Perception and Planning\nGrowing interest in autonomous driving (AD) and intelligent vehicles (IVs) is\nfueled by their promise for enhanced safety, efficiency, and economic benefits.\nWhile previous surveys have captured progress in this field, a comprehensive\nand forward-looking summary is needed. Our work fills this gap through three\ndistinct articles. The first part, a \"Survey of Surveys\" (SoS), outlines the\nhistory, surveys, ethics, and future directions of AD and IV technologies. The\nsecond part, \"Milestones in Autonomous Driving and Intelligent Vehicles Part I:\nControl, Computing System Design, Communication, HD Map, Testing, and Human\nBehaviors\" delves into the development of control, computing system,\ncommunication, HD map, testing, and human behaviors in IVs. This part, the\nthird part, reviews perception and planning in the context of IVs. Aiming to\nprovide a comprehensive overview of the latest advancements in AD and IVs, this\nwork caters to both newcomers and seasoned researchers. By integrating the SoS\nand Part I, we offer unique insights and strive to serve as a bridge between\npast achievements and future possibilities in this dynamic field."
            ],
            "interesting paper": 4
        }
    ],
    "Casey Brown": [
        {
            "papers": [
                "Deep Learning and Ethics\nThis article appears as chapter 21 of Prince (2023, Understanding Deep\nLearning); a complete draft of the textbook is available here:\nhttp://udlbook.com. This chapter considers potential harms arising from the\ndesign and use of AI systems. These include algorithmic bias, lack of\nexplainability, data privacy violations, militarization, fraud, and\nenvironmental concerns. The aim is not to provide advice on being more ethical.\nInstead, the goal is to express ideas and start conversations in key areas that\nhave received attention in philosophy, political science, and the broader\nsocial sciences.",
                "Towards a Capability Assessment Model for the Comprehension and Adoption\n  of AI in Organisations\nThe comprehension and adoption of Artificial Intelligence (AI) are beset with\npractical and ethical problems. This article presents a 5-level AI Capability\nAssessment Model (AI-CAM) and a related AI Capabilities Matrix (AI-CM) to\nassist practitioners in AI comprehension and adoption. These practical tools\nwere developed with business executives, technologists, and other\norganisational stakeholders in mind. They are founded on a comprehensive\nconception of AI compared to those in other AI adoption models and are also\nopen-source artefacts. Thus, the AI-CAM and AI-CM present an accessible\nresource to help inform organisational decision-makers on the capability\nrequirements for (1) AI-based data analytics use cases based on machine\nlearning technologies; (2) Knowledge representation to engineer and represent\ndata, information and knowledge using semantic technologies; and (3) AI-based\nsolutions that seek to emulate human reasoning and decision-making. The AI-CAM\ncovers the core capability dimensions (business, data, technology,\norganisation, AI skills, risks, and ethical considerations) required at the\nfive capability maturity levels to achieve optimal use of AI in organisations.",
                "Trends and Challenges Towards an Effective Data-Driven Decision Making\n  in UK SMEs: Case Studies and Lessons Learnt from the Analysis of 85 SMEs\nThe adoption of data science brings vast benefits to Small and Medium-sized\nEnterprises (SMEs) including business productivity, economic growth, innovation\nand jobs creation. Data Science can support SMEs to optimise production\nprocesses, anticipate customers' needs, predict machinery failures and deliver\nefficient smart services. Businesses can also harness the power of Artificial\nIntelligence (AI) and Big Data and the smart use of digital technologies to\nenhance productivity and performance, paving the way for innovation. However,\nintegrating data science decisions into an SME requires both skills and IT\ninvestments. In most cases, such expenses are beyond the means of SMEs due to\nlimited resources and restricted access to financing. This paper presents\ntrends and challenges towards an effective data-driven decision making for\norganisations based on a case study of 85 SMEs, mostly from the West Midlands\nregion of England. The work is supported as part of a 3 years ERDF (European\nRegional Development Funded project) in the areas of big data management,\nanalytics and business intelligence. We present two case studies that\ndemonstrates the potential of Digitisation, AI and Machine Learning and use\nthese as examples to unveil challenges and showcase the wealth of current\navailable opportunities for SMEs.",
                "Applications of Intelligent Systems in Green Technology\nIntelligent Systems (ISs) are technologically advanced machines, which\nperceive and respond to the environment around them. They are usually of\nvarious forms ranging from software to hardware. ISs are generally the fusion\nof Artificial Intelligence (AI), robotics and Internet of things (IoT). In\norder to strengthen ISs, one of the key technologies is green technology (GT).\nIt refers to the continuously advancing methods and materials, which cover\ntechniques for producing energy to non-toxic cleaning products. It may also be\nbroadened to saving energy, and reducing toxic and waste materials in the\nenvironment. The motto of GT can be achieved by using the ISs. In this paper,\nwe present various applications of ISs in GT. Moreover, we discuss various\npossible solutions using ISs in order to overcome the on-going real-life\nproblems.",
                "Model evaluation for extreme risks\nCurrent approaches to building general-purpose AI systems tend to produce\nsystems with both beneficial and harmful capabilities. Further progress in AI\ndevelopment could lead to capabilities that pose extreme risks, such as\noffensive cyber capabilities or strong manipulation skills. We explain why\nmodel evaluation is critical for addressing extreme risks. Developers must be\nable to identify dangerous capabilities (through \"dangerous capability\nevaluations\") and the propensity of models to apply their capabilities for harm\n(through \"alignment evaluations\"). These evaluations will become critical for\nkeeping policymakers and other stakeholders informed, and for making\nresponsible decisions about model training, deployment, and security.",
                "Distributed Online Rollout for Multivehicle Routing in Unmapped\n  Environments\nIn this work we consider a generalization of the well-known multivehicle\nrouting problem: given a network, a set of agents occupying a subset of its\nnodes, and a set of tasks, we seek a minimum cost sequence of movements subject\nto the constraint that each task is visited by some agent at least once. The\nclassical version of this problem assumes a central computational server that\nobserves the entire state of the system perfectly and directs individual agents\naccording to a centralized control scheme. In contrast, we assume that there is\nno centralized server and that each agent is an individual processor with no a\npriori knowledge of the underlying network (including task and agent\nlocations). Moreover, our agents possess strictly local communication and\nsensing capabilities (restricted to a fixed radius around their respective\nlocations), aligning more closely with several real-world multiagent\napplications. These restrictions introduce many challenges that are overcome\nthrough local information sharing and direct coordination between agents. We\npresent a fully distributed, online, and scalable reinforcement learning\nalgorithm for this problem whereby agents self-organize into local clusters and\nindependently apply a multiagent rollout scheme locally to each cluster. We\ndemonstrate empirically via extensive simulations that there exists a critical\nsensing radius beyond which the distributed rollout algorithm begins to improve\nover a greedy base policy. This critical sensing radius grows proportionally to\nthe $\\log^*$ function of the size of the network, and is, therefore, a small\nconstant for any relevant network. Our decentralized reinforcement learning\nalgorithm achieves approximately a factor of two cost improvement over the base\npolicy for a range of radii bounded from below and above by two and three times\nthe critical sensing radius, respectively."
            ],
            "interesting paper": 3
        },
        {
            "papers": [
                "Alert of the Second Decision-maker: An Introduction to Human-AI Conflict\nThe collaboration between humans and artificial intelligence (AI) is a\nsignificant feature in this digital age. However, humans and AI may have\nobservation, interpretation, and action conflicts when working synchronously.\nThis phenomenon is often masked by faults and, unfortunately, overlooked. This\npaper systematically introduces the human-AI conflict concept, causes,\nmeasurement methods, and risk assessment. The results highlight that there is a\npotential second decision-maker besides the human, which is the AI; the\nhuman-AI conflict is a unique and emerging risk in digitalized process systems;\nand this is an interdisciplinary field that needs to be distinguished from\ntraditional fault and failure analysis; the conflict risk is significant and\ncannot be ignored.",
                "AI Techniques in the Microservices Life-Cycle: A Survey\nMicroservices is a popular architectural style for the development of\ndistributed software, with an emphasis on modularity, scalability, and\nflexibility. Indeed, in microservice systems, functionalities are provided by\nloosely coupled, small services, each focusing on a specific business\ncapability. Building a system according to the microservices architectural\nstyle brings a number of challenges, mainly related to how the different\nmicroservices are deployed and coordinated and how they interact. In this\npaper, we provide a survey about how techniques in the area of Artificial\nIntelligence have been used to tackle these challenges.",
                "Attention Paper: How Generative AI Reshapes Digital Shadow Industry?\nThe rapid development of digital economy has led to the emergence of various\nblack and shadow internet industries, which pose potential risks that can be\nidentified and managed through digital risk management (DRM) that uses\ndifferent techniques such as machine learning and deep learning. The evolution\nof DRM architecture has been driven by changes in data forms. However, the\ndevelopment of AI-generated content (AIGC) technology, such as ChatGPT and\nStable Diffusion, has given black and shadow industries powerful tools to\npersonalize data and generate realistic images and conversations for fraudulent\nactivities. This poses a challenge for DRM systems to control risks from the\nsource of data generation and to respond quickly to the fast-changing risk\nenvironment. This paper aims to provide a technical analysis of the challenges\nand opportunities of AIGC from upstream, midstream, and downstream paths of\nblack/shadow industries and suggest future directions for improving existing\nrisk control systems. The paper will explore the new black and shadow\ntechniques triggered by generative AI technology and provide insights for\nbuilding the next-generation DRM system.",
                "On Computing Universal Plans for Partially Observable Multi-Agent Path\n  Finding\nMulti-agent routing problems have drawn significant attention nowadays due to\ntheir broad industrial applications in, e.g., warehouse robots, logistics\nautomation, and traffic control. Conventionally, they are modelled as classical\nplanning problems. In this paper, we argue that it is beneficial to formulate\nthem as universal planning problems. We therefore propose universal plans, also\nknown as policies, as the solution concepts, and implement a system called\nASP-MAUPF (Answer Set Programming for Multi-Agent Universal Plan Finding) for\ncomputing them. Given an arbitrary two-dimensional map and a profile of goals\nfor the agents, the system finds a feasible universal plan for each agent that\nensures no collision with others. We use the system to conduct some\nexperiments, and make some observations on the types of goal profiles and\nenvironments that will have feasible policies, and how they may depend on\nagents' sensors. We also demonstrate how users can customize action preferences\nto compute more efficient policies, even (near-)optimal ones.",
                "Enhancing Human Capabilities through Symbiotic Artificial Intelligence\n  with Shared Sensory Experiences\nThe merging of human intelligence and artificial intelligence has long been a\nsubject of interest in both science fiction and academia. In this paper, we\nintroduce a novel concept in Human-AI interaction called Symbiotic Artificial\nIntelligence with Shared Sensory Experiences (SAISSE), which aims to establish\na mutually beneficial relationship between AI systems and human users through\nshared sensory experiences. By integrating multiple sensory input channels and\nprocessing human experiences, SAISSE fosters a strong human-AI bond, enabling\nAI systems to learn from and adapt to individual users, providing personalized\nsupport, assistance, and enhancement. Furthermore, we discuss the incorporation\nof memory storage units for long-term growth and development of both the AI\nsystem and its human user. As we address user privacy and ethical guidelines\nfor responsible AI-human symbiosis, we also explore potential biases and\ninequalities in AI-human symbiosis and propose strategies to mitigate these\nchallenges. Our research aims to provide a comprehensive understanding of the\nSAISSE concept and its potential to effectively support and enhance individual\nhuman users through symbiotic AI systems. This position article aims at\ndiscussing poteintial AI-human interaction related topics within the scientific\ncommunity, rather than providing experimental or theoretical results.",
                "Applying Interdisciplinary Frameworks to Understand Algorithmic\n  Decision-Making\nWe argue that explanations for \"algorithmic decision-making\" (ADM) systems\ncan profit by adopting practices that are already used in the learning\nsciences. We shortly introduce the importance of explaining ADM systems, give a\nbrief overview of approaches drawing from other disciplines to improve\nexplanations, and present the results of our qualitative task-based study\nincorporating the \"six facets of understanding\" framework. We close with\nquestions guiding the discussion of how future studies can leverage an\ninterdisciplinary approach."
            ],
            "interesting paper": 3
        },
        {
            "papers": [
                "Towards Cognitive Bots: Architectural Research Challenges\nSoftware bots operating in multiple virtual digital platforms must understand\nthe platforms' affordances and behave like human users. Platform affordances or\nfeatures differ from one application platform to another or through a life\ncycle, requiring such bots to be adaptable. Moreover, bots in such platforms\ncould cooperate with humans or other software agents for work or to learn\nspecific behavior patterns. However, present-day bots, particularly chatbots,\nother than language processing and prediction, are far from reaching a human\nuser's behavior level within complex business information systems. They lack\nthe cognitive capabilities to sense and act in such virtual environments,\nrendering their development a challenge to artificial general intelligence\nresearch. In this study, we problematize and investigate assumptions in\nconceptualizing software bot architecture by directing attention to significant\narchitectural research challenges in developing cognitive bots endowed with\ncomplex behavior for operation on information systems. As an outlook, we\npropose alternate architectural assumptions to consider in future bot design\nand bot development frameworks.",
                "How Do UX Practitioners Communicate AI as a Design Material? Artifacts,\n  Conceptions, and Propositions\nUX practitioners (UXPs) face novel challenges when working with and\ncommunicating artificial intelligence (AI) as a design material. We explore how\nUXPs communicate AI concepts when given hands-on experience training and\nexperimenting with AI models. To do so, we conducted a task-based design study\nwith 27 UXPs in which they prototyped and created a design presentation for a\nAI-enabled interface while having access to a simple AI model training tool.\nThrough analyzing UXPs' design presentations and post-activity interviews, we\nfound that although UXPs struggled to clearly communicate some AI concepts,\ntinkering with AI broadened common ground when communicating with technical\nstakeholders. UXPs also identified key risks and benefits of AI in their\ndesigns, and proposed concrete next steps for both UX and AI work. We conclude\nwith a sensitizing concept and recommendations for design and AI tools to\nenhance multi-stakeholder communication and collaboration when crafting\nhuman-centered AI experiences.",
                "Mindstorms in Natural Language-Based Societies of Mind\nBoth Minsky's \"society of mind\" and Schmidhuber's \"learning to think\" inspire\ndiverse societies of large multimodal neural networks (NNs) that solve problems\nby interviewing each other in a \"mindstorm.\" Recent implementations of NN-based\nsocieties of minds consist of large language models (LLMs) and other NN-based\nexperts communicating through a natural language interface. In doing so, they\novercome the limitations of single LLMs, improving multimodal zero-shot\nreasoning. In these natural language-based societies of mind (NLSOMs), new\nagents -- all communicating through the same universal symbolic language -- are\neasily added in a modular fashion. To demonstrate the power of NLSOMs, we\nassemble and experiment with several of them (having up to 129 members),\nleveraging mindstorms in them to solve some practical AI tasks: visual question\nanswering, image captioning, text-to-image synthesis, 3D generation, egocentric\nretrieval, embodied AI, and general language-based task solving. We view this\nas a starting point towards much larger NLSOMs with billions of agents-some of\nwhich may be humans. And with this emergence of great societies of\nheterogeneous minds, many new research questions have suddenly become paramount\nto the future of artificial intelligence. What should be the social structure\nof an NLSOM? What would be the (dis)advantages of having a monarchical rather\nthan a democratic structure? How can principles of NN economies be used to\nmaximize the total reward of a reinforcement learning NLSOM? In this work, we\nidentify, discuss, and try to answer some of these questions.",
                "MADiff: Offline Multi-agent Learning with Diffusion Models\nDiffusion model (DM) recently achieved huge success in various scenarios\nincluding offline reinforcement learning, where the diffusion planner learn to\ngenerate desired trajectories during online evaluations. However, despite the\neffectiveness in single-agent learning, it remains unclear how DMs can operate\nin multi-agent problems, where agents can hardly complete teamwork without good\ncoordination by independently modeling each agent's trajectories. In this\npaper, we propose MADiff, a novel generative multi-agent learning framework to\ntackle this problem. MADiff is realized with an attention-based diffusion model\nto model the complex coordination among behaviors of multiple agents. To the\nbest of our knowledge, MADiff is the first diffusion-based multi-agent learning\nframework, which behaves as both a decentralized policy and a centralized\ncontroller. During decentralized executions, MADiff simultaneously performs\nteammate modeling, and the centralized controller can also be applied in\nmulti-agent trajectory predictions. Our experiments show the superior\nperformance of MADiff compared to baseline algorithms in a wide range of\nmulti-agent learning tasks, which emphasizes the effectiveness of MADiff in\nmodeling complex multi-agent interactions. Our code is available at\nhttps://github.com/zbzhu99/madiff.",
                "Attention Schema in Neural Agents\nAttention has become a common ingredient in deep learning architectures. It\nadds a dynamical selection of information on top of the static selection of\ninformation supported by weights. In the same way, we can imagine a\nhigher-order informational filter built on top of attention: an Attention\nSchema (AS), namely, a descriptive and predictive model of attention. In\ncognitive neuroscience, Attention Schema Theory (AST) supports this idea of\ndistinguishing attention from AS. A strong prediction of this theory is that an\nagent can use its own AS to also infer the states of other agents' attention\nand consequently enhance coordination with other agents. As such, multi-agent\nreinforcement learning would be an ideal setting to experimentally test the\nvalidity of AST. We explore different ways in which attention and AS interact\nwith each other. Our preliminary results indicate that agents that implement\nthe AS as a recurrent internal control achieve the best performance. In\ngeneral, these exploratory experiments suggest that equipping artificial agents\nwith a model of attention can enhance their social intelligence.",
                "A Categorical Representation Language and Computational System for\n  Knowledge-Based Planning\nClassical planning representation languages based on first-order logic have\npreliminarily been used to model and solve robotic task planning problems.\nWider adoption of these representation languages, however, is hindered by the\nlimitations present when managing implicit world changes with concise action\nmodels. To address this problem, we propose an alternative approach to\nrepresenting and managing updates to world states during planning. Based on the\ncategory-theoretic concepts of $\\mathsf{C}$-sets and double-pushout rewriting\n(DPO), our proposed representation can effectively handle structured knowledge\nabout world states that support domain abstractions at all levels. It\nformalizes the semantics of predicates according to a user-provided ontology\nand preserves the semantics when transitioning between world states. This\nmethod provides a formal semantics for using knowledge graphs and relational\ndatabases to model world states and updates in planning. In this paper, we\nconceptually compare our category-theoretic representation with the classical\nplanning representation. We show that our proposed representation has\nadvantages over the classical representation in terms of handling implicit\npreconditions and effects, and provides a more structured framework in which to\nmodel and solve planning problems."
            ],
            "interesting paper": 3
        },
        {
            "papers": [
                "AI Coach Assist: An Automated Approach for Call Recommendation in\n  Contact Centers for Agent Coaching\nIn recent years, the utilization of Artificial Intelligence (AI) in the\ncontact center industry is on the rise. One area where AI can have a\nsignificant impact is in the coaching of contact center agents. By analyzing\ncall transcripts using Natural Language Processing (NLP) techniques, it would\nbe possible to quickly determine which calls are most relevant for coaching\npurposes. In this paper, we present AI Coach Assist, which leverages the\npre-trained transformer-based language models to determine whether a given call\nis coachable or not based on the quality assurance (QA) questions asked by the\ncontact center managers or supervisors. The system was trained and evaluated on\na large dataset collected from real-world contact centers and provides an\neffective way to recommend calls to the contact center managers that are more\nlikely to contain coachable moments. Our experimental findings demonstrate the\npotential of AI Coach Assist to improve the coaching process, resulting in\nenhancing the performance of contact center agents.",
                "MLOps: A Step Forward to Enterprise Machine Learning\nMachine Learning Operations (MLOps) is becoming a highly crucial part of\nbusinesses looking to capitalize on the benefits of AI and ML models. This\nresearch presents a detailed review of MLOps, its benefits, difficulties,\nevolutions, and important underlying technologies such as MLOps frameworks,\nDocker, GitHub actions, and Kubernetes. The MLOps workflow, which includes\nmodel design, deployment, and operations, is explained in detail along with the\nvarious tools necessary for both model and data exploration and deployment.\nThis article also puts light on the end-to-end production of ML projects using\nvarious maturity levels of automated pipelines, with the least at no automation\nat all and the highest with complete CI/CD and CT capabilities. Furthermore, a\ndetailed example of an enterprise-level MLOps project for an object detection\nservice is used to explain the workflow of the technology in a real-world\nscenario. For this purpose, a web application hosting a pre-trained model from\nTensorFlow 2 Model Zoo is packaged and deployed to the internet making sure\nthat the system is scalable, reliable, and optimized for deployment at an\nenterprise level.",
                "Modeling Dynamic Environments with Scene Graph Memory\nEmbodied AI agents that search for objects in large environments such as\nhouseholds often need to make efficient decisions by predicting object\nlocations based on partial information. We pose this as a new type of link\nprediction problem: link prediction on partially observable dynamic graphs. Our\ngraph is a representation of a scene in which rooms and objects are nodes, and\ntheir relationships are encoded in the edges; only parts of the changing graph\nare known to the agent at each timestep. This partial observability poses a\nchallenge to existing link prediction approaches, which we address. We propose\na novel state representation -- Scene Graph Memory (SGM) -- with captures the\nagent's accumulated set of observations, as well as a neural net architecture\ncalled a Node Edge Predictor (NEP) that extracts information from the SGM to\nsearch efficiently. We evaluate our method in the Dynamic House Simulator, a\nnew benchmark that creates diverse dynamic graphs following the semantic\npatterns typically seen at homes, and show that NEP can be trained to predict\nthe locations of objects in a variety of environments with diverse object\nmovement dynamics, outperforming baselines both in terms of new scene\nadaptability and overall accuracy. The codebase and more can be found at\nhttps://www.scenegraphmemory.com.",
                "Integrating Action Knowledge and LLMs for Task Planning and Situation\n  Handling in Open Worlds\nTask planning systems have been developed to help robots use human knowledge\n(about actions) to complete long-horizon tasks. Most of them have been\ndeveloped for \"closed worlds\" while assuming the robot is provided with\ncomplete world knowledge. However, the real world is generally open, and the\nrobots frequently encounter unforeseen situations that can potentially break\nthe planner's completeness. Could we leverage the recent advances on\npre-trained Large Language Models (LLMs) to enable classical planning systems\nto deal with novel situations?\n  This paper introduces a novel framework, called COWP, for open-world task\nplanning and situation handling. COWP dynamically augments the robot's action\nknowledge, including the preconditions and effects of actions, with\ntask-oriented commonsense knowledge. COWP embraces the openness from LLMs, and\nis grounded to specific domains via action knowledge. For systematic\nevaluations, we collected a dataset that includes 1,085 execution-time\nsituations. Each situation corresponds to a state instance wherein a robot is\npotentially unable to complete a task using a solution that normally works.\nExperimental results show that our approach outperforms competitive baselines\nfrom the literature in the success rate of service tasks. Additionally, we have\ndemonstrated COWP using a mobile manipulator. Supplementary materials are\navailable at: https://cowplanning.github.io/",
                "On the Value of Myopic Behavior in Policy Reuse\nLeveraging learned strategies in unfamiliar scenarios is fundamental to human\nintelligence. In reinforcement learning, rationally reusing the policies\nacquired from other tasks or human experts is critical for tackling problems\nthat are difficult to learn from scratch. In this work, we present a framework\ncalled Selective Myopic bEhavior Control~(SMEC), which results from the insight\nthat the short-term behaviors of prior policies are sharable across tasks. By\nevaluating the behaviors of prior policies via a hybrid value function\narchitecture, SMEC adaptively aggregates the sharable short-term behaviors of\nprior policies and the long-term behaviors of the task policy, leading to\ncoordinated decisions. Empirical results on a collection of manipulation and\nlocomotion tasks demonstrate that SMEC outperforms existing methods, and\nvalidate the ability of SMEC to leverage related prior policies.",
                "Assumption Generation for the Verification of Learning-Enabled\n  Autonomous Systems\nProviding safety guarantees for autonomous systems is difficult as these\nsystems operate in complex environments that require the use of\nlearning-enabled components, such as deep neural networks (DNNs) for visual\nperception. DNNs are hard to analyze due to their size (they can have thousands\nor millions of parameters), lack of formal specifications (DNNs are typically\nlearnt from labeled data, in the absence of any formal requirements), and\nsensitivity to small changes in the environment. We present an assume-guarantee\nstyle compositional approach for the formal verification of system-level safety\nproperties of such autonomous systems. Our insight is that we can analyze the\nsystem in the absence of the DNN perception components by automatically\nsynthesizing assumptions on the DNN behaviour that guarantee the satisfaction\nof the required safety properties. The synthesized assumptions are the weakest\nin the sense that they characterize the output sequences of all the possible\nDNNs that, plugged into the autonomous system, guarantee the required safety\nproperties. The assumptions can be leveraged as run-time monitors over a\ndeployed DNN to guarantee the safety of the overall system; they can also be\nmined to extract local specifications for use during training and testing of\nDNNs. We illustrate our approach on a case study taken from the autonomous\nairplanes domain that uses a complex DNN for perception."
            ],
            "interesting paper": 4
        },
        {
            "papers": [
                "Re-imagining health and well-being in low resource African settings\n  using an augmented AI system and a 3D digital twin\nThis paper discusses and explores the potential and relevance of recent\ndevelopments in artificial intelligence (AI) and digital twins for health and\nwell-being in low-resource African countries. We use the case of public health\nemergency response to disease outbreaks and epidemic control. There is\npotential to take advantage of the increasing availability of data and\ndigitization to develop advanced AI methods for analysis and prediction. Using\nan AI systems perspective, we review emerging trends in AI systems and digital\ntwins and propose an initial augmented AI system architecture to illustrate how\nan AI system can work with a 3D digital twin to address public health goals. We\nhighlight scientific knowledge discovery, continual learning, pragmatic\ninteroperability, and interactive explanation and decision-making as essential\nresearch challenges for AI systems and digital twins.",
                "Towards Autonomous and Safe Last-mile Deliveries with AI-augmented\n  Self-driving Delivery Robots\nIn addition to its crucial impact on customer satisfaction, last-mile\ndelivery (LMD) is notorious for being the most time-consuming and costly stage\nof the shipping process. Pressing environmental concerns combined with the\nrecent surge of e-commerce sales have sparked renewed interest in automation\nand electrification of last-mile logistics. To address the hurdles faced by\nexisting robotic couriers, this paper introduces a customer-centric and\nsafety-conscious LMD system for small urban communities based on AI-assisted\nautonomous delivery robots. The presented framework enables end-to-end\nautomation and optimization of the logistic process while catering for\nreal-world imposed operational uncertainties, clients' preferred time\nschedules, and safety of pedestrians. To this end, the integrated optimization\ncomponent is modeled as a robust variant of the Cumulative Capacitated Vehicle\nRouting Problem with Time Windows, where routes are constructed under uncertain\ntravel times with an objective to minimize the total latency of deliveries\n(i.e., the overall waiting time of customers, which can negatively affect their\nsatisfaction). We demonstrate the proposed LMD system's utility through\nreal-world trials in a university campus with a single robotic courier.\nImplementation aspects as well as the findings and practical insights gained\nfrom the deployment are discussed in detail. Lastly, we round up the\ncontributions with numerical simulations to investigate the scalability of the\ndeveloped mathematical formulation with respect to the number of robotic\nvehicles and customers.",
                "The Digital Divide in Process Safety: Quantitative Risk Analysis of\n  Human-AI Collaboration\nDigital technologies have dramatically accelerated the digital transformation\nin process industries, boosted new industrial applications, upgraded the\nproduction system, and enhanced operational efficiency. In contrast, the\nchallenges and gaps between human and artificial intelligence (AI) have become\nmore and more prominent, whereas the digital divide in process safety is\naggregating. The study attempts to address the following questions: (i)What is\nAI in the process safety context? (ii)What is the difference between AI and\nhumans in process safety? (iii)How do AI and humans collaborate in process\nsafety? (iv)What are the challenges and gaps in human-AI collaboration? (v)How\nto quantify the risk of human-AI collaboration in process safety? Qualitative\nrisk analysis based on brainstorming and literature review, and quantitative\nrisk analysis based on layer of protection analysis (LOPA) and Bayesian network\n(BN), were applied to explore and model. The importance of human reliability\nshould be stressed in the digital age, not usually to increase the reliability\nof AI, and human-centered AI design in process safety needs to be propagated.",
                "Towards a Technology-Driven Adaptive Decision Support System for\n  Integrated Pavement and Maintenance strategies (TDADSS-IPM): focus on risk\n  assessment framework for climate change adaptation\nDecision Support Systems for pavement and maintenance strategies have\ntraditionally been designed as silos led to local optimum systems. Moreover,\nsince big data usage didn't exist as result of Industry 4.0 as of today, DSSs\nwere not initially designed adaptive to the sources of uncertainties led to\nrigid decisions. Motivated by the vulnerability of the road assets to the\nclimate phenomena, this paper takes a visionary step towards introducing a\nTechnology-Driven Adaptive Decision Support System for Integrated Pavement and\nMaintenance activities called TDADSS-IPM. As part of such DSS, a bottom-up risk\nassessment model is met via Bayesian Belief Networks (BBN) to realize the\nactual condition of the Danish roads due to weather condition. Such model fills\nthe gaps in the knowledge domain and develops a platform that can be trained\nover time, and applied in real-time to the actual event.",
                "AI Audit: A Card Game to Reflect on Everyday AI Systems\nAn essential element of K-12 AI literacy is educating learners about the\nethical and societal implications of AI systems. Previous work in AI ethics\nliteracy have developed curriculum and classroom activities that engage\nlearners in reflecting on the ethical implications of AI systems and developing\nresponsible AI. There is little work in using game-based learning methods in AI\nliteracy. Games are known to be compelling media to teach children about\ncomplex STEM concepts. In this work, we developed a competitive card game for\nmiddle and high school students called \"AI Audit\" where they play as AI\nstart-up founders building novel AI-powered technology. Players can challenge\nother players with potential harms of their technology or defend their own\nbusinesses by features that mitigate these harms. The game mechanics reward\nsystems that are ethically developed or that take steps to mitigate potential\nharms. In this paper, we present the game design, teacher resources for\nclassroom deployment and early playtesting results. We discuss our reflections\nabout using games as teaching tools for AI literacy in K-12 classrooms.",
                "IoT based Personal Voice Assistant\nToday, technological advancement is increasing day by day. Earlier, there was\nonly a computer system in which we could only perform a few tasks. But now,\nmachine learning, artificial intelligence, deep learning, and a few more\ntechnologies have made computer systems so advanced that we can perform any\ntype of task. In this era of advancement, if people are still struggling to\ninteract using various input devices, then it's not worth it. For this reason,\nwe developed a voice assistant using Python that allows the user to run any\ntype of command in Linux without interaction with the keyboard. The main task\nof the voice assistant is to minimize the use of input devices like the\nkeyboard and mouse. It will also reduce hardware space and cost."
            ],
            "interesting paper": 2
        },
        {
            "papers": [
                "RE-centric Recommendations for the Development of Trustworthy(er)\n  Autonomous Systems\nComplying with the EU AI Act (AIA) guidelines while developing and\nimplementing AI systems will soon be mandatory within the EU. However,\npractitioners lack actionable instructions to operationalise ethics during AI\nsystems development. A literature review of different ethical guidelines\nrevealed inconsistencies in the principles addressed and the terminology used\nto describe them. Furthermore, requirements engineering (RE), which is\nidentified to foster trustworthiness in the AI development process from the\nearly stages was observed to be absent in a lot of frameworks that support the\ndevelopment of ethical and trustworthy AI. This incongruous phrasing combined\nwith a lack of concrete development practices makes trustworthy AI development\nharder. To address this concern, we formulated a comparison table for the\nterminology used and the coverage of the ethical AI principles in major ethical\nAI guidelines. We then examined the applicability of ethical AI development\nframeworks for performing effective RE during the development of trustworthy AI\nsystems. A tertiary review and meta-analysis of literature discussing ethical\nAI frameworks revealed their limitations when developing trustworthy AI. Based\non our findings, we propose recommendations to address such limitations during\nthe development of trustworthy AI.",
                "An Emergency Disposal Decision-making Method with Human--Machine\n  Collaboration\nRapid developments in artificial intelligence technology have led to unmanned\nsystems replacing human beings in many fields requiring high-precision\npredictions and decisions. In modern operational environments, all job plans\nare affected by emergency events such as equipment failures and resource\nshortages, making a quick resolution critical. The use of unmanned systems to\nassist decision-making can improve resolution efficiency, but their\ndecision-making is not interpretable and may make the wrong decisions. Current\nunmanned systems require human supervision and control. Based on this, we\npropose a collaborative human--machine method for resolving unplanned events\nusing two phases: task filtering and task scheduling. In the task filtering\nphase, we propose a human--machine collaborative decision-making algorithm for\ndynamic tasks. The GACRNN model is used to predict the state of the job nodes,\nlocate the key nodes, and generate a machine-predicted resolution task list. A\nhuman decision-maker supervises the list in real time and modifies and confirms\nthe machine-predicted list through the human--machine interface. In the task\nscheduling phase, we propose a scheduling algorithm that integrates human\nexperience constraints. The steps to resolve an event are inserted into the\nnormal job sequence to schedule the resolution. We propose several\nhuman--machine collaboration methods in each phase to generate steps to resolve\nan unplanned event while minimizing the impact on the original job plan.",
                "Towards a Unifying Model of Rationality in Multiagent Systems\nMultiagent systems deployed in the real world need to cooperate with other\nagents (including humans) nearly as effectively as these agents cooperate with\none another. To design such AI, and provide guarantees of its effectiveness, we\nneed to clearly specify what types of agents our AI must be able to cooperate\nwith. In this work we propose a generic model of socially intelligent agents,\nwhich are individually rational learners that are also able to cooperate with\none another (in the sense that their joint behavior is Pareto efficient). We\ndefine rationality in terms of the regret incurred by each agent over its\nlifetime, and show how we can construct socially intelligent agents for\ndifferent forms of regret. We then discuss the implications of this model for\nthe development of \"robust\" MAS that can cooperate with a wide variety of\nsocially intelligent agents.",
                "Development of a ROS-based Architecture for Intelligent Autonomous on\n  Demand Last Mile Delivery\nThis paper presents the development of the JKU-ITS Last Mile Delivery Robot.\nThe proposed approach utilizes a combination of one 3D LIDAR, RGB-D camera, IMU\nand GPS sensor on top of a mobile robot slope mower. An embedded computer,\nrunning ROS1, is utilized to process the sensor data streams to enable 2D and\n3D Simultaneous Localization and Mapping, 2D localization and object detection\nusing a convolutional neural network.",
                "Graph Neural Network for spatiotemporal data: methods and applications\nIn the era of big data, there has been a surge in the availability of data\ncontaining rich spatial and temporal information, offering valuable insights\ninto dynamic systems and processes for applications such as weather\nforecasting, natural disaster management, intelligent transport systems, and\nprecision agriculture. Graph neural networks (GNNs) have emerged as a powerful\ntool for modeling and understanding data with dependencies to each other such\nas spatial and temporal dependencies. There is a large amount of existing work\nthat focuses on addressing the complex spatial and temporal dependencies in\nspatiotemporal data using GNNs. However, the strong interdisciplinary nature of\nspatiotemporal data has created numerous GNNs variants specifically designed\nfor distinct application domains. Although the techniques are generally\napplicable across various domains, cross-referencing these methods remains\nessential yet challenging due to the absence of a comprehensive literature\nreview on GNNs for spatiotemporal data. This article aims to provide a\nsystematic and comprehensive overview of the technologies and applications of\nGNNs in the spatiotemporal domain. First, the ways of constructing graphs from\nspatiotemporal data are summarized to help domain experts understand how to\ngenerate graphs from various types of spatiotemporal data. Then, a systematic\ncategorization and summary of existing spatiotemporal GNNs are presented to\nenable domain experts to identify suitable techniques and to support model\ndevelopers in advancing their research. Moreover, a comprehensive overview of\nsignificant applications in the spatiotemporal domain is offered to introduce a\nbroader range of applications to model developers and domain experts, assisting\nthem in exploring potential research topics and enhancing the impact of their\nwork. Finally, open challenges and future directions are discussed.",
                "Generalized Disparate Impact for Configurable Fairness Solutions in ML\nWe make two contributions in the field of AI fairness over continuous\nprotected attributes. First, we show that the Hirschfeld-Gebelein-Renyi (HGR)\nindicator (the only one currently available for such a case) is valuable but\nsubject to a few crucial limitations regarding semantics, interpretability, and\nrobustness. Second, we introduce a family of indicators that are: 1)\ncomplementary to HGR in terms of semantics; 2) fully interpretable and\ntransparent; 3) robust over finite samples; 4) configurable to suit specific\napplications. Our approach also allows us to define fine-grained constraints to\npermit certain types of dependence and forbid others selectively. By expanding\nthe available options for continuous protected attributes, our approach\nrepresents a significant contribution to the area of fair artificial\nintelligence."
            ],
            "interesting paper": 5
        },
        {
            "papers": [
                "Intent-aligned AI systems deplete human agency: the need for agency\n  foundations research in AI safety\nThe rapid advancement of artificial intelligence (AI) systems suggests that\nartificial general intelligence (AGI) systems may soon arrive. Many researchers\nare concerned that AIs and AGIs will harm humans via intentional misuse\n(AI-misuse) or through accidents (AI-accidents). In respect of AI-accidents,\nthere is an increasing effort focused on developing algorithms and paradigms\nthat ensure AI systems are aligned to what humans intend, e.g. AI systems that\nyield actions or recommendations that humans might judge as consistent with\ntheir intentions and goals. Here we argue that alignment to human intent is\ninsufficient for safe AI systems and that preservation of long-term agency of\nhumans may be a more robust standard, and one that needs to be separated\nexplicitly and a priori during optimization. We argue that AI systems can\nreshape human intention and discuss the lack of biological and psychological\nmechanisms that protect humans from loss of agency. We provide the first formal\ndefinition of agency-preserving AI-human interactions which focuses on\nforward-looking agency evaluations and argue that AI systems - not humans -\nmust be increasingly tasked with making these evaluations. We show how agency\nloss can occur in simple environments containing embedded agents that use\ntemporal-difference learning to make action recommendations. Finally, we\npropose a new area of research called \"agency foundations\" and pose four\ninitial topics designed to improve our understanding of agency in AI-human\ninteractions: benevolent game theory, algorithmic foundations of human rights,\nmechanistic interpretability of agency representation in neural-networks and\nreinforcement learning from internal states.",
                "Disentangling and Operationalizing AI Fairness at LinkedIn\nOperationalizing AI fairness at LinkedIn's scale is challenging not only\nbecause there are multiple mutually incompatible definitions of fairness but\nalso because determining what is fair depends on the specifics and context of\nthe product where AI is deployed. Moreover, AI practitioners need clarity on\nwhat fairness expectations need to be addressed at the AI level. In this paper,\nwe present the evolving AI fairness framework used at LinkedIn to address these\nthree challenges. The framework disentangles AI fairness by separating out\nequal treatment and equitable product expectations. Rather than imposing a\ntrade-off between these two commonly opposing interpretations of fairness, the\nframework provides clear guidelines for operationalizing equal AI treatment\ncomplemented with a product equity strategy. This paper focuses on the equal AI\ntreatment component of LinkedIn's AI fairness framework, shares the principles\nthat support it, and illustrates their application through a case study. We\nhope this paper will encourage other big tech companies to join us in sharing\ntheir approach to operationalizing AI fairness at scale, so that together we\ncan keep advancing this constantly evolving field.",
                "Evaluating GPT's Programming Capability through CodeWars' Katas\nIn the burgeoning field of artificial intelligence (AI), understanding the\ncapabilities and limitations of programming-oriented models is crucial. This\npaper presents a novel evaluation of the programming proficiency of Generative\nPretrained Transformer (GPT) models, specifically GPT-3.5 and GPT-4, against\ncoding problems of varying difficulty levels drawn from Codewars. The\nexperiments reveal a distinct boundary at the 3kyu level, beyond which these\nGPT models struggle to provide solutions. These findings led to the proposal of\na measure for coding problem complexity that incorporates both problem\ndifficulty and the time required for solution. The research emphasizes the need\nfor validation and creative thinking capabilities in AI models to better\nemulate human problem-solving techniques. Future work aims to refine this\nproposed complexity measure, enhance AI models with these suggested\ncapabilities, and develop an objective measure for programming problem\ndifficulty. The results of this research offer invaluable insights for\nimproving AI programming capabilities and advancing the frontier of AI\nproblem-solving abilities.",
                "Traffic Prediction using Artificial Intelligence: Review of Recent\n  Advances and Emerging Opportunities\nTraffic prediction plays a crucial role in alleviating traffic congestion\nwhich represents a critical problem globally, resulting in negative\nconsequences such as lost hours of additional travel time and increased fuel\nconsumption. Integrating emerging technologies into transportation systems\nprovides opportunities for improving traffic prediction significantly and\nbrings about new research problems. In order to lay the foundation for\nunderstanding the open research challenges in traffic prediction, this survey\naims to provide a comprehensive overview of traffic prediction methodologies.\nSpecifically, we focus on the recent advances and emerging research\nopportunities in Artificial Intelligence (AI)-based traffic prediction methods,\ndue to their recent success and potential in traffic prediction, with an\nemphasis on multivariate traffic time series modeling. We first provide a list\nand explanation of the various data types and resources used in the literature.\nNext, the essential data preprocessing methods within the traffic prediction\ncontext are categorized, and the prediction methods and applications are\nsubsequently summarized. Lastly, we present primary research challenges in\ntraffic prediction and discuss some directions for future research.",
                "Data and Knowledge for Overtaking Scenarios in Autonomous Driving\nAutonomous driving has become one of the most popular research topics within\nArtificial Intelligence. An autonomous vehicle is understood as a system that\ncombines perception, decision-making, planning, and control. All of those tasks\nrequire that the vehicle collects surrounding data in order to make a good\ndecision and action. In particular, the overtaking maneuver is one of the most\ncritical actions of driving. The process involves lane changes, acceleration\nand deceleration actions, and estimation of the speed and distance of the\nvehicle in front or in the lane in which it is moving. Despite the amount of\nwork available in the literature, just a few handle overtaking maneuvers and,\nbecause overtaking can be risky, no real-world dataset is available. This work\ncontributes in this area by presenting a new synthetic dataset whose focus is\nthe overtaking maneuver. We start by performing a thorough review of the state\nof the art in autonomous driving and then explore the main datasets found in\nthe literature (public and private, synthetic and real), highlighting their\nlimitations, and suggesting a new set of features whose focus is the overtaking\nmaneuver.",
                "Bigger, Better, Faster: Human-level Atari with human-level efficiency\nWe introduce a value-based RL agent, which we call BBF, that achieves\nsuper-human performance in the Atari 100K benchmark. BBF relies on scaling the\nneural networks used for value estimation, as well as a number of other design\nchoices that enable this scaling in a sample-efficient manner. We conduct\nextensive analyses of these design choices and provide insights for future\nwork. We end with a discussion about updating the goalposts for\nsample-efficient RL research on the ALE. We make our code and data publicly\navailable at\nhttps://github.com/google-research/google-research/tree/master/bigger_better_faster."
            ],
            "interesting paper": 3
        },
        {
            "papers": [
                "Citizen Perspectives on Necessary Safeguards to the Use of AI by Law\n  Enforcement Agencies\nIn the light of modern technological advances, Artificial Intelligence (AI)\nis relied upon to enhance performance, increase efficiency, and maximize gains.\nFor Law Enforcement Agencies (LEAs), it can prove valuable in optimizing\nevidence analysis and establishing proactive prevention measures. Nevertheless,\ncitizens raise legitimate concerns around privacy invasions, biases,\ninequalities, and inaccurate decisions. This study explores the views of 111\ncitizens towards AI use by police through interviews, and integrates societal\nconcerns along with propositions of safeguards from negative effects of AI use\nby LEAs in the context of cybercrime and terrorism.",
                "Traffic Road Congestion System using by the internet of vehicles (IoV)\nTraffic problems have increased in modern life due to a huge number of\nvehicles, big cities, and ignoring the traffic rules. Vehicular ad hoc network\n(VANET) has improved the traffic system in previous some and plays a vital role\nin the best traffic control system in big cities. But due to some limitations,\nit is not enough to control some problems in specific conditions. Now a day\ninvention of new technologies of the Internet of Things (IoT) is used for\ncollaboratively and efficiently performing tasks. This technology was also\nintroduced in the transportation system which makes it an intelligent\ntransportation system (ITS), this is called the Internet of vehicles (IOV). We\nwill elaborate on traffic problems in the traditional system and elaborate on\nthe benefits, enhancements, and reasons to better IOV by Systematic Literature\nReview (SLR). This technique will be implemented by targeting needed papers\nthrough many search phrases. A systematic literature review is used for 121\narticles between 2014 and 2023. The IoV technologies and tools are required to\ncreate the IoV and resolve some traffic rules through SUMO (simulation of urban\nmobility) which is used for the design and simulation the road traffic. We have\ntried to contribute to the best model of the traffic control system. This paper\nwill analysis two vehicular congestion control models in term of select the\noptimized and efficient model and elaborate on the reasons for efficiency by\nsearching the solution SLR based questions. Due to some efficient features, we\nhave suggested the IOV based on vehicular clouds. These efficient features make\nthis model the best and most effective than the traditional model which is a\ngreat reason to enhance the network system.",
                "Responsible Design Patterns for Machine Learning Pipelines\nIntegrating ethical practices into the AI development process for artificial\nintelligence (AI) is essential to ensure safe, fair, and responsible operation.\nAI ethics involves applying ethical principles to the entire life cycle of AI\nsystems. This is essential to mitigate potential risks and harms associated\nwith AI, such as algorithm biases. To achieve this goal, responsible design\npatterns (RDPs) are critical for Machine Learning (ML) pipelines to guarantee\nethical and fair outcomes. In this paper, we propose a comprehensive framework\nincorporating RDPs into ML pipelines to mitigate risks and ensure the ethical\ndevelopment of AI systems. Our framework comprises new responsible AI design\npatterns for ML pipelines identified through a survey of AI ethics and data\nmanagement experts and validated through real-world scenarios with expert\nfeedback. The framework guides AI developers, data scientists, and\npolicy-makers to implement ethical practices in AI development and deploy\nresponsible AI systems in production.",
                "Human or Not? A Gamified Approach to the Turing Test\nWe present \"Human or Not?\", an online game inspired by the Turing test, that\nmeasures the capability of AI chatbots to mimic humans in dialog, and of humans\nto tell bots from other humans. Over the course of a month, the game was played\nby over 1.5 million users who engaged in anonymous two-minute chat sessions\nwith either another human or an AI language model which was prompted to behave\nlike humans. The task of the players was to correctly guess whether they spoke\nto a person or to an AI. This largest scale Turing-style test conducted to date\nrevealed some interesting facts. For example, overall users guessed the\nidentity of their partners correctly in only 68% of the games. In the subset of\nthe games in which users faced an AI bot, users had even lower correct guess\nrates of 60% (that is, not much higher than chance). This white paper details\nthe development, deployment, and results of this unique experiment. While this\nexperiment calls for many extensions and refinements, these findings already\nbegin to shed light on the inevitable near future which will commingle humans\nand AI.",
                "Human Control: Definitions and Algorithms\nHow can humans stay in control of advanced artificial intelligence systems?\nOne proposal is corrigibility, which requires the agent to follow the\ninstructions of a human overseer, without inappropriately influencing them. In\nthis paper, we formally define a variant of corrigibility called shutdown\ninstructability, and show that it implies appropriate shutdown behavior,\nretention of human autonomy, and avoidance of user harm. We also analyse the\nrelated concepts of non-obstruction and shutdown alignment, three previously\nproposed algorithms for human control, and one new algorithm.",
                "From Human-Centered to Social-Centered Artificial Intelligence:\n  Assessing ChatGPT's Impact through Disruptive Events\nLarge language models (LLMs) and dialogue agents have existed for years, but\nthe release of recent GPT models has been a watershed moment for artificial\nintelligence (AI) research and society at large. Immediately recognized for its\ngenerative capabilities and versatility, ChatGPT's impressive proficiency\nacross technical and creative domains led to its widespread adoption. While\nsociety grapples with the emerging cultural impacts of ChatGPT, critiques of\nChatGPT's impact within the machine learning community have coalesced around\nits performance or other conventional Responsible AI evaluations relating to\nbias, toxicity, and 'hallucination.' We argue that these latter critiques draw\nheavily on a particular conceptualization of the 'human-centered' framework,\nwhich tends to cast atomized individuals as the key recipients of both the\nbenefits and detriments of technology. In this article, we direct attention to\nanother dimension of LLMs and dialogue agents' impact: their effect on social\ngroups, institutions, and accompanying norms and practices. By illustrating\nChatGPT's social impact through three disruptive events, we challenge\nindividualistic approaches in AI development and contribute to ongoing debates\naround the ethical and responsible implementation of AI systems. We hope this\neffort will call attention to more comprehensive and longitudinal evaluation\ntools and compel technologists to go beyond human-centered thinking and ground\ntheir efforts through social-centered AI."
            ],
            "interesting paper": 5
        },
        {
            "papers": [
                "AI Liability Insurance With an Example in AI-Powered E-diagnosis System\nArtificial Intelligence (AI) has received an increasing amount of attention\nin multiple areas. The uncertainties and risks in AI-powered systems have\ncreated reluctance in their wild adoption. As an economic solution to\ncompensate for potential damages, AI liability insurance is a promising market\nto enhance the integration of AI into daily life. In this work, we use an\nAI-powered E-diagnosis system as an example to study AI liability insurance. We\nprovide a quantitative risk assessment model with evidence-based numerical\nanalysis. We discuss the insurability criteria for AI technologies and suggest\nnecessary adjustments to accommodate the features of AI products. We show that\nAI liability insurance can act as a regulatory mechanism to incentivize\ncompliant behaviors and serve as a certificate of high-quality AI systems.\nFurthermore, we suggest premium adjustment to reflect the dynamic evolution of\nthe inherent uncertainty in AI. Moral hazard problems are discussed and\nsuggestions for AI liability insurance are provided.",
                "AI and the creative realm: A short review of current and future\n  applications\nThis study explores the concept of creativity and artificial intelligence\n(AI) and their recent integration. While AI has traditionally been perceived as\nincapable of generating new ideas or creating art, the development of more\nsophisticated AI models and the proliferation of human-computer interaction\ntools have opened up new possibilities for AI in artistic creation. This study\ninvestigates the various applications of AI in a creative context,\ndifferentiating between the type of art, language, and algorithms used. It also\nconsiders the philosophical implications of AI and creativity, questioning\nwhether consciousness can be researched in machines and AI's potential\ninterests and decision-making capabilities. Overall, we aim to stimulate a\nreflection on AI's use and ethical implications in creative contexts.",
                "Recent Advances in Graph-based Machine Learning for Applications in\n  Smart Urban Transportation Systems\nThe Intelligent Transportation System (ITS) is an important part of modern\ntransportation infrastructure, employing a combination of communication\ntechnology, information processing and control systems to manage transportation\nnetworks. This integration of various components such as roads, vehicles, and\ncommunication systems, is expected to improve efficiency and safety by\nproviding better information, services, and coordination of transportation\nmodes. In recent years, graph-based machine learning has become an increasingly\nimportant research focus in the field of ITS aiming at the development of\ncomplex, data-driven solutions to address various ITS-related challenges. This\nchapter presents background information on the key technical challenges for ITS\ndesign, along with a review of research methods ranging from classic\nstatistical approaches to modern machine learning and deep learning-based\napproaches. Specifically, we provide an in-depth review of graph-based machine\nlearning methods, including basic concepts of graphs, graph data\nrepresentation, graph neural network architectures and their relation to ITS\napplications. Additionally, two case studies of graph-based ITS applications\nproposed in our recent work are presented in detail to demonstrate the\npotential of graph-based machine learning in the ITS domain.",
                "An Overview on Generative AI at Scale with Edge-Cloud Computing\nAs a specific category of artificial intelligence (AI), generative artificial\nintelligence (GenAI) generates new content that resembles what is created by\nhumans. The rapid development of GenAI systems has created a huge amount of new\ndata on the Internet, posing new challenges to current computing and\ncommunication frameworks. Currently, GenAI services rely on the traditional\ncloud computing framework due to the need for large computation resources.\nHowever, such services will encounter high latency because of data transmission\nand a high volume of requests. On the other hand, edge-cloud computing can\nprovide adequate computation power and low latency at the same time through the\ncollaboration between edges and the cloud. Thus, it is attractive to build\nGenAI systems at scale by leveraging the edge-cloud computing paradigm. In this\noverview paper, we review recent developments in GenAI and edge-cloud\ncomputing, respectively. Then, we use two exemplary GenAI applications to\ndiscuss technical challenges in scaling up their solutions using edge-cloud\ncollaborative systems. Finally, we list design considerations for training and\ndeploying GenAI systems at scale and point out future research directions.",
                "The ethical ambiguity of AI data enrichment: Measuring gaps in research\n  ethics norms and practices\nThe technical progression of artificial intelligence (AI) research has been\nbuilt on breakthroughs in fields such as computer science, statistics, and\nmathematics. However, in the past decade AI researchers have increasingly\nlooked to the social sciences, turning to human interactions to solve the\nchallenges of model development. Paying crowdsourcing workers to generate or\ncurate data, or data enrichment, has become indispensable for many areas of AI\nresearch, from natural language processing to reinforcement learning from human\nfeedback (RLHF). Other fields that routinely interact with crowdsourcing\nworkers, such as Psychology, have developed common governance requirements and\nnorms to ensure research is undertaken ethically. This study explores how, and\nto what extent, comparable research ethics requirements and norms have\ndeveloped for AI research and data enrichment. We focus on the approach taken\nby two leading conferences: ICLR and NeurIPS, and journal publisher Springer.\nIn a longitudinal study of accepted papers, and via a comparison with\nPsychology and CHI papers, this work finds that leading AI venues have begun to\nestablish protocols for human data collection, but these are are inconsistently\nfollowed by authors. Whilst Psychology papers engaging with crowdsourcing\nworkers frequently disclose ethics reviews, payment data, demographic data and\nother information, similar disclosures are far less common in leading AI venues\ndespite similar guidance. The work concludes with hypotheses to explain these\ngaps in research ethics practices and considerations for its implications.",
                "Integrated Sensing-Communication-Computation for Edge Artificial\n  Intelligence\nEdge artificial intelligence (AI) has been a promising solution towards 6G to\nempower a series of advanced techniques such as digital twins, holographic\nprojection, semantic communications, and auto-driving, for achieving\nintelligence of everything. The performance of edge AI tasks, including edge\nlearning and edge AI inference, depends on the quality of three highly coupled\nprocesses, i.e., sensing for data acquisition, computation for information\nextraction, and communication for information transmission. However, these\nthree modules need to compete for network resources for enhancing their own\nquality-of-services. To this end, integrated sensing-communication-computation\n(ISCC) is of paramount significance for improving resource utilization as well\nas achieving the customized goals of edge AI tasks. By investigating the\ninterplay among the three modules, this article presents various kinds of ISCC\nschemes for federated edge learning tasks and edge AI inference tasks in both\napplication and physical layers."
            ],
            "interesting paper": 3
        },
        {
            "papers": [
                "Enough With \"Human-AI Collaboration\"\nDescribing our interaction with Artificial Intelligence (AI) systems as\n'collaboration' is well-intentioned, but flawed. Not only is it misleading, but\nit also takes away the credit of AI 'labour' from the humans behind it, and\nerases and obscures an often exploitative arrangement between AI producers and\nconsumers. The AI 'collaboration' metaphor is merely the latest episode in a\nlong history of labour appropriation and credit reassignment that\ndisenfranchises labourers in the Global South. I propose that viewing AI as a\ntool or an instrument, rather than a collaborator, is more accurate, and\nultimately fairer.",
                "Milestones in Autonomous Driving and Intelligent Vehicles Part II:\n  Perception and Planning\nGrowing interest in autonomous driving (AD) and intelligent vehicles (IVs) is\nfueled by their promise for enhanced safety, efficiency, and economic benefits.\nWhile previous surveys have captured progress in this field, a comprehensive\nand forward-looking summary is needed. Our work fills this gap through three\ndistinct articles. The first part, a \"Survey of Surveys\" (SoS), outlines the\nhistory, surveys, ethics, and future directions of AD and IV technologies. The\nsecond part, \"Milestones in Autonomous Driving and Intelligent Vehicles Part I:\nControl, Computing System Design, Communication, HD Map, Testing, and Human\nBehaviors\" delves into the development of control, computing system,\ncommunication, HD map, testing, and human behaviors in IVs. This part, the\nthird part, reviews perception and planning in the context of IVs. Aiming to\nprovide a comprehensive overview of the latest advancements in AD and IVs, this\nwork caters to both newcomers and seasoned researchers. By integrating the SoS\nand Part I, we offer unique insights and strive to serve as a bridge between\npast achievements and future possibilities in this dynamic field.",
                "AI Transparency in the Age of LLMs: A Human-Centered Research Roadmap\nThe rise of powerful large language models (LLMs) brings about tremendous\nopportunities for innovation but also looming risks for individuals and society\nat large. We have reached a pivotal moment for ensuring that LLMs and\nLLM-infused applications are developed and deployed responsibly. However, a\ncentral pillar of responsible AI -- transparency -- is largely missing from the\ncurrent discourse around LLMs. It is paramount to pursue new approaches to\nprovide transparency for LLMs, and years of research at the intersection of AI\nand human-computer interaction (HCI) highlight that we must do so with a\nhuman-centered perspective: Transparency is fundamentally about supporting\nappropriate human understanding, and this understanding is sought by different\nstakeholders with different goals in different contexts. In this new era of\nLLMs, we must develop and design approaches to transparency by considering the\nneeds of stakeholders in the emerging LLM ecosystem, the novel types of\nLLM-infused applications being built, and the new usage patterns and challenges\naround LLMs, all while building on lessons learned about how people process,\ninteract with, and make use of information. We reflect on the unique challenges\nthat arise in providing transparency for LLMs, along with lessons learned from\nHCI and responsible AI research that has taken a human-centered perspective on\nAI transparency. We then lay out four common approaches that the community has\ntaken to achieve transparency -- model reporting, publishing evaluation\nresults, providing explanations, and communicating uncertainty -- and call out\nopen questions around how these approaches may or may not be applied to LLMs.\nWe hope this provides a starting point for discussion and a useful roadmap for\nfuture research.",
                "DataAI-6G: A System Parameters Configurable Channel Dataset for AI-6G\n  Research\nWith the acceleration of the commercialization of fifth generation (5G)\nmobile communication technology and the research for 6G communication systems,\nthe communication system has the characteristics of high frequency, multi-band,\nhigh speed movement of users and large antenna array. These bring many\ndifficulties to obtain accurate channel state information (CSI), which makes\nthe performance of traditional communication methods be greatly restricted.\nTherefore, there has been a lot of interest in using artificial intelligence\n(AI) instead of traditional methods to improve performance. A common and\naccurate dataset is essential for the research of AI communication. However,\nthe common datasets nowadays still lack some important features, such as mobile\nfeatures, spatial non-stationary features etc. To address these issues, we give\na dataset for future 6G communication. In this dataset, we address these issues\nwith specific simulation methods and accompanying code processing.",
                "XAI Renaissance: Redefining Interpretability in Medical Diagnostic\n  Models\nAs machine learning models become increasingly prevalent in medical\ndiagnostics, the need for interpretability and transparency becomes paramount.\nThe XAI Renaissance signifies a significant shift in the field, aiming to\nredefine the interpretability of medical diagnostic models. This paper explores\nthe innovative approaches and methodologies within the realm of Explainable AI\n(XAI) that are revolutionizing the interpretability of medical diagnostic\nmodels. By shedding light on the underlying decision-making process, XAI\ntechniques empower healthcare professionals to understand, trust, and\neffectively utilize these models for accurate and reliable medical diagnoses.\nThis review highlights the key advancements in XAI for medical diagnostics and\ntheir potential to transform the healthcare landscape, ultimately improving\npatient outcomes and fostering trust in AI-driven diagnostic systems.",
                "A Hybrid Approach for Smart Alert Generation\nAnomaly detection is an important task in network management. However,\ndeploying intelligent alert systems in real-world large-scale networking\nsystems is challenging when we take into account (i) scalability, (ii) data\nheterogeneity, and (iii) generalizability and maintainability. In this paper,\nwe propose a hybrid model for an alert system that combines statistical models\nwith a whitelist mechanism to tackle these challenges and reduce false positive\nalerts. The statistical models take advantage of a large database to detect\nanomalies in time-series data, while the whitelist filters out persistently\nalerted nodes to further reduce false positives. Our model is validated using\nqualitative data from customer support cases. Future work includes more feature\nengineering and input data, as well as including human feedback in the model\ndevelopment process."
            ],
            "interesting paper": 2
        }
    ],
    "Charlie Brown": [
        {
            "papers": [
                "Applications of Intelligent Systems in Green Technology\nIntelligent Systems (ISs) are technologically advanced machines, which\nperceive and respond to the environment around them. They are usually of\nvarious forms ranging from software to hardware. ISs are generally the fusion\nof Artificial Intelligence (AI), robotics and Internet of things (IoT). In\norder to strengthen ISs, one of the key technologies is green technology (GT).\nIt refers to the continuously advancing methods and materials, which cover\ntechniques for producing energy to non-toxic cleaning products. It may also be\nbroadened to saving energy, and reducing toxic and waste materials in the\nenvironment. The motto of GT can be achieved by using the ISs. In this paper,\nwe present various applications of ISs in GT. Moreover, we discuss various\npossible solutions using ISs in order to overcome the on-going real-life\nproblems.",
                "Towards a Capability Assessment Model for the Comprehension and Adoption\n  of AI in Organisations\nThe comprehension and adoption of Artificial Intelligence (AI) are beset with\npractical and ethical problems. This article presents a 5-level AI Capability\nAssessment Model (AI-CAM) and a related AI Capabilities Matrix (AI-CM) to\nassist practitioners in AI comprehension and adoption. These practical tools\nwere developed with business executives, technologists, and other\norganisational stakeholders in mind. They are founded on a comprehensive\nconception of AI compared to those in other AI adoption models and are also\nopen-source artefacts. Thus, the AI-CAM and AI-CM present an accessible\nresource to help inform organisational decision-makers on the capability\nrequirements for (1) AI-based data analytics use cases based on machine\nlearning technologies; (2) Knowledge representation to engineer and represent\ndata, information and knowledge using semantic technologies; and (3) AI-based\nsolutions that seek to emulate human reasoning and decision-making. The AI-CAM\ncovers the core capability dimensions (business, data, technology,\norganisation, AI skills, risks, and ethical considerations) required at the\nfive capability maturity levels to achieve optimal use of AI in organisations.",
                "Reliability Scores from Saliency Map Clusters for Improved Image-based\n  Harvest-Readiness Prediction in Cauliflower\nCauliflower is a hand-harvested crop that must fulfill high-quality standards\nin sales making the timing of harvest important. However, accurately\ndetermining harvest-readiness can be challenging due to the cauliflower head\nbeing covered by its canopy. While deep learning enables automated\nharvest-readiness estimation, errors can occur due to field-variability and\nlimited training data. In this paper, we analyze the reliability of a\nharvest-readiness classifier with interpretable machine learning. By\nidentifying clusters of saliency maps, we derive reliability scores for each\nclassification result using knowledge about the domain and the image\nproperties. For unseen data, the reliability can be used to (i) inform farmers\nto improve their decision-making and (ii) increase the model prediction\naccuracy. Using RGB images of single cauliflower plants at different\ndevelopmental stages from the GrowliFlower dataset, we investigate various\nsaliency mapping approaches and find that they result in different quality of\nreliability scores. With the most suitable interpretation tool, we adjust the\nclassification result and achieve a 15.72% improvement of the overall accuracy\nto 88.14% and a 15.44% improvement of the average class accuracy to 88.52% for\nthe GrowliFlower dataset.",
                "Deep Learning and Ethics\nThis article appears as chapter 21 of Prince (2023, Understanding Deep\nLearning); a complete draft of the textbook is available here:\nhttp://udlbook.com. This chapter considers potential harms arising from the\ndesign and use of AI systems. These include algorithmic bias, lack of\nexplainability, data privacy violations, militarization, fraud, and\nenvironmental concerns. The aim is not to provide advice on being more ethical.\nInstead, the goal is to express ideas and start conversations in key areas that\nhave received attention in philosophy, political science, and the broader\nsocial sciences.",
                "Trends and Challenges Towards an Effective Data-Driven Decision Making\n  in UK SMEs: Case Studies and Lessons Learnt from the Analysis of 85 SMEs\nThe adoption of data science brings vast benefits to Small and Medium-sized\nEnterprises (SMEs) including business productivity, economic growth, innovation\nand jobs creation. Data Science can support SMEs to optimise production\nprocesses, anticipate customers' needs, predict machinery failures and deliver\nefficient smart services. Businesses can also harness the power of Artificial\nIntelligence (AI) and Big Data and the smart use of digital technologies to\nenhance productivity and performance, paving the way for innovation. However,\nintegrating data science decisions into an SME requires both skills and IT\ninvestments. In most cases, such expenses are beyond the means of SMEs due to\nlimited resources and restricted access to financing. This paper presents\ntrends and challenges towards an effective data-driven decision making for\norganisations based on a case study of 85 SMEs, mostly from the West Midlands\nregion of England. The work is supported as part of a 3 years ERDF (European\nRegional Development Funded project) in the areas of big data management,\nanalytics and business intelligence. We present two case studies that\ndemonstrates the potential of Digitisation, AI and Machine Learning and use\nthese as examples to unveil challenges and showcase the wealth of current\navailable opportunities for SMEs.",
                "Model evaluation for extreme risks\nCurrent approaches to building general-purpose AI systems tend to produce\nsystems with both beneficial and harmful capabilities. Further progress in AI\ndevelopment could lead to capabilities that pose extreme risks, such as\noffensive cyber capabilities or strong manipulation skills. We explain why\nmodel evaluation is critical for addressing extreme risks. Developers must be\nable to identify dangerous capabilities (through \"dangerous capability\nevaluations\") and the propensity of models to apply their capabilities for harm\n(through \"alignment evaluations\"). These evaluations will become critical for\nkeeping policymakers and other stakeholders informed, and for making\nresponsible decisions about model training, deployment, and security."
            ],
            "interesting paper": 3
        },
        {
            "papers": [
                "AI Techniques in the Microservices Life-Cycle: A Survey\nMicroservices is a popular architectural style for the development of\ndistributed software, with an emphasis on modularity, scalability, and\nflexibility. Indeed, in microservice systems, functionalities are provided by\nloosely coupled, small services, each focusing on a specific business\ncapability. Building a system according to the microservices architectural\nstyle brings a number of challenges, mainly related to how the different\nmicroservices are deployed and coordinated and how they interact. In this\npaper, we provide a survey about how techniques in the area of Artificial\nIntelligence have been used to tackle these challenges.",
                "Enhancing Human Capabilities through Symbiotic Artificial Intelligence\n  with Shared Sensory Experiences\nThe merging of human intelligence and artificial intelligence has long been a\nsubject of interest in both science fiction and academia. In this paper, we\nintroduce a novel concept in Human-AI interaction called Symbiotic Artificial\nIntelligence with Shared Sensory Experiences (SAISSE), which aims to establish\na mutually beneficial relationship between AI systems and human users through\nshared sensory experiences. By integrating multiple sensory input channels and\nprocessing human experiences, SAISSE fosters a strong human-AI bond, enabling\nAI systems to learn from and adapt to individual users, providing personalized\nsupport, assistance, and enhancement. Furthermore, we discuss the incorporation\nof memory storage units for long-term growth and development of both the AI\nsystem and its human user. As we address user privacy and ethical guidelines\nfor responsible AI-human symbiosis, we also explore potential biases and\ninequalities in AI-human symbiosis and propose strategies to mitigate these\nchallenges. Our research aims to provide a comprehensive understanding of the\nSAISSE concept and its potential to effectively support and enhance individual\nhuman users through symbiotic AI systems. This position article aims at\ndiscussing poteintial AI-human interaction related topics within the scientific\ncommunity, rather than providing experimental or theoretical results.",
                "Attention Paper: How Generative AI Reshapes Digital Shadow Industry?\nThe rapid development of digital economy has led to the emergence of various\nblack and shadow internet industries, which pose potential risks that can be\nidentified and managed through digital risk management (DRM) that uses\ndifferent techniques such as machine learning and deep learning. The evolution\nof DRM architecture has been driven by changes in data forms. However, the\ndevelopment of AI-generated content (AIGC) technology, such as ChatGPT and\nStable Diffusion, has given black and shadow industries powerful tools to\npersonalize data and generate realistic images and conversations for fraudulent\nactivities. This poses a challenge for DRM systems to control risks from the\nsource of data generation and to respond quickly to the fast-changing risk\nenvironment. This paper aims to provide a technical analysis of the challenges\nand opportunities of AIGC from upstream, midstream, and downstream paths of\nblack/shadow industries and suggest future directions for improving existing\nrisk control systems. The paper will explore the new black and shadow\ntechniques triggered by generative AI technology and provide insights for\nbuilding the next-generation DRM system.",
                "Applying Interdisciplinary Frameworks to Understand Algorithmic\n  Decision-Making\nWe argue that explanations for \"algorithmic decision-making\" (ADM) systems\ncan profit by adopting practices that are already used in the learning\nsciences. We shortly introduce the importance of explaining ADM systems, give a\nbrief overview of approaches drawing from other disciplines to improve\nexplanations, and present the results of our qualitative task-based study\nincorporating the \"six facets of understanding\" framework. We close with\nquestions guiding the discussion of how future studies can leverage an\ninterdisciplinary approach.",
                "Image Classification of Stroke Blood Clot Origin using Deep\n  Convolutional Neural Networks and Visual Transformers\nStroke is one of two main causes of death worldwide. Many individuals suffer\nfrom ischemic stroke every year. Only in US more over 700,000 individuals meet\nischemic stroke due to blood clot blocking an artery to the brain every year.\nThe paper describes particular approach how to apply Artificial Intelligence\nfor purposes of separating two major acute ischemic stroke (AIS) etiology\nsubtypes: cardiac and large artery atherosclerosis. Four deep neural network\narchitectures and simple ensemble method are used in the approach.",
                "Alert of the Second Decision-maker: An Introduction to Human-AI Conflict\nThe collaboration between humans and artificial intelligence (AI) is a\nsignificant feature in this digital age. However, humans and AI may have\nobservation, interpretation, and action conflicts when working synchronously.\nThis phenomenon is often masked by faults and, unfortunately, overlooked. This\npaper systematically introduces the human-AI conflict concept, causes,\nmeasurement methods, and risk assessment. The results highlight that there is a\npotential second decision-maker besides the human, which is the AI; the\nhuman-AI conflict is a unique and emerging risk in digitalized process systems;\nand this is an interdisciplinary field that needs to be distinguished from\ntraditional fault and failure analysis; the conflict risk is significant and\ncannot be ignored."
            ],
            "interesting paper": 3
        },
        {
            "papers": [
                "Mindstorms in Natural Language-Based Societies of Mind\nBoth Minsky's \"society of mind\" and Schmidhuber's \"learning to think\" inspire\ndiverse societies of large multimodal neural networks (NNs) that solve problems\nby interviewing each other in a \"mindstorm.\" Recent implementations of NN-based\nsocieties of minds consist of large language models (LLMs) and other NN-based\nexperts communicating through a natural language interface. In doing so, they\novercome the limitations of single LLMs, improving multimodal zero-shot\nreasoning. In these natural language-based societies of mind (NLSOMs), new\nagents -- all communicating through the same universal symbolic language -- are\neasily added in a modular fashion. To demonstrate the power of NLSOMs, we\nassemble and experiment with several of them (having up to 129 members),\nleveraging mindstorms in them to solve some practical AI tasks: visual question\nanswering, image captioning, text-to-image synthesis, 3D generation, egocentric\nretrieval, embodied AI, and general language-based task solving. We view this\nas a starting point towards much larger NLSOMs with billions of agents-some of\nwhich may be humans. And with this emergence of great societies of\nheterogeneous minds, many new research questions have suddenly become paramount\nto the future of artificial intelligence. What should be the social structure\nof an NLSOM? What would be the (dis)advantages of having a monarchical rather\nthan a democratic structure? How can principles of NN economies be used to\nmaximize the total reward of a reinforcement learning NLSOM? In this work, we\nidentify, discuss, and try to answer some of these questions.",
                "Frontier AI developers need an internal audit function\nThis article argues that frontier artificial intelligence (AI) developers\nneed an internal audit function. First, it describes the role of internal audit\nin corporate governance: internal audit evaluates the adequacy and\neffectiveness of a company's risk management, control, and governance\nprocesses. It is organizationally independent from senior management and\nreports directly to the board of directors, typically its audit committee. In\nthe IIA's Three Lines Model, internal audit serves as the third line and is\nresponsible for providing assurance to the board, while the Combined Assurance\nFramework highlights the need to coordinate the activities of internal and\nexternal assurance providers. Next, the article provides an overview of key\ngovernance challenges in frontier AI development: dangerous capabilities can\narise unpredictably and undetected; it is difficult to prevent a deployed model\nfrom causing harm; frontier models can proliferate rapidly; it is inherently\ndifficult to assess frontier AI risks; and frontier AI developers do not seem\nto follow best practices in risk governance. Finally, the article discusses how\nan internal audit function could address some of these challenges: internal\naudit could identify ineffective risk management practices; it could ensure\nthat the board of directors has a more accurate understanding of the current\nlevel of risk and the adequacy of the developer's risk management practices;\nand it could serve as a contact point for whistleblowers. In light of rapid\nprogress in AI research and development, frontier AI developers need to\nstrengthen their risk governance. Instead of reinventing the wheel, they should\nfollow existing best practices. While this might not be sufficient, they should\nnot skip this obvious first step.",
                "How Do UX Practitioners Communicate AI as a Design Material? Artifacts,\n  Conceptions, and Propositions\nUX practitioners (UXPs) face novel challenges when working with and\ncommunicating artificial intelligence (AI) as a design material. We explore how\nUXPs communicate AI concepts when given hands-on experience training and\nexperimenting with AI models. To do so, we conducted a task-based design study\nwith 27 UXPs in which they prototyped and created a design presentation for a\nAI-enabled interface while having access to a simple AI model training tool.\nThrough analyzing UXPs' design presentations and post-activity interviews, we\nfound that although UXPs struggled to clearly communicate some AI concepts,\ntinkering with AI broadened common ground when communicating with technical\nstakeholders. UXPs also identified key risks and benefits of AI in their\ndesigns, and proposed concrete next steps for both UX and AI work. We conclude\nwith a sensitizing concept and recommendations for design and AI tools to\nenhance multi-stakeholder communication and collaboration when crafting\nhuman-centered AI experiences.",
                "Towards Cognitive Bots: Architectural Research Challenges\nSoftware bots operating in multiple virtual digital platforms must understand\nthe platforms' affordances and behave like human users. Platform affordances or\nfeatures differ from one application platform to another or through a life\ncycle, requiring such bots to be adaptable. Moreover, bots in such platforms\ncould cooperate with humans or other software agents for work or to learn\nspecific behavior patterns. However, present-day bots, particularly chatbots,\nother than language processing and prediction, are far from reaching a human\nuser's behavior level within complex business information systems. They lack\nthe cognitive capabilities to sense and act in such virtual environments,\nrendering their development a challenge to artificial general intelligence\nresearch. In this study, we problematize and investigate assumptions in\nconceptualizing software bot architecture by directing attention to significant\narchitectural research challenges in developing cognitive bots endowed with\ncomplex behavior for operation on information systems. As an outlook, we\npropose alternate architectural assumptions to consider in future bot design\nand bot development frameworks.",
                "Exploiting Large Neuroimaging Datasets to Create Connectome-Constrained\n  Approaches for more Robust, Efficient, and Adaptable Artificial Intelligence\nDespite the progress in deep learning networks, efficient learning at the\nedge (enabling adaptable, low-complexity machine learning solutions) remains a\ncritical need for defense and commercial applications. We envision a pipeline\nto utilize large neuroimaging datasets, including maps of the brain which\ncapture neuron and synapse connectivity, to improve machine learning\napproaches. We have pursued different approaches within this pipeline\nstructure. First, as a demonstration of data-driven discovery, the team has\ndeveloped a technique for discovery of repeated subcircuits, or motifs. These\nwere incorporated into a neural architecture search approach to evolve network\narchitectures. Second, we have conducted analysis of the heading direction\ncircuit in the fruit fly, which performs fusion of visual and angular velocity\nfeatures, to explore augmenting existing computational models with new insight.\nOur team discovered a novel pattern of connectivity, implemented a new model,\nand demonstrated sensor fusion on a robotic platform. Third, the team analyzed\ncircuitry for memory formation in the fruit fly connectome, enabling the design\nof a novel generative replay approach. Finally, the team has begun analysis of\nconnectivity in mammalian cortex to explore potential improvements to\ntransformer networks. These constraints increased network robustness on the\nmost challenging examples in the CIFAR-10-C computer vision robustness\nbenchmark task, while reducing learnable attention parameters by over an order\nof magnitude. Taken together, these results demonstrate multiple potential\napproaches to utilize insight from neural systems for developing robust and\nefficient machine learning techniques.",
                "Training Socially Aligned Language Models on Simulated Social\n  Interactions\nSocial alignment in AI systems aims to ensure that these models behave\naccording to established societal values. However, unlike humans, who derive\nconsensus on value judgments through social interaction, current language\nmodels (LMs) are trained to rigidly replicate their training corpus in\nisolation, leading to subpar generalization in unfamiliar scenarios and\nvulnerability to adversarial attacks. This work presents a novel training\nparadigm that permits LMs to learn from simulated social interactions. In\ncomparison to existing methodologies, our approach is considerably more\nscalable and efficient, demonstrating superior performance in alignment\nbenchmarks and human evaluations. This paradigm shift in the training of LMs\nbrings us a step closer to developing AI systems that can robustly and\naccurately reflect societal norms and values."
            ],
            "interesting paper": 2
        },
        {
            "papers": [
                "MLOps: A Step Forward to Enterprise Machine Learning\nMachine Learning Operations (MLOps) is becoming a highly crucial part of\nbusinesses looking to capitalize on the benefits of AI and ML models. This\nresearch presents a detailed review of MLOps, its benefits, difficulties,\nevolutions, and important underlying technologies such as MLOps frameworks,\nDocker, GitHub actions, and Kubernetes. The MLOps workflow, which includes\nmodel design, deployment, and operations, is explained in detail along with the\nvarious tools necessary for both model and data exploration and deployment.\nThis article also puts light on the end-to-end production of ML projects using\nvarious maturity levels of automated pipelines, with the least at no automation\nat all and the highest with complete CI/CD and CT capabilities. Furthermore, a\ndetailed example of an enterprise-level MLOps project for an object detection\nservice is used to explain the workflow of the technology in a real-world\nscenario. For this purpose, a web application hosting a pre-trained model from\nTensorFlow 2 Model Zoo is packaged and deployed to the internet making sure\nthat the system is scalable, reliable, and optimized for deployment at an\nenterprise level.",
                "AI Coach Assist: An Automated Approach for Call Recommendation in\n  Contact Centers for Agent Coaching\nIn recent years, the utilization of Artificial Intelligence (AI) in the\ncontact center industry is on the rise. One area where AI can have a\nsignificant impact is in the coaching of contact center agents. By analyzing\ncall transcripts using Natural Language Processing (NLP) techniques, it would\nbe possible to quickly determine which calls are most relevant for coaching\npurposes. In this paper, we present AI Coach Assist, which leverages the\npre-trained transformer-based language models to determine whether a given call\nis coachable or not based on the quality assurance (QA) questions asked by the\ncontact center managers or supervisors. The system was trained and evaluated on\na large dataset collected from real-world contact centers and provides an\neffective way to recommend calls to the contact center managers that are more\nlikely to contain coachable moments. Our experimental findings demonstrate the\npotential of AI Coach Assist to improve the coaching process, resulting in\nenhancing the performance of contact center agents.",
                "A Comprehensive Overview and Comparative Analysis on Deep Learning\n  Models: CNN, RNN, LSTM, GRU\nDeep learning (DL) has emerged as a powerful subset of machine learning (ML)\nand artificial intelligence (AI), outperforming traditional ML methods,\nespecially in handling unstructured and large datasets. Its impact spans across\nvarious domains, including speech recognition, healthcare, autonomous vehicles,\ncybersecurity, predictive analytics, and more. However, the complexity and\ndynamic nature of real-world problems present challenges in designing effective\ndeep learning models. Consequently, several deep learning models have been\ndeveloped to address different problems and applications. In this article, we\nconduct a comprehensive survey of various deep learning models, including\nConvolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs),\nGenerative Models, Deep Reinforcement Learning (DRL), and Deep Transfer\nLearning. We examine the structure, applications, benefits, and limitations of\neach model. Furthermore, we perform an analysis using three publicly available\ndatasets: IMDB, ARAS, and Fruit-360. We compare the performance of six renowned\ndeep learning models: CNN, Simple RNN, Long Short-Term Memory (LSTM),\nBidirectional LSTM, Gated Recurrent Unit (GRU), and Bidirectional GRU.",
                "Synthesizing a Progression of Subtasks for Block-Based Visual\n  Programming Tasks\nBlock-based visual programming environments play an increasingly important\nrole in introducing computing concepts to K-12 students. In recent years, they\nhave also gained popularity in neuro-symbolic AI, serving as a benchmark to\nevaluate general problem-solving and logical reasoning skills. The open-ended\nand conceptual nature of these visual programming tasks make them challenging,\nboth for state-of-the-art AI agents as well as for novice programmers. A\nnatural approach to providing assistance for problem-solving is breaking down a\ncomplex task into a progression of simpler subtasks; however, this is not\ntrivial given that the solution codes are typically nested and have non-linear\nexecution behavior. In this paper, we formalize the problem of synthesizing\nsuch a progression for a given reference block-based visual programming task.\nWe propose a novel synthesis algorithm that generates a progression of subtasks\nthat are high-quality, well-spaced in terms of their complexity, and solving\nthis progression leads to solving the reference task. We show the utility of\nour synthesis algorithm in improving the efficacy of AI agents (in this case,\nneural program synthesizers) for solving tasks in the Karel programming\nenvironment. Then, we conduct a user study to demonstrate that our synthesized\nprogression of subtasks can assist a novice programmer in solving tasks in the\nHour of Code: Maze Challenge by Code-dot-org.",
                "Integrating Action Knowledge and LLMs for Task Planning and Situation\n  Handling in Open Worlds\nTask planning systems have been developed to help robots use human knowledge\n(about actions) to complete long-horizon tasks. Most of them have been\ndeveloped for \"closed worlds\" while assuming the robot is provided with\ncomplete world knowledge. However, the real world is generally open, and the\nrobots frequently encounter unforeseen situations that can potentially break\nthe planner's completeness. Could we leverage the recent advances on\npre-trained Large Language Models (LLMs) to enable classical planning systems\nto deal with novel situations?\n  This paper introduces a novel framework, called COWP, for open-world task\nplanning and situation handling. COWP dynamically augments the robot's action\nknowledge, including the preconditions and effects of actions, with\ntask-oriented commonsense knowledge. COWP embraces the openness from LLMs, and\nis grounded to specific domains via action knowledge. For systematic\nevaluations, we collected a dataset that includes 1,085 execution-time\nsituations. Each situation corresponds to a state instance wherein a robot is\npotentially unable to complete a task using a solution that normally works.\nExperimental results show that our approach outperforms competitive baselines\nfrom the literature in the success rate of service tasks. Additionally, we have\ndemonstrated COWP using a mobile manipulator. Supplementary materials are\navailable at: https://cowplanning.github.io/",
                "Counterfactual Formulation of Patient-Specific Root Causes of Disease\nRoot causes of disease intuitively correspond to root vertices that increase\nthe likelihood of a diagnosis. This description of a root cause nevertheless\nlacks the rigorous mathematical formulation needed for the development of\ncomputer algorithms designed to automatically detect root causes from data.\nPrior work defined patient-specific root causes of disease using an\ninterventionalist account that only climbs to the second rung of Pearl's Ladder\nof Causation. In this theoretical piece, we climb to the third rung by\nproposing a counterfactual definition matching clinical intuition based on\nfixed factual data alone. We then show how to assign a root causal contribution\nscore to each variable using Shapley values from explainable artificial\nintelligence. The proposed counterfactual formulation of patient-specific root\ncauses of disease accounts for noisy labels, adapts to disease prevalence and\nadmits fast computation without the need for counterfactual simulation."
            ],
            "interesting paper": 5
        },
        {
            "papers": [
                "JutePestDetect: An Intelligent Approach for Jute Pest Identification\n  Using Fine-Tuned Transfer Learning\nIn certain Asian countries, Jute is one of the primary sources of income and\nGross Domestic Product (GDP) for the agricultural sector. Like many other\ncrops, Jute is prone to pest infestations, and its identification is typically\nmade visually in countries like Bangladesh, India, Myanmar, and China. In\naddition, this method is time-consuming, challenging, and somewhat imprecise,\nwhich poses a substantial financial risk. To address this issue, the study\nproposes a high-performing and resilient transfer learning (TL) based\nJutePestDetect model to identify jute pests at the early stage. Firstly, we\nprepared jute pest dataset containing 17 classes and around 380 photos per pest\nclass, which were evaluated after manual and automatic pre-processing and\ncleaning, such as background removal and resizing. Subsequently, five prominent\npre-trained models -DenseNet201, InceptionV3, MobileNetV2, VGG19, and ResNet50\nwere selected from a previous study to design the JutePestDetect model. Each\nmodel was revised by replacing the classification layer with a global average\npooling layer and incorporating a dropout layer for regularization. To evaluate\nthe models performance, various metrics such as precision, recall, F1 score,\nROC curve, and confusion matrix were employed. These analyses provided\nadditional insights for determining the efficacy of the models. Among them, the\ncustomized regularized DenseNet201-based proposed JutePestDetect model\noutperformed the others, achieving an impressive accuracy of 99%. As a result,\nour proposed method and strategy offer an enhanced approach to pest\nidentification in the case of Jute, which can significantly benefit farmers\nworldwide.",
                "Re-imagining health and well-being in low resource African settings\n  using an augmented AI system and a 3D digital twin\nThis paper discusses and explores the potential and relevance of recent\ndevelopments in artificial intelligence (AI) and digital twins for health and\nwell-being in low-resource African countries. We use the case of public health\nemergency response to disease outbreaks and epidemic control. There is\npotential to take advantage of the increasing availability of data and\ndigitization to develop advanced AI methods for analysis and prediction. Using\nan AI systems perspective, we review emerging trends in AI systems and digital\ntwins and propose an initial augmented AI system architecture to illustrate how\nan AI system can work with a 3D digital twin to address public health goals. We\nhighlight scientific knowledge discovery, continual learning, pragmatic\ninteroperability, and interactive explanation and decision-making as essential\nresearch challenges for AI systems and digital twins.",
                "AI Audit: A Card Game to Reflect on Everyday AI Systems\nAn essential element of K-12 AI literacy is educating learners about the\nethical and societal implications of AI systems. Previous work in AI ethics\nliteracy have developed curriculum and classroom activities that engage\nlearners in reflecting on the ethical implications of AI systems and developing\nresponsible AI. There is little work in using game-based learning methods in AI\nliteracy. Games are known to be compelling media to teach children about\ncomplex STEM concepts. In this work, we developed a competitive card game for\nmiddle and high school students called \"AI Audit\" where they play as AI\nstart-up founders building novel AI-powered technology. Players can challenge\nother players with potential harms of their technology or defend their own\nbusinesses by features that mitigate these harms. The game mechanics reward\nsystems that are ethically developed or that take steps to mitigate potential\nharms. In this paper, we present the game design, teacher resources for\nclassroom deployment and early playtesting results. We discuss our reflections\nabout using games as teaching tools for AI literacy in K-12 classrooms.",
                "The Digital Divide in Process Safety: Quantitative Risk Analysis of\n  Human-AI Collaboration\nDigital technologies have dramatically accelerated the digital transformation\nin process industries, boosted new industrial applications, upgraded the\nproduction system, and enhanced operational efficiency. In contrast, the\nchallenges and gaps between human and artificial intelligence (AI) have become\nmore and more prominent, whereas the digital divide in process safety is\naggregating. The study attempts to address the following questions: (i)What is\nAI in the process safety context? (ii)What is the difference between AI and\nhumans in process safety? (iii)How do AI and humans collaborate in process\nsafety? (iv)What are the challenges and gaps in human-AI collaboration? (v)How\nto quantify the risk of human-AI collaboration in process safety? Qualitative\nrisk analysis based on brainstorming and literature review, and quantitative\nrisk analysis based on layer of protection analysis (LOPA) and Bayesian network\n(BN), were applied to explore and model. The importance of human reliability\nshould be stressed in the digital age, not usually to increase the reliability\nof AI, and human-centered AI design in process safety needs to be propagated.",
                "The Social Value of Dark Energy\nAstrophysics is a social enterprise exemplified here by the Dark Energy\nSurvey (DES) which completed its fieldwork in 2019 after 16 years of\npreparation and observation, while data analysis continues. Society funds\nastrophysics on a grand scale. For human capital and for governance the\ndiscipline draws on a self-governing \"republic of science\", while the funds\nwere provided by philanthropists in the past, and by governments today. The\nbenefits accrue initially to scientists themselves, in the form of a rewarding\nvocation. For the social benefit it is tempting to apply formal cost benefit\nanalysis, but that approach ignores the option value of science and imposes\nquestionable assumptions from welfare economics. Astrophysics generates some\nuseful spinoffs, offers attractive careers, appeals to the popular imagination,\nspeaks to metaphysical cravings and constitutes a good in itself. The rise of\nAI also suggests a role in exploring future habitats for intelligence and\ncognition.",
                "Towards a Technology-Driven Adaptive Decision Support System for\n  Integrated Pavement and Maintenance strategies (TDADSS-IPM): focus on risk\n  assessment framework for climate change adaptation\nDecision Support Systems for pavement and maintenance strategies have\ntraditionally been designed as silos led to local optimum systems. Moreover,\nsince big data usage didn't exist as result of Industry 4.0 as of today, DSSs\nwere not initially designed adaptive to the sources of uncertainties led to\nrigid decisions. Motivated by the vulnerability of the road assets to the\nclimate phenomena, this paper takes a visionary step towards introducing a\nTechnology-Driven Adaptive Decision Support System for Integrated Pavement and\nMaintenance activities called TDADSS-IPM. As part of such DSS, a bottom-up risk\nassessment model is met via Bayesian Belief Networks (BBN) to realize the\nactual condition of the Danish roads due to weather condition. Such model fills\nthe gaps in the knowledge domain and develops a platform that can be trained\nover time, and applied in real-time to the actual event."
            ],
            "interesting paper": 1
        },
        {
            "papers": [
                "RE-centric Recommendations for the Development of Trustworthy(er)\n  Autonomous Systems\nComplying with the EU AI Act (AIA) guidelines while developing and\nimplementing AI systems will soon be mandatory within the EU. However,\npractitioners lack actionable instructions to operationalise ethics during AI\nsystems development. A literature review of different ethical guidelines\nrevealed inconsistencies in the principles addressed and the terminology used\nto describe them. Furthermore, requirements engineering (RE), which is\nidentified to foster trustworthiness in the AI development process from the\nearly stages was observed to be absent in a lot of frameworks that support the\ndevelopment of ethical and trustworthy AI. This incongruous phrasing combined\nwith a lack of concrete development practices makes trustworthy AI development\nharder. To address this concern, we formulated a comparison table for the\nterminology used and the coverage of the ethical AI principles in major ethical\nAI guidelines. We then examined the applicability of ethical AI development\nframeworks for performing effective RE during the development of trustworthy AI\nsystems. A tertiary review and meta-analysis of literature discussing ethical\nAI frameworks revealed their limitations when developing trustworthy AI. Based\non our findings, we propose recommendations to address such limitations during\nthe development of trustworthy AI.",
                "Towards a Unifying Model of Rationality in Multiagent Systems\nMultiagent systems deployed in the real world need to cooperate with other\nagents (including humans) nearly as effectively as these agents cooperate with\none another. To design such AI, and provide guarantees of its effectiveness, we\nneed to clearly specify what types of agents our AI must be able to cooperate\nwith. In this work we propose a generic model of socially intelligent agents,\nwhich are individually rational learners that are also able to cooperate with\none another (in the sense that their joint behavior is Pareto efficient). We\ndefine rationality in terms of the regret incurred by each agent over its\nlifetime, and show how we can construct socially intelligent agents for\ndifferent forms of regret. We then discuss the implications of this model for\nthe development of \"robust\" MAS that can cooperate with a wide variety of\nsocially intelligent agents.",
                "Shapley Based Residual Decomposition for Instance Analysis\nIn this paper, we introduce the idea of decomposing the residuals of\nregression with respect to the data instances instead of features. This allows\nus to determine the effects of each individual instance on the model and each\nother, and in doing so makes for a model-agnostic method of identifying\ninstances of interest. In doing so, we can also determine the appropriateness\nof the model and data in the wider context of a given study. The paper focuses\non the possible applications that such a framework brings to the relatively\nunexplored field of instance analysis in the context of Explainable AI tasks.",
                "An Emergency Disposal Decision-making Method with Human--Machine\n  Collaboration\nRapid developments in artificial intelligence technology have led to unmanned\nsystems replacing human beings in many fields requiring high-precision\npredictions and decisions. In modern operational environments, all job plans\nare affected by emergency events such as equipment failures and resource\nshortages, making a quick resolution critical. The use of unmanned systems to\nassist decision-making can improve resolution efficiency, but their\ndecision-making is not interpretable and may make the wrong decisions. Current\nunmanned systems require human supervision and control. Based on this, we\npropose a collaborative human--machine method for resolving unplanned events\nusing two phases: task filtering and task scheduling. In the task filtering\nphase, we propose a human--machine collaborative decision-making algorithm for\ndynamic tasks. The GACRNN model is used to predict the state of the job nodes,\nlocate the key nodes, and generate a machine-predicted resolution task list. A\nhuman decision-maker supervises the list in real time and modifies and confirms\nthe machine-predicted list through the human--machine interface. In the task\nscheduling phase, we propose a scheduling algorithm that integrates human\nexperience constraints. The steps to resolve an event are inserted into the\nnormal job sequence to schedule the resolution. We propose several\nhuman--machine collaboration methods in each phase to generate steps to resolve\nan unplanned event while minimizing the impact on the original job plan.",
                "Development of a ROS-based Architecture for Intelligent Autonomous on\n  Demand Last Mile Delivery\nThis paper presents the development of the JKU-ITS Last Mile Delivery Robot.\nThe proposed approach utilizes a combination of one 3D LIDAR, RGB-D camera, IMU\nand GPS sensor on top of a mobile robot slope mower. An embedded computer,\nrunning ROS1, is utilized to process the sensor data streams to enable 2D and\n3D Simultaneous Localization and Mapping, 2D localization and object detection\nusing a convolutional neural network.",
                "Generalized Disparate Impact for Configurable Fairness Solutions in ML\nWe make two contributions in the field of AI fairness over continuous\nprotected attributes. First, we show that the Hirschfeld-Gebelein-Renyi (HGR)\nindicator (the only one currently available for such a case) is valuable but\nsubject to a few crucial limitations regarding semantics, interpretability, and\nrobustness. Second, we introduce a family of indicators that are: 1)\ncomplementary to HGR in terms of semantics; 2) fully interpretable and\ntransparent; 3) robust over finite samples; 4) configurable to suit specific\napplications. Our approach also allows us to define fine-grained constraints to\npermit certain types of dependence and forbid others selectively. By expanding\nthe available options for continuous protected attributes, our approach\nrepresents a significant contribution to the area of fair artificial\nintelligence."
            ],
            "interesting paper": 1
        },
        {
            "papers": [
                "Intent-aligned AI systems deplete human agency: the need for agency\n  foundations research in AI safety\nThe rapid advancement of artificial intelligence (AI) systems suggests that\nartificial general intelligence (AGI) systems may soon arrive. Many researchers\nare concerned that AIs and AGIs will harm humans via intentional misuse\n(AI-misuse) or through accidents (AI-accidents). In respect of AI-accidents,\nthere is an increasing effort focused on developing algorithms and paradigms\nthat ensure AI systems are aligned to what humans intend, e.g. AI systems that\nyield actions or recommendations that humans might judge as consistent with\ntheir intentions and goals. Here we argue that alignment to human intent is\ninsufficient for safe AI systems and that preservation of long-term agency of\nhumans may be a more robust standard, and one that needs to be separated\nexplicitly and a priori during optimization. We argue that AI systems can\nreshape human intention and discuss the lack of biological and psychological\nmechanisms that protect humans from loss of agency. We provide the first formal\ndefinition of agency-preserving AI-human interactions which focuses on\nforward-looking agency evaluations and argue that AI systems - not humans -\nmust be increasingly tasked with making these evaluations. We show how agency\nloss can occur in simple environments containing embedded agents that use\ntemporal-difference learning to make action recommendations. Finally, we\npropose a new area of research called \"agency foundations\" and pose four\ninitial topics designed to improve our understanding of agency in AI-human\ninteractions: benevolent game theory, algorithmic foundations of human rights,\nmechanistic interpretability of agency representation in neural-networks and\nreinforcement learning from internal states.",
                "Evaluating GPT's Programming Capability through CodeWars' Katas\nIn the burgeoning field of artificial intelligence (AI), understanding the\ncapabilities and limitations of programming-oriented models is crucial. This\npaper presents a novel evaluation of the programming proficiency of Generative\nPretrained Transformer (GPT) models, specifically GPT-3.5 and GPT-4, against\ncoding problems of varying difficulty levels drawn from Codewars. The\nexperiments reveal a distinct boundary at the 3kyu level, beyond which these\nGPT models struggle to provide solutions. These findings led to the proposal of\na measure for coding problem complexity that incorporates both problem\ndifficulty and the time required for solution. The research emphasizes the need\nfor validation and creative thinking capabilities in AI models to better\nemulate human problem-solving techniques. Future work aims to refine this\nproposed complexity measure, enhance AI models with these suggested\ncapabilities, and develop an objective measure for programming problem\ndifficulty. The results of this research offer invaluable insights for\nimproving AI programming capabilities and advancing the frontier of AI\nproblem-solving abilities.",
                "Explainable AI for Malnutrition Risk Prediction from m-Health and\n  Clinical Data\nMalnutrition is a serious and prevalent health problem in the older\npopulation, and especially in hospitalised or institutionalised subjects.\nAccurate and early risk detection is essential for malnutrition management and\nprevention. M-health services empowered with Artificial Intelligence (AI) may\nlead to important improvements in terms of a more automatic, objective, and\ncontinuous monitoring and assessment. Moreover, the latest Explainable AI (XAI)\nmethodologies may make AI decisions interpretable and trustworthy for end\nusers. This paper presents a novel AI framework for early and explainable\nmalnutrition risk detection based on heterogeneous m-health data. We performed\nan extensive model evaluation including both subject-independent and\npersonalised predictions, and the obtained results indicate Random Forest (RF)\nand Gradient Boosting as the best performing classifiers, especially when\nincorporating body composition assessment data. We also investigated several\nbenchmark XAI methods to extract global model explanations. Model-specific\nexplanation consistency assessment indicates that each selected model\nprivileges similar subsets of the most relevant predictors, with the highest\nagreement shown between SHapley Additive ExPlanations (SHAP) and feature\npermutation method. Furthermore, we performed a preliminary clinical validation\nto verify that the learned feature-output trends are compliant with the current\nevidence-based assessment.",
                "Towards Explainable and Language-Agnostic LLMs: Symbolic Reverse\n  Engineering of Language at Scale\nLarge language models (LLMs) have achieved a milestone that undenia-bly\nchanged many held beliefs in artificial intelligence (AI). However, there\nremains many limitations of these LLMs when it comes to true language\nunderstanding, limitations that are a byproduct of the under-lying architecture\nof deep neural networks. Moreover, and due to their subsymbolic nature,\nwhatever knowledge these models acquire about how language works will always be\nburied in billions of microfeatures (weights), none of which is meaningful on\nits own, making such models hopelessly unexplainable. To address these\nlimitations, we suggest com-bining the strength of symbolic representations\nwith what we believe to be the key to the success of LLMs, namely a successful\nbottom-up re-verse engineering of language at scale. As such we argue for a\nbottom-up reverse engineering of language in a symbolic setting. Hints on what\nthis project amounts to have been suggested by several authors, and we discuss\nin some detail here how this project could be accomplished.",
                "Disentangling and Operationalizing AI Fairness at LinkedIn\nOperationalizing AI fairness at LinkedIn's scale is challenging not only\nbecause there are multiple mutually incompatible definitions of fairness but\nalso because determining what is fair depends on the specifics and context of\nthe product where AI is deployed. Moreover, AI practitioners need clarity on\nwhat fairness expectations need to be addressed at the AI level. In this paper,\nwe present the evolving AI fairness framework used at LinkedIn to address these\nthree challenges. The framework disentangles AI fairness by separating out\nequal treatment and equitable product expectations. Rather than imposing a\ntrade-off between these two commonly opposing interpretations of fairness, the\nframework provides clear guidelines for operationalizing equal AI treatment\ncomplemented with a product equity strategy. This paper focuses on the equal AI\ntreatment component of LinkedIn's AI fairness framework, shares the principles\nthat support it, and illustrates their application through a case study. We\nhope this paper will encourage other big tech companies to join us in sharing\ntheir approach to operationalizing AI fairness at scale, so that together we\ncan keep advancing this constantly evolving field.",
                "NetHack is Hard to Hack\nNeural policy learning methods have achieved remarkable results in various\ncontrol problems, ranging from Atari games to simulated locomotion. However,\nthese methods struggle in long-horizon tasks, especially in open-ended\nenvironments with multi-modal observations, such as the popular dungeon-crawler\ngame, NetHack. Intriguingly, the NeurIPS 2021 NetHack Challenge revealed that\nsymbolic agents outperformed neural approaches by over four times in median\ngame score. In this paper, we delve into the reasons behind this performance\ngap and present an extensive study on neural policy learning for NetHack. To\nconduct this study, we analyze the winning symbolic agent, extending its\ncodebase to track internal strategy selection in order to generate one of the\nlargest available demonstration datasets. Utilizing this dataset, we examine\n(i) the advantages of an action hierarchy; (ii) enhancements in neural\narchitecture; and (iii) the integration of reinforcement learning with\nimitation learning. Our investigations produce a state-of-the-art neural agent\nthat surpasses previous fully neural policies by 127% in offline settings and\n25% in online settings on median game score. However, we also demonstrate that\nmere scaling is insufficient to bridge the performance gap with the best\nsymbolic models or even the top human players."
            ],
            "interesting paper": 1
        },
        {
            "papers": [
                "Credit Card Fraud Detection Using Asexual Reproduction Optimization\nAs the number of credit card users has increased, detecting fraud in this\ndomain has become a vital issue. Previous literature has applied various\nsupervised and unsupervised machine learning methods to find an effective fraud\ndetection system. However, some of these methods require an enormous amount of\ntime to achieve reasonable accuracy. In this paper, an Asexual Reproduction\nOptimization (ARO) approach was employed, which is a supervised method to\ndetect credit card fraud. ARO refers to a kind of production in which one\nparent produces some offspring. By applying this method and sampling just from\nthe majority class, the effectiveness of the classification is increased. A\ncomparison to Artificial Immune Systems (AIS), which is one of the best methods\nimplemented on current datasets, has shown that the proposed method is able to\nremarkably reduce the required training time and at the same time increase the\nrecall that is important in fraud detection problems. The obtained results show\nthat ARO achieves the best cost in a short time, and consequently, it can be\nconsidered a real-time fraud detection system.",
                "The Canadian Cropland Dataset: A New Land Cover Dataset for\n  Multitemporal Deep Learning Classification in Agriculture\nMonitoring land cover using remote sensing is vital for studying\nenvironmental changes and ensuring global food security through crop yield\nforecasting. Specifically, multitemporal remote sensing imagery provides\nrelevant information about the dynamics of a scene, which has proven to lead to\nbetter land cover classification results. Nevertheless, few studies have\nbenefited from high spatial and temporal resolution data due to the difficulty\nof accessing reliable, fine-grained and high-quality annotated samples to\nsupport their hypotheses. Therefore, we introduce a temporal patch-based\ndataset of Canadian croplands, enriched with labels retrieved from the Canadian\nAnnual Crop Inventory. The dataset contains 78,536 manually verified\nhigh-resolution (10 m/pixel, 640 x 640 m) geo-referenced images from 10 crop\nclasses collected over four crop production years (2017-2020) and five months\n(June-October). Each instance contains 12 spectral bands, an RGB image, and\nadditional vegetation index bands. Individually, each category contains at\nleast 4,800 images. Moreover, as a benchmark, we provide models and source code\nthat allow a user to predict the crop class using a single image (ResNet,\nDenseNet, EfficientNet) or a sequence of images (LRCN, 3D-CNN) from the same\nlocation. In perspective, we expect this evolving dataset to propel the\ncreation of robust agro-environmental models that can accelerate the\ncomprehension of complex agricultural regions by providing accurate and\ncontinuous monitoring of land cover.",
                "Responsible Design Patterns for Machine Learning Pipelines\nIntegrating ethical practices into the AI development process for artificial\nintelligence (AI) is essential to ensure safe, fair, and responsible operation.\nAI ethics involves applying ethical principles to the entire life cycle of AI\nsystems. This is essential to mitigate potential risks and harms associated\nwith AI, such as algorithm biases. To achieve this goal, responsible design\npatterns (RDPs) are critical for Machine Learning (ML) pipelines to guarantee\nethical and fair outcomes. In this paper, we propose a comprehensive framework\nincorporating RDPs into ML pipelines to mitigate risks and ensure the ethical\ndevelopment of AI systems. Our framework comprises new responsible AI design\npatterns for ML pipelines identified through a survey of AI ethics and data\nmanagement experts and validated through real-world scenarios with expert\nfeedback. The framework guides AI developers, data scientists, and\npolicy-makers to implement ethical practices in AI development and deploy\nresponsible AI systems in production.",
                "EMOTE: An Explainable architecture for Modelling the Other Through\n  Empathy\nWe can usually assume others have goals analogous to our own. This assumption\ncan also, at times, be applied to multi-agent games - e.g. Agent 1's attraction\nto green pellets is analogous to Agent 2's attraction to red pellets. This\n\"analogy\" assumption is tied closely to the cognitive process known as empathy.\nInspired by empathy, we design a simple and explainable architecture to model\nanother agent's action-value function. This involves learning an \"Imagination\nNetwork\" to transform the other agent's observed state in order to produce a\nhuman-interpretable \"empathetic state\" which, when presented to the learning\nagent, produces behaviours that mimic the other agent. Our approach is\napplicable to multi-agent scenarios consisting of a single learning agent and\nother (independent) agents acting according to fixed policies. This\narchitecture is particularly beneficial for (but not limited to) algorithms\nusing a composite value or reward function. We show our method produces better\nperformance in multi-agent games, where it robustly estimates the other's model\nin different environment configurations. Additionally, we show that the\nempathetic states are human interpretable, and thus verifiable.",
                "From Human-Centered to Social-Centered Artificial Intelligence:\n  Assessing ChatGPT's Impact through Disruptive Events\nLarge language models (LLMs) and dialogue agents have existed for years, but\nthe release of recent GPT models has been a watershed moment for artificial\nintelligence (AI) research and society at large. Immediately recognized for its\ngenerative capabilities and versatility, ChatGPT's impressive proficiency\nacross technical and creative domains led to its widespread adoption. While\nsociety grapples with the emerging cultural impacts of ChatGPT, critiques of\nChatGPT's impact within the machine learning community have coalesced around\nits performance or other conventional Responsible AI evaluations relating to\nbias, toxicity, and 'hallucination.' We argue that these latter critiques draw\nheavily on a particular conceptualization of the 'human-centered' framework,\nwhich tends to cast atomized individuals as the key recipients of both the\nbenefits and detriments of technology. In this article, we direct attention to\nanother dimension of LLMs and dialogue agents' impact: their effect on social\ngroups, institutions, and accompanying norms and practices. By illustrating\nChatGPT's social impact through three disruptive events, we challenge\nindividualistic approaches in AI development and contribute to ongoing debates\naround the ethical and responsible implementation of AI systems. We hope this\neffort will call attention to more comprehensive and longitudinal evaluation\ntools and compel technologists to go beyond human-centered thinking and ground\ntheir efforts through social-centered AI.",
                "AI Imagery and the Overton Window\nAI-based text-to-image generation has undergone a significant leap in the\nproduction of visually comprehensive and aesthetic imagery over the past year,\nto the point where differentiating between a man-made piece of art and an\nAI-generated image is becoming more difficult. Generative Models such as Stable\nDiffusion, Midjourney and others are expected to affect several major\nindustries in technological and ethical aspects. Striking the balance between\nraising human standard of life and work vs exploiting one group of people to\nenrich another is a complex and crucial part of the discussion. Due to the\nrapid growth of this technology, the way in which its models operate, and gray\narea legalities, visual and artistic domains - including the video game\nindustry, are at risk of being taken over from creators by AI infrastructure\nowners. This paper is a literature review examining the concerns facing both AI\ndevelopers and users today, including identity theft, data laundering and more.\nIt discusses legalization challenges and ethical concerns, and concludes with\nhow AI generative models can be tremendously useful in streamlining the process\nof visual creativity in both static and interactive media given proper\nregulation.\n  Keywords: AI text-to-image generation, Midjourney, Stable Diffusion, AI\nEthics, Game Design, Digital Art, Data Laundering"
            ],
            "interesting paper": 2
        },
        {
            "papers": [
                "AI Liability Insurance With an Example in AI-Powered E-diagnosis System\nArtificial Intelligence (AI) has received an increasing amount of attention\nin multiple areas. The uncertainties and risks in AI-powered systems have\ncreated reluctance in their wild adoption. As an economic solution to\ncompensate for potential damages, AI liability insurance is a promising market\nto enhance the integration of AI into daily life. In this work, we use an\nAI-powered E-diagnosis system as an example to study AI liability insurance. We\nprovide a quantitative risk assessment model with evidence-based numerical\nanalysis. We discuss the insurability criteria for AI technologies and suggest\nnecessary adjustments to accommodate the features of AI products. We show that\nAI liability insurance can act as a regulatory mechanism to incentivize\ncompliant behaviors and serve as a certificate of high-quality AI systems.\nFurthermore, we suggest premium adjustment to reflect the dynamic evolution of\nthe inherent uncertainty in AI. Moral hazard problems are discussed and\nsuggestions for AI liability insurance are provided.",
                "Cook-Gen: Robust Generative Modeling of Cooking Actions from Recipes\nAs people become more aware of their food choices, food computation models\nhave become increasingly popular in assisting people in maintaining healthy\neating habits. For example, food recommendation systems analyze recipe\ninstructions to assess nutritional contents and provide recipe recommendations.\nThe recent and remarkable successes of generative AI methods, such as\nauto-regressive large language models, can lead to robust methods for a more\ncomprehensive understanding of recipes for healthy food recommendations beyond\nsurface-level nutrition content assessments. In this study, we explore the use\nof generative AI methods to extend current food computation models, primarily\ninvolving the analysis of nutrition and ingredients, to also incorporate\ncooking actions (e.g., add salt, fry the meat, boil the vegetables, etc.).\nCooking actions are notoriously hard to model using statistical learning\nmethods due to irregular data patterns - significantly varying natural language\ndescriptions for the same action (e.g., marinate the meat vs. marinate the meat\nand leave overnight) and infrequently occurring patterns (e.g., add salt occurs\nfar more frequently than marinating the meat). The prototypical approach to\nhandling irregular data patterns is to increase the volume of data that the\nmodel ingests by orders of magnitude. Unfortunately, in the cooking domain,\nthese problems are further compounded with larger data volumes presenting a\nunique challenge that is not easily handled by simply scaling up. In this work,\nwe propose novel aggregation-based generative AI methods, Cook-Gen, that\nreliably generate cooking actions from recipes, despite difficulties with\nirregular data patterns, while also outperforming Large Language Models and\nother strong baselines.",
                "Prediction of Citrus Diseases Using Machine Learning And Deep Learning:\n  Classifier, Models SLR\nCitrus diseases have been major issues for citrus growing worldwide for many\nyears they can lead significantly reduce fruit quality. the most harmful citrus\ndiseases are citrus canker, citrus greening, citrus black spot, citrus leaf\nminer which can have significant economic losses of citrus industry in\nworldwide prevention and management strategies like chemical treatments. Citrus\ndiseases existing in all over the world where citrus is growing its effects the\ncitrus tree root, citrus tree leaf, citrus tree orange etc. Existing of citrus\ndiseases is highly impact on economic factor that can also produce low quality\nfruits and increased the rate for diseases management. Sanitation and routine\nmonitoring can be effective in managing certain citrus diseases, but others may\nrequire more intensive treatments like chemical or biological control methods.",
                "AI and the creative realm: A short review of current and future\n  applications\nThis study explores the concept of creativity and artificial intelligence\n(AI) and their recent integration. While AI has traditionally been perceived as\nincapable of generating new ideas or creating art, the development of more\nsophisticated AI models and the proliferation of human-computer interaction\ntools have opened up new possibilities for AI in artistic creation. This study\ninvestigates the various applications of AI in a creative context,\ndifferentiating between the type of art, language, and algorithms used. It also\nconsiders the philosophical implications of AI and creativity, questioning\nwhether consciousness can be researched in machines and AI's potential\ninterests and decision-making capabilities. Overall, we aim to stimulate a\nreflection on AI's use and ethical implications in creative contexts.",
                "The ethical ambiguity of AI data enrichment: Measuring gaps in research\n  ethics norms and practices\nThe technical progression of artificial intelligence (AI) research has been\nbuilt on breakthroughs in fields such as computer science, statistics, and\nmathematics. However, in the past decade AI researchers have increasingly\nlooked to the social sciences, turning to human interactions to solve the\nchallenges of model development. Paying crowdsourcing workers to generate or\ncurate data, or data enrichment, has become indispensable for many areas of AI\nresearch, from natural language processing to reinforcement learning from human\nfeedback (RLHF). Other fields that routinely interact with crowdsourcing\nworkers, such as Psychology, have developed common governance requirements and\nnorms to ensure research is undertaken ethically. This study explores how, and\nto what extent, comparable research ethics requirements and norms have\ndeveloped for AI research and data enrichment. We focus on the approach taken\nby two leading conferences: ICLR and NeurIPS, and journal publisher Springer.\nIn a longitudinal study of accepted papers, and via a comparison with\nPsychology and CHI papers, this work finds that leading AI venues have begun to\nestablish protocols for human data collection, but these are are inconsistently\nfollowed by authors. Whilst Psychology papers engaging with crowdsourcing\nworkers frequently disclose ethics reviews, payment data, demographic data and\nother information, similar disclosures are far less common in leading AI venues\ndespite similar guidance. The work concludes with hypotheses to explain these\ngaps in research ethics practices and considerations for its implications.",
                "Deep Q-Learning versus Proximal Policy Optimization: Performance\n  Comparison in a Material Sorting Task\nThis paper presents a comparison between two well-known deep Reinforcement\nLearning (RL) algorithms: Deep Q-Learning (DQN) and Proximal Policy\nOptimization (PPO) in a simulated production system. We utilize a Petri Net\n(PN)-based simulation environment, which was previously proposed in related\nwork. The performance of the two algorithms is compared based on several\nevaluation metrics, including average percentage of correctly assembled and\nsorted products, average episode length, and percentage of successful episodes.\nThe results show that PPO outperforms DQN in terms of all evaluation metrics.\nThe study highlights the advantages of policy-based algorithms in problems with\nhigh-dimensional state and action spaces. The study contributes to the field of\ndeep RL in context of production systems by providing insights into the\neffectiveness of different algorithms and their suitability for different\ntasks."
            ],
            "interesting paper": 2
        },
        {
            "papers": [
                "Enough With \"Human-AI Collaboration\"\nDescribing our interaction with Artificial Intelligence (AI) systems as\n'collaboration' is well-intentioned, but flawed. Not only is it misleading, but\nit also takes away the credit of AI 'labour' from the humans behind it, and\nerases and obscures an often exploitative arrangement between AI producers and\nconsumers. The AI 'collaboration' metaphor is merely the latest episode in a\nlong history of labour appropriation and credit reassignment that\ndisenfranchises labourers in the Global South. I propose that viewing AI as a\ntool or an instrument, rather than a collaborator, is more accurate, and\nultimately fairer.",
                "Accelerating science with human-aware artificial intelligence\nArtificial intelligence (AI) models trained on published scientific findings\nhave been used to invent valuable materials and targeted therapies, but they\ntypically ignore the human scientists who continually alter the landscape of\ndiscovery. Here we show that incorporating the distribution of human expertise\nby training unsupervised models on simulated inferences cognitively accessible\nto experts dramatically improves (up to 400%) AI prediction of future\ndiscoveries beyond those focused on research content alone, especially when\nrelevant literature is sparse. These models succeed by predicting human\npredictions and the scientists who will make them. By tuning human-aware AI to\navoid the crowd, we can generate scientifically promising \"alien\" hypotheses\nunlikely to be imagined or pursued without intervention until the distant\nfuture, which hold promise to punctuate scientific advance beyond questions\ncurrently pursued. Accelerating human discovery or probing its blind spots,\nhuman-aware AI enables us to move toward and beyond the contemporary scientific\nfrontier.",
                "XAI Renaissance: Redefining Interpretability in Medical Diagnostic\n  Models\nAs machine learning models become increasingly prevalent in medical\ndiagnostics, the need for interpretability and transparency becomes paramount.\nThe XAI Renaissance signifies a significant shift in the field, aiming to\nredefine the interpretability of medical diagnostic models. This paper explores\nthe innovative approaches and methodologies within the realm of Explainable AI\n(XAI) that are revolutionizing the interpretability of medical diagnostic\nmodels. By shedding light on the underlying decision-making process, XAI\ntechniques empower healthcare professionals to understand, trust, and\neffectively utilize these models for accurate and reliable medical diagnoses.\nThis review highlights the key advancements in XAI for medical diagnostics and\ntheir potential to transform the healthcare landscape, ultimately improving\npatient outcomes and fostering trust in AI-driven diagnostic systems.",
                "AlerTiger: Deep Learning for AI Model Health Monitoring at LinkedIn\nData-driven companies use AI models extensively to develop products and\nintelligent business solutions, making the health of these models crucial for\nbusiness success. Model monitoring and alerting in industries pose unique\nchallenges, including a lack of clear model health metrics definition, label\nsparsity, and fast model iterations that result in short-lived models and\nfeatures. As a product, there are also requirements for scalability,\ngeneralizability, and explainability. To tackle these challenges, we propose\nAlerTiger, a deep-learning-based MLOps model monitoring system that helps AI\nteams across the company monitor their AI models' health by detecting anomalies\nin models' input features and output score over time. The system consists of\nfour major steps: model statistics generation, deep-learning-based anomaly\ndetection, anomaly post-processing, and user alerting. Our solution generates\nthree categories of statistics to indicate AI model health, offers a two-stage\ndeep anomaly detection solution to address label sparsity and attain the\ngeneralizability of monitoring new models, and provides holistic reports for\nactionable alerts. This approach has been deployed to most of LinkedIn's\nproduction AI models for over a year and has identified several model issues\nthat later led to significant business metric gains after fixing.",
                "A Novel Correlation-optimized Deep Learning Method for Wind Speed\n  Forecast\nThe increasing installation rate of wind power poses great challenges to the\nglobal power system. In order to ensure the reliable operation of the power\nsystem, it is necessary to accurately forecast the wind speed and power of the\nwind turbines. At present, deep learning is progressively applied to the wind\nspeed prediction. Nevertheless, the recent deep learning methods still reflect\nthe embarrassment for practical applications due to model interpretability and\nhardware limitation. To this end, a novel deep knowledge-based learning method\nis proposed in this paper. The proposed method hybridizes pre-training method\nand auto-encoder structure to improve data representation and modeling of the\ndeep knowledge-based learning framework. In order to form knowledge and\ncorresponding absorbers, the original data is preprocessed by an optimization\nmodel based on correlation to construct multi-layer networks (knowledge) which\nare absorbed by sequence to sequence (Seq2Seq) models. Specifically, new\ncognition and memory units (CMU) are designed to reinforce traditional deep\nlearning framework. Finally, the effectiveness of the proposed method is\nverified by three wind prediction cases from a wind farm in Liaoning, China.\nExperimental results show that the proposed method increases the stability and\ntraining efficiency compared to the traditional LSTM method and LSTM/GRU-based\nSeq2Seq method for applications of wind speed forecasting.",
                "Generative Adversarial Networks for Data Augmentation\nOne way to expand the available dataset for training AI models in the medical\nfield is through the use of Generative Adversarial Networks (GANs) for data\naugmentation. GANs work by employing a generator network to create new data\nsamples that are then assessed by a discriminator network to determine their\nsimilarity to real samples. The discriminator network is taught to\ndifferentiate between actual and synthetic samples, while the generator system\nis trained to generate data that closely resemble real ones. The process is\nrepeated until the generator network can produce synthetic data that is\nindistinguishable from genuine data. GANs have been utilized in medical image\nanalysis for various tasks, including data augmentation, image creation, and\ndomain adaptation. They can generate synthetic samples that can be used to\nincrease the available dataset, especially in cases where obtaining large\namounts of genuine data is difficult or unethical. However, it is essential to\nnote that the use of GANs in medical imaging is still an active area of\nresearch to ensure that the produced images are of high quality and suitable\nfor use in clinical settings."
            ],
            "interesting paper": 2
        }
    ],
    "Riley Garcia": [
        {
            "papers": [
                "Asking Before Acting: Gather Information in Embodied Decision Making\n  with Language Models\nWith strong capabilities of reasoning and a broad understanding of the world,\nLarge Language Models (LLMs) have demonstrated immense potential in building\nversatile embodied decision-making agents capable of executing a wide array of\ntasks. Nevertheless, when deployed in unfamiliar environments, we show that LLM\nagents encounter challenges in efficiently gathering essential information,\nleading to suboptimal performance. Conversely, human individuals often seek\nadditional information from their peers prior to taking action, harnessing\nexternal knowledge to avoid unnecessary trial and error. Drawing inspiration\nfrom this behavior, we propose \\textit{Asking Before Acting} (ABA), a method\nthat empowers the agent to proactively inquire with external sources for\npertinent information using natural language during their interactions within\nthe environment. In this way, the agent is able to enhance its efficiency and\nperformance by circumventing potentially laborious steps and combating the\ndifficulties associated with exploration in unfamiliar environments and\nvagueness of the instructions. We conduct extensive experiments involving a\nspectrum of environments including text-based household everyday tasks, robot\narm manipulation tasks, and real world open domain image based embodied tasks.\nThe experiments involve various models from Vicuna to GPT-4. The results\ndemonstrate that, even with modest prompts modifications, ABA exhibits\nsubstantial advantages on both performance and efficiency over baseline LLM\nagents. Further finetuning ABA with reformulated metadata (ABA-FT) faciliates\nlearning the rationale for asking and allows for additional enhancements\nespecially in tasks that baselines struggle to solve.",
                "From Interactive to Co-Constructive Task Learning\nHumans have developed the capability to teach relevant aspects of new or\nadapted tasks to a social peer with very few task demonstrations by making use\nof scaffolding strategies that leverage prior knowledge and importantly prior\njoint experience to yield a joint understanding and a joint execution of the\nrequired steps to solve the task. This process has been discovered and analyzed\nin parent-infant interaction and constitutes a ``co-construction'' as it allows\nboth, the teacher and the learner, to jointly contribute to the task. We\npropose to focus research in robot interactive learning on this co-construction\nprocess to enable robots to learn from non-expert users in everyday situations.\nIn the following, we will review current proposals for interactive task\nlearning and discuss their main contributions with respect to the entailing\ninteraction. We then discuss our notion of co-construction and summarize\nresearch insights from adult-child and human-robot interactions to elucidate\nits nature in more detail. From this overview we finally derive research\ndesiderata that entail the dimensions architecture, representation, interaction\nand explainability.",
                "Role-Play with Large Language Models\nAs dialogue agents become increasingly human-like in their performance, it is\nimperative that we develop effective ways to describe their behaviour in\nhigh-level terms without falling into the trap of anthropomorphism. In this\npaper, we foreground the concept of role-play. Casting dialogue agent behaviour\nin terms of role-play allows us to draw on familiar folk psychological terms,\nwithout ascribing human characteristics to language models they in fact lack.\nTwo important cases of dialogue agent behaviour are addressed this way, namely\n(apparent) deception and (apparent) self-awareness.",
                "An Experimental Investigation into the Evaluation of Explainability\n  Methods\nEXplainable Artificial Intelligence (XAI) aims to help users to grasp the\nreasoning behind the predictions of an Artificial Intelligence (AI) system.\nMany XAI approaches have emerged in recent years. Consequently, a subfield\nrelated to the evaluation of XAI methods has gained considerable attention,\nwith the aim to determine which methods provide the best explanation using\nvarious approaches and criteria. However, the literature lacks a comparison of\nthe evaluation metrics themselves, that one can use to evaluate XAI methods.\nThis work aims to fill this gap by comparing 14 different metrics when applied\nto nine state-of-the-art XAI methods and three dummy methods (e.g., random\nsaliency maps) used as references. Experimental results show which of these\nmetrics produces highly correlated results, indicating potential redundancy. We\nalso demonstrate the significant impact of varying the baseline hyperparameter\non the evaluation metric values. Finally, we use dummy methods to assess the\nreliability of metrics in terms of ranking, pointing out their limitations.",
                "HuatuoGPT, towards Taming Language Model to Be a Doctor\nIn this paper, we present HuatuoGPT, a large language model (LLM) for medical\nconsultation. The core recipe of HuatuoGPT is to leverage both\n\\textit{distilled data from ChatGPT} and \\textit{real-world data from doctors}\nin the supervised fine-tuned stage. The responses of ChatGPT are usually\ndetailed, well-presented and informative while it cannot perform like a doctor\nin many aspects, e.g. for integrative diagnosis. We argue that real-world data\nfrom doctors would be complementary to distilled data in the sense the former\ncould tame a distilled language model to perform like doctors. To better\nleverage the strengths of both data, we train a reward model to align the\nlanguage model with the merits that both data bring, following an RLAIF\n(reinforced learning from AI feedback) fashion. To evaluate and benchmark the\nmodels, we propose a comprehensive evaluation scheme (including automatic and\nmanual metrics). Experimental results demonstrate that HuatuoGPT achieves\nstate-of-the-art results in performing medical consultation among open-source\nLLMs in GPT-4 evaluation, human evaluation, and medical benchmark datasets. It\nis worth noting that by using additional real-world data and RLAIF, the\ndistilled language model (i.e., HuatuoGPT) outperforms its teacher model\nChatGPT in most cases. Our code, data, and models are publicly available at\n\\url{https://github.com/FreedomIntelligence/HuatuoGPT}. The online demo is\navailable at \\url{https://www.HuatuoGPT.cn/}.",
                "MPE4G: Multimodal Pretrained Encoder for Co-Speech Gesture Generation\nWhen virtual agents interact with humans, gestures are crucial to delivering\ntheir intentions with speech. Previous multimodal co-speech gesture generation\nmodels required encoded features of all modalities to generate gestures. If\nsome input modalities are removed or contain noise, the model may not generate\nthe gestures properly. To acquire robust and generalized encodings, we propose\na novel framework with a multimodal pre-trained encoder for co-speech gesture\ngeneration. In the proposed method, the multi-head-attention-based encoder is\ntrained with self-supervised learning to contain the information on each\nmodality. Moreover, we collect full-body gestures that consist of 3D joint\nrotations to improve visualization and apply gestures to the extensible body\nmodel. Through the series of experiments and human evaluation, the proposed\nmethod renders realistic co-speech gestures not only when all input modalities\nare given but also when the input modalities are missing or noisy."
            ],
            "interesting paper": 1
        },
        {
            "papers": [
                "Alert of the Second Decision-maker: An Introduction to Human-AI Conflict\nThe collaboration between humans and artificial intelligence (AI) is a\nsignificant feature in this digital age. However, humans and AI may have\nobservation, interpretation, and action conflicts when working synchronously.\nThis phenomenon is often masked by faults and, unfortunately, overlooked. This\npaper systematically introduces the human-AI conflict concept, causes,\nmeasurement methods, and risk assessment. The results highlight that there is a\npotential second decision-maker besides the human, which is the AI; the\nhuman-AI conflict is a unique and emerging risk in digitalized process systems;\nand this is an interdisciplinary field that needs to be distinguished from\ntraditional fault and failure analysis; the conflict risk is significant and\ncannot be ignored.",
                "Enhancing Human Capabilities through Symbiotic Artificial Intelligence\n  with Shared Sensory Experiences\nThe merging of human intelligence and artificial intelligence has long been a\nsubject of interest in both science fiction and academia. In this paper, we\nintroduce a novel concept in Human-AI interaction called Symbiotic Artificial\nIntelligence with Shared Sensory Experiences (SAISSE), which aims to establish\na mutually beneficial relationship between AI systems and human users through\nshared sensory experiences. By integrating multiple sensory input channels and\nprocessing human experiences, SAISSE fosters a strong human-AI bond, enabling\nAI systems to learn from and adapt to individual users, providing personalized\nsupport, assistance, and enhancement. Furthermore, we discuss the incorporation\nof memory storage units for long-term growth and development of both the AI\nsystem and its human user. As we address user privacy and ethical guidelines\nfor responsible AI-human symbiosis, we also explore potential biases and\ninequalities in AI-human symbiosis and propose strategies to mitigate these\nchallenges. Our research aims to provide a comprehensive understanding of the\nSAISSE concept and its potential to effectively support and enhance individual\nhuman users through symbiotic AI systems. This position article aims at\ndiscussing poteintial AI-human interaction related topics within the scientific\ncommunity, rather than providing experimental or theoretical results.",
                "A Hierarchical Approach to Population Training for Human-AI\n  Collaboration\nA major challenge for deep reinforcement learning (DRL) agents is to\ncollaborate with novel partners that were not encountered by them during the\ntraining phase. This is specifically worsened by an increased variance in\naction responses when the DRL agents collaborate with human partners due to the\nlack of consistency in human behaviors. Recent work have shown that training a\nsingle agent as the best response to a diverse population of training partners\nsignificantly increases an agent's robustness to novel partners. We further\nenhance the population-based training approach by introducing a Hierarchical\nReinforcement Learning (HRL) based method for Human-AI Collaboration. Our agent\nis able to learn multiple best-response policies as its low-level policy while\nat the same time, it learns a high-level policy that acts as a manager which\nallows the agent to dynamically switch between the low-level best-response\npolicies based on its current partner. We demonstrate that our method is able\nto dynamically adapt to novel partners of different play styles and skill\nlevels in the 2-player collaborative Overcooked game environment. We also\nconducted a human study in the same environment to test the effectiveness of\nour method when partnering with real human subjects.",
                "Passive learning of active causal strategies in agents and language\n  models\nWhat can be learned about causality and experimentation from passive data?\nThis question is salient given recent successes of passively-trained language\nmodels in interactive domains such as tool use. Passive learning is inherently\nlimited. However, we show that purely passive learning can in fact allow an\nagent to learn generalizable strategies for determining and using causal\nstructures, as long as the agent can intervene at test time. We formally\nillustrate that learning a strategy of first experimenting, then seeking goals,\ncan allow generalization from passive learning in principle. We then show\nempirically that agents trained via imitation on expert data can indeed\ngeneralize at test time to infer and use causal links which are never present\nin the training data; these agents can also generalize experimentation\nstrategies to novel variable sets never observed in training. We then show that\nstrategies for causal intervention and exploitation can be generalized from\npassive data even in a more complex environment with high-dimensional\nobservations, with the support of natural language explanations. Explanations\ncan even allow passive learners to generalize out-of-distribution from\nperfectly-confounded training data. Finally, we show that language models,\ntrained only on passive next-word prediction, can generalize causal\nintervention strategies from a few-shot prompt containing examples of\nexperimentation, together with explanations and reasoning. These results\nhighlight the surprising power of passive learning of active causal strategies,\nand may help to understand the behaviors and capabilities of language models.",
                "Ghost in the Minecraft: Generally Capable Agents for Open-World\n  Environments via Large Language Models with Text-based Knowledge and Memory\nThe captivating realm of Minecraft has attracted substantial research\ninterest in recent years, serving as a rich platform for developing intelligent\nagents capable of functioning in open-world environments. However, the current\nresearch landscape predominantly focuses on specific objectives, such as the\npopular \"ObtainDiamond\" task, and has not yet shown effective generalization to\na broader spectrum of tasks. Furthermore, the current leading success rate for\nthe \"ObtainDiamond\" task stands at around 20%, highlighting the limitations of\nReinforcement Learning (RL) based controllers used in existing methods. To\ntackle these challenges, we introduce Ghost in the Minecraft (GITM), a novel\nframework integrates Large Language Models (LLMs) with text-based knowledge and\nmemory, aiming to create Generally Capable Agents (GCAs) in Minecraft. These\nagents, equipped with the logic and common sense capabilities of LLMs, can\nskillfully navigate complex, sparse-reward environments with text-based\ninteractions. We develop a set of structured actions and leverage LLMs to\ngenerate action plans for the agents to execute. The resulting LLM-based agent\nmarkedly surpasses previous methods, achieving a remarkable improvement of\n+47.5% in success rate on the \"ObtainDiamond\" task, demonstrating superior\nrobustness compared to traditional RL-based controllers. Notably, our agent is\nthe first to procure all items in the Minecraft Overworld technology tree,\ndemonstrating its extensive capabilities. GITM does not need any GPU for\ntraining, but a single CPU node with 32 CPU cores is enough. This research\nshows the potential of LLMs in developing capable agents for handling\nlong-horizon, complex tasks and adapting to uncertainties in open-world\nenvironments. See the project website at https://github.com/OpenGVLab/GITM.",
                "Combining Gamification and Intelligent Tutoring Systems in a Serious\n  Game for Engineering Education\nWe provide ongoing results from the development of a personalized learning\nsystem integrated into a serious game. Given limited instructor resources, the\nuse of computerized systems to help tutor students offers a way to provide\nhigher quality education and to improve educational efficacy. Personalized\nlearning systems like the one proposed in this paper offer an accessible\nsolution. Furthermore, by combining such a system with a serious game, students\nare further engaged in interacting with the system. The proposed learning\nsystem combines expert-driven structure and lesson planning with computational\nintelligence methods and gamification to provide students with a fun and\neducational experience. As the project is ongoing from past years, numerous\ndesign iterations have been made on the system based on feedback from students\nand classroom observations. Using computational intelligence, the system\nadaptively provides support to students based on data collected from both their\nin-game actions and by estimating their emotional state from webcam images. For\nour evaluation, we focus on student data gathered from in-classroom testing in\nrelevant courses, with both educational efficacy, results and student\nobservations. To demonstrate the effect of our proposed system, students in an\nearly electrical engineering course were instructed to interact with the system\nin place of a standard lab assignment. The system would then measure and help\nthem improve their background knowledge before allowing them to complete the\nlab assignment. As they played through the game, we observed their interactions\nwith the system to gather insights for future work. Additionally, we\ndemonstrate the system's educational efficacy through pre-post-test results\nfrom students who played the game with and without the personalized learning\nsystem."
            ],
            "interesting paper": 2
        },
        {
            "papers": [
                "Mindstorms in Natural Language-Based Societies of Mind\nBoth Minsky's \"society of mind\" and Schmidhuber's \"learning to think\" inspire\ndiverse societies of large multimodal neural networks (NNs) that solve problems\nby interviewing each other in a \"mindstorm.\" Recent implementations of NN-based\nsocieties of minds consist of large language models (LLMs) and other NN-based\nexperts communicating through a natural language interface. In doing so, they\novercome the limitations of single LLMs, improving multimodal zero-shot\nreasoning. In these natural language-based societies of mind (NLSOMs), new\nagents -- all communicating through the same universal symbolic language -- are\neasily added in a modular fashion. To demonstrate the power of NLSOMs, we\nassemble and experiment with several of them (having up to 129 members),\nleveraging mindstorms in them to solve some practical AI tasks: visual question\nanswering, image captioning, text-to-image synthesis, 3D generation, egocentric\nretrieval, embodied AI, and general language-based task solving. We view this\nas a starting point towards much larger NLSOMs with billions of agents-some of\nwhich may be humans. And with this emergence of great societies of\nheterogeneous minds, many new research questions have suddenly become paramount\nto the future of artificial intelligence. What should be the social structure\nof an NLSOM? What would be the (dis)advantages of having a monarchical rather\nthan a democratic structure? How can principles of NN economies be used to\nmaximize the total reward of a reinforcement learning NLSOM? In this work, we\nidentify, discuss, and try to answer some of these questions.",
                "How Do UX Practitioners Communicate AI as a Design Material? Artifacts,\n  Conceptions, and Propositions\nUX practitioners (UXPs) face novel challenges when working with and\ncommunicating artificial intelligence (AI) as a design material. We explore how\nUXPs communicate AI concepts when given hands-on experience training and\nexperimenting with AI models. To do so, we conducted a task-based design study\nwith 27 UXPs in which they prototyped and created a design presentation for a\nAI-enabled interface while having access to a simple AI model training tool.\nThrough analyzing UXPs' design presentations and post-activity interviews, we\nfound that although UXPs struggled to clearly communicate some AI concepts,\ntinkering with AI broadened common ground when communicating with technical\nstakeholders. UXPs also identified key risks and benefits of AI in their\ndesigns, and proposed concrete next steps for both UX and AI work. We conclude\nwith a sensitizing concept and recommendations for design and AI tools to\nenhance multi-stakeholder communication and collaboration when crafting\nhuman-centered AI experiences.",
                "SwiftSage: A Generative Agent with Fast and Slow Thinking for Complex\n  Interactive Tasks\nWe introduce SwiftSage, a novel agent framework inspired by the dual-process\ntheory of human cognition, designed to excel in action planning for complex\ninteractive reasoning tasks. SwiftSage integrates the strengths of behavior\ncloning and prompting large language models (LLMs) to enhance task completion\nperformance. The framework comprises two primary modules: the Swift module,\nrepresenting fast and intuitive thinking, and the Sage module, emulating\ndeliberate thought processes. The Swift module is a small encoder-decoder LM\nfine-tuned on the oracle agent's action trajectories, while the Sage module\nemploys LLMs such as GPT-4 for subgoal planning and grounding. We develop a\nheuristic method to harmoniously integrate the two modules, resulting in a more\nefficient and robust problem-solving process. In 30 tasks from the ScienceWorld\nbenchmark, SwiftSage significantly outperforms other methods such as SayCan,\nReAct, and Reflexion, demonstrating its effectiveness in solving complex\ninteractive tasks.",
                "Acting as Inverse Inverse Planning\nGreat storytellers know how to take us on a journey. They direct characters\nto act -- not necessarily in the most rational way -- but rather in a way that\nleads to interesting situations, and ultimately creates an impactful experience\nfor audience members looking on.\n  If audience experience is what matters most, then can we help artists and\nanimators *directly* craft such experiences, independent of the concrete\ncharacter actions needed to evoke those experiences? In this paper, we offer a\nnovel computational framework for such tools. Our key idea is to optimize\nanimations with respect to *simulated* audience members' experiences. To\nsimulate the audience, we borrow an established principle from cognitive\nscience: that human social intuition can be modeled as \"inverse planning,\" the\ntask of inferring an agent's (hidden) goals from its (observed) actions.\nBuilding on this model, we treat storytelling as \"*inverse* inverse planning,\"\nthe task of choosing actions to manipulate an inverse planner's inferences. Our\nframework is grounded in literary theory, naturally capturing many storytelling\nelements from first principles. We give a series of examples to demonstrate\nthis, with supporting evidence from human subject studies.",
                "Attention Schema in Neural Agents\nAttention has become a common ingredient in deep learning architectures. It\nadds a dynamical selection of information on top of the static selection of\ninformation supported by weights. In the same way, we can imagine a\nhigher-order informational filter built on top of attention: an Attention\nSchema (AS), namely, a descriptive and predictive model of attention. In\ncognitive neuroscience, Attention Schema Theory (AST) supports this idea of\ndistinguishing attention from AS. A strong prediction of this theory is that an\nagent can use its own AS to also infer the states of other agents' attention\nand consequently enhance coordination with other agents. As such, multi-agent\nreinforcement learning would be an ideal setting to experimentally test the\nvalidity of AST. We explore different ways in which attention and AS interact\nwith each other. Our preliminary results indicate that agents that implement\nthe AS as a recurrent internal control achieve the best performance. In\ngeneral, these exploratory experiments suggest that equipping artificial agents\nwith a model of attention can enhance their social intelligence.",
                "Towards Cognitive Bots: Architectural Research Challenges\nSoftware bots operating in multiple virtual digital platforms must understand\nthe platforms' affordances and behave like human users. Platform affordances or\nfeatures differ from one application platform to another or through a life\ncycle, requiring such bots to be adaptable. Moreover, bots in such platforms\ncould cooperate with humans or other software agents for work or to learn\nspecific behavior patterns. However, present-day bots, particularly chatbots,\nother than language processing and prediction, are far from reaching a human\nuser's behavior level within complex business information systems. They lack\nthe cognitive capabilities to sense and act in such virtual environments,\nrendering their development a challenge to artificial general intelligence\nresearch. In this study, we problematize and investigate assumptions in\nconceptualizing software bot architecture by directing attention to significant\narchitectural research challenges in developing cognitive bots endowed with\ncomplex behavior for operation on information systems. As an outlook, we\npropose alternate architectural assumptions to consider in future bot design\nand bot development frameworks."
            ],
            "interesting paper": 1
        },
        {
            "papers": [
                "Integrating Action Knowledge and LLMs for Task Planning and Situation\n  Handling in Open Worlds\nTask planning systems have been developed to help robots use human knowledge\n(about actions) to complete long-horizon tasks. Most of them have been\ndeveloped for \"closed worlds\" while assuming the robot is provided with\ncomplete world knowledge. However, the real world is generally open, and the\nrobots frequently encounter unforeseen situations that can potentially break\nthe planner's completeness. Could we leverage the recent advances on\npre-trained Large Language Models (LLMs) to enable classical planning systems\nto deal with novel situations?\n  This paper introduces a novel framework, called COWP, for open-world task\nplanning and situation handling. COWP dynamically augments the robot's action\nknowledge, including the preconditions and effects of actions, with\ntask-oriented commonsense knowledge. COWP embraces the openness from LLMs, and\nis grounded to specific domains via action knowledge. For systematic\nevaluations, we collected a dataset that includes 1,085 execution-time\nsituations. Each situation corresponds to a state instance wherein a robot is\npotentially unable to complete a task using a solution that normally works.\nExperimental results show that our approach outperforms competitive baselines\nfrom the literature in the success rate of service tasks. Additionally, we have\ndemonstrated COWP using a mobile manipulator. Supplementary materials are\navailable at: https://cowplanning.github.io/",
                "AI Coach Assist: An Automated Approach for Call Recommendation in\n  Contact Centers for Agent Coaching\nIn recent years, the utilization of Artificial Intelligence (AI) in the\ncontact center industry is on the rise. One area where AI can have a\nsignificant impact is in the coaching of contact center agents. By analyzing\ncall transcripts using Natural Language Processing (NLP) techniques, it would\nbe possible to quickly determine which calls are most relevant for coaching\npurposes. In this paper, we present AI Coach Assist, which leverages the\npre-trained transformer-based language models to determine whether a given call\nis coachable or not based on the quality assurance (QA) questions asked by the\ncontact center managers or supervisors. The system was trained and evaluated on\na large dataset collected from real-world contact centers and provides an\neffective way to recommend calls to the contact center managers that are more\nlikely to contain coachable moments. Our experimental findings demonstrate the\npotential of AI Coach Assist to improve the coaching process, resulting in\nenhancing the performance of contact center agents.",
                "On the Value of Myopic Behavior in Policy Reuse\nLeveraging learned strategies in unfamiliar scenarios is fundamental to human\nintelligence. In reinforcement learning, rationally reusing the policies\nacquired from other tasks or human experts is critical for tackling problems\nthat are difficult to learn from scratch. In this work, we present a framework\ncalled Selective Myopic bEhavior Control~(SMEC), which results from the insight\nthat the short-term behaviors of prior policies are sharable across tasks. By\nevaluating the behaviors of prior policies via a hybrid value function\narchitecture, SMEC adaptively aggregates the sharable short-term behaviors of\nprior policies and the long-term behaviors of the task policy, leading to\ncoordinated decisions. Empirical results on a collection of manipulation and\nlocomotion tasks demonstrate that SMEC outperforms existing methods, and\nvalidate the ability of SMEC to leverage related prior policies.",
                "In-Context Analogical Reasoning with Pre-Trained Language Models\nAnalogical reasoning is a fundamental capacity of human cognition that allows\nus to reason abstractly about novel situations by relating them to past\nexperiences. While it is thought to be essential for robust reasoning in AI\nsystems, conventional approaches require significant training and/or\nhard-coding of domain knowledge to be applied to benchmark tasks. Inspired by\ncognitive science research that has found connections between human language\nand analogy-making, we explore the use of intuitive language-based abstractions\nto support analogy in AI systems. Specifically, we apply large pre-trained\nlanguage models (PLMs) to visual Raven's Progressive Matrices (RPM), a common\nrelational reasoning test. By simply encoding the perceptual features of the\nproblem into language form, we find that PLMs exhibit a striking capacity for\nzero-shot relational reasoning, exceeding human performance and nearing\nsupervised vision-based methods. We explore different encodings that vary the\nlevel of abstraction over task features, finding that higher-level abstractions\nfurther strengthen PLMs' analogical reasoning. Our detailed analysis reveals\ninsights on the role of model complexity, in-context learning, and prior\nknowledge in solving RPM tasks.",
                "Modeling Dynamic Environments with Scene Graph Memory\nEmbodied AI agents that search for objects in large environments such as\nhouseholds often need to make efficient decisions by predicting object\nlocations based on partial information. We pose this as a new type of link\nprediction problem: link prediction on partially observable dynamic graphs. Our\ngraph is a representation of a scene in which rooms and objects are nodes, and\ntheir relationships are encoded in the edges; only parts of the changing graph\nare known to the agent at each timestep. This partial observability poses a\nchallenge to existing link prediction approaches, which we address. We propose\na novel state representation -- Scene Graph Memory (SGM) -- with captures the\nagent's accumulated set of observations, as well as a neural net architecture\ncalled a Node Edge Predictor (NEP) that extracts information from the SGM to\nsearch efficiently. We evaluate our method in the Dynamic House Simulator, a\nnew benchmark that creates diverse dynamic graphs following the semantic\npatterns typically seen at homes, and show that NEP can be trained to predict\nthe locations of objects in a variety of environments with diverse object\nmovement dynamics, outperforming baselines both in terms of new scene\nadaptability and overall accuracy. The codebase and more can be found at\nhttps://www.scenegraphmemory.com.",
                "Probing reaction channels via reinforcement learning\nWe propose a reinforcement learning based method to identify important\nconfigurations that connect reactant and product states along chemical reaction\npaths. By shooting multiple trajectories from these configurations, we can\ngenerate an ensemble of configurations that concentrate on the transition path\nensemble. This configuration ensemble can be effectively employed in a neural\nnetwork-based partial differential equation solver to obtain an approximation\nsolution of a restricted Backward Kolmogorov equation, even when the dimension\nof the problem is very high. The resulting solution, known as the committor\nfunction, encodes mechanistic information for the reaction and can in turn be\nused to evaluate reaction rates."
            ],
            "interesting paper": 4
        },
        {
            "papers": [
                "The Digital Divide in Process Safety: Quantitative Risk Analysis of\n  Human-AI Collaboration\nDigital technologies have dramatically accelerated the digital transformation\nin process industries, boosted new industrial applications, upgraded the\nproduction system, and enhanced operational efficiency. In contrast, the\nchallenges and gaps between human and artificial intelligence (AI) have become\nmore and more prominent, whereas the digital divide in process safety is\naggregating. The study attempts to address the following questions: (i)What is\nAI in the process safety context? (ii)What is the difference between AI and\nhumans in process safety? (iii)How do AI and humans collaborate in process\nsafety? (iv)What are the challenges and gaps in human-AI collaboration? (v)How\nto quantify the risk of human-AI collaboration in process safety? Qualitative\nrisk analysis based on brainstorming and literature review, and quantitative\nrisk analysis based on layer of protection analysis (LOPA) and Bayesian network\n(BN), were applied to explore and model. The importance of human reliability\nshould be stressed in the digital age, not usually to increase the reliability\nof AI, and human-centered AI design in process safety needs to be propagated.",
                "IoT based Personal Voice Assistant\nToday, technological advancement is increasing day by day. Earlier, there was\nonly a computer system in which we could only perform a few tasks. But now,\nmachine learning, artificial intelligence, deep learning, and a few more\ntechnologies have made computer systems so advanced that we can perform any\ntype of task. In this era of advancement, if people are still struggling to\ninteract using various input devices, then it's not worth it. For this reason,\nwe developed a voice assistant using Python that allows the user to run any\ntype of command in Linux without interaction with the keyboard. The main task\nof the voice assistant is to minimize the use of input devices like the\nkeyboard and mouse. It will also reduce hardware space and cost.",
                "Visual Affordance Prediction for Guiding Robot Exploration\nMotivated by the intuitive understanding humans have about the space of\npossible interactions, and the ease with which they can generalize this\nunderstanding to previously unseen scenes, we develop an approach for learning\nvisual affordances for guiding robot exploration. Given an input image of a\nscene, we infer a distribution over plausible future states that can be\nachieved via interactions with it. We use a Transformer-based model to learn a\nconditional distribution in the latent embedding space of a VQ-VAE and show\nthat these models can be trained using large-scale and diverse passive data,\nand that the learned models exhibit compositional generalization to diverse\nobjects beyond the training distribution. We show how the trained affordance\nmodel can be used for guiding exploration by acting as a goal-sampling\ndistribution, during visual goal-conditioned policy learning in robotic\nmanipulation.",
                "Taming AI Bots: Controllability of Neural States in Large Language\n  Models\nWe tackle the question of whether an agent can, by suitable choice of\nprompts, control an AI bot to any state. To that end, we first introduce a\nformal definition of ``meaning'' that is amenable to analysis. Then, we\ncharacterize ``meaningful data'' on which large language models (LLMs) are\nostensibly trained, and ``well-trained LLMs'' through conditions that are\nlargely met by today's LLMs. While a well-trained LLM constructs an embedding\nspace of meanings that is Euclidean, meanings themselves do not form a vector\n(linear) subspace, but rather a quotient space within. We then characterize the\nsubset of meanings that can be reached by the state of the LLMs for some input\nprompt, and show that a well-trained bot can reach any meaning albeit with\nsmall probability. We then introduce a stronger notion of controllability as\n{\\em almost certain reachability}, and show that, when restricted to the space\nof meanings, an AI bot is controllable. We do so after introducing a functional\ncharacterization of attentive AI bots, and finally derive necessary and\nsufficient conditions for controllability. The fact that AI bots are\ncontrollable means that an adversary could steer them towards any state.\nHowever, the sampling process can be designed to counteract adverse actions and\navoid reaching undesirable regions of state space before their boundary is\ncrossed.",
                "Re-imagining health and well-being in low resource African settings\n  using an augmented AI system and a 3D digital twin\nThis paper discusses and explores the potential and relevance of recent\ndevelopments in artificial intelligence (AI) and digital twins for health and\nwell-being in low-resource African countries. We use the case of public health\nemergency response to disease outbreaks and epidemic control. There is\npotential to take advantage of the increasing availability of data and\ndigitization to develop advanced AI methods for analysis and prediction. Using\nan AI systems perspective, we review emerging trends in AI systems and digital\ntwins and propose an initial augmented AI system architecture to illustrate how\nan AI system can work with a 3D digital twin to address public health goals. We\nhighlight scientific knowledge discovery, continual learning, pragmatic\ninteroperability, and interactive explanation and decision-making as essential\nresearch challenges for AI systems and digital twins.",
                "Reinforcement Learning with Human Feedback: Learning Dynamic Choices via\n  Pessimism\nIn this paper, we study offline Reinforcement Learning with Human Feedback\n(RLHF) where we aim to learn the human's underlying reward and the MDP's\noptimal policy from a set of trajectories induced by human choices. RLHF is\nchallenging for multiple reasons: large state space but limited human feedback,\nthe bounded rationality of human decisions, and the off-policy distribution\nshift. In this paper, we focus on the Dynamic Discrete Choice (DDC) model for\nmodeling and understanding human choices. DCC, rooted in econometrics and\ndecision theory, is widely used to model a human decision-making process with\nforward-looking and bounded rationality. We propose a\n\\underline{D}ynamic-\\underline{C}hoice-\\underline{P}essimistic-\\underline{P}olicy-\\underline{O}ptimization\n(DCPPO) method. \\ The method involves a three-stage process: The first step is\nto estimate the human behavior policy and the state-action value function via\nmaximum likelihood estimation (MLE); the second step recovers the human reward\nfunction via minimizing Bellman mean squared error using the learned value\nfunctions; the third step is to plug in the learned reward and invoke\npessimistic value iteration for finding a near-optimal policy. With only\nsingle-policy coverage (i.e., optimal policy) of the dataset, we prove that the\nsuboptimality of DCPPO almost matches the classical pessimistic offline RL\nalgorithm in terms of suboptimality's dependency on distribution shift and\ndimension. To the best of our knowledge, this paper presents the first\ntheoretical guarantees for off-policy offline RLHF with dynamic discrete choice\nmodel."
            ],
            "interesting paper": 1
        },
        {
            "papers": [
                "AlphaBlock: Embodied Finetuning for Vision-Language Reasoning in Robot\n  Manipulation\nWe propose a novel framework for learning high-level cognitive capabilities\nin robot manipulation tasks, such as making a smiley face using building\nblocks. These tasks often involve complex multi-step reasoning, presenting\nsignificant challenges due to the limited paired data connecting human\ninstructions (e.g., making a smiley face) and robot actions (e.g., end-effector\nmovement). Existing approaches relieve this challenge by adopting an open-loop\nparadigm decomposing high-level instructions into simple sub-task plans, and\nexecuting them step-by-step using low-level control models. However, these\napproaches are short of instant observations in multi-step reasoning, leading\nto sub-optimal results. To address this issue, we propose to automatically\ncollect a cognitive robot dataset by Large Language Models (LLMs). The\nresulting dataset AlphaBlock consists of 35 comprehensive high-level tasks of\nmulti-step text plans and paired observation sequences. To enable efficient\ndata acquisition, we employ elaborated multi-round prompt designs that\neffectively reduce the burden of extensive human involvement. We further\npropose a closed-loop multi-modal embodied planning model that autoregressively\ngenerates plans by taking image observations as input. To facilitate effective\nlearning, we leverage MiniGPT-4 with a frozen visual encoder and LLM, and\nfinetune additional vision adapter and Q-former to enable fine-grained spatial\nperception for manipulation tasks. We conduct experiments to verify the\nsuperiority over existing open and closed-loop methods, and achieve a\nsignificant increase in success rate by 21.4% and 14.5% over ChatGPT and GPT-4\nbased robot tasks. Real-world demos are shown in\nhttps://www.youtube.com/watch?v=ayAzID1_qQk .",
                "An Emergency Disposal Decision-making Method with Human--Machine\n  Collaboration\nRapid developments in artificial intelligence technology have led to unmanned\nsystems replacing human beings in many fields requiring high-precision\npredictions and decisions. In modern operational environments, all job plans\nare affected by emergency events such as equipment failures and resource\nshortages, making a quick resolution critical. The use of unmanned systems to\nassist decision-making can improve resolution efficiency, but their\ndecision-making is not interpretable and may make the wrong decisions. Current\nunmanned systems require human supervision and control. Based on this, we\npropose a collaborative human--machine method for resolving unplanned events\nusing two phases: task filtering and task scheduling. In the task filtering\nphase, we propose a human--machine collaborative decision-making algorithm for\ndynamic tasks. The GACRNN model is used to predict the state of the job nodes,\nlocate the key nodes, and generate a machine-predicted resolution task list. A\nhuman decision-maker supervises the list in real time and modifies and confirms\nthe machine-predicted list through the human--machine interface. In the task\nscheduling phase, we propose a scheduling algorithm that integrates human\nexperience constraints. The steps to resolve an event are inserted into the\nnormal job sequence to schedule the resolution. We propose several\nhuman--machine collaboration methods in each phase to generate steps to resolve\nan unplanned event while minimizing the impact on the original job plan.",
                "Towards a Unifying Model of Rationality in Multiagent Systems\nMultiagent systems deployed in the real world need to cooperate with other\nagents (including humans) nearly as effectively as these agents cooperate with\none another. To design such AI, and provide guarantees of its effectiveness, we\nneed to clearly specify what types of agents our AI must be able to cooperate\nwith. In this work we propose a generic model of socially intelligent agents,\nwhich are individually rational learners that are also able to cooperate with\none another (in the sense that their joint behavior is Pareto efficient). We\ndefine rationality in terms of the regret incurred by each agent over its\nlifetime, and show how we can construct socially intelligent agents for\ndifferent forms of regret. We then discuss the implications of this model for\nthe development of \"robust\" MAS that can cooperate with a wide variety of\nsocially intelligent agents.",
                "Reason to explain: Interactive contrastive explanations (REASONX)\nMany high-performing machine learning models are not interpretable. As they\nare increasingly used in decision scenarios that can critically affect\nindividuals, it is necessary to develop tools to better understand their\noutputs. Popular explanation methods include contrastive explanations. However,\nthey suffer several shortcomings, among others an insufficient incorporation of\nbackground knowledge, and a lack of interactivity. While (dialogue-like)\ninteractivity is important to better communicate an explanation, background\nknowledge has the potential to significantly improve their quality, e.g., by\nadapting the explanation to the needs of the end-user. To close this gap, we\npresent REASONX, an explanation tool based on Constraint Logic Programming\n(CLP). REASONX provides interactive contrastive explanations that can be\naugmented by background knowledge, and allows to operate under a setting of\nunder-specified information, leading to increased flexibility in the provided\nexplanations. REASONX computes factual and constrative decision rules, as well\nas closest constrative examples. It provides explanations for decision trees,\nwhich can be the ML models under analysis, or global/local surrogate models of\nany ML model. While the core part of REASONX is built on CLP, we also provide a\nprogram layer that allows to compute the explanations via Python, making the\ntool accessible to a wider audience. We illustrate the capability of REASONX on\na synthetic data set, and on a a well-developed example in the credit domain.\nIn both cases, we can show how REASONX can be flexibly used and tailored to the\nneeds of the user.",
                "Contextual Object Detection with Multimodal Large Language Models\nRecent Multimodal Large Language Models (MLLMs) are remarkable in\nvision-language tasks, such as image captioning and question answering, but\nlack the essential perception ability, i.e., object detection. In this work, we\naddress this limitation by introducing a novel research problem of contextual\nobject detection -- understanding visible objects within different human-AI\ninteractive contexts. Three representative scenarios are investigated,\nincluding the language cloze test, visual captioning, and question answering.\nMoreover, we present ContextDET, a unified multimodal model that is capable of\nend-to-end differentiable modeling of visual-language contexts, so as to\nlocate, identify, and associate visual objects with language inputs for\nhuman-AI interaction. Our ContextDET involves three key submodels: (i) a visual\nencoder for extracting visual representations, (ii) a pre-trained LLM for\nmultimodal context decoding, and (iii) a visual decoder for predicting bounding\nboxes given contextual object words. The new generate-then-detect framework\nenables us to detect object words within human vocabulary. Extensive\nexperiments show the advantages of ContextDET on our proposed CODE benchmark,\nopen-vocabulary detection, and referring image segmentation. Github:\nhttps://github.com/yuhangzang/ContextDET.",
                "Code Prompting: a Neural Symbolic Method for Complex Reasoning in Large\n  Language Models\nLarge language models (LLMs) have scaled up to unlock a wide range of complex\nreasoning tasks with the aid of various prompting methods. However, current\nprompting methods generate natural language intermediate steps to help\nreasoning, which can cause imperfect task reduction and confusion. To mitigate\nsuch limitations, we explore code prompting, a neural symbolic prompting method\nwith both zero-shot and few-shot versions which triggers code as intermediate\nsteps. We conduct experiments on 7 widely-used benchmarks involving symbolic\nreasoning and arithmetic reasoning. Code prompting generally outperforms\nchain-of-thought (CoT) prompting. To further understand the performance and\nlimitations of code prompting, we perform extensive ablation studies and error\nanalyses, and identify several exclusive advantages of using symbolic\npromptings compared to natural language. We also consider the ensemble of code\nprompting and CoT prompting to combine the strengths of both. Finally, we show\nthrough experiments how code annotations and their locations affect code\nprompting."
            ],
            "interesting paper": 1
        },
        {
            "papers": [
                "Intent-aligned AI systems deplete human agency: the need for agency\n  foundations research in AI safety\nThe rapid advancement of artificial intelligence (AI) systems suggests that\nartificial general intelligence (AGI) systems may soon arrive. Many researchers\nare concerned that AIs and AGIs will harm humans via intentional misuse\n(AI-misuse) or through accidents (AI-accidents). In respect of AI-accidents,\nthere is an increasing effort focused on developing algorithms and paradigms\nthat ensure AI systems are aligned to what humans intend, e.g. AI systems that\nyield actions or recommendations that humans might judge as consistent with\ntheir intentions and goals. Here we argue that alignment to human intent is\ninsufficient for safe AI systems and that preservation of long-term agency of\nhumans may be a more robust standard, and one that needs to be separated\nexplicitly and a priori during optimization. We argue that AI systems can\nreshape human intention and discuss the lack of biological and psychological\nmechanisms that protect humans from loss of agency. We provide the first formal\ndefinition of agency-preserving AI-human interactions which focuses on\nforward-looking agency evaluations and argue that AI systems - not humans -\nmust be increasingly tasked with making these evaluations. We show how agency\nloss can occur in simple environments containing embedded agents that use\ntemporal-difference learning to make action recommendations. Finally, we\npropose a new area of research called \"agency foundations\" and pose four\ninitial topics designed to improve our understanding of agency in AI-human\ninteractions: benevolent game theory, algorithmic foundations of human rights,\nmechanistic interpretability of agency representation in neural-networks and\nreinforcement learning from internal states.",
                "A Computational Account Of Self-Supervised Visual Learning From\n  Egocentric Object Play\nResearch in child development has shown that embodied experience handling\nphysical objects contributes to many cognitive abilities, including visual\nlearning. One characteristic of such experience is that the learner sees the\nsame object from several different viewpoints. In this paper, we study how\nlearning signals that equate different viewpoints -- e.g., assigning similar\nrepresentations to different views of a single object -- can support robust\nvisual learning. We use the Toybox dataset, which contains egocentric videos of\nhumans manipulating different objects, and conduct experiments using a computer\nvision framework for self-supervised contrastive learning. We find that\nrepresentations learned by equating different physical viewpoints of an object\nbenefit downstream image classification accuracy. Further experiments show that\nthis performance improvement is robust to variations in the gaps between\nviewpoints, and that the benefits transfer to several different image\nclassification tasks.",
                "Strategic Reasoning with Language Models\nStrategic reasoning enables agents to cooperate, communicate, and compete\nwith other agents in diverse situations. Existing approaches to solving\nstrategic games rely on extensive training, yielding strategies that do not\ngeneralize to new scenarios or games without retraining. Large Language Models\n(LLMs), with their ability to comprehend and generate complex, context-rich\nlanguage, could prove powerful as tools for strategic gameplay. This paper\nintroduces an approach that uses pretrained LLMs with few-shot chain-of-thought\nexamples to enable strategic reasoning for AI agents. Our approach uses\nsystematically generated demonstrations of reasoning about states, values, and\nbeliefs to prompt the model. Using extensive variations of simple matrix games,\nwe show that strategies that are derived based on systematically generated\nprompts generalize almost perfectly to new game structures, alternate\nobjectives, and hidden information. Additionally, we demonstrate our approach\ncan lead to human-like negotiation strategies in realistic scenarios without\nany extra training or fine-tuning. Our results highlight the ability of LLMs,\nguided by systematic reasoning demonstrations, to adapt and excel in diverse\nstrategic scenarios.",
                "Less Likely Brainstorming: Using Language Models to Generate Alternative\n  Hypotheses\nA human decision-maker benefits the most from an AI assistant that corrects\nfor their biases. For problems such as generating interpretation of a radiology\nreport given findings, a system predicting only highly likely outcomes may be\nless useful, where such outcomes are already obvious to the user. To alleviate\nbiases in human decision-making, it is worth considering a broad differential\ndiagnosis, going beyond the most likely options. We introduce a new task, \"less\nlikely brainstorming,\" that asks a model to generate outputs that humans think\nare relevant but less likely to happen. We explore the task in two settings: a\nbrain MRI interpretation generation setting and an everyday commonsense\nreasoning setting. We found that a baseline approach of training with less\nlikely hypotheses as targets generates outputs that humans evaluate as either\nlikely or irrelevant nearly half of the time; standard MLE training is not\neffective. To tackle this problem, we propose a controlled text generation\nmethod that uses a novel contrastive learning strategy to encourage models to\ndifferentiate between generating likely and less likely outputs according to\nhumans. We compare our method with several state-of-the-art controlled text\ngeneration models via automatic and human evaluations and show that our models'\ncapability of generating less likely outputs is improved.",
                "Bigger, Better, Faster: Human-level Atari with human-level efficiency\nWe introduce a value-based RL agent, which we call BBF, that achieves\nsuper-human performance in the Atari 100K benchmark. BBF relies on scaling the\nneural networks used for value estimation, as well as a number of other design\nchoices that enable this scaling in a sample-efficient manner. We conduct\nextensive analyses of these design choices and provide insights for future\nwork. We end with a discussion about updating the goalposts for\nsample-efficient RL research on the ALE. We make our code and data publicly\navailable at\nhttps://github.com/google-research/google-research/tree/master/bigger_better_faster.",
                "Language-Conditioned Imitation Learning with Base Skill Priors under\n  Unstructured Data\nThe growing interest in language-conditioned robot manipulation aims to\ndevelop robots capable of understanding and executing complex tasks, with the\nobjective of enabling robots to interpret language commands and manipulate\nobjects accordingly. While language-conditioned approaches demonstrate\nimpressive capabilities for addressing tasks in familiar environments, they\nencounter limitations in adapting to unfamiliar environment settings. In this\nstudy, we propose a general-purpose, language-conditioned approach that\ncombines base skill priors and imitation learning under unstructured data to\nenhance the algorithm's generalization in adapting to unfamiliar environments.\nWe assess our model's performance in both simulated and real-world environments\nusing a zero-shot setting. In the simulated environment, the proposed approach\nsurpasses previously reported scores for CALVIN benchmark, especially in the\nchallenging Zero-Shot Multi-Environment setting. The average completed task\nlength, indicating the average number of tasks the agent can continuously\ncomplete, improves more than 2.5 times compared to the state-of-the-art method\nHULC. In addition, we conduct a zero-shot evaluation of our policy in a\nreal-world setting, following training exclusively in simulated environments\nwithout additional specific adaptations. In this evaluation, we set up ten\ntasks and achieved an average 30% improvement in our approach compared to the\ncurrent state-of-the-art approach, demonstrating a high generalization\ncapability in both simulated environments and the real world. For further\ndetails, including access to our code and videos, please refer to\nhttps://hk-zh.github.io/spil/"
            ],
            "interesting paper": 1
        },
        {
            "papers": [
                "Human Control: Definitions and Algorithms\nHow can humans stay in control of advanced artificial intelligence systems?\nOne proposal is corrigibility, which requires the agent to follow the\ninstructions of a human overseer, without inappropriately influencing them. In\nthis paper, we formally define a variant of corrigibility called shutdown\ninstructability, and show that it implies appropriate shutdown behavior,\nretention of human autonomy, and avoidance of user harm. We also analyse the\nrelated concepts of non-obstruction and shutdown alignment, three previously\nproposed algorithms for human control, and one new algorithm.",
                "Human or Not? A Gamified Approach to the Turing Test\nWe present \"Human or Not?\", an online game inspired by the Turing test, that\nmeasures the capability of AI chatbots to mimic humans in dialog, and of humans\nto tell bots from other humans. Over the course of a month, the game was played\nby over 1.5 million users who engaged in anonymous two-minute chat sessions\nwith either another human or an AI language model which was prompted to behave\nlike humans. The task of the players was to correctly guess whether they spoke\nto a person or to an AI. This largest scale Turing-style test conducted to date\nrevealed some interesting facts. For example, overall users guessed the\nidentity of their partners correctly in only 68% of the games. In the subset of\nthe games in which users faced an AI bot, users had even lower correct guess\nrates of 60% (that is, not much higher than chance). This white paper details\nthe development, deployment, and results of this unique experiment. While this\nexperiment calls for many extensions and refinements, these findings already\nbegin to shed light on the inevitable near future which will commingle humans\nand AI.",
                "Decision-Oriented Dialogue for Human-AI Collaboration\nWe describe a class of tasks called decision-oriented dialogues, in which AI\nassistants such as large language models (LMs) must collaborate with one or\nmore humans via natural language to help them make complex decisions. We\nformalize three domains in which users face everyday decisions: (1) choosing an\nassignment of reviewers to conference papers, (2) planning a multi-step\nitinerary in a city, and (3) negotiating travel plans for a group of friends.\nIn each of these settings, AI assistants and users have disparate abilities\nthat they must combine to arrive at the best decision: assistants can access\nand process large amounts of information, while users have preferences and\nconstraints external to the system. For each task, we build a dialogue\nenvironment where agents receive a reward based on the quality of the final\ndecision they reach. We evaluate LMs in self-play and in collaboration with\nhumans and find that they fall short compared to human assistants, achieving\nmuch lower rewards despite engaging in longer dialogues. We highlight a number\nof challenges models face in decision-oriented dialogues, ranging from\ngoal-directed behavior to reasoning and optimization, and release our\nenvironments as a testbed for future work.",
                "From Human-Centered to Social-Centered Artificial Intelligence:\n  Assessing ChatGPT's Impact through Disruptive Events\nLarge language models (LLMs) and dialogue agents have existed for years, but\nthe release of recent GPT models has been a watershed moment for artificial\nintelligence (AI) research and society at large. Immediately recognized for its\ngenerative capabilities and versatility, ChatGPT's impressive proficiency\nacross technical and creative domains led to its widespread adoption. While\nsociety grapples with the emerging cultural impacts of ChatGPT, critiques of\nChatGPT's impact within the machine learning community have coalesced around\nits performance or other conventional Responsible AI evaluations relating to\nbias, toxicity, and 'hallucination.' We argue that these latter critiques draw\nheavily on a particular conceptualization of the 'human-centered' framework,\nwhich tends to cast atomized individuals as the key recipients of both the\nbenefits and detriments of technology. In this article, we direct attention to\nanother dimension of LLMs and dialogue agents' impact: their effect on social\ngroups, institutions, and accompanying norms and practices. By illustrating\nChatGPT's social impact through three disruptive events, we challenge\nindividualistic approaches in AI development and contribute to ongoing debates\naround the ethical and responsible implementation of AI systems. We hope this\neffort will call attention to more comprehensive and longitudinal evaluation\ntools and compel technologists to go beyond human-centered thinking and ground\ntheir efforts through social-centered AI.",
                "EMOTE: An Explainable architecture for Modelling the Other Through\n  Empathy\nWe can usually assume others have goals analogous to our own. This assumption\ncan also, at times, be applied to multi-agent games - e.g. Agent 1's attraction\nto green pellets is analogous to Agent 2's attraction to red pellets. This\n\"analogy\" assumption is tied closely to the cognitive process known as empathy.\nInspired by empathy, we design a simple and explainable architecture to model\nanother agent's action-value function. This involves learning an \"Imagination\nNetwork\" to transform the other agent's observed state in order to produce a\nhuman-interpretable \"empathetic state\" which, when presented to the learning\nagent, produces behaviours that mimic the other agent. Our approach is\napplicable to multi-agent scenarios consisting of a single learning agent and\nother (independent) agents acting according to fixed policies. This\narchitecture is particularly beneficial for (but not limited to) algorithms\nusing a composite value or reward function. We show our method produces better\nperformance in multi-agent games, where it robustly estimates the other's model\nin different environment configurations. Additionally, we show that the\nempathetic states are human interpretable, and thus verifiable.",
                "Speaking the Language of Your Listener: Audience-Aware Adaptation via\n  Plug-and-Play Theory of Mind\nDialogue participants may have varying levels of knowledge about the topic\nunder discussion. In such cases, it is essential for speakers to adapt their\nutterances by taking their audience into account. Yet, it is an open question\nhow such adaptation can be modelled in computational agents. In this paper, we\nmodel a visually grounded referential game between a knowledgeable speaker and\na listener with more limited visual and linguistic experience. Inspired by\npsycholinguistic theories, we endow our speaker with the ability to adapt its\nreferring expressions via a simulation module that monitors the effectiveness\nof planned utterances from the listener's perspective. We propose an adaptation\nmechanism building on plug-and-play approaches to controlled language\ngeneration, where utterance generation is steered on the fly by the simulator\nwithout finetuning the speaker's underlying language model. Our results and\nanalyses show that our approach is effective: the speaker's utterances become\ncloser to the listener's domain of expertise, which leads to higher\ncommunicative success."
            ],
            "interesting paper": 2
        },
        {
            "papers": [
                "Egocentric Planning for Scalable Embodied Task Achievement\nEmbodied agents face significant challenges when tasked with performing\nactions in diverse environments, particularly in generalizing across object\ntypes and executing suitable actions to accomplish tasks. Furthermore, agents\nshould exhibit robustness, minimizing the execution of illegal actions. In this\nwork, we present Egocentric Planning, an innovative approach that combines\nsymbolic planning and Object-oriented POMDPs to solve tasks in complex\nenvironments, harnessing existing models for visual perception and natural\nlanguage processing. We evaluated our approach in ALFRED, a simulated\nenvironment designed for domestic tasks, and demonstrated its high scalability,\nachieving an impressive 36.07% unseen success rate in the ALFRED benchmark and\nwinning the ALFRED challenge at CVPR Embodied AI workshop. Our method requires\nreliable perception and the specification or learning of a symbolic description\nof the preconditions and effects of the agent's actions, as well as what object\ntypes reveal information about others. It is capable of naturally scaling to\nsolve new tasks beyond ALFRED, as long as they can be solved using the\navailable skills. This work offers a solid baseline for studying end-to-end and\nhybrid methods that aim to generalize to new tasks, including recent approaches\nrelying on LLMs, but often struggle to scale to long sequences of actions or\nproduce robust plans for novel tasks.",
                "AI and the creative realm: A short review of current and future\n  applications\nThis study explores the concept of creativity and artificial intelligence\n(AI) and their recent integration. While AI has traditionally been perceived as\nincapable of generating new ideas or creating art, the development of more\nsophisticated AI models and the proliferation of human-computer interaction\ntools have opened up new possibilities for AI in artistic creation. This study\ninvestigates the various applications of AI in a creative context,\ndifferentiating between the type of art, language, and algorithms used. It also\nconsiders the philosophical implications of AI and creativity, questioning\nwhether consciousness can be researched in machines and AI's potential\ninterests and decision-making capabilities. Overall, we aim to stimulate a\nreflection on AI's use and ethical implications in creative contexts.",
                "AI Liability Insurance With an Example in AI-Powered E-diagnosis System\nArtificial Intelligence (AI) has received an increasing amount of attention\nin multiple areas. The uncertainties and risks in AI-powered systems have\ncreated reluctance in their wild adoption. As an economic solution to\ncompensate for potential damages, AI liability insurance is a promising market\nto enhance the integration of AI into daily life. In this work, we use an\nAI-powered E-diagnosis system as an example to study AI liability insurance. We\nprovide a quantitative risk assessment model with evidence-based numerical\nanalysis. We discuss the insurability criteria for AI technologies and suggest\nnecessary adjustments to accommodate the features of AI products. We show that\nAI liability insurance can act as a regulatory mechanism to incentivize\ncompliant behaviors and serve as a certificate of high-quality AI systems.\nFurthermore, we suggest premium adjustment to reflect the dynamic evolution of\nthe inherent uncertainty in AI. Moral hazard problems are discussed and\nsuggestions for AI liability insurance are provided.",
                "STEVE-1: A Generative Model for Text-to-Behavior in Minecraft\nConstructing AI models that respond to text instructions is challenging,\nespecially for sequential decision-making tasks. This work introduces a\nmethodology, inspired by unCLIP, for instruction-tuning generative models of\nbehavior without relying on a large dataset of instruction-labeled\ntrajectories. Using this methodology, we create an instruction-tuned Video\nPretraining (VPT) model called STEVE-1, which can follow short-horizon\nopen-ended text and visual instructions in Minecraft. STEVE-1 is trained in two\nsteps: adapting the pretrained VPT model to follow commands in MineCLIP's\nlatent space, then training a prior to predict latent codes from text. This\nallows us to finetune VPT through self-supervised behavioral cloning and\nhindsight relabeling, reducing the need for costly human text annotations, and\nall for only $60 of compute. By leveraging pretrained models like VPT and\nMineCLIP and employing best practices from text-conditioned image generation,\nSTEVE-1 sets a new bar for open-ended instruction-following in Minecraft with\nlow-level controls (mouse and keyboard) and raw pixel inputs, far outperforming\nprevious baselines and robustly completing 12 of 13 tasks in our early-game\nevaluation suite. We provide experimental evidence highlighting key factors for\ndownstream performance, including pretraining, classifier-free guidance, and\ndata scaling. All resources, including our model weights, training scripts, and\nevaluation tools are made available for further research.",
                "The ethical ambiguity of AI data enrichment: Measuring gaps in research\n  ethics norms and practices\nThe technical progression of artificial intelligence (AI) research has been\nbuilt on breakthroughs in fields such as computer science, statistics, and\nmathematics. However, in the past decade AI researchers have increasingly\nlooked to the social sciences, turning to human interactions to solve the\nchallenges of model development. Paying crowdsourcing workers to generate or\ncurate data, or data enrichment, has become indispensable for many areas of AI\nresearch, from natural language processing to reinforcement learning from human\nfeedback (RLHF). Other fields that routinely interact with crowdsourcing\nworkers, such as Psychology, have developed common governance requirements and\nnorms to ensure research is undertaken ethically. This study explores how, and\nto what extent, comparable research ethics requirements and norms have\ndeveloped for AI research and data enrichment. We focus on the approach taken\nby two leading conferences: ICLR and NeurIPS, and journal publisher Springer.\nIn a longitudinal study of accepted papers, and via a comparison with\nPsychology and CHI papers, this work finds that leading AI venues have begun to\nestablish protocols for human data collection, but these are are inconsistently\nfollowed by authors. Whilst Psychology papers engaging with crowdsourcing\nworkers frequently disclose ethics reviews, payment data, demographic data and\nother information, similar disclosures are far less common in leading AI venues\ndespite similar guidance. The work concludes with hypotheses to explain these\ngaps in research ethics practices and considerations for its implications.",
                "LIV: Language-Image Representations and Rewards for Robotic Control\nWe present Language-Image Value learning (LIV), a unified objective for\nvision-language representation and reward learning from action-free videos with\ntext annotations. Exploiting a novel connection between dual reinforcement\nlearning and mutual information contrastive learning, the LIV objective trains\na multi-modal representation that implicitly encodes a universal value function\nfor tasks specified as language or image goals. We use LIV to pre-train the\nfirst control-centric vision-language representation from large human video\ndatasets such as EpicKitchen. Given only a language or image goal, the\npre-trained LIV model can assign dense rewards to each frame in videos of\nunseen robots or humans attempting that task in unseen environments. Further,\nwhen some target domain-specific data is available, the same objective can be\nused to fine-tune and improve LIV and even other pre-trained representations\nfor robotic control and reward specification in that domain. In our experiments\non several simulated and real-world robot environments, LIV models consistently\noutperform the best prior input state representations for imitation learning,\nas well as reward specification methods for policy synthesis. Our results\nvalidate the advantages of joint vision-language representation and reward\nlearning within the unified, compact LIV framework."
            ],
            "interesting paper": 4
        },
        {
            "papers": [
                "Enough With \"Human-AI Collaboration\"\nDescribing our interaction with Artificial Intelligence (AI) systems as\n'collaboration' is well-intentioned, but flawed. Not only is it misleading, but\nit also takes away the credit of AI 'labour' from the humans behind it, and\nerases and obscures an often exploitative arrangement between AI producers and\nconsumers. The AI 'collaboration' metaphor is merely the latest episode in a\nlong history of labour appropriation and credit reassignment that\ndisenfranchises labourers in the Global South. I propose that viewing AI as a\ntool or an instrument, rather than a collaborator, is more accurate, and\nultimately fairer.",
                "AI Transparency in the Age of LLMs: A Human-Centered Research Roadmap\nThe rise of powerful large language models (LLMs) brings about tremendous\nopportunities for innovation but also looming risks for individuals and society\nat large. We have reached a pivotal moment for ensuring that LLMs and\nLLM-infused applications are developed and deployed responsibly. However, a\ncentral pillar of responsible AI -- transparency -- is largely missing from the\ncurrent discourse around LLMs. It is paramount to pursue new approaches to\nprovide transparency for LLMs, and years of research at the intersection of AI\nand human-computer interaction (HCI) highlight that we must do so with a\nhuman-centered perspective: Transparency is fundamentally about supporting\nappropriate human understanding, and this understanding is sought by different\nstakeholders with different goals in different contexts. In this new era of\nLLMs, we must develop and design approaches to transparency by considering the\nneeds of stakeholders in the emerging LLM ecosystem, the novel types of\nLLM-infused applications being built, and the new usage patterns and challenges\naround LLMs, all while building on lessons learned about how people process,\ninteract with, and make use of information. We reflect on the unique challenges\nthat arise in providing transparency for LLMs, along with lessons learned from\nHCI and responsible AI research that has taken a human-centered perspective on\nAI transparency. We then lay out four common approaches that the community has\ntaken to achieve transparency -- model reporting, publishing evaluation\nresults, providing explanations, and communicating uncertainty -- and call out\nopen questions around how these approaches may or may not be applied to LLMs.\nWe hope this provides a starting point for discussion and a useful roadmap for\nfuture research.",
                "Accelerating science with human-aware artificial intelligence\nArtificial intelligence (AI) models trained on published scientific findings\nhave been used to invent valuable materials and targeted therapies, but they\ntypically ignore the human scientists who continually alter the landscape of\ndiscovery. Here we show that incorporating the distribution of human expertise\nby training unsupervised models on simulated inferences cognitively accessible\nto experts dramatically improves (up to 400%) AI prediction of future\ndiscoveries beyond those focused on research content alone, especially when\nrelevant literature is sparse. These models succeed by predicting human\npredictions and the scientists who will make them. By tuning human-aware AI to\navoid the crowd, we can generate scientifically promising \"alien\" hypotheses\nunlikely to be imagined or pursued without intervention until the distant\nfuture, which hold promise to punctuate scientific advance beyond questions\ncurrently pursued. Accelerating human discovery or probing its blind spots,\nhuman-aware AI enables us to move toward and beyond the contemporary scientific\nfrontier.",
                "Guided scenarios with simulated expert personae: a remarkable strategy\n  to perform cognitive work\nLarge language models (LLMs) trained on a substantial corpus of human\nknowledge and literature productively work with a large array of facts from\nthat corpus. Surprisingly, they are also able to re-create the behaviors of\npersonae that are captured within the corpus. By forming teams of simulated\npersonae, supplying contexts that set the stage, and providing gentle prompts,\none can move through scenarios that elicit expert behavior to perform\nmeaningful cognitive work. The power of this strategy is demonstrated with two\nexamples, one attacking factuality of LLM responses and the other reproducing a\nvery recently published result in quantum optics.",
                "Milestones in Autonomous Driving and Intelligent Vehicles Part II:\n  Perception and Planning\nGrowing interest in autonomous driving (AD) and intelligent vehicles (IVs) is\nfueled by their promise for enhanced safety, efficiency, and economic benefits.\nWhile previous surveys have captured progress in this field, a comprehensive\nand forward-looking summary is needed. Our work fills this gap through three\ndistinct articles. The first part, a \"Survey of Surveys\" (SoS), outlines the\nhistory, surveys, ethics, and future directions of AD and IV technologies. The\nsecond part, \"Milestones in Autonomous Driving and Intelligent Vehicles Part I:\nControl, Computing System Design, Communication, HD Map, Testing, and Human\nBehaviors\" delves into the development of control, computing system,\ncommunication, HD map, testing, and human behaviors in IVs. This part, the\nthird part, reviews perception and planning in the context of IVs. Aiming to\nprovide a comprehensive overview of the latest advancements in AD and IVs, this\nwork caters to both newcomers and seasoned researchers. By integrating the SoS\nand Part I, we offer unique insights and strive to serve as a bridge between\npast achievements and future possibilities in this dynamic field.",
                "OMNI: Open-endedness via Models of human Notions of Interestingness\nOpen-ended algorithms aim to learn new, interesting behaviors forever. That\nrequires a vast environment search space, but there are thus infinitely many\npossible tasks. Even after filtering for tasks the current agent can learn\n(i.e., learning progress), countless learnable yet uninteresting tasks remain\n(e.g., minor variations of previously learned tasks). An Achilles Heel of\nopen-endedness research is the inability to quantify (and thus prioritize)\ntasks that are not just learnable, but also $\\textit{interesting}$ (e.g.,\nworthwhile and novel). We propose solving this problem by\n$\\textit{Open-endedness via Models of human Notions of Interestingness}$\n(OMNI). The insight is that we can utilize foundation models (FMs) as a model\nof interestingness (MoI), because they $\\textit{already}$ internalize human\nconcepts of interestingness from training on vast amounts of human-generated\ndata, where humans naturally write about what they find interesting or boring.\nWe show that FM-based MoIs improve open-ended learning by focusing on tasks\nthat are both learnable $\\textit{and interesting}$, outperforming baselines\nbased on uniform task sampling or learning progress alone. This approach has\nthe potential to dramatically advance the ability to intelligently select which\ntasks to focus on next (i.e., auto-curricula), and could be seen as AI\nselecting its own next task to learn, facilitating self-improving AI and\nAI-Generating Algorithms. Project website at https://www.jennyzhangzt.com/omni/"
            ],
            "interesting paper": 2
        }
    ],
    "Drew Clark": [
        {
            "papers": [
                "Reverse Engineering Self-Supervised Learning\nSelf-supervised learning (SSL) is a powerful tool in machine learning, but\nunderstanding the learned representations and their underlying mechanisms\nremains a challenge. This paper presents an in-depth empirical analysis of\nSSL-trained representations, encompassing diverse models, architectures, and\nhyperparameters. Our study reveals an intriguing aspect of the SSL training\nprocess: it inherently facilitates the clustering of samples with respect to\nsemantic labels, which is surprisingly driven by the SSL objective's\nregularization term. This clustering process not only enhances downstream\nclassification but also compresses the data information. Furthermore, we\nestablish that SSL-trained representations align more closely with semantic\nclasses rather than random classes. Remarkably, we show that learned\nrepresentations align with semantic classes across various hierarchical levels,\nand this alignment increases during training and when moving deeper into the\nnetwork. Our findings provide valuable insights into SSL's representation\nlearning mechanisms and their impact on performance across different sets of\nclasses.",
                "Classic machine learning methods\nIn this chapter, we present the main classic machine learning methods. A\nlarge part of the chapter is devoted to supervised learning techniques for\nclassification and regression, including nearest-neighbor methods, linear and\nlogistic regressions, support vector machines and tree-based algorithms. We\nalso describe the problem of overfitting as well as strategies to overcome it.\nWe finally provide a brief overview of unsupervised learning methods, namely\nfor clustering and dimensionality reduction.",
                "Rethinking the Evaluation Protocol of Domain Generalization\nDomain generalization aims to solve the challenge of Out-of-Distribution\n(OOD) generalization by leveraging common knowledge learned from multiple\ntraining domains to generalize to unseen test domains. To accurately evaluate\nthe OOD generalization ability, it is required that test data information is\nunavailable. However, the current domain generalization protocol may still have\npotential test data information leakage. This paper examines the risks of test\ndata information leakage from two aspects of the current evaluation protocol:\nsupervised pretraining on ImageNet and oracle model selection. We propose\nmodifications to the current protocol that we should employ self-supervised\npretraining or train from scratch instead of employing the current supervised\npretraining, and we should use multiple test domains. These would result in a\nmore precise evaluation of OOD generalization ability. We also rerun the\nalgorithms with the modified protocol and introduce new leaderboards to\nencourage future research in domain generalization with a fairer comparison.",
                "Differentiable Clustering with Perturbed Spanning Forests\nWe introduce a differentiable clustering method based on stochastic\nperturbations of minimum-weight spanning forests. This allows us to include\nclustering in end-to-end trainable pipelines, with efficient gradients. We show\nthat our method performs well even in difficult settings, such as data sets\nwith high noise and challenging geometries. We also formulate an ad hoc loss to\nefficiently learn from partial clustering data using this operation. We\ndemonstrate its performance on several data sets for supervised and\nsemi-supervised tasks.",
                "How to escape sharp minima with random perturbations\nModern machine learning applications have witnessed the remarkable success of\noptimization algorithms that are designed to find flat minima. Motivated by\nthis design choice, we undertake a formal study that (i) formulates the notion\nof flat minima, and (ii) studies the complexity of finding them. Specifically,\nwe adopt the trace of the Hessian of the cost function as a measure of\nflatness, and use it to formally define the notion of approximate flat minima.\nUnder this notion, we then analyze algorithms that find approximate flat minima\nefficiently. For general cost functions, we discuss a gradient-based algorithm\nthat finds an approximate flat local minimum efficiently. The main component of\nthe algorithm is to use gradients computed from randomly perturbed iterates to\nestimate a direction that leads to flatter minima. For the setting where the\ncost function is an empirical risk over training data, we present a faster\nalgorithm that is inspired by a recently proposed practical algorithm called\nsharpness-aware minimization, supporting its success in practice.",
                "Generalizable Low-Resource Activity Recognition with Diverse and\n  Discriminative Representation Learning\nHuman activity recognition (HAR) is a time series classification task that\nfocuses on identifying the motion patterns from human sensor readings. Adequate\ndata is essential but a major bottleneck for training a generalizable HAR\nmodel, which assists customization and optimization of online web applications.\nHowever, it is costly in time and economy to collect large-scale labeled data\nin reality, i.e., the low-resource challenge. Meanwhile, data collected from\ndifferent persons have distribution shifts due to different living habits, body\nshapes, age groups, etc. The low-resource and distribution shift challenges are\ndetrimental to HAR when applying the trained model to new unseen subjects. In\nthis paper, we propose a novel approach called Diverse and Discriminative\nrepresentation Learning (DDLearn) for generalizable low-resource HAR. DDLearn\nsimultaneously considers diversity and discrimination learning. With the\nconstructed self-supervised learning task, DDLearn enlarges the data diversity\nand explores the latent activity properties. Then, we propose a diversity\npreservation module to preserve the diversity of learned features by enlarging\nthe distribution divergence between the original and augmented domains.\nMeanwhile, DDLearn also enhances semantic discrimination by learning\ndiscriminative representations with supervised contrastive learning. Extensive\nexperiments on three public HAR datasets demonstrate that our method\nsignificantly outperforms state-of-art methods by an average accuracy\nimprovement of 9.5% under the low-resource distribution shift scenarios, while\nbeing a generic, explainable, and flexible framework. Code is available at:\nhttps://github.com/microsoft/robustlearn."
            ],
            "interesting paper": 1
        },
        {
            "papers": [
                "ISLE: An Intelligent Streaming Framework for High-Throughput AI\n  Inference in Medical Imaging\nAs the adoption of Artificial Intelligence (AI) systems within the clinical\nenvironment grows, limitations in bandwidth and compute can create\ncommunication bottlenecks when streaming imaging data, leading to delays in\npatient care and increased cost. As such, healthcare providers and AI vendors\nwill require greater computational infrastructure, therefore dramatically\nincreasing costs. To that end, we developed ISLE, an intelligent streaming\nframework for high-throughput, compute- and bandwidth- optimized, and cost\neffective AI inference for clinical decision making at scale. In our\nexperiments, ISLE on average reduced data transmission by 98.02% and decoding\ntime by 98.09%, while increasing throughput by 2,730%. We show that ISLE\nresults in faster turnaround times, and reduced overall cost of data,\ntransmission, and compute, without negatively impacting clinical decision\nmaking using AI systems.",
                "Artificial Intelligence-Based Methods for Precision Medicine: Diabetes\n  Risk Prediction\nThe rising prevalence of type 2 diabetes mellitus (T2DM) necessitates the\ndevelopment of predictive models for T2DM risk assessment. Artificial\nintelligence (AI) models are being extensively used for this purpose, but a\ncomprehensive review of their advancements and challenges is lacking. This\nscoping review analyzes existing literature on AI-based models for T2DM risk\nprediction. Forty studies were included, mainly published in the past four\nyears. Traditional machine learning models were more prevalent than deep\nlearning models. Electronic health records were the most commonly used data\nsource. Unimodal AI models relying on EHR data were prominent, while only a few\nutilized multimodal models. Both unimodal and multimodal models showed\npromising performance, with the latter outperforming the former. Internal\nvalidation was common, while external validation was limited. Interpretability\nmethods were reported in half of the studies. Few studies reported novel\nbiomarkers, and open-source code availability was limited. This review provides\ninsights into the current state and limitations of AI-based T2DM risk\nprediction models and highlights challenges for their development and clinical\nimplementation.",
                "HuatuoGPT, towards Taming Language Model to Be a Doctor\nIn this paper, we present HuatuoGPT, a large language model (LLM) for medical\nconsultation. The core recipe of HuatuoGPT is to leverage both\n\\textit{distilled data from ChatGPT} and \\textit{real-world data from doctors}\nin the supervised fine-tuned stage. The responses of ChatGPT are usually\ndetailed, well-presented and informative while it cannot perform like a doctor\nin many aspects, e.g. for integrative diagnosis. We argue that real-world data\nfrom doctors would be complementary to distilled data in the sense the former\ncould tame a distilled language model to perform like doctors. To better\nleverage the strengths of both data, we train a reward model to align the\nlanguage model with the merits that both data bring, following an RLAIF\n(reinforced learning from AI feedback) fashion. To evaluate and benchmark the\nmodels, we propose a comprehensive evaluation scheme (including automatic and\nmanual metrics). Experimental results demonstrate that HuatuoGPT achieves\nstate-of-the-art results in performing medical consultation among open-source\nLLMs in GPT-4 evaluation, human evaluation, and medical benchmark datasets. It\nis worth noting that by using additional real-world data and RLAIF, the\ndistilled language model (i.e., HuatuoGPT) outperforms its teacher model\nChatGPT in most cases. Our code, data, and models are publicly available at\n\\url{https://github.com/FreedomIntelligence/HuatuoGPT}. The online demo is\navailable at \\url{https://www.HuatuoGPT.cn/}.",
                "Deep Learning and Ethics\nThis article appears as chapter 21 of Prince (2023, Understanding Deep\nLearning); a complete draft of the textbook is available here:\nhttp://udlbook.com. This chapter considers potential harms arising from the\ndesign and use of AI systems. These include algorithmic bias, lack of\nexplainability, data privacy violations, militarization, fraud, and\nenvironmental concerns. The aim is not to provide advice on being more ethical.\nInstead, the goal is to express ideas and start conversations in key areas that\nhave received attention in philosophy, political science, and the broader\nsocial sciences.",
                "Towards a Capability Assessment Model for the Comprehension and Adoption\n  of AI in Organisations\nThe comprehension and adoption of Artificial Intelligence (AI) are beset with\npractical and ethical problems. This article presents a 5-level AI Capability\nAssessment Model (AI-CAM) and a related AI Capabilities Matrix (AI-CM) to\nassist practitioners in AI comprehension and adoption. These practical tools\nwere developed with business executives, technologists, and other\norganisational stakeholders in mind. They are founded on a comprehensive\nconception of AI compared to those in other AI adoption models and are also\nopen-source artefacts. Thus, the AI-CAM and AI-CM present an accessible\nresource to help inform organisational decision-makers on the capability\nrequirements for (1) AI-based data analytics use cases based on machine\nlearning technologies; (2) Knowledge representation to engineer and represent\ndata, information and knowledge using semantic technologies; and (3) AI-based\nsolutions that seek to emulate human reasoning and decision-making. The AI-CAM\ncovers the core capability dimensions (business, data, technology,\norganisation, AI skills, risks, and ethical considerations) required at the\nfive capability maturity levels to achieve optimal use of AI in organisations.",
                "Patient Outcome Predictions Improve Operations at a Large Hospital\n  Network\nProblem definition: Access to accurate predictions of patients' outcomes can\nenhance medical staff's decision-making, which ultimately benefits all\nstakeholders in the hospitals. A large hospital network in the US has been\ncollaborating with academics and consultants to predict short-term and\nlong-term outcomes for all inpatients across their seven hospitals.\nMethodology/results: We develop machine learning models that predict the\nprobabilities of next 24-hr/48-hr discharge and intensive care unit transfers,\nend-of-stay mortality and discharge dispositions. All models achieve high\nout-of-sample AUC (75.7%-92.5%) and are well calibrated. In addition, combining\n48-hr discharge predictions with doctors' predictions simultaneously enables\nmore patient discharges (10%-28.7%) and fewer 7-day/30-day readmissions\n($p$-value $<0.001$). We implement an automated pipeline that extracts data and\nupdates predictions every morning, as well as user-friendly software and a\ncolor-coded alert system to communicate these patient-level predictions\n(alongside explanations) to clinical teams. Managerial implications: Since we\nhave been gradually deploying the tool, and training medical staff, over 200\ndoctors, nurses, and case managers across seven hospitals use it in their daily\npatient review process. We observe a significant reduction in the average\nlength of stay (0.67 days per patient) following its adoption and anticipate\nsubstantial financial benefits (between \\$55 and \\$72 million annually) for the\nhealthcare system."
            ],
            "interesting paper": 3
        },
        {
            "papers": [
                "Revisiting Structured Variational Autoencoders\nStructured variational autoencoders (SVAEs) combine probabilistic graphical\nmodel priors on latent variables, deep neural networks to link latent variables\nto observed data, and structure-exploiting algorithms for approximate posterior\ninference. These models are particularly appealing for sequential data, where\nthe prior can capture temporal dependencies. However, despite their conceptual\nelegance, SVAEs have proven difficult to implement, and more general approaches\nhave been favored in practice. Here, we revisit SVAEs using modern machine\nlearning tools and demonstrate their advantages over more general alternatives\nin terms of both accuracy and efficiency. First, we develop a modern\nimplementation for hardware acceleration, parallelization, and automatic\ndifferentiation of the message passing algorithms at the core of the SVAE.\nSecond, we show that by exploiting structure in the prior, the SVAE learns more\naccurate models and posterior distributions, which translate into improved\nperformance on prediction tasks. Third, we show how the SVAE can naturally\nhandle missing data, and we leverage this ability to develop a novel,\nself-supervised training approach. Altogether, these results show that the time\nis ripe to revisit structured variational autoencoders.",
                "Detecting Heart Disease from Multi-View Ultrasound Images via Supervised\n  Attention Multiple Instance Learning\nAortic stenosis (AS) is a degenerative valve condition that causes\nsubstantial morbidity and mortality. This condition is under-diagnosed and\nunder-treated. In clinical practice, AS is diagnosed with expert review of\ntransthoracic echocardiography, which produces dozens of ultrasound images of\nthe heart. Only some of these views show the aortic valve. To automate\nscreening for AS, deep networks must learn to mimic a human expert's ability to\nidentify views of the aortic valve then aggregate across these relevant images\nto produce a study-level diagnosis. We find previous approaches to AS detection\nyield insufficient accuracy due to relying on inflexible averages across\nimages. We further find that off-the-shelf attention-based multiple instance\nlearning (MIL) performs poorly. We contribute a new end-to-end MIL approach\nwith two key methodological innovations. First, a supervised attention\ntechnique guides the learned attention mechanism to favor relevant views.\nSecond, a novel self-supervised pretraining strategy applies contrastive\nlearning on the representation of the whole study instead of individual images\nas commonly done in prior literature. Experiments on an open-access dataset and\nan external validation set show that our approach yields higher accuracy while\nreducing model size.",
                "Scan and Snap: Understanding Training Dynamics and Token Composition in\n  1-layer Transformer\nTransformer architecture has shown impressive performance in multiple\nresearch domains and has become the backbone of many neural network models.\nHowever, there is limited understanding on how it works. In particular, with a\nsimple predictive loss, how the representation emerges from the gradient\n\\emph{training dynamics} remains a mystery. In this paper, for 1-layer\ntransformer with one self-attention layer plus one decoder layer, we analyze\nits SGD training dynamics for the task of next token prediction in a\nmathematically rigorous manner. We open the black box of the dynamic process of\nhow the self-attention layer combines input tokens, and reveal the nature of\nunderlying inductive bias. More specifically, with the assumption (a) no\npositional encoding, (b) long input sequence, and (c) the decoder layer learns\nfaster than the self-attention layer, we prove that self-attention acts as a\n\\emph{discriminative scanning algorithm}: starting from uniform attention, it\ngradually attends more to distinct key tokens for a specific next token to be\npredicted, and pays less attention to common key tokens that occur across\ndifferent next tokens. Among distinct tokens, it progressively drops attention\nweights, following the order of low to high co-occurrence between the key and\nthe query token in the training set. Interestingly, this procedure does not\nlead to winner-takes-all, but decelerates due to a \\emph{phase transition} that\nis controllable by the learning rates of the two layers, leaving (almost) fixed\ntoken combination. We verify this \\textbf{\\emph{scan and snap}} dynamics on\nsynthetic and real-world data (WikiText).",
                "Diversify Your Vision Datasets with Automatic Diffusion-Based\n  Augmentation\nMany fine-grained classification tasks, like rare animal identification, have\nlimited training data and consequently classifiers trained on these datasets\noften fail to generalize to variations in the domain like changes in weather or\nlocation. As such, we explore how natural language descriptions of the domains\nseen in training data can be used with large vision models trained on diverse\npretraining datasets to generate useful variations of the training data. We\nintroduce ALIA (Automated Language-guided Image Augmentation), a method which\nutilizes large vision and language models to automatically generate natural\nlanguage descriptions of a dataset's domains and augment the training data via\nlanguage-guided image editing. To maintain data integrity, a model trained on\nthe original dataset filters out minimal image edits and those which corrupt\nclass-relevant information. The resulting dataset is visually consistent with\nthe original training data and offers significantly enhanced diversity. We show\nthat ALIA is able to surpasses traditional data augmentation and text-to-image\ngenerated data on fine-grained classification tasks, including cases of domain\ngeneralization and contextual bias. Code is available at\nhttps://github.com/lisadunlap/ALIA.",
                "On convex decision regions in deep network representations\nCurrent work on human-machine alignment aims at understanding machine-learned\nlatent spaces and their correspondence to human representations.\nG{\\\"a}rdenfors' conceptual spaces is a prominent framework for understanding\nhuman representations. Convexity of object regions in conceptual spaces is\nargued to promote generalizability, few-shot learning, and interpersonal\nalignment. Based on these insights, we investigate the notion of convexity of\nconcept regions in machine-learned latent spaces. We develop a set of tools for\nmeasuring convexity in sampled data and evaluate emergent convexity in layered\nrepresentations of state-of-the-art deep networks. We show that convexity is\nrobust to basic re-parametrization and, hence, meaningful as a quality of\nmachine-learned latent spaces. We find that approximate convexity is pervasive\nin neural representations in multiple application domains, including models of\nimages, audio, human activity, text, and medical images. Generally, we observe\nthat fine-tuning increases the convexity of label regions. We find evidence\nthat pretraining convexity of class label regions predicts subsequent\nfine-tuning performance.",
                "Differentiable Random Partition Models\nPartitioning a set of elements into an unknown number of mutually exclusive\nsubsets is essential in many machine learning problems. However, assigning\nelements, such as samples in a dataset or neurons in a network layer, to an\nunknown and discrete number of subsets is inherently non-differentiable,\nprohibiting end-to-end gradient-based optimization of parameters. We overcome\nthis limitation by proposing a novel two-step method for inferring partitions,\nwhich allows its usage in variational inference tasks. This new approach\nenables reparameterized gradients with respect to the parameters of the new\nrandom partition model. Our method works by inferring the number of elements\nper subset and, second, by filling these subsets in a learned order. We\nhighlight the versatility of our general-purpose approach on three different\nchallenging experiments: variational clustering, inference of shared and\nindependent generative factors under weak supervision, and multitask learning."
            ],
            "interesting paper": 3
        },
        {
            "papers": [
                "Image Classification of Stroke Blood Clot Origin using Deep\n  Convolutional Neural Networks and Visual Transformers\nStroke is one of two main causes of death worldwide. Many individuals suffer\nfrom ischemic stroke every year. Only in US more over 700,000 individuals meet\nischemic stroke due to blood clot blocking an artery to the brain every year.\nThe paper describes particular approach how to apply Artificial Intelligence\nfor purposes of separating two major acute ischemic stroke (AIS) etiology\nsubtypes: cardiac and large artery atherosclerosis. Four deep neural network\narchitectures and simple ensemble method are used in the approach.",
                "Enhancing Human Capabilities through Symbiotic Artificial Intelligence\n  with Shared Sensory Experiences\nThe merging of human intelligence and artificial intelligence has long been a\nsubject of interest in both science fiction and academia. In this paper, we\nintroduce a novel concept in Human-AI interaction called Symbiotic Artificial\nIntelligence with Shared Sensory Experiences (SAISSE), which aims to establish\na mutually beneficial relationship between AI systems and human users through\nshared sensory experiences. By integrating multiple sensory input channels and\nprocessing human experiences, SAISSE fosters a strong human-AI bond, enabling\nAI systems to learn from and adapt to individual users, providing personalized\nsupport, assistance, and enhancement. Furthermore, we discuss the incorporation\nof memory storage units for long-term growth and development of both the AI\nsystem and its human user. As we address user privacy and ethical guidelines\nfor responsible AI-human symbiosis, we also explore potential biases and\ninequalities in AI-human symbiosis and propose strategies to mitigate these\nchallenges. Our research aims to provide a comprehensive understanding of the\nSAISSE concept and its potential to effectively support and enhance individual\nhuman users through symbiotic AI systems. This position article aims at\ndiscussing poteintial AI-human interaction related topics within the scientific\ncommunity, rather than providing experimental or theoretical results.",
                "Alert of the Second Decision-maker: An Introduction to Human-AI Conflict\nThe collaboration between humans and artificial intelligence (AI) is a\nsignificant feature in this digital age. However, humans and AI may have\nobservation, interpretation, and action conflicts when working synchronously.\nThis phenomenon is often masked by faults and, unfortunately, overlooked. This\npaper systematically introduces the human-AI conflict concept, causes,\nmeasurement methods, and risk assessment. The results highlight that there is a\npotential second decision-maker besides the human, which is the AI; the\nhuman-AI conflict is a unique and emerging risk in digitalized process systems;\nand this is an interdisciplinary field that needs to be distinguished from\ntraditional fault and failure analysis; the conflict risk is significant and\ncannot be ignored.",
                "Detecting Heart Disease from Multi-View Ultrasound Images via Supervised\n  Attention Multiple Instance Learning\nAortic stenosis (AS) is a degenerative valve condition that causes\nsubstantial morbidity and mortality. This condition is under-diagnosed and\nunder-treated. In clinical practice, AS is diagnosed with expert review of\ntransthoracic echocardiography, which produces dozens of ultrasound images of\nthe heart. Only some of these views show the aortic valve. To automate\nscreening for AS, deep networks must learn to mimic a human expert's ability to\nidentify views of the aortic valve then aggregate across these relevant images\nto produce a study-level diagnosis. We find previous approaches to AS detection\nyield insufficient accuracy due to relying on inflexible averages across\nimages. We further find that off-the-shelf attention-based multiple instance\nlearning (MIL) performs poorly. We contribute a new end-to-end MIL approach\nwith two key methodological innovations. First, a supervised attention\ntechnique guides the learned attention mechanism to favor relevant views.\nSecond, a novel self-supervised pretraining strategy applies contrastive\nlearning on the representation of the whole study instead of individual images\nas commonly done in prior literature. Experiments on an open-access dataset and\nan external validation set show that our approach yields higher accuracy while\nreducing model size.",
                "Applying Interdisciplinary Frameworks to Understand Algorithmic\n  Decision-Making\nWe argue that explanations for \"algorithmic decision-making\" (ADM) systems\ncan profit by adopting practices that are already used in the learning\nsciences. We shortly introduce the importance of explaining ADM systems, give a\nbrief overview of approaches drawing from other disciplines to improve\nexplanations, and present the results of our qualitative task-based study\nincorporating the \"six facets of understanding\" framework. We close with\nquestions guiding the discussion of how future studies can leverage an\ninterdisciplinary approach.",
                "AI Techniques in the Microservices Life-Cycle: A Survey\nMicroservices is a popular architectural style for the development of\ndistributed software, with an emphasis on modularity, scalability, and\nflexibility. Indeed, in microservice systems, functionalities are provided by\nloosely coupled, small services, each focusing on a specific business\ncapability. Building a system according to the microservices architectural\nstyle brings a number of challenges, mainly related to how the different\nmicroservices are deployed and coordinated and how they interact. In this\npaper, we provide a survey about how techniques in the area of Artificial\nIntelligence have been used to tackle these challenges."
            ],
            "interesting paper": 1
        },
        {
            "papers": [
                "Instance-based Max-margin for Practical Few-shot Recognition\nIn order to mimic the human few-shot learning (FSL) ability better and to\nmake FSL closer to real-world applications, this paper proposes a practical FSL\n(pFSL) setting. pFSL is based on unsupervised pretrained models (analogous to\nhuman prior knowledge) and recognizes many novel classes simultaneously.\nCompared to traditional FSL, pFSL is simpler in its formulation, easier to\nevaluate, more challenging and more practical. To cope with the rarity of\ntraining examples, this paper proposes IbM2, an instance-based max-margin\nmethod not only for the new pFSL setting, but also works well in traditional\nFSL scenarios. Based on the Gaussian Annulus Theorem, IbM2 converts random\nnoise applied to the instances into a mechanism to achieve maximum margin in\nthe many-way pFSL (or traditional FSL) recognition task. Experiments with\nvarious self-supervised pretraining methods and diverse many- or few-way FSL\ntasks show that IbM2 almost always leads to improvements compared to its\nrespective baseline methods, and in most cases the improvements are\nsignificant. With both the new pFSL setting and novel IbM2 method, this paper\nshows that practical few-shot learning is both viable and promising.",
                "Self-Supervised Reinforcement Learning that Transfers using Random\n  Features\nModel-free reinforcement learning algorithms have exhibited great potential\nin solving single-task sequential decision-making problems with\nhigh-dimensional observations and long horizons, but are known to be hard to\ngeneralize across tasks. Model-based RL, on the other hand, learns\ntask-agnostic models of the world that naturally enables transfer across\ndifferent reward functions, but struggles to scale to complex environments due\nto the compounding error. To get the best of both worlds, we propose a\nself-supervised reinforcement learning method that enables the transfer of\nbehaviors across tasks with different rewards, while circumventing the\nchallenges of model-based RL. In particular, we show self-supervised\npre-training of model-free reinforcement learning with a number of random\nfeatures as rewards allows implicit modeling of long-horizon environment\ndynamics. Then, planning techniques like model-predictive control using these\nimplicit models enable fast adaptation to problems with new reward functions.\nOur method is self-supervised in that it can be trained on offline datasets\nwithout reward labels, but can then be quickly deployed on new tasks. We\nvalidate that our proposed method enables transfer across tasks on a variety of\nmanipulation and locomotion domains in simulation, opening the door to\ngeneralist decision-making agents.",
                "Matrix Information Theory for Self-Supervised Learning\nThe maximum entropy encoding framework provides a unified perspective for\nmany non-contrastive learning methods like SimSiam, Barlow Twins, and MEC.\nInspired by this framework, we introduce Matrix-SSL, a novel approach that\nleverages matrix information theory to interpret the maximum entropy encoding\nloss as matrix uniformity loss. Furthermore, Matrix-SSL enhances the maximum\nentropy encoding method by seamlessly incorporating matrix alignment loss,\ndirectly aligning covariance matrices in different branches. Experimental\nresults reveal that Matrix-SSL outperforms state-of-the-art methods on the\nImageNet dataset under linear evaluation settings and on MS-COCO for transfer\nlearning tasks. Specifically, when performing transfer learning tasks on\nMS-COCO, our method outperforms previous SOTA methods such as MoCo v2 and BYOL\nup to 3.3% with only 400 epochs compared to 800 epochs pre-training. We also\ntry to introduce representation learning into the language modeling regime by\nfine-tuning a 7B model using matrix cross-entropy loss, with a margin of 3.1%\non the GSM8K dataset over the standard cross-entropy loss. Code available at\nhttps://github.com/yifanzhang-pro/Matrix-SSL.",
                "Statistically Significant Concept-based Explanation of Image Classifiers\n  via Model Knockoffs\nA concept-based classifier can explain the decision process of a deep\nlearning model by human-understandable concepts in image classification\nproblems. However, sometimes concept-based explanations may cause false\npositives, which misregards unrelated concepts as important for the prediction\ntask. Our goal is to find the statistically significant concept for\nclassification to prevent misinterpretation. In this study, we propose a method\nusing a deep learning model to learn the image concept and then using the\nKnockoff samples to select the important concepts for prediction by controlling\nthe False Discovery Rate (FDR) under a certain value. We evaluate the proposed\nmethod in our synthetic and real data experiments. Also, it shows that our\nmethod can control the FDR properly while selecting highly interpretable\nconcepts to improve the trustworthiness of the model.",
                "Visualizing Self-Regulated Learner Profiles in Dashboards: Design\n  Insights from Teachers\nFlipped Classrooms (FC) are a promising teaching strategy, where students\nengage with the learning material before attending face-to-face sessions. While\npre-class activities are critical for course success, many students struggle to\nengage effectively in them due to inadequate of self-regulated learning (SRL)\nskills. Thus, tools enabling teachers to monitor students' SRL and provide\npersonalized guidance have the potential to improve learning outcomes. However,\nexisting dashboards mostly focus on aggregated information, disregarding recent\nwork leveraging machine learning (ML) approaches that have identified\ncomprehensive, multi-dimensional SRL behaviors. Unfortunately, the complexity\nof such findings makes them difficult to communicate and act on. In this paper,\nwe follow a teacher-centered approach to study how to make thorough findings\naccessible to teachers. We design and implement FlippED, a dashboard for\nmonitoring students' SRL behavior. We evaluate the usability and actionability\nof the tool in semi-structured interviews with ten university teachers. We find\nthat communicating ML-based profiles spark a range of potential interventions\nfor students and course modifications.",
                "Causal Component Analysis\nIndependent Component Analysis (ICA) aims to recover independent latent\nvariables from observed mixtures thereof. Causal Representation Learning (CRL)\naims instead to infer causally related (thus often statistically dependent)\nlatent variables, together with the unknown graph encoding their causal\nrelationships. We introduce an intermediate problem termed Causal Component\nAnalysis (CauCA). CauCA can be viewed as a generalization of ICA, modelling the\ncausal dependence among the latent components, and as a special case of CRL. In\ncontrast to CRL, it presupposes knowledge of the causal graph, focusing solely\non learning the unmixing function and the causal mechanisms. Any impossibility\nresults regarding the recovery of the ground truth in CauCA also apply for CRL,\nwhile possibility results may serve as a stepping stone for extensions to CRL.\nWe characterize CauCA identifiability from multiple datasets generated through\ndifferent types of interventions on the latent causal variables. As a\ncorollary, this interventional perspective also leads to new identifiability\nresults for nonlinear ICA -- a special case of CauCA with an empty graph --\nrequiring strictly fewer datasets than previous results. We introduce a\nlikelihood-based approach using normalizing flows to estimate both the unmixing\nfunction and the causal mechanisms, and demonstrate its effectiveness through\nextensive synthetic experiments in the CauCA and ICA setting."
            ],
            "interesting paper": 3
        },
        {
            "papers": [
                "How Do UX Practitioners Communicate AI as a Design Material? Artifacts,\n  Conceptions, and Propositions\nUX practitioners (UXPs) face novel challenges when working with and\ncommunicating artificial intelligence (AI) as a design material. We explore how\nUXPs communicate AI concepts when given hands-on experience training and\nexperimenting with AI models. To do so, we conducted a task-based design study\nwith 27 UXPs in which they prototyped and created a design presentation for a\nAI-enabled interface while having access to a simple AI model training tool.\nThrough analyzing UXPs' design presentations and post-activity interviews, we\nfound that although UXPs struggled to clearly communicate some AI concepts,\ntinkering with AI broadened common ground when communicating with technical\nstakeholders. UXPs also identified key risks and benefits of AI in their\ndesigns, and proposed concrete next steps for both UX and AI work. We conclude\nwith a sensitizing concept and recommendations for design and AI tools to\nenhance multi-stakeholder communication and collaboration when crafting\nhuman-centered AI experiences.",
                "Frontier AI developers need an internal audit function\nThis article argues that frontier artificial intelligence (AI) developers\nneed an internal audit function. First, it describes the role of internal audit\nin corporate governance: internal audit evaluates the adequacy and\neffectiveness of a company's risk management, control, and governance\nprocesses. It is organizationally independent from senior management and\nreports directly to the board of directors, typically its audit committee. In\nthe IIA's Three Lines Model, internal audit serves as the third line and is\nresponsible for providing assurance to the board, while the Combined Assurance\nFramework highlights the need to coordinate the activities of internal and\nexternal assurance providers. Next, the article provides an overview of key\ngovernance challenges in frontier AI development: dangerous capabilities can\narise unpredictably and undetected; it is difficult to prevent a deployed model\nfrom causing harm; frontier models can proliferate rapidly; it is inherently\ndifficult to assess frontier AI risks; and frontier AI developers do not seem\nto follow best practices in risk governance. Finally, the article discusses how\nan internal audit function could address some of these challenges: internal\naudit could identify ineffective risk management practices; it could ensure\nthat the board of directors has a more accurate understanding of the current\nlevel of risk and the adequacy of the developer's risk management practices;\nand it could serve as a contact point for whistleblowers. In light of rapid\nprogress in AI research and development, frontier AI developers need to\nstrengthen their risk governance. Instead of reinventing the wheel, they should\nfollow existing best practices. While this might not be sufficient, they should\nnot skip this obvious first step.",
                "An Improved Model Ensembled of Different Hyper-parameter Tuned Machine\n  Learning Algorithms for Fetal Health Prediction\nFetal health is a critical concern during pregnancy as it can impact the\nwell-being of both the mother and the baby. Regular monitoring and timely\ninterventions are necessary to ensure the best possible outcomes. While there\nare various methods to monitor fetal health in the mother's womb, the use of\nartificial intelligence (AI) can improve the accuracy, efficiency, and speed of\ndiagnosis. In this study, we propose a robust ensemble model called ensemble of\ntuned Support Vector Machine and ExtraTrees (ETSE) for predicting fetal health.\nInitially, we employed various data preprocessing techniques such as outlier\nrejection, missing value imputation, data standardization, and data sampling.\nThen, seven machine learning (ML) classifiers including Support Vector Machine\n(SVM), XGBoost (XGB), Light Gradient Boosting Machine (LGBM), Decision Tree\n(DT), Random Forest (RF), ExtraTrees (ET), and K-Neighbors were implemented.\nThese models were evaluated and then optimized by hyperparameter tuning using\nthe grid search technique. Finally, we analyzed the performance of our proposed\nETSE model. The performance analysis of each model revealed that our proposed\nETSE model outperformed the other models with 100% precision, 100% recall, 100%\nF1-score, and 99.66% accuracy. This indicates that the ETSE model can\neffectively predict fetal health, which can aid in timely interventions and\nimprove outcomes for both the mother and the baby.",
                "BiomedGPT: A Generalist Vision-Language Foundation Model for Diverse\n  Biomedical Tasks\nTraditional biomedical artificial intelligence (AI) models, designed for\nspecific tasks or modalities, often exhibit limited flexibility in real-world\ndeployment and struggle to utilize holistic information. Generalist AI holds\nthe potential to address these limitations due to its versatility in\ninterpreting different data types and generating tailored outputs for diverse\nneeds. However, existing biomedical generalist AI solutions are typically\nheavyweight and closed source to researchers, practitioners, and patients.\nHere, we propose BiomedGPT, the first open-source and lightweight\nvision-language foundation model, designed as a generalist capable of\nperforming various biomedical tasks. BiomedGPT achieved state-of-the-art\nresults in 16 out of 25 experiments while maintaining a computing-friendly\nmodel scale. We also conducted human evaluations to assess the capabilities of\nBiomedGPT in radiology visual question answering, report generation, and\nsummarization. BiomedGPT exhibits robust prediction ability with a low error\nrate of 3.8% in question answering, satisfactory performance with an error rate\nof 8.3% in writing complex radiology reports, and competitive summarization\nability with a nearly equivalent preference score to human experts. Our method\ndemonstrates that effective training with diverse data can lead to more\npractical biomedical AI for improving diagnosis and workflow efficiency.",
                "Mindstorms in Natural Language-Based Societies of Mind\nBoth Minsky's \"society of mind\" and Schmidhuber's \"learning to think\" inspire\ndiverse societies of large multimodal neural networks (NNs) that solve problems\nby interviewing each other in a \"mindstorm.\" Recent implementations of NN-based\nsocieties of minds consist of large language models (LLMs) and other NN-based\nexperts communicating through a natural language interface. In doing so, they\novercome the limitations of single LLMs, improving multimodal zero-shot\nreasoning. In these natural language-based societies of mind (NLSOMs), new\nagents -- all communicating through the same universal symbolic language -- are\neasily added in a modular fashion. To demonstrate the power of NLSOMs, we\nassemble and experiment with several of them (having up to 129 members),\nleveraging mindstorms in them to solve some practical AI tasks: visual question\nanswering, image captioning, text-to-image synthesis, 3D generation, egocentric\nretrieval, embodied AI, and general language-based task solving. We view this\nas a starting point towards much larger NLSOMs with billions of agents-some of\nwhich may be humans. And with this emergence of great societies of\nheterogeneous minds, many new research questions have suddenly become paramount\nto the future of artificial intelligence. What should be the social structure\nof an NLSOM? What would be the (dis)advantages of having a monarchical rather\nthan a democratic structure? How can principles of NN economies be used to\nmaximize the total reward of a reinforcement learning NLSOM? In this work, we\nidentify, discuss, and try to answer some of these questions.",
                "Towards Cognitive Bots: Architectural Research Challenges\nSoftware bots operating in multiple virtual digital platforms must understand\nthe platforms' affordances and behave like human users. Platform affordances or\nfeatures differ from one application platform to another or through a life\ncycle, requiring such bots to be adaptable. Moreover, bots in such platforms\ncould cooperate with humans or other software agents for work or to learn\nspecific behavior patterns. However, present-day bots, particularly chatbots,\nother than language processing and prediction, are far from reaching a human\nuser's behavior level within complex business information systems. They lack\nthe cognitive capabilities to sense and act in such virtual environments,\nrendering their development a challenge to artificial general intelligence\nresearch. In this study, we problematize and investigate assumptions in\nconceptualizing software bot architecture by directing attention to significant\narchitectural research challenges in developing cognitive bots endowed with\ncomplex behavior for operation on information systems. As an outlook, we\npropose alternate architectural assumptions to consider in future bot design\nand bot development frameworks."
            ],
            "interesting paper": 3
        },
        {
            "papers": [
                "Data Minimization at Inference Time\nIn domains with high stakes such as law, recruitment, and healthcare,\nlearning models frequently rely on sensitive user data for inference,\nnecessitating the complete set of features. This not only poses significant\nprivacy risks for individuals but also demands substantial human effort from\norganizations to verify information accuracy. This paper asks whether it is\nnecessary to use \\emph{all} input features for accurate predictions at\ninference time. The paper demonstrates that, in a personalized setting,\nindividuals may only need to disclose a small subset of their features without\ncompromising decision-making accuracy. The paper also provides an efficient\nsequential algorithm to determine the appropriate attributes for each\nindividual to provide. Evaluations across various learning tasks show that\nindividuals can potentially report as little as 10\\% of their information while\nmaintaining the same accuracy level as a model that employs the full set of\nuser information.",
                "Distill Gold from Massive Ores: Bi-level Data Pruning towards Efficient\n  Dataset Distillation\nData-efficient learning has garnered significant attention, especially given\nthe current trend of large multi-modal models. Recently, dataset distillation\nhas become an effective approach by synthesizing data samples that are\nessential for network training. However, it remains to be explored which\nsamples are essential for the dataset distillation process itself. In this\nwork, we study the data efficiency and selection for the dataset distillation\ntask. By re-formulating the dynamics of distillation, we provide insight into\nthe inherent redundancy in the real dataset, both theoretically and\nempirically. We propose to use the empirical loss value as a static data\npruning criterion. To further compensate for the variation of the data value in\ntraining, we find the most contributing samples based on their causal effects\non the distillation. The proposed selection strategy can efficiently exploit\nthe training dataset, outperform the previous SOTA distillation algorithms, and\nconsistently enhance the distillation algorithms, even on much larger-scale and\nmore heterogeneous datasets, e.g., full ImageNet-1K and Kinetics-400. We\nbelieve this paradigm will open up new avenues in the dynamics of distillation\nand pave the way for efficient dataset distillation. Our code is available on\nhttps://github.com/silicx/GoldFromOres-BiLP.",
                "The Curse of Recursion: Training on Generated Data Makes Models Forget\nStable Diffusion revolutionised image creation from descriptive text. GPT-2,\nGPT-3(.5) and GPT-4 demonstrated astonishing performance across a variety of\nlanguage tasks. ChatGPT introduced such language models to the general public.\nIt is now clear that large language models (LLMs) are here to stay, and will\nbring about drastic change in the whole ecosystem of online text and images. In\nthis paper we consider what the future might hold. What will happen to GPT-{n}\nonce LLMs contribute much of the language found online? We find that use of\nmodel-generated content in training causes irreversible defects in the\nresulting models, where tails of the original content distribution disappear.\nWe refer to this effect as Model Collapse and show that it can occur in\nVariational Autoencoders, Gaussian Mixture Models and LLMs. We build\ntheoretical intuition behind the phenomenon and portray its ubiquity amongst\nall learned generative models. We demonstrate that it has to be taken seriously\nif we are to sustain the benefits of training from large-scale data scraped\nfrom the web. Indeed, the value of data collected about genuine human\ninteractions with systems will be increasingly valuable in the presence of\ncontent generated by LLMs in data crawled from the Internet.",
                "One Network, Many Masks: Towards More Parameter-Efficient Transfer\n  Learning\nFine-tuning pre-trained language models for multiple tasks tends to be\nexpensive in terms of storage. To mitigate this, parameter-efficient transfer\nlearning (PETL) methods have been proposed to address this issue, but they\nstill require a significant number of parameters and storage when being applied\nto broader ranges of tasks. To achieve even greater storage reduction, we\npropose PROPETL, a novel method that enables efficient sharing of a single PETL\nmodule which we call prototype network (e.g., adapter, LoRA, and prefix-tuning)\nacross layers and tasks. We then learn binary masks to select different\nsub-networks from the shared prototype network and apply them as PETL modules\ninto different layers. We find that the binary masks can determine crucial\ninformation from the network, which is often ignored in previous studies. Our\nwork can also be seen as a type of pruning method, where we find that\noverparameterization also exists in the seemingly small PETL modules. We\nevaluate PROPETL on various downstream tasks and show that it can outperform\nother PETL methods with approximately 10% of the parameter storage required by\nthe latter.",
                "Diagnosing Transformers: Illuminating Feature Spaces for Clinical\n  Decision-Making\nPre-trained transformers are often fine-tuned to aid clinical decision-making\nusing limited clinical notes. Model interpretability is crucial, especially in\nhigh-stakes domains like medicine, to establish trust and ensure safety, which\nrequires human engagement. We introduce SUFO, a systematic framework that\nenhances interpretability of fine-tuned transformer feature spaces. SUFO\nutilizes a range of analytic and visualization techniques, including Supervised\nprobing, Unsupervised similarity analysis, Feature dynamics, and Outlier\nanalysis to address key questions about model trust and interpretability. We\nconduct a case study investigating the impact of pre-training data where we\nfocus on real-world pathology classification tasks, and validate our findings\non MedNLI. We evaluate five 110M-sized pre-trained transformer models,\ncategorized into general-domain (BERT, TNLR), mixed-domain (BioBERT, Clinical\nBioBERT), and domain-specific (PubMedBERT) groups. Our SUFO analyses reveal\nthat: (1) while PubMedBERT, the domain-specific model, contains valuable\ninformation for fine-tuning, it can overfit to minority classes when class\nimbalances exist. In contrast, mixed-domain models exhibit greater resistance\nto overfitting, suggesting potential improvements in domain-specific model\nrobustness; (2) in-domain pre-training accelerates feature disambiguation\nduring fine-tuning; and (3) feature spaces undergo significant sparsification\nduring this process, enabling clinicians to identify common outlier modes among\nfine-tuned models as demonstrated in this paper. These findings showcase the\nutility of SUFO in enhancing trust and safety when using transformers in\nmedicine, and we believe SUFO can aid practitioners in evaluating fine-tuned\nlanguage models for other applications in medicine and in more critical\ndomains.",
                "USIM-DAL: Uncertainty-aware Statistical Image Modeling-based Dense\n  Active Learning for Super-resolution\nDense regression is a widely used approach in computer vision for tasks such\nas image super-resolution, enhancement, depth estimation, etc. However, the\nhigh cost of annotation and labeling makes it challenging to achieve accurate\nresults. We propose incorporating active learning into dense regression models\nto address this problem. Active learning allows models to select the most\ninformative samples for labeling, reducing the overall annotation cost while\nimproving performance. Despite its potential, active learning has not been\nwidely explored in high-dimensional computer vision regression tasks like\nsuper-resolution. We address this research gap and propose a new framework\ncalled USIM-DAL that leverages the statistical properties of colour images to\nlearn informative priors using probabilistic deep neural networks that model\nthe heteroscedastic predictive distribution allowing uncertainty\nquantification. Moreover, the aleatoric uncertainty from the network serves as\na proxy for error that is used for active learning. Our experiments on a wide\nvariety of datasets spanning applications in natural images (visual genome,\nBSD100), medical imaging (histopathology slides), and remote sensing (satellite\nimages) demonstrate the efficacy of the newly proposed USIM-DAL and superiority\nover several dense regression active learning methods."
            ],
            "interesting paper": 2
        },
        {
            "papers": [
                "A Comprehensive Overview and Comparative Analysis on Deep Learning\n  Models: CNN, RNN, LSTM, GRU\nDeep learning (DL) has emerged as a powerful subset of machine learning (ML)\nand artificial intelligence (AI), outperforming traditional ML methods,\nespecially in handling unstructured and large datasets. Its impact spans across\nvarious domains, including speech recognition, healthcare, autonomous vehicles,\ncybersecurity, predictive analytics, and more. However, the complexity and\ndynamic nature of real-world problems present challenges in designing effective\ndeep learning models. Consequently, several deep learning models have been\ndeveloped to address different problems and applications. In this article, we\nconduct a comprehensive survey of various deep learning models, including\nConvolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs),\nGenerative Models, Deep Reinforcement Learning (DRL), and Deep Transfer\nLearning. We examine the structure, applications, benefits, and limitations of\neach model. Furthermore, we perform an analysis using three publicly available\ndatasets: IMDB, ARAS, and Fruit-360. We compare the performance of six renowned\ndeep learning models: CNN, Simple RNN, Long Short-Term Memory (LSTM),\nBidirectional LSTM, Gated Recurrent Unit (GRU), and Bidirectional GRU.",
                "Counterfactual Formulation of Patient-Specific Root Causes of Disease\nRoot causes of disease intuitively correspond to root vertices that increase\nthe likelihood of a diagnosis. This description of a root cause nevertheless\nlacks the rigorous mathematical formulation needed for the development of\ncomputer algorithms designed to automatically detect root causes from data.\nPrior work defined patient-specific root causes of disease using an\ninterventionalist account that only climbs to the second rung of Pearl's Ladder\nof Causation. In this theoretical piece, we climb to the third rung by\nproposing a counterfactual definition matching clinical intuition based on\nfixed factual data alone. We then show how to assign a root causal contribution\nscore to each variable using Shapley values from explainable artificial\nintelligence. The proposed counterfactual formulation of patient-specific root\ncauses of disease accounts for noisy labels, adapts to disease prevalence and\nadmits fast computation without the need for counterfactual simulation.",
                "Diagnosing Transformers: Illuminating Feature Spaces for Clinical\n  Decision-Making\nPre-trained transformers are often fine-tuned to aid clinical decision-making\nusing limited clinical notes. Model interpretability is crucial, especially in\nhigh-stakes domains like medicine, to establish trust and ensure safety, which\nrequires human engagement. We introduce SUFO, a systematic framework that\nenhances interpretability of fine-tuned transformer feature spaces. SUFO\nutilizes a range of analytic and visualization techniques, including Supervised\nprobing, Unsupervised similarity analysis, Feature dynamics, and Outlier\nanalysis to address key questions about model trust and interpretability. We\nconduct a case study investigating the impact of pre-training data where we\nfocus on real-world pathology classification tasks, and validate our findings\non MedNLI. We evaluate five 110M-sized pre-trained transformer models,\ncategorized into general-domain (BERT, TNLR), mixed-domain (BioBERT, Clinical\nBioBERT), and domain-specific (PubMedBERT) groups. Our SUFO analyses reveal\nthat: (1) while PubMedBERT, the domain-specific model, contains valuable\ninformation for fine-tuning, it can overfit to minority classes when class\nimbalances exist. In contrast, mixed-domain models exhibit greater resistance\nto overfitting, suggesting potential improvements in domain-specific model\nrobustness; (2) in-domain pre-training accelerates feature disambiguation\nduring fine-tuning; and (3) feature spaces undergo significant sparsification\nduring this process, enabling clinicians to identify common outlier modes among\nfine-tuned models as demonstrated in this paper. These findings showcase the\nutility of SUFO in enhancing trust and safety when using transformers in\nmedicine, and we believe SUFO can aid practitioners in evaluating fine-tuned\nlanguage models for other applications in medicine and in more critical\ndomains.",
                "AI Coach Assist: An Automated Approach for Call Recommendation in\n  Contact Centers for Agent Coaching\nIn recent years, the utilization of Artificial Intelligence (AI) in the\ncontact center industry is on the rise. One area where AI can have a\nsignificant impact is in the coaching of contact center agents. By analyzing\ncall transcripts using Natural Language Processing (NLP) techniques, it would\nbe possible to quickly determine which calls are most relevant for coaching\npurposes. In this paper, we present AI Coach Assist, which leverages the\npre-trained transformer-based language models to determine whether a given call\nis coachable or not based on the quality assurance (QA) questions asked by the\ncontact center managers or supervisors. The system was trained and evaluated on\na large dataset collected from real-world contact centers and provides an\neffective way to recommend calls to the contact center managers that are more\nlikely to contain coachable moments. Our experimental findings demonstrate the\npotential of AI Coach Assist to improve the coaching process, resulting in\nenhancing the performance of contact center agents.",
                "Assumption Generation for the Verification of Learning-Enabled\n  Autonomous Systems\nProviding safety guarantees for autonomous systems is difficult as these\nsystems operate in complex environments that require the use of\nlearning-enabled components, such as deep neural networks (DNNs) for visual\nperception. DNNs are hard to analyze due to their size (they can have thousands\nor millions of parameters), lack of formal specifications (DNNs are typically\nlearnt from labeled data, in the absence of any formal requirements), and\nsensitivity to small changes in the environment. We present an assume-guarantee\nstyle compositional approach for the formal verification of system-level safety\nproperties of such autonomous systems. Our insight is that we can analyze the\nsystem in the absence of the DNN perception components by automatically\nsynthesizing assumptions on the DNN behaviour that guarantee the satisfaction\nof the required safety properties. The synthesized assumptions are the weakest\nin the sense that they characterize the output sequences of all the possible\nDNNs that, plugged into the autonomous system, guarantee the required safety\nproperties. The assumptions can be leveraged as run-time monitors over a\ndeployed DNN to guarantee the safety of the overall system; they can also be\nmined to extract local specifications for use during training and testing of\nDNNs. We illustrate our approach on a case study taken from the autonomous\nairplanes domain that uses a complex DNN for perception.",
                "Data Minimization at Inference Time\nIn domains with high stakes such as law, recruitment, and healthcare,\nlearning models frequently rely on sensitive user data for inference,\nnecessitating the complete set of features. This not only poses significant\nprivacy risks for individuals but also demands substantial human effort from\norganizations to verify information accuracy. This paper asks whether it is\nnecessary to use \\emph{all} input features for accurate predictions at\ninference time. The paper demonstrates that, in a personalized setting,\nindividuals may only need to disclose a small subset of their features without\ncompromising decision-making accuracy. The paper also provides an efficient\nsequential algorithm to determine the appropriate attributes for each\nindividual to provide. Evaluations across various learning tasks show that\nindividuals can potentially report as little as 10\\% of their information while\nmaintaining the same accuracy level as a model that employs the full set of\nuser information."
            ],
            "interesting paper": 3
        },
        {
            "papers": [
                "Full High-Dimensional Intelligible Learning In 2-D Lossless\n  Visualization Space\nThis study explores a new methodology for machine learning classification\ntasks in 2-dimensional visualization space (2-D ML) using Visual knowledge\nDiscovery in lossless General Line Coordinates. It is shown that this is a full\nmachine learning approach that does not require processing n-dimensional data\nin an abstract n-dimensional space. It enables discovering n-D patterns in 2-D\nspace without loss of n-D information using graph representations of n-D data\nin 2-D. Specifically, this study shows that it can be done with static and\ndynamic In-line Based Coordinates in different modifications, which are a\ncategory of General Line Coordinates. Based on these inline coordinates,\nclassification and regression methods were developed. The viability of the\nstrategy was shown by two case studies based on benchmark datasets (Wisconsin\nBreast Cancer and Page Block Classification datasets). The characteristics of\npage block classification data led to the development of an algorithm for\nimbalanced high-resolution data with multiple classes, which exploits the\ndecision trees as a model design facilitator producing a model, which is more\ngeneral than a decision tree. This work accelerates the ongoing consolidation\nof an emerging field of full 2-D machine learning and its methodology. Within\nthis methodology the end users can discover models and justify them as\nself-service. Providing interpretable ML models is another benefit of this\napproach.",
                "Mitigating Label Biases for In-context Learning\nVarious design settings for in-context learning (ICL), such as the choice and\norder of the in-context examples, can bias a model toward a particular\nprediction without being reflective of an understanding of the task. While many\nstudies discuss these design choices, there have been few systematic\ninvestigations into categorizing them and mitigating their impact. In this\nwork, we define a typology for three types of label biases in ICL for text\nclassification: vanilla-label bias, context-label bias, and domain-label bias\n(which we conceptualize and detect for the first time).\n  Our analysis demonstrates that prior label bias calibration methods fall\nshort of addressing all three types of biases. Specifically, domain-label bias\nrestricts LLMs to random-level performance on many tasks regardless of the\nchoice of in-context examples. To mitigate the effect of these biases, we\npropose a simple bias calibration method that estimates a language model's\nlabel bias using random in-domain words from the task corpus. After controlling\nfor this estimated bias when making predictions, our novel domain-context\ncalibration significantly improves the ICL performance of GPT-J and GPT-3 on a\nwide range of tasks. The gain is substantial on tasks with large domain-label\nbias (up to 37% in Macro-F1). Furthermore, our results generalize to models\nwith different scales, pretraining methods, and manually-designed task\ninstructions, showing the prevalence of label biases in ICL.",
                "Learning to Learn from APIs: Black-Box Data-Free Meta-Learning\nData-free meta-learning (DFML) aims to enable efficient learning of new tasks\nby meta-learning from a collection of pre-trained models without access to the\ntraining data. Existing DFML work can only meta-learn from (i) white-box and\n(ii) small-scale pre-trained models (iii) with the same architecture,\nneglecting the more practical setting where the users only have inference\naccess to the APIs with arbitrary model architectures and model scale inside.\nTo solve this issue, we propose a Bi-level Data-free Meta Knowledge\nDistillation (BiDf-MKD) framework to transfer more general meta knowledge from\na collection of black-box APIs to one single meta model. Specifically, by just\nquerying APIs, we inverse each API to recover its training data via a\nzero-order gradient estimator and then perform meta-learning via a novel\nbi-level meta knowledge distillation structure, in which we design a boundary\nquery set recovery technique to recover a more informative query set near the\ndecision boundary. In addition, to encourage better generalization within the\nsetting of limited API budgets, we propose task memory replay to diversify the\nunderlying task distribution by covering more interpolated tasks. Extensive\nexperiments in various real-world scenarios show the superior performance of\nour BiDf-MKD framework.",
                "A Meta-learning Framework for Tuning Parameters of Protection Mechanisms\n  in Trustworthy Federated Learning\nTrustworthy Federated Learning (TFL) typically leverages protection\nmechanisms to guarantee privacy. However, protection mechanisms inevitably\nintroduce utility loss or efficiency reduction while protecting data privacy.\nTherefore, protection mechanisms and their parameters should be carefully\nchosen to strike an optimal tradeoff between \\textit{privacy leakage},\n\\textit{utility loss}, and \\textit{efficiency reduction}. To this end,\nfederated learning practitioners need tools to measure the three factors and\noptimize the tradeoff between them to choose the protection mechanism that is\nmost appropriate to the application at hand. Motivated by this requirement, we\npropose a framework that (1) formulates TFL as a problem of finding a\nprotection mechanism to optimize the tradeoff between privacy leakage, utility\nloss, and efficiency reduction and (2) formally defines bounded measurements of\nthe three factors. We then propose a meta-learning algorithm to approximate\nthis optimization problem and find optimal protection parameters for\nrepresentative protection mechanisms, including Randomization, Homomorphic\nEncryption, Secret Sharing, and Compression. We further design estimation\nalgorithms to quantify these found optimal protection parameters in a practical\nhorizontal federated learning setting and provide a theoretical analysis of the\nestimation error.",
                "JutePestDetect: An Intelligent Approach for Jute Pest Identification\n  Using Fine-Tuned Transfer Learning\nIn certain Asian countries, Jute is one of the primary sources of income and\nGross Domestic Product (GDP) for the agricultural sector. Like many other\ncrops, Jute is prone to pest infestations, and its identification is typically\nmade visually in countries like Bangladesh, India, Myanmar, and China. In\naddition, this method is time-consuming, challenging, and somewhat imprecise,\nwhich poses a substantial financial risk. To address this issue, the study\nproposes a high-performing and resilient transfer learning (TL) based\nJutePestDetect model to identify jute pests at the early stage. Firstly, we\nprepared jute pest dataset containing 17 classes and around 380 photos per pest\nclass, which were evaluated after manual and automatic pre-processing and\ncleaning, such as background removal and resizing. Subsequently, five prominent\npre-trained models -DenseNet201, InceptionV3, MobileNetV2, VGG19, and ResNet50\nwere selected from a previous study to design the JutePestDetect model. Each\nmodel was revised by replacing the classification layer with a global average\npooling layer and incorporating a dropout layer for regularization. To evaluate\nthe models performance, various metrics such as precision, recall, F1 score,\nROC curve, and confusion matrix were employed. These analyses provided\nadditional insights for determining the efficacy of the models. Among them, the\ncustomized regularized DenseNet201-based proposed JutePestDetect model\noutperformed the others, achieving an impressive accuracy of 99%. As a result,\nour proposed method and strategy offer an enhanced approach to pest\nidentification in the case of Jute, which can significantly benefit farmers\nworldwide.",
                "Dink-Net: Neural Clustering on Large Graphs\nDeep graph clustering, which aims to group the nodes of a graph into disjoint\nclusters with deep neural networks, has achieved promising progress in recent\nyears. However, the existing methods fail to scale to the large graph with\nmillion nodes. To solve this problem, a scalable deep graph clustering method\n(Dink-Net) is proposed with the idea of dilation and shrink. Firstly, by\ndiscriminating nodes, whether being corrupted by augmentations, representations\nare learned in a self-supervised manner. Meanwhile, the cluster centres are\ninitialized as learnable neural parameters. Subsequently, the clustering\ndistribution is optimized by minimizing the proposed cluster dilation loss and\ncluster shrink loss in an adversarial manner. By these settings, we unify the\ntwo-step clustering, i.e., representation learning and clustering optimization,\ninto an end-to-end framework, guiding the network to learn clustering-friendly\nfeatures. Besides, Dink-Net scales well to large graphs since the designed loss\nfunctions adopt the mini-batch data to optimize the clustering distribution\neven without performance drops. Both experimental results and theoretical\nanalyses demonstrate the superiority of our method. Compared to the runner-up,\nDink-Net achieves 9.62% NMI improvement on the ogbn-papers100M dataset with 111\nmillion nodes and 1.6 billion edges. The source code is released at\nhttps://github.com/yueliu1999/Dink-Net. Besides, a collection (papers, codes,\nand datasets) of deep graph clustering is shared at\nhttps://github.com/yueliu1999/Awesome-Deep-Graph-Clustering."
            ],
            "interesting paper": 3
        },
        {
            "papers": [
                "Re-imagining health and well-being in low resource African settings\n  using an augmented AI system and a 3D digital twin\nThis paper discusses and explores the potential and relevance of recent\ndevelopments in artificial intelligence (AI) and digital twins for health and\nwell-being in low-resource African countries. We use the case of public health\nemergency response to disease outbreaks and epidemic control. There is\npotential to take advantage of the increasing availability of data and\ndigitization to develop advanced AI methods for analysis and prediction. Using\nan AI systems perspective, we review emerging trends in AI systems and digital\ntwins and propose an initial augmented AI system architecture to illustrate how\nan AI system can work with a 3D digital twin to address public health goals. We\nhighlight scientific knowledge discovery, continual learning, pragmatic\ninteroperability, and interactive explanation and decision-making as essential\nresearch challenges for AI systems and digital twins.",
                "The Digital Divide in Process Safety: Quantitative Risk Analysis of\n  Human-AI Collaboration\nDigital technologies have dramatically accelerated the digital transformation\nin process industries, boosted new industrial applications, upgraded the\nproduction system, and enhanced operational efficiency. In contrast, the\nchallenges and gaps between human and artificial intelligence (AI) have become\nmore and more prominent, whereas the digital divide in process safety is\naggregating. The study attempts to address the following questions: (i)What is\nAI in the process safety context? (ii)What is the difference between AI and\nhumans in process safety? (iii)How do AI and humans collaborate in process\nsafety? (iv)What are the challenges and gaps in human-AI collaboration? (v)How\nto quantify the risk of human-AI collaboration in process safety? Qualitative\nrisk analysis based on brainstorming and literature review, and quantitative\nrisk analysis based on layer of protection analysis (LOPA) and Bayesian network\n(BN), were applied to explore and model. The importance of human reliability\nshould be stressed in the digital age, not usually to increase the reliability\nof AI, and human-centered AI design in process safety needs to be propagated.",
                "AI Audit: A Card Game to Reflect on Everyday AI Systems\nAn essential element of K-12 AI literacy is educating learners about the\nethical and societal implications of AI systems. Previous work in AI ethics\nliteracy have developed curriculum and classroom activities that engage\nlearners in reflecting on the ethical implications of AI systems and developing\nresponsible AI. There is little work in using game-based learning methods in AI\nliteracy. Games are known to be compelling media to teach children about\ncomplex STEM concepts. In this work, we developed a competitive card game for\nmiddle and high school students called \"AI Audit\" where they play as AI\nstart-up founders building novel AI-powered technology. Players can challenge\nother players with potential harms of their technology or defend their own\nbusinesses by features that mitigate these harms. The game mechanics reward\nsystems that are ethically developed or that take steps to mitigate potential\nharms. In this paper, we present the game design, teacher resources for\nclassroom deployment and early playtesting results. We discuss our reflections\nabout using games as teaching tools for AI literacy in K-12 classrooms.",
                "Key-Value Transformer\nTransformers have emerged as the prevailing standard solution for various AI\ntasks, including computer vision and natural language processing. The widely\nadopted Query, Key, and Value formulation (QKV) has played a significant role\nin this. Nevertheless, no research has examined the essentiality of these three\ncomponents for transformer performance. Therefore, we conducted an evaluation\nof the key-value formulation (KV), which generates symmetric attention maps,\nalong with an asymmetric version that incorporates a 2D positional encoding\ninto the attention matrix. Remarkably, this transformer requires fewer\nparameters and computation than the original one. Through experiments\nencompassing three task types -- synthetics (such as reversing or sorting a\nlist), vision (mnist or cifar classification), and NLP (character generation\nand translation) -- we discovered that the KV transformer occasionally\noutperforms the QKV transformer. However, it also exhibits instances of\nunderperformance compared to QKV, making it challenging to draw a definitive\nconclusion. Nonetheless, we consider the reported results to be encouraging and\nanticipate that they may pave the way for more efficient transformers in the\nfuture.",
                "Ask an Expert: Leveraging Language Models to Improve Strategic Reasoning\n  in Goal-Oriented Dialogue Models\nExisting dialogue models may encounter scenarios which are not\nwell-represented in the training data, and as a result generate responses that\nare unnatural, inappropriate, or unhelpful. We propose the \"Ask an Expert\"\nframework in which the model is trained with access to an \"expert\" which it can\nconsult at each turn. Advice is solicited via a structured dialogue with the\nexpert, and the model is optimized to selectively utilize (or ignore) it given\nthe context and dialogue history. In this work the expert takes the form of an\nLLM. We evaluate this framework in a mental health support domain, where the\nstructure of the expert conversation is outlined by pre-specified prompts which\nreflect a reasoning strategy taught to practitioners in the field. Blenderbot\nmodels utilizing \"Ask an Expert\" show quality improvements across all expert\nsizes, including those with fewer parameters than the dialogue model itself.\nOur best model provides a $\\sim 10\\%$ improvement over baselines, approaching\nhuman-level scores on \"engingingness\" and \"helpfulness\" metrics.",
                "Analysis of Perceived Stress Test using Machine Learning\nThe aim of this study is to determine the perceived stress levels of 150\nindividuals and analyze the responses given to adapted questions in Turkish\nusing machine learning. The test consists of 14 questions, each scored on a\nscale of 0 to 4, resulting in a total score range of 0-56. Out of these\nquestions, 7 are formulated in a negative context and scored accordingly, while\nthe remaining 7 are formulated in a positive context and scored in reverse. The\ntest is also designed to identify two sub-factors: perceived self-efficacy and\nstress/discomfort perception. The main objectives of this research are to\ndemonstrate that test questions may not have equal importance using artificial\nintelligence techniques, reveal which questions exhibit variations in the\nsociety using machine learning, and ultimately demonstrate the existence of\ndistinct patterns observed psychologically. This study provides a different\nperspective from the existing psychology literature by repeating the test\nthrough machine learning. Additionally, it questions the accuracy of the scale\nused to interpret the results of the perceived stress test and emphasizes the\nimportance of considering differences in the prioritization of test questions.\nThe findings of this study offer new insights into coping strategies and\ntherapeutic approaches in dealing with stress. Source code:\nhttps://github.com/toygarr/ppl-r-stressed"
            ],
            "interesting paper": 4
        },
        {
            "papers": [
                "Learning Off-Road Terrain Traversability with Self-Supervisions Only\nEstimating the traversability of terrain should be reliable and accurate in\ndiverse conditions for autonomous driving in off-road environments. However,\nlearning-based approaches often yield unreliable results when confronted with\nunfamiliar contexts, and it is challenging to obtain manual annotations\nfrequently for new circumstances. In this paper, we introduce a method for\nlearning traversability from images that utilizes only self-supervision and no\nmanual labels, enabling it to easily learn traversability in new circumstances.\nTo this end, we first generate self-supervised traversability labels from past\ndriving trajectories by labeling regions traversed by the vehicle as highly\ntraversable. Using the self-supervised labels, we then train a neural network\nthat identifies terrains that are safe to traverse from an image using a\none-class classification algorithm. Additionally, we supplement the limitations\nof self-supervised labels by incorporating methods of self-supervised learning\nof visual representations. To conduct a comprehensive evaluation, we collect\ndata in a variety of driving environments and perceptual conditions and show\nthat our method produces reliable estimations in various environments. In\naddition, the experimental results validate that our method outperforms other\nself-supervised traversability estimation methods and achieves comparable\nperformances with supervised learning methods trained on manually labeled data.",
                "Partially Personalized Federated Learning: Breaking the Curse of Data\n  Heterogeneity\nWe present a partially personalized formulation of Federated Learning (FL)\nthat strikes a balance between the flexibility of personalization and\ncooperativeness of global training. In our framework, we split the variables\ninto global parameters, which are shared across all clients, and individual\nlocal parameters, which are kept private. We prove that under the right split\nof parameters, it is possible to find global parameters that allow each client\nto fit their data perfectly, and refer to the obtained problem as\noverpersonalized. For instance, the shared global parameters can be used to\nlearn good data representations, whereas the personalized layers are fine-tuned\nfor a specific client. Moreover, we present a simple algorithm for the\npartially personalized formulation that offers significant benefits to all\nclients. In particular, it breaks the curse of data heterogeneity in several\nsettings, such as training with local steps, asynchronous training, and\nByzantine-robust training.",
                "Task-Equivariant Graph Few-shot Learning\nAlthough Graph Neural Networks (GNNs) have been successful in node\nclassification tasks, their performance heavily relies on the availability of a\nsufficient number of labeled nodes per class. In real-world situations, not all\nclasses have many labeled nodes and there may be instances where the model\nneeds to classify new classes, making manual labeling difficult. To solve this\nproblem, it is important for GNNs to be able to classify nodes with a limited\nnumber of labeled nodes, known as few-shot node classification. Previous\nepisodic meta-learning based methods have demonstrated success in few-shot node\nclassification, but our findings suggest that optimal performance can only be\nachieved with a substantial amount of diverse training meta-tasks. To address\nthis challenge of meta-learning based few-shot learning (FSL), we propose a new\napproach, the Task-Equivariant Graph few-shot learning (TEG) framework. Our TEG\nframework enables the model to learn transferable task-adaptation strategies\nusing a limited number of training meta-tasks, allowing it to acquire\nmeta-knowledge for a wide range of meta-tasks. By incorporating equivariant\nneural networks, TEG can utilize their strong generalization abilities to learn\nhighly adaptable task-specific strategies. As a result, TEG achieves\nstate-of-the-art performance with limited training meta-tasks. Our experiments\non various benchmark datasets demonstrate TEG's superiority in terms of\naccuracy and generalization ability, even when using minimal meta-training\ndata, highlighting the effectiveness of our proposed approach in addressing the\nchallenges of meta-learning based few-shot node classification. Our code is\navailable at the following link: https://github.com/sung-won-kim/TEG",
                "Controllable Path of Destruction\nPath of Destruction (PoD) is a self-supervised method for learning iterative\ngenerators. The core idea is to produce a training set by destroying a set of\nartifacts, and for each destructive step create a training instance based on\nthe corresponding repair action. A generator trained on this dataset can then\ngenerate new artifacts by repairing from arbitrary states. The PoD method is\nvery data-efficient in terms of original training examples and well-suited to\nfunctional artifacts composed of categorical data, such as game levels and\ndiscrete 3D structures. In this paper, we extend the Path of Destruction method\nto allow designer control over aspects of the generated artifacts.\nControllability is introduced by adding conditional inputs to the state-action\npairs that make up the repair trajectories. We test the controllable PoD method\nin a 2D dungeon setting, as well as in the domain of small 3D Lego cars.",
                "Autoencoding Conditional Neural Processes for Representation Learning\nConditional neural processes (CNPs) are a flexible and efficient family of\nmodels that learn to learn a stochastic process from data. They have seen\nparticular application in contextual image completion - observing pixel values\nat some locations to predict a distribution over values at other unobserved\nlocations. However, the choice of pixels in learning CNPs is typically either\nrandom or derived from a simple statistical measure (e.g. pixel variance).\nHere, we turn the problem on its head and ask: which pixels would a CNP like to\nobserve - do they facilitate fitting better CNPs, and do such pixels tell us\nsomething meaningful about the underlying image? To this end we develop the\nPartial Pixel Space Variational Autoencoder (PPS-VAE), an amortised variational\nframework that casts CNP context as latent variables learnt simultaneously with\nthe CNP. We evaluate PPS-VAE over a number of tasks across different visual\ndata, and find that not only can it facilitate better-fit CNPs, but also that\nthe spatial arrangement and values meaningfully characterise image information\n- evaluated through the lens of classification on both within and out-of-data\ndistributions. Our model additionally allows for dynamic adaption of\ncontext-set size and the ability to scale-up to larger images, providing a\npromising avenue to explore learning meaningful and effective visual\nrepresentations.",
                "DMS: Differentiable Mean Shift for Dataset Agnostic Task Specific\n  Clustering Using Side Information\nWe present a novel approach, in which we learn to cluster data directly from\nside information, in the form of a small set of pairwise examples. Unlike\nprevious methods, with or without side information, we do not need to know the\nnumber of clusters, their centers or any kind of distance metric for\nsimilarity. Our method is able to divide the same data points in various ways\ndependant on the needs of a specific task, defined by the side information.\nContrastingly, other work generally finds only the intrinsic, most obvious,\nclusters. Inspired by the mean shift algorithm, we implement our new clustering\napproach using a custom iterative neural network to create Differentiable Mean\nShift (DMS), a state of the art, dataset agnostic, clustering method. We found\nthat it was possible to train a strong cluster definition without enforcing a\nconstraint that each cluster must be presented during training. DMS outperforms\ncurrent methods in both the intrinsic and non-intrinsic dataset tasks."
            ],
            "interesting paper": 1
        },
        {
            "papers": [
                "RE-centric Recommendations for the Development of Trustworthy(er)\n  Autonomous Systems\nComplying with the EU AI Act (AIA) guidelines while developing and\nimplementing AI systems will soon be mandatory within the EU. However,\npractitioners lack actionable instructions to operationalise ethics during AI\nsystems development. A literature review of different ethical guidelines\nrevealed inconsistencies in the principles addressed and the terminology used\nto describe them. Furthermore, requirements engineering (RE), which is\nidentified to foster trustworthiness in the AI development process from the\nearly stages was observed to be absent in a lot of frameworks that support the\ndevelopment of ethical and trustworthy AI. This incongruous phrasing combined\nwith a lack of concrete development practices makes trustworthy AI development\nharder. To address this concern, we formulated a comparison table for the\nterminology used and the coverage of the ethical AI principles in major ethical\nAI guidelines. We then examined the applicability of ethical AI development\nframeworks for performing effective RE during the development of trustworthy AI\nsystems. A tertiary review and meta-analysis of literature discussing ethical\nAI frameworks revealed their limitations when developing trustworthy AI. Based\non our findings, we propose recommendations to address such limitations during\nthe development of trustworthy AI.",
                "Shapley Based Residual Decomposition for Instance Analysis\nIn this paper, we introduce the idea of decomposing the residuals of\nregression with respect to the data instances instead of features. This allows\nus to determine the effects of each individual instance on the model and each\nother, and in doing so makes for a model-agnostic method of identifying\ninstances of interest. In doing so, we can also determine the appropriateness\nof the model and data in the wider context of a given study. The paper focuses\non the possible applications that such a framework brings to the relatively\nunexplored field of instance analysis in the context of Explainable AI tasks.",
                "Game of Tones: Faculty detection of GPT-4 generated content in\n  university assessments\nThis study explores the robustness of university assessments against the use\nof Open AI's Generative Pre-Trained Transformer 4 (GPT-4) generated content and\nevaluates the ability of academic staff to detect its use when supported by the\nTurnitin Artificial Intelligence (AI) detection tool. The research involved\ntwenty-two GPT-4 generated submissions being created and included in the\nassessment process to be marked by fifteen different faculty members. The study\nreveals that although the detection tool identified 91% of the experimental\nsubmissions as containing some AI-generated content, the total detected content\nwas only 54.8%. This suggests that the use of adversarial techniques regarding\nprompt engineering is an effective method in evading AI detection tools and\nhighlights that improvements to AI detection software are needed. Using the\nTurnitin AI detect tool, faculty reported 54.5% of the experimental submissions\nto the academic misconduct process, suggesting the need for increased awareness\nand training into these tools. Genuine submissions received a mean score of\n54.4, whereas AI-generated content scored 52.3, indicating the comparable\nperformance of GPT-4 in real-life situations. Recommendations include adjusting\nassessment strategies to make them more resistant to the use of AI tools, using\nAI-inclusive assessment where possible, and providing comprehensive training\nprograms for faculty and students. This research contributes to understanding\nthe relationship between AI-generated content and academic assessment, urging\nfurther investigation to preserve academic integrity.",
                "An Emergency Disposal Decision-making Method with Human--Machine\n  Collaboration\nRapid developments in artificial intelligence technology have led to unmanned\nsystems replacing human beings in many fields requiring high-precision\npredictions and decisions. In modern operational environments, all job plans\nare affected by emergency events such as equipment failures and resource\nshortages, making a quick resolution critical. The use of unmanned systems to\nassist decision-making can improve resolution efficiency, but their\ndecision-making is not interpretable and may make the wrong decisions. Current\nunmanned systems require human supervision and control. Based on this, we\npropose a collaborative human--machine method for resolving unplanned events\nusing two phases: task filtering and task scheduling. In the task filtering\nphase, we propose a human--machine collaborative decision-making algorithm for\ndynamic tasks. The GACRNN model is used to predict the state of the job nodes,\nlocate the key nodes, and generate a machine-predicted resolution task list. A\nhuman decision-maker supervises the list in real time and modifies and confirms\nthe machine-predicted list through the human--machine interface. In the task\nscheduling phase, we propose a scheduling algorithm that integrates human\nexperience constraints. The steps to resolve an event are inserted into the\nnormal job sequence to schedule the resolution. We propose several\nhuman--machine collaboration methods in each phase to generate steps to resolve\nan unplanned event while minimizing the impact on the original job plan.",
                "Generalized Disparate Impact for Configurable Fairness Solutions in ML\nWe make two contributions in the field of AI fairness over continuous\nprotected attributes. First, we show that the Hirschfeld-Gebelein-Renyi (HGR)\nindicator (the only one currently available for such a case) is valuable but\nsubject to a few crucial limitations regarding semantics, interpretability, and\nrobustness. Second, we introduce a family of indicators that are: 1)\ncomplementary to HGR in terms of semantics; 2) fully interpretable and\ntransparent; 3) robust over finite samples; 4) configurable to suit specific\napplications. Our approach also allows us to define fine-grained constraints to\npermit certain types of dependence and forbid others selectively. By expanding\nthe available options for continuous protected attributes, our approach\nrepresents a significant contribution to the area of fair artificial\nintelligence.",
                "Medical Dialogue Generation via Dual Flow Modeling\nMedical dialogue systems (MDS) aim to provide patients with medical services,\nsuch as diagnosis and prescription. Since most patients cannot precisely\ndescribe their symptoms, dialogue understanding is challenging for MDS.\nPrevious studies mainly addressed this by extracting the mentioned medical\nentities as critical dialogue history information. In this work, we argue that\nit is also essential to capture the transitions of the medical entities and the\ndoctor's dialogue acts in each turn, as they help the understanding of how the\ndialogue flows and enhance the prediction of the entities and dialogue acts to\nbe adopted in the following turn. Correspondingly, we propose a Dual Flow\nenhanced Medical (DFMed) dialogue generation framework. It extracts the medical\nentities and dialogue acts used in the dialogue history and models their\ntransitions with an entity-centric graph flow and a sequential act flow,\nrespectively. We employ two sequential models to encode them and devise an\ninterweaving component to enhance their interactions. Experiments on two\ndatasets demonstrate that our method exceeds baselines in both automatic and\nmanual evaluations."
            ],
            "interesting paper": 3
        },
        {
            "papers": [
                "A Computational Account Of Self-Supervised Visual Learning From\n  Egocentric Object Play\nResearch in child development has shown that embodied experience handling\nphysical objects contributes to many cognitive abilities, including visual\nlearning. One characteristic of such experience is that the learner sees the\nsame object from several different viewpoints. In this paper, we study how\nlearning signals that equate different viewpoints -- e.g., assigning similar\nrepresentations to different views of a single object -- can support robust\nvisual learning. We use the Toybox dataset, which contains egocentric videos of\nhumans manipulating different objects, and conduct experiments using a computer\nvision framework for self-supervised contrastive learning. We find that\nrepresentations learned by equating different physical viewpoints of an object\nbenefit downstream image classification accuracy. Further experiments show that\nthis performance improvement is robust to variations in the gaps between\nviewpoints, and that the benefits transfer to several different image\nclassification tasks.",
                "Contextual Vision Transformers for Robust Representation Learning\nWe introduce Contextual Vision Transformers (ContextViT), a method designed\nto generate robust image representations for datasets experiencing shifts in\nlatent factors across various groups. Derived from the concept of in-context\nlearning, ContextViT incorporates an additional context token to encapsulate\ngroup-specific information. This integration allows the model to adjust the\nimage representation in accordance with the group-specific context.\nSpecifically, for a given input image, ContextViT maps images with identical\ngroup membership into this context token, which is appended to the input image\ntokens. Additionally, we introduce a context inference network to predict such\ntokens on-the-fly, given a batch of samples from the group. This enables\nContextViT to adapt to new testing distributions during inference time. We\ndemonstrate the efficacy of ContextViT across a wide range of applications. In\nsupervised fine-tuning, we show that augmenting pre-trained ViTs with our\nproposed context conditioning mechanism results in consistent improvements in\nout-of-distribution generalization on iWildCam and FMoW. We also investigate\nself-supervised representation learning with ContextViT. Our experiments on the\nCamelyon17 pathology imaging benchmark and the JUMP-CP microscopy imaging\nbenchmark demonstrate that ContextViT excels in learning stable image\nfeaturizations amidst distribution shift, consistently outperforming its ViT\ncounterpart.",
                "Spotlight Attention: Robust Object-Centric Learning With a Spatial\n  Locality Prior\nThe aim of object-centric vision is to construct an explicit representation\nof the objects in a scene. This representation is obtained via a set of\ninterchangeable modules called \\emph{slots} or \\emph{object files} that compete\nfor local patches of an image. The competition has a weak inductive bias to\npreserve spatial continuity; consequently, one slot may claim patches scattered\ndiffusely throughout the image. In contrast, the inductive bias of human vision\nis strong, to the degree that attention has classically been described with a\nspotlight metaphor. We incorporate a spatial-locality prior into\nstate-of-the-art object-centric vision models and obtain significant\nimprovements in segmenting objects in both synthetic and real-world datasets.\nSimilar to human visual attention, the combination of image content and spatial\nconstraints yield robust unsupervised object-centric learning, including less\nsensitivity to model hyperparameters.",
                "DENTEX: An Abnormal Tooth Detection with Dental Enumeration and\n  Diagnosis Benchmark for Panoramic X-rays\nPanoramic X-rays are frequently used in dentistry for treatment planning, but\ntheir interpretation can be both time-consuming and prone to error. Artificial\nintelligence (AI) has the potential to aid in the analysis of these X-rays,\nthereby improving the accuracy of dental diagnoses and treatment plans.\nNevertheless, designing automated algorithms for this purpose poses significant\nchallenges, mainly due to the scarcity of annotated data and variations in\nanatomical structure. To address these issues, the Dental Enumeration and\nDiagnosis on Panoramic X-rays Challenge (DENTEX) has been organized in\nassociation with the International Conference on Medical Image Computing and\nComputer-Assisted Intervention (MICCAI) in 2023. This challenge aims to promote\nthe development of algorithms for multi-label detection of abnormal teeth,\nusing three types of hierarchically annotated data: partially annotated\nquadrant data, partially annotated quadrant-enumeration data, and fully\nannotated quadrant-enumeration-diagnosis data, inclusive of four different\ndiagnoses. In this paper, we present the results of evaluating participant\nalgorithms on the fully annotated data, additionally investigating performance\nvariation for quadrant, enumeration, and diagnosis labels in the detection of\nabnormal teeth. The provision of this annotated dataset, alongside the results\nof this challenge, may lay the groundwork for the creation of AI-powered tools\nthat can offer more precise and efficient diagnosis and treatment planning in\nthe field of dentistry. The evaluation code and datasets can be accessed at\nhttps://github.com/ibrahimethemhamamci/DENTEX",
                "FedCSD: A Federated Learning Based Approach for Code-Smell Detection\nThis paper proposes a Federated Learning Code Smell Detection (FedCSD)\napproach that allows organizations to collaboratively train federated ML models\nwhile preserving their data privacy. These assertions have been supported by\nthree experiments that have significantly leveraged three manually validated\ndatasets aimed at detecting and examining different code smell scenarios. In\nexperiment 1, which was concerned with a centralized training experiment,\ndataset two achieved the lowest accuracy (92.30%) with fewer smells, while\ndatasets one and three achieved the highest accuracy with a slight difference\n(98.90% and 99.5%, respectively). This was followed by experiment 2, which was\nconcerned with cross-evaluation, where each ML model was trained using one\ndataset, which was then evaluated over the other two datasets. Results from\nthis experiment show a significant drop in the model's accuracy (lowest\naccuracy: 63.80\\%) where fewer smells exist in the training dataset, which has\na noticeable reflection (technical debt) on the model's performance. Finally,\nthe last and third experiments evaluate our approach by splitting the dataset\ninto 10 companies. The ML model was trained on the company's site, then all\nmodel-updated weights were transferred to the server. Ultimately, an accuracy\nof 98.34% was achieved by the global model that has been trained using 10\ncompanies for 100 training rounds. The results reveal a slight difference in\nthe global model's accuracy compared to the highest accuracy of the centralized\nmodel, which can be ignored in favour of the global model's comprehensive\nknowledge, lower training cost, preservation of data privacy, and avoidance of\nthe technical debt problem.",
                "A rule-general abductive learning by rough sets\nIn real-world tasks, there is usually a large amount of unlabeled data and\nlabeled data. The task of combining the two to learn is known as\nsemi-supervised learning. Experts can use logical rules to label unlabeled\ndata, but this operation is costly. The combination of perception and reasoning\nhas a good effect in processing such semi-supervised tasks with domain\nknowledge. However, acquiring domain knowledge and the correction, reduction\nand generation of rules remain complex problems to be solved. Rough set theory\nis an important method for solving knowledge processing in information systems.\nIn this paper, we propose a rule general abductive learning by rough set\n(RS-ABL). By transforming the target concept and sub-concepts of rules into\ninformation tables, rough set theory is used to solve the acquisition of domain\nknowledge and the correction, reduction and generation of rules at a lower\ncost. This framework can also generate more extensive negative rules to enhance\nthe breadth of the knowledge base. Compared with the traditional\nsemi-supervised learning method, RS-ABL has higher accuracy in dealing with\nsemi-supervised tasks."
            ],
            "interesting paper": 2
        },
        {
            "papers": [
                "Building Extractive Question Answering System to Support Human-AI Health\n  Coaching Model for Sleep Domain\nNon-communicable diseases (NCDs) are a leading cause of global deaths,\nnecessitating a focus on primary prevention and lifestyle behavior change.\nHealth coaching, coupled with Question Answering (QA) systems, has the\npotential to transform preventive healthcare. This paper presents a\nhuman-Artificial Intelligence (AI) health coaching model incorporating a\ndomain-specific extractive QA system. A sleep-focused dataset, SleepQA, was\nmanually assembled and used to fine-tune domain-specific BERT models. The QA\nsystem was evaluated using automatic and human methods. A data-centric\nframework enhanced the system's performance by improving passage retrieval and\nquestion reformulation. Although the system did not outperform the baseline in\nautomatic evaluation, it excelled in the human evaluation of real-world\nquestions. Integration into a Human-AI health coaching model was tested in a\npilot Randomized Controlled Trial (RCT).",
                "Intent-aligned AI systems deplete human agency: the need for agency\n  foundations research in AI safety\nThe rapid advancement of artificial intelligence (AI) systems suggests that\nartificial general intelligence (AGI) systems may soon arrive. Many researchers\nare concerned that AIs and AGIs will harm humans via intentional misuse\n(AI-misuse) or through accidents (AI-accidents). In respect of AI-accidents,\nthere is an increasing effort focused on developing algorithms and paradigms\nthat ensure AI systems are aligned to what humans intend, e.g. AI systems that\nyield actions or recommendations that humans might judge as consistent with\ntheir intentions and goals. Here we argue that alignment to human intent is\ninsufficient for safe AI systems and that preservation of long-term agency of\nhumans may be a more robust standard, and one that needs to be separated\nexplicitly and a priori during optimization. We argue that AI systems can\nreshape human intention and discuss the lack of biological and psychological\nmechanisms that protect humans from loss of agency. We provide the first formal\ndefinition of agency-preserving AI-human interactions which focuses on\nforward-looking agency evaluations and argue that AI systems - not humans -\nmust be increasingly tasked with making these evaluations. We show how agency\nloss can occur in simple environments containing embedded agents that use\ntemporal-difference learning to make action recommendations. Finally, we\npropose a new area of research called \"agency foundations\" and pose four\ninitial topics designed to improve our understanding of agency in AI-human\ninteractions: benevolent game theory, algorithmic foundations of human rights,\nmechanistic interpretability of agency representation in neural-networks and\nreinforcement learning from internal states.",
                "Explainable AI for Malnutrition Risk Prediction from m-Health and\n  Clinical Data\nMalnutrition is a serious and prevalent health problem in the older\npopulation, and especially in hospitalised or institutionalised subjects.\nAccurate and early risk detection is essential for malnutrition management and\nprevention. M-health services empowered with Artificial Intelligence (AI) may\nlead to important improvements in terms of a more automatic, objective, and\ncontinuous monitoring and assessment. Moreover, the latest Explainable AI (XAI)\nmethodologies may make AI decisions interpretable and trustworthy for end\nusers. This paper presents a novel AI framework for early and explainable\nmalnutrition risk detection based on heterogeneous m-health data. We performed\nan extensive model evaluation including both subject-independent and\npersonalised predictions, and the obtained results indicate Random Forest (RF)\nand Gradient Boosting as the best performing classifiers, especially when\nincorporating body composition assessment data. We also investigated several\nbenchmark XAI methods to extract global model explanations. Model-specific\nexplanation consistency assessment indicates that each selected model\nprivileges similar subsets of the most relevant predictors, with the highest\nagreement shown between SHapley Additive ExPlanations (SHAP) and feature\npermutation method. Furthermore, we performed a preliminary clinical validation\nto verify that the learned feature-output trends are compliant with the current\nevidence-based assessment.",
                "Mining Themes in Clinical Notes to Identify Phenotypes and to Predict\n  Length of Stay in Patients admitted with Heart Failure\nHeart failure is a syndrome which occurs when the heart is not able to pump\nblood and oxygen to support other organs in the body. Identifying the\nunderlying themes in the diagnostic codes and procedure reports of patients\nadmitted for heart failure could reveal the clinical phenotypes associated with\nheart failure and to group patients based on their similar characteristics\nwhich could also help in predicting patient outcomes like length of stay. These\nclinical phenotypes usually have a probabilistic latent structure and hence, as\nthere has been no previous work on identifying phenotypes in clinical notes of\nheart failure patients using a probabilistic framework and to predict length of\nstay of these patients using data-driven artificial intelligence-based methods,\nwe apply natural language processing technique, topic modeling, to identify the\nthemes present in diagnostic codes and in procedure reports of 1,200 patients\nadmitted for heart failure at the University of Illinois Hospital and Health\nSciences System (UI Health). Topic modeling identified twelve themes each in\ndiagnostic codes and procedure reports which revealed information about\ndifferent phenotypes related to various perspectives about heart failure, to\nstudy patients' profiles and to discover new relationships among medical\nconcepts. Each theme had a set of keywords and each clinical note was labeled\nwith two themes - one corresponding to its diagnostic code and the other\ncorresponding to its procedure reports along with their percentage\ncontribution. We used these themes and their percentage contribution to predict\nlength of stay. We found that the themes discovered in diagnostic codes and\nprocedure reports using topic modeling together were able to predict length of\nstay of the patients with an accuracy of 61.1% and an Area under the Receiver\nOperating Characteristic Curve (ROC AUC) value of 0.828.",
                "Disentangling and Operationalizing AI Fairness at LinkedIn\nOperationalizing AI fairness at LinkedIn's scale is challenging not only\nbecause there are multiple mutually incompatible definitions of fairness but\nalso because determining what is fair depends on the specifics and context of\nthe product where AI is deployed. Moreover, AI practitioners need clarity on\nwhat fairness expectations need to be addressed at the AI level. In this paper,\nwe present the evolving AI fairness framework used at LinkedIn to address these\nthree challenges. The framework disentangles AI fairness by separating out\nequal treatment and equitable product expectations. Rather than imposing a\ntrade-off between these two commonly opposing interpretations of fairness, the\nframework provides clear guidelines for operationalizing equal AI treatment\ncomplemented with a product equity strategy. This paper focuses on the equal AI\ntreatment component of LinkedIn's AI fairness framework, shares the principles\nthat support it, and illustrates their application through a case study. We\nhope this paper will encourage other big tech companies to join us in sharing\ntheir approach to operationalizing AI fairness at scale, so that together we\ncan keep advancing this constantly evolving field.",
                "Predicting Heart Disease and Reducing Survey Time Using Machine Learning\n  Algorithms\nCurrently, many researchers and analysts are working toward medical diagnosis\nenhancement for various diseases. Heart disease is one of the common diseases\nthat can be considered a significant cause of mortality worldwide. Early\ndetection of heart disease significantly helps in reducing the risk of heart\nfailure. Consequently, the Centers for Disease Control and Prevention (CDC)\nconducts a health-related telephone survey yearly from over 400,000\nparticipants. However, several concerns arise regarding the reliability of the\ndata in predicting heart disease and whether all of the survey questions are\nstrongly related. This study aims to utilize several machine learning\ntechniques, such as support vector machines and logistic regression, to\ninvestigate the accuracy of the CDC's heart disease survey in the United\nStates. Furthermore, we use various feature selection methods to identify the\nmost relevant subset of questions that can be utilized to forecast heart\nconditions. To reach a robust conclusion, we perform stability analysis by\nrandomly sampling the data 300 times. The experimental results show that the\nsurvey data can be useful up to 80% in terms of predicting heart disease, which\nsignificantly improves the diagnostic process before bloodwork and tests. In\naddition, the amount of time spent conducting the survey can be reduced by 77%\nwhile maintaining the same level of performance."
            ],
            "interesting paper": 2
        },
        {
            "papers": [
                "Quantifying Representation Reliability in Self-Supervised Learning\n  Models\nSelf-supervised learning models extract general-purpose representations from\ndata. Quantifying the reliability of these representations is crucial, as many\ndownstream models rely on them as input for their own tasks. To this end, we\nintroduce a formal definition of representation reliability: the representation\nfor a given test point is considered to be reliable if the downstream models\nbuilt on top of that representation can consistently generate accurate\npredictions for that test point. However, accessing downstream data to quantify\nthe representation reliability is often infeasible or restricted due to privacy\nconcerns. We propose an ensemble-based method for estimating the representation\nreliability without knowing the downstream tasks a priori. Our method is based\non the concept of neighborhood consistency across distinct pre-trained\nrepresentation spaces. The key insight is to find shared neighboring points as\nanchors to align these representation spaces before comparing them. We\ndemonstrate through comprehensive numerical experiments that our method\neffectively captures the representation reliability with a high degree of\ncorrelation, achieving robust and favorable performance compared with baseline\nmethods.",
                "Doubly Robust Self-Training\nSelf-training is an important technique for solving semi-supervised learning\nproblems. It leverages unlabeled data by generating pseudo-labels and combining\nthem with a limited labeled dataset for training. The effectiveness of\nself-training heavily relies on the accuracy of these pseudo-labels. In this\npaper, we introduce doubly robust self-training, a novel semi-supervised\nalgorithm that provably balances between two extremes. When the pseudo-labels\nare entirely incorrect, our method reduces to a training process solely using\nlabeled data. Conversely, when the pseudo-labels are completely accurate, our\nmethod transforms into a training process utilizing all pseudo-labeled data and\nlabeled data, thus increasing the effective sample size. Through empirical\nevaluations on both the ImageNet dataset for image classification and the\nnuScenes autonomous driving dataset for 3D object detection, we demonstrate the\nsuperiority of the doubly robust loss over the standard self-training baseline.",
                "SSL-CPCD: Self-supervised learning with composite pretext-class\n  discrimination for improved generalisability in endoscopic image analysis\nData-driven methods have shown tremendous progress in medical image analysis.\nIn this context, deep learning-based supervised methods are widely popular.\nHowever, they require a large amount of training data and face issues in\ngeneralisability to unseen datasets that hinder clinical translation.\nEndoscopic imaging data incorporates large inter- and intra-patient variability\nthat makes these models more challenging to learn representative features for\ndownstream tasks. Thus, despite the publicly available datasets and datasets\nthat can be generated within hospitals, most supervised models still\nunderperform. While self-supervised learning has addressed this problem to some\nextent in natural scene data, there is a considerable performance gap in the\nmedical image domain. In this paper, we propose to explore patch-level\ninstance-group discrimination and penalisation of inter-class variation using\nadditive angular margin within the cosine similarity metrics. Our novel\napproach enables models to learn to cluster similar representative patches,\nthereby improving their ability to provide better separation between different\nclasses. Our results demonstrate significant improvement on all metrics over\nthe state-of-the-art (SOTA) methods on the test set from the same and diverse\ndatasets. We evaluated our approach for classification, detection, and\nsegmentation. SSL-CPCD achieves 79.77% on Top 1 accuracy for ulcerative colitis\nclassification, 88.62% on mAP for polyp detection, and 82.32% on dice\nsimilarity coefficient for segmentation tasks are nearly over 4%, 2%, and 3%,\nrespectively, compared to the baseline architectures. We also demonstrate that\nour method generalises better than all SOTA methods to unseen datasets,\nreporting nearly 7% improvement in our generalisability assessment.",
                "MERT: Acoustic Music Understanding Model with Large-Scale\n  Self-supervised Training\nSelf-supervised learning (SSL) has recently emerged as a promising paradigm\nfor training generalisable models on large-scale data in the fields of vision,\ntext, and speech. Although SSL has been proven effective in speech and audio,\nits application to music audio has yet to be thoroughly explored. This is\npartially due to the distinctive challenges associated with modelling musical\nknowledge, particularly tonal and pitched characteristics of music. To address\nthis research gap, we propose an acoustic Music undERstanding model with\nlarge-scale self-supervised Training (MERT), which incorporates teacher models\nto provide pseudo labels in the masked language modelling (MLM) style acoustic\npre-training. In our exploration, we identified an effective combination of\nteacher models, which outperforms conventional speech and audio approaches in\nterms of performance. This combination includes an acoustic teacher based on\nResidual Vector Quantisation - Variational AutoEncoder (RVQ-VAE) and a musical\nteacher based on the Constant-Q Transform (CQT). Furthermore, we explore a wide\nrange of settings to overcome the instability in acoustic language model\npre-training, which allows our designed paradigm to scale from 95M to 330M\nparameters. Experimental results indicate that our model can generalise and\nperform well on 14 music understanding tasks and attain state-of-the-art (SOTA)\noverall scores.",
                "Feature Learning in Image Hierarchies using Functional Maximal\n  Correlation\nThis paper proposes the Hierarchical Functional Maximal Correlation Algorithm\n(HFMCA), a hierarchical methodology that characterizes dependencies across two\nhierarchical levels in multiview systems. By framing view similarities as\ndependencies and ensuring contrastivity by imposing orthonormality, HFMCA\nachieves faster convergence and increased stability in self-supervised\nlearning. HFMCA defines and measures dependencies within image hierarchies,\nfrom pixels and patches to full images. We find that the network topology for\napproximating orthonormal basis functions aligns with a vanilla CNN, enabling\nthe decomposition of density ratios between neighboring layers of feature maps.\nThis approach provides powerful interpretability, revealing the resemblance\nbetween supervision and self-supervision through the lens of internal\nrepresentations.",
                "Primal-Attention: Self-attention through Asymmetric Kernel SVD in Primal\n  Representation\nRecently, a new line of works has emerged to understand and improve\nself-attention in Transformers by treating it as a kernel machine. However,\nexisting works apply the methods for symmetric kernels to the asymmetric\nself-attention, resulting in a nontrivial gap between the analytical\nunderstanding and numerical implementation. In this paper, we provide a new\nperspective to represent and optimize self-attention through asymmetric Kernel\nSingular Value Decomposition (KSVD), which is also motivated by the low-rank\nproperty of self-attention normally observed in deep layers. Through asymmetric\nKSVD, $i$) a primal-dual representation of self-attention is formulated, where\nthe optimization objective is cast to maximize the projection variances in the\nattention outputs; $ii$) a novel attention mechanism, i.e., Primal-Attention,\nis proposed via the primal representation of KSVD, avoiding explicit\ncomputation of the kernel matrix in the dual; $iii$) with KKT conditions, we\nprove that the stationary solution to the KSVD optimization in Primal-Attention\nyields a zero-value objective. In this manner, KSVD optimization can be\nimplemented by simply minimizing a regularization loss, so that low-rank\nproperty is promoted without extra decomposition. Numerical experiments show\nstate-of-the-art performance of our Primal-Attention with improved efficiency.\nMoreover, we demonstrate that the deployed KSVD optimization regularizes\nPrimal-Attention with a sharper singular value decay than that of the canonical\nself-attention, further verifying the great potential of our method. To the\nbest of our knowledge, this is the first work that provides a primal-dual\nrepresentation for the asymmetric kernel in self-attention and successfully\napplies it to modeling and optimization."
            ],
            "interesting paper": 3
        },
        {
            "papers": [
                "Breast Cancer Detection and Diagnosis: A comparative study of\n  state-of-the-arts deep learning architectures\nBreast cancer is a prevalent form of cancer among women, with over 1.5\nmillion women being diagnosed each year. Unfortunately, the survival rates for\nbreast cancer patients in certain third-world countries, like South Africa, are\nalarmingly low, with only 40% of diagnosed patients surviving beyond five\nyears. The inadequate availability of resources, including qualified\npathologists, delayed diagnoses, and ineffective therapy planning, contribute\nto this low survival rate. To address this pressing issue, medical specialists\nand researchers have turned to domain-specific AI approaches, specifically deep\nlearning models, to develop end-to-end solutions that can be integrated into\ncomputer-aided diagnosis (CAD) systems. By improving the workflow of\npathologists, these AI models have the potential to enhance the detection and\ndiagnosis of breast cancer. This research focuses on evaluating the performance\nof various cutting-edge convolutional neural network (CNN) architectures in\ncomparison to a relatively new model called the Vision Trans-former (ViT). The\nobjective is to determine the superiority of these models in terms of their\naccuracy and effectiveness. The experimental results reveal that the ViT models\noutperform the other selected state-of-the-art CNN architectures, achieving an\nimpressive accuracy rate of 95.15%. This study signifies a significant\nadvancement in the field, as it explores the utilization of data augmentation\nand other relevant preprocessing techniques in conjunction with deep learning\nmodels for the detection and diagnosis of breast cancer using datasets of\nBreast Cancer Histopathological Image Classification.",
                "Responsible Design Patterns for Machine Learning Pipelines\nIntegrating ethical practices into the AI development process for artificial\nintelligence (AI) is essential to ensure safe, fair, and responsible operation.\nAI ethics involves applying ethical principles to the entire life cycle of AI\nsystems. This is essential to mitigate potential risks and harms associated\nwith AI, such as algorithm biases. To achieve this goal, responsible design\npatterns (RDPs) are critical for Machine Learning (ML) pipelines to guarantee\nethical and fair outcomes. In this paper, we propose a comprehensive framework\nincorporating RDPs into ML pipelines to mitigate risks and ensure the ethical\ndevelopment of AI systems. Our framework comprises new responsible AI design\npatterns for ML pipelines identified through a survey of AI ethics and data\nmanagement experts and validated through real-world scenarios with expert\nfeedback. The framework guides AI developers, data scientists, and\npolicy-makers to implement ethical practices in AI development and deploy\nresponsible AI systems in production.",
                "Human Control: Definitions and Algorithms\nHow can humans stay in control of advanced artificial intelligence systems?\nOne proposal is corrigibility, which requires the agent to follow the\ninstructions of a human overseer, without inappropriately influencing them. In\nthis paper, we formally define a variant of corrigibility called shutdown\ninstructability, and show that it implies appropriate shutdown behavior,\nretention of human autonomy, and avoidance of user harm. We also analyse the\nrelated concepts of non-obstruction and shutdown alignment, three previously\nproposed algorithms for human control, and one new algorithm.",
                "Enrichment of the NLST and NSCLC-Radiomics computed tomography\n  collections with AI-derived annotations\nPublic imaging datasets are critical for the development and evaluation of\nautomated tools in cancer imaging. Unfortunately, many do not include\nannotations or image-derived features, complicating their downstream analysis.\nArtificial intelligence-based annotation tools have been shown to achieve\nacceptable performance and thus can be used to automatically annotate large\ndatasets. As part of the effort to enrich public data available within NCI\nImaging Data Commons (IDC), here we introduce AI-generated annotations for two\ncollections of computed tomography images of the chest, NSCLC-Radiomics, and\nthe National Lung Screening Trial. Using publicly available AI algorithms we\nderived volumetric annotations of thoracic organs at risk, their corresponding\nradiomics features, and slice-level annotations of anatomical landmarks and\nregions. The resulting annotations are publicly available within IDC, where the\nDICOM format is used to harmonize the data and achieve FAIR principles. The\nannotations are accompanied by cloud-enabled notebooks demonstrating their use.\nThis study reinforces the need for large, publicly accessible curated datasets\nand demonstrates how AI can be used to aid in cancer imaging.",
                "Citizen Perspectives on Necessary Safeguards to the Use of AI by Law\n  Enforcement Agencies\nIn the light of modern technological advances, Artificial Intelligence (AI)\nis relied upon to enhance performance, increase efficiency, and maximize gains.\nFor Law Enforcement Agencies (LEAs), it can prove valuable in optimizing\nevidence analysis and establishing proactive prevention measures. Nevertheless,\ncitizens raise legitimate concerns around privacy invasions, biases,\ninequalities, and inaccurate decisions. This study explores the views of 111\ncitizens towards AI use by police through interviews, and integrates societal\nconcerns along with propositions of safeguards from negative effects of AI use\nby LEAs in the context of cybercrime and terrorism.",
                "Assessing Language Disorders using Artificial Intelligence: a Paradigm\n  Shift\nSpeech, language, and communication deficits are present in most\nneurodegenerative syndromes. They enable the early detection, diagnosis,\ntreatment planning, and monitoring of neurocognitive disease progression as\npart of traditional neurological assessment. Nevertheless, standard speech and\nlanguage evaluation is time-consuming and resource-intensive for clinicians. We\nargue that using machine learning methodologies, natural language processing,\nand modern artificial intelligence (AI) for Language Assessment is an\nimprovement over conventional manual assessment. Using these methodologies,\nComputational Language Assessment (CLA) accomplishes three goals: (i) provides\na neuro-cognitive evaluation of speech, language, and communication in elderly\nand high-risk individuals for dementia; (ii) facilitates the diagnosis,\nprognosis, and therapy efficacy in at-risk and language-impaired populations;\nand (iii) allows easier extensibility to assess patients from a wide range of\nlanguages. By employing AI models, CLA may inform neurocognitive theory on the\nrelationship between language symptoms and their neural bases. Finally, it\nsignals a paradigm shift by significantly advancing our ability to optimize the\nprevention and treatment of elderly individuals with communication disorders,\nallowing them to age gracefully with social engagement."
            ],
            "interesting paper": 1
        },
        {
            "papers": [
                "Scaling Up Semi-supervised Learning with Unconstrained Unlabelled Data\nWe propose UnMixMatch, a semi-supervised learning framework which can learn\neffective representations from unconstrained unlabelled data in order to scale\nup performance. Most existing semi-supervised methods rely on the assumption\nthat labelled and unlabelled samples are drawn from the same distribution,\nwhich limits the potential for improvement through the use of free-living\nunlabeled data. Consequently, the generalizability and scalability of\nsemi-supervised learning are often hindered by this assumption. Our method aims\nto overcome these constraints and effectively utilize unconstrained unlabelled\ndata in semi-supervised learning. UnMixMatch consists of three main components:\na supervised learner with hard augmentations that provides strong\nregularization, a contrastive consistency regularizer to learn underlying\nrepresentations from the unlabelled data, and a self-supervised loss to enhance\nthe representations that are learnt from the unlabelled data. We perform\nextensive experiments on 4 commonly used datasets and demonstrate superior\nperformance over existing semi-supervised methods with a performance boost of\n4.79%. Extensive ablation and sensitivity studies show the effectiveness and\nimpact of each of the proposed components of our method.",
                "Efficient Failure Pattern Identification of Predictive Algorithms\nGiven a (machine learning) classifier and a collection of unlabeled data, how\ncan we efficiently identify misclassification patterns presented in this\ndataset? To address this problem, we propose a human-machine collaborative\nframework that consists of a team of human annotators and a sequential\nrecommendation algorithm. The recommendation algorithm is conceptualized as a\nstochastic sampler that, in each round, queries the annotators a subset of\nsamples for their true labels and obtains the feedback information on whether\nthe samples are misclassified. The sampling mechanism needs to balance between\ndiscovering new patterns of misclassification (exploration) and confirming the\npotential patterns of classification (exploitation). We construct a\ndeterminantal point process, whose intensity balances the\nexploration-exploitation trade-off through the weighted update of the posterior\nat each round to form the generator of the stochastic sampler. The numerical\nresults empirically demonstrate the competitive performance of our framework on\nmultiple datasets at various signal-to-noise ratios.",
                "Rotating Features for Object Discovery\nThe binding problem in human cognition, concerning how the brain represents\nand connects objects within a fixed network of neural connections, remains a\nsubject of intense debate. Most machine learning efforts addressing this issue\nin an unsupervised setting have focused on slot-based methods, which may be\nlimiting due to their discrete nature and difficulty to express uncertainty.\nRecently, the Complex AutoEncoder was proposed as an alternative that learns\ncontinuous and distributed object-centric representations. However, it is only\napplicable to simple toy data. In this paper, we present Rotating Features, a\ngeneralization of complex-valued features to higher dimensions, and a new\nevaluation procedure for extracting objects from distributed representations.\nAdditionally, we show the applicability of our approach to pre-trained\nfeatures. Together, these advancements enable us to scale distributed\nobject-centric representations from simple toy to real-world data. We believe\nthis work advances a new paradigm for addressing the binding problem in machine\nlearning and has the potential to inspire further innovation in the field.",
                "Federated Domain Generalization: A Survey\nMachine learning typically relies on the assumption that training and testing\ndistributions are identical and that data is centrally stored for training and\ntesting. However, in real-world scenarios, distributions may differ\nsignificantly and data is often distributed across different devices,\norganizations, or edge nodes. Consequently, it is imperative to develop models\nthat can effectively generalize to unseen distributions where data is\ndistributed across different domains. In response to this challenge, there has\nbeen a surge of interest in federated domain generalization (FDG) in recent\nyears. FDG combines the strengths of federated learning (FL) and domain\ngeneralization (DG) techniques to enable multiple source domains to\ncollaboratively learn a model capable of directly generalizing to unseen\ndomains while preserving data privacy. However, generalizing the federated\nmodel under domain shifts is a technically challenging problem that has\nreceived scant attention in the research area so far. This paper presents the\nfirst survey of recent advances in this area. Initially, we discuss the\ndevelopment process from traditional machine learning to domain adaptation and\ndomain generalization, leading to FDG as well as provide the corresponding\nformal definition. Then, we categorize recent methodologies into four classes:\nfederated domain alignment, data manipulation, learning strategies, and\naggregation optimization, and present suitable algorithms in detail for each\ncategory. Next, we introduce commonly used datasets, applications, evaluations,\nand benchmarks. Finally, we conclude this survey by providing some potential\nresearch topics for the future.",
                "Learning Causally Disentangled Representations via the Principle of\n  Independent Causal Mechanisms\nLearning disentangled causal representations is a challenging problem that\nhas gained significant attention recently due to its implications for\nextracting meaningful information for downstream tasks. In this work, we define\na new notion of causal disentanglement from the perspective of independent\ncausal mechanisms. We propose ICM-VAE, a framework for learning causally\ndisentangled representations supervised by causally related observed labels. We\nmodel causal mechanisms using nonlinear learnable flow-based diffeomorphic\nfunctions to map noise variables to latent causal variables. Further, to\npromote the disentanglement of causal factors, we propose a causal\ndisentanglement prior learned from auxiliary labels and the latent causal\nstructure. We theoretically show the identifiability of causal factors and\nmechanisms up to permutation and elementwise reparameterization. We empirically\ndemonstrate that our framework induces highly disentangled causal factors,\nimproves interventional robustness, and is compatible with counterfactual\ngeneration.",
                "Joint Learning of Label and Environment Causal Independence for Graph\n  Out-of-Distribution Generalization\nWe tackle the problem of graph out-of-distribution (OOD) generalization.\nExisting graph OOD algorithms either rely on restricted assumptions or fail to\nexploit environment information in training data. In this work, we propose to\nsimultaneously incorporate label and environment causal independence (LECI) to\nfully make use of label and environment information, thereby addressing the\nchallenges faced by prior methods on identifying causal and invariant\nsubgraphs. We further develop an adversarial training strategy to jointly\noptimize these two properties for causal subgraph discovery with theoretical\nguarantees. Extensive experiments and analysis show that LECI significantly\noutperforms prior methods on both synthetic and real-world datasets,\nestablishing LECI as a practical and effective solution for graph OOD\ngeneralization.\n  Our code is available at https://github.com/divelab/LECI."
            ],
            "interesting paper": 3
        },
        {
            "papers": [
                "AI Liability Insurance With an Example in AI-Powered E-diagnosis System\nArtificial Intelligence (AI) has received an increasing amount of attention\nin multiple areas. The uncertainties and risks in AI-powered systems have\ncreated reluctance in their wild adoption. As an economic solution to\ncompensate for potential damages, AI liability insurance is a promising market\nto enhance the integration of AI into daily life. In this work, we use an\nAI-powered E-diagnosis system as an example to study AI liability insurance. We\nprovide a quantitative risk assessment model with evidence-based numerical\nanalysis. We discuss the insurability criteria for AI technologies and suggest\nnecessary adjustments to accommodate the features of AI products. We show that\nAI liability insurance can act as a regulatory mechanism to incentivize\ncompliant behaviors and serve as a certificate of high-quality AI systems.\nFurthermore, we suggest premium adjustment to reflect the dynamic evolution of\nthe inherent uncertainty in AI. Moral hazard problems are discussed and\nsuggestions for AI liability insurance are provided.",
                "AI and the creative realm: A short review of current and future\n  applications\nThis study explores the concept of creativity and artificial intelligence\n(AI) and their recent integration. While AI has traditionally been perceived as\nincapable of generating new ideas or creating art, the development of more\nsophisticated AI models and the proliferation of human-computer interaction\ntools have opened up new possibilities for AI in artistic creation. This study\ninvestigates the various applications of AI in a creative context,\ndifferentiating between the type of art, language, and algorithms used. It also\nconsiders the philosophical implications of AI and creativity, questioning\nwhether consciousness can be researched in machines and AI's potential\ninterests and decision-making capabilities. Overall, we aim to stimulate a\nreflection on AI's use and ethical implications in creative contexts.",
                "Navigating Fairness in Radiology AI: Concepts, Consequences,and Crucial\n  Considerations\nArtificial Intelligence (AI) has significantly revolutionized radiology,\npromising improved patient outcomes and streamlined processes. However, it's\ncritical to ensure the fairness of AI models to prevent stealthy bias and\ndisparities from leading to unequal outcomes. This review discusses the concept\nof fairness in AI, focusing on bias auditing using the Aequitas toolkit, and\nits real-world implications in radiology, particularly in disease screening\nscenarios. Aequitas, an open-source bias audit toolkit, scrutinizes AI models'\ndecisions, identifying hidden biases that may result in disparities across\ndifferent demographic groups and imaging equipment brands. This toolkit\noperates on statistical theories, analyzing a large dataset to reveal a model's\nfairness. It excels in its versatility to handle various variables\nsimultaneously, especially in a field as diverse as radiology. The review\nexplicates essential fairness metrics: Equal and Proportional Parity, False\nPositive Rate Parity, False Discovery Rate Parity, False Negative Rate Parity,\nand False Omission Rate Parity. Each metric serves unique purposes and offers\ndifferent insights. We present hypothetical scenarios to demonstrate their\nrelevance in disease screening settings, and how disparities can lead to\nsignificant real-world impacts.",
                "The ethical ambiguity of AI data enrichment: Measuring gaps in research\n  ethics norms and practices\nThe technical progression of artificial intelligence (AI) research has been\nbuilt on breakthroughs in fields such as computer science, statistics, and\nmathematics. However, in the past decade AI researchers have increasingly\nlooked to the social sciences, turning to human interactions to solve the\nchallenges of model development. Paying crowdsourcing workers to generate or\ncurate data, or data enrichment, has become indispensable for many areas of AI\nresearch, from natural language processing to reinforcement learning from human\nfeedback (RLHF). Other fields that routinely interact with crowdsourcing\nworkers, such as Psychology, have developed common governance requirements and\nnorms to ensure research is undertaken ethically. This study explores how, and\nto what extent, comparable research ethics requirements and norms have\ndeveloped for AI research and data enrichment. We focus on the approach taken\nby two leading conferences: ICLR and NeurIPS, and journal publisher Springer.\nIn a longitudinal study of accepted papers, and via a comparison with\nPsychology and CHI papers, this work finds that leading AI venues have begun to\nestablish protocols for human data collection, but these are are inconsistently\nfollowed by authors. Whilst Psychology papers engaging with crowdsourcing\nworkers frequently disclose ethics reviews, payment data, demographic data and\nother information, similar disclosures are far less common in leading AI venues\ndespite similar guidance. The work concludes with hypotheses to explain these\ngaps in research ethics practices and considerations for its implications.",
                "Heart Diseases Prediction Using Block-chain and Machine Learning\nMost people around the globe are dying due to heart disease. The main reason\nbehind the rapid increase in the death rate due to heart disease is that there\nis no infrastructure developed for the healthcare department that can provide a\nsecure way of data storage and transmission. Due to redundancy in the patient\ndata, it is difficult for cardiac Professionals to predict the disease early\non. This rapid increase in the death rate due to heart disease can be\ncontrolled by monitoring and eliminating some of the key attributes in the\nearly stages such as blood pressure, cholesterol level, body weight, and\naddiction to smoking. Patient data can be monitored by cardiac Professionals\n(Cp) by using the advanced framework in the healthcare departments. Blockchain\nis the world's most reliable provider. The use of advanced systems in the\nhealthcare departments providing new ways of dealing with diseases has been\ndeveloped as well. In this article Machine Learning (ML) algorithm known as a\nsine-cosine weighted k-nearest neighbor (SCA-WKNN) is used for predicting the\nHearth disease with the maximum accuracy among the existing approaches.\nBlockchain technology has been used in the research to secure the data\nthroughout the session and can give more accurate results using this\ntechnology. The performance of the system can be improved by using this\nalgorithm and the dataset proposed has been improved by using different\nresources as well.",
                "An Architecture for Deploying Reinforcement Learning in Industrial\n  Environments\nIndustry 4.0 is driven by demands like shorter time-to-market, mass\ncustomization of products, and batch size one production. Reinforcement\nLearning (RL), a machine learning paradigm shown to possess a great potential\nin improving and surpassing human level performance in numerous complex tasks,\nallows coping with the mentioned demands. In this paper, we present an OPC UA\nbased Operational Technology (OT)-aware RL architecture, which extends the\nstandard RL setting, combining it with the setting of digital twins. Moreover,\nwe define an OPC UA information model allowing for a generalized plug-and-play\nlike approach for exchanging the RL agent used. In conclusion, we demonstrate\nand evaluate the architecture, by creating a proof of concept. By means of\nsolving a toy example, we show that this architecture can be used to determine\nthe optimal policy using a real control system."
            ],
            "interesting paper": 3
        },
        {
            "papers": [
                "The Information Pathways Hypothesis: Transformers are Dynamic\n  Self-Ensembles\nTransformers use the dense self-attention mechanism which gives a lot of\nflexibility for long-range connectivity. Over multiple layers of a deep\ntransformer, the number of possible connectivity patterns increases\nexponentially. However, very few of these contribute to the performance of the\nnetwork, and even fewer are essential. We hypothesize that there are sparsely\nconnected sub-networks within a transformer, called information pathways which\ncan be trained independently. However, the dynamic (i.e., input-dependent)\nnature of these pathways makes it difficult to prune dense self-attention\nduring training. But the overall distribution of these pathways is often\npredictable. We take advantage of this fact to propose Stochastically\nSubsampled self-Attention (SSA) - a general-purpose training strategy for\ntransformers that can reduce both the memory and computational cost of\nself-attention by 4 to 8 times during training while also serving as a\nregularization method - improving generalization over dense training. We show\nthat an ensemble of sub-models can be formed from the subsampled pathways\nwithin a network, which can achieve better performance than its densely\nattended counterpart. We perform experiments on a variety of NLP, computer\nvision and graph learning tasks in both generative and discriminative settings\nto provide empirical evidence for our claims and show the effectiveness of the\nproposed method.",
                "HomE: Homography-Equivariant Video Representation Learning\nRecent advances in self-supervised representation learning have enabled more\nefficient and robust model performance without relying on extensive labeled\ndata. However, most works are still focused on images, with few working on\nvideos and even fewer on multi-view videos, where more powerful inductive\nbiases can be leveraged for self-supervision. In this work, we propose a novel\nmethod for representation learning of multi-view videos, where we explicitly\nmodel the representation space to maintain Homography Equivariance (HomE). Our\nmethod learns an implicit mapping between different views, culminating in a\nrepresentation space that maintains the homography relationship between\nneighboring views. We evaluate our HomE representation via action recognition\nand pedestrian intent prediction as downstream tasks. On action classification,\nour method obtains 96.4% 3-fold accuracy on the UCF101 dataset, better than\nmost state-of-the-art self-supervised learning methods. Similarly, on the STIP\ndataset, we outperform the state-of-the-art by 6% for pedestrian intent\nprediction one second into the future while also obtaining an accuracy of 91.2%\nfor pedestrian action (cross vs. not-cross) classification. Code is available\nat https://github.com/anirudhs123/HomE.",
                "Affinity Clustering Framework for Data Debiasing Using Pairwise\n  Distribution Discrepancy\nGroup imbalance, resulting from inadequate or unrepresentative data\ncollection methods, is a primary cause of representation bias in datasets.\nRepresentation bias can exist with respect to different groups of one or more\nprotected attributes and might lead to prejudicial and discriminatory outcomes\ntoward certain groups of individuals; in cases where a learning model is\ntrained on such biased data. This paper presents MASC, a data augmentation\napproach that leverages affinity clustering to balance the representation of\nnon-protected and protected groups of a target dataset by utilizing instances\nof the same protected attributes from similar datasets that are categorized in\nthe same cluster as the target dataset by sharing instances of the protected\nattribute. The proposed method involves constructing an affinity matrix by\nquantifying distribution discrepancies between dataset pairs and transforming\nthem into a symmetric pairwise similarity matrix. A non-parametric spectral\nclustering is then applied to this affinity matrix, automatically categorizing\nthe datasets into an optimal number of clusters. We perform a step-by-step\nexperiment as a demo of our method to show the procedure of the proposed data\naugmentation method and evaluate and discuss its performance. A comparison with\nother data augmentation methods, both pre- and post-augmentation, is conducted,\nalong with a model evaluation analysis of each method. Our method can handle\nnon-binary protected attributes so, in our experiments, bias is measured in a\nnon-binary protected attribute setup w.r.t. racial groups distribution for two\nseparate minority groups in comparison with the majority group before and after\ndebiasing. Empirical results imply that our method of augmenting dataset biases\nusing real (genuine) data from similar contexts can effectively debias the\ntarget datasets comparably to existing data augmentation strategies.",
                "Comparing a composite model versus chained models to locate a nearest\n  visual object\nExtracting information from geographic images and text is crucial for\nautonomous vehicles to determine in advance the best cell stations to connect\nto along their future path. Multiple artificial neural network models can\naddress this challenge; however, there is no definitive guidance on the\nselection of an appropriate model for such use cases. Therefore, we\nexperimented two architectures to solve such a task: a first architecture with\nchained models where each model in the chain addresses a sub-task of the task;\nand a second architecture with a single model that addresses the whole task.\nOur results showed that these two architectures achieved the same level\nperformance with a root mean square error (RMSE) of 0.055 and 0.056; The\nfindings further revealed that when the task can be decomposed into sub-tasks,\nthe chain architecture exhibits a twelve-fold increase in training speed\ncompared to the composite model. Nevertheless, the composite model\nsignificantly alleviates the burden of data labeling.",
                "Simple Data Augmentation Techniques for Chinese Disease Normalization\nDisease name normalization is an important task in the medical domain. It\nclassifies disease names written in various formats into standardized names,\nserving as a fundamental component in smart healthcare systems for various\ndisease-related functions. Nevertheless, the most significant obstacle to\nexisting disease name normalization systems is the severe shortage of training\ndata. Consequently, we present a novel data augmentation approach that includes\na series of data augmentation techniques and some supporting modules to help\nmitigate the problem. Our proposed methods rely on the Structural Invariance\nproperty of disease names and the Hierarchy property of the disease\nclassification system. The goal is to equip the models with extensive\nunderstanding of the disease names and the hierarchical structure of the\ndisease name classification system. Through extensive experimentation, we\nillustrate that our proposed approach exhibits significant performance\nimprovements across various baseline models and training objectives,\nparticularly in scenarios with limited training data.",
                "Unifying (Machine) Vision via Counterfactual World Modeling\nLeading approaches in machine vision employ different architectures for\ndifferent tasks, trained on costly task-specific labeled datasets. This\ncomplexity has held back progress in areas, such as robotics, where robust\ntask-general perception remains a bottleneck. In contrast, \"foundation models\"\nof natural language have shown how large pre-trained neural networks can\nprovide zero-shot solutions to a broad spectrum of apparently distinct tasks.\nHere we introduce Counterfactual World Modeling (CWM), a framework for\nconstructing a visual foundation model: a unified, unsupervised network that\ncan be prompted to perform a wide variety of visual computations. CWM has two\nkey components, which resolve the core issues that have hindered application of\nthe foundation model concept to vision. The first is structured masking, a\ngeneralization of masked prediction methods that encourages a prediction model\nto capture the low-dimensional structure in visual data. The model thereby\nfactors the key physical components of a scene and exposes an interface to them\nvia small sets of visual tokens. This in turn enables CWM's second main idea --\ncounterfactual prompting -- the observation that many apparently distinct\nvisual representations can be computed, in a zero-shot manner, by comparing the\nprediction model's output on real inputs versus slightly modified\n(\"counterfactual\") inputs. We show that CWM generates high-quality readouts on\nreal-world images and videos for a diversity of tasks, including estimation of\nkeypoints, optical flow, occlusions, object segments, and relative depth. Taken\ntogether, our results show that CWM is a promising path to unifying the\nmanifold strands of machine vision in a conceptually simple foundation."
            ],
            "interesting paper": 2
        }
    ],
    "Riley Hernandez": [
        {
            "papers": [
                "Sharpness-Aware Minimization Revisited: Weighted Sharpness as a\n  Regularization Term\nDeep Neural Networks (DNNs) generalization is known to be closely related to\nthe flatness of minima, leading to the development of Sharpness-Aware\nMinimization (SAM) for seeking flatter minima and better generalization. In\nthis paper, we revisit the loss of SAM and propose a more general method,\ncalled WSAM, by incorporating sharpness as a regularization term. We prove its\ngeneralization bound through the combination of PAC and Bayes-PAC techniques,\nand evaluate its performance on various public datasets. The results\ndemonstrate that WSAM achieves improved generalization, or is at least highly\ncompetitive, compared to the vanilla optimizer, SAM and its variants. The code\nis available at\nhttps://github.com/intelligent-machine-learning/dlrover/tree/master/atorch/atorch/optimizers.",
                "Deep Learning and Ethics\nThis article appears as chapter 21 of Prince (2023, Understanding Deep\nLearning); a complete draft of the textbook is available here:\nhttp://udlbook.com. This chapter considers potential harms arising from the\ndesign and use of AI systems. These include algorithmic bias, lack of\nexplainability, data privacy violations, militarization, fraud, and\nenvironmental concerns. The aim is not to provide advice on being more ethical.\nInstead, the goal is to express ideas and start conversations in key areas that\nhave received attention in philosophy, political science, and the broader\nsocial sciences.",
                "PDE+: Enhancing Generalization via PDE with Adaptive Distributional\n  Diffusion\nThe generalization of neural networks is a central challenge in machine\nlearning, especially concerning the performance under distributions that differ\nfrom training ones. Current methods, mainly based on the data-driven paradigm\nsuch as data augmentation, adversarial training, and noise injection, may\nencounter limited generalization due to model non-smoothness. In this paper, we\npropose to investigate generalization from a Partial Differential Equation\n(PDE) perspective, aiming to enhance it directly through the underlying\nfunction of neural networks, rather than focusing on adjusting input data.\nSpecifically, we first establish the connection between neural network\ngeneralization and the smoothness of the solution to a specific PDE, namely\n\"transport equation\". Building upon this, we propose a general framework that\nintroduces adaptive distributional diffusion into transport equation to enhance\nthe smoothness of its solution, thereby improving generalization. In the\ncontext of neural networks, we put this theoretical framework into practice as\n$\\textbf{PDE+}$ ($\\textbf{PDE}$ with $\\textbf{A}$daptive\n$\\textbf{D}$istributional $\\textbf{D}$iffusion) which diffuses each sample into\na distribution covering semantically similar inputs. This enables better\ncoverage of potentially unobserved distributions in training, thus improving\ngeneralization beyond merely data-driven methods. The effectiveness of PDE+ is\nvalidated through extensive experimental settings, demonstrating its superior\nperformance compared to SOTA methods.",
                "READ: Recurrent Adaptation of Large Transformers\nFine-tuning large-scale Transformers has led to the explosion of many AI\napplications across Natural Language Processing and Computer Vision tasks.\nHowever, fine-tuning all pre-trained model parameters becomes impractical as\nthe model size and number of tasks increase. Parameter-efficient transfer\nlearning (PETL) methods aim to address these challenges. While effective in\nreducing the number of trainable parameters, PETL methods still require\nsignificant energy and computational resources to fine-tune. In this paper, we\nintroduce \\textbf{RE}current \\textbf{AD}aption (READ) -- a lightweight and\nmemory-efficient fine-tuning method -- to overcome the limitations of the\ncurrent PETL approaches. Specifically, READ inserts a small RNN network\nalongside the backbone model so that the model does not have to back-propagate\nthrough the large backbone network. Through comprehensive empirical evaluation\nof the GLUE benchmark, we demonstrate READ can achieve a $56\\%$ reduction in\nthe training memory consumption and an $84\\%$ reduction in the GPU energy usage\nwhile retraining high model quality compared to full-tuning. Additionally, the\nmodel size of READ does not grow with the backbone model size, making it a\nhighly scalable solution for fine-tuning large Transformers.",
                "Unifying gradient regularization for Heterogeneous Graph Neural Networks\nHeterogeneous Graph Neural Networks (HGNNs) are a class of powerful deep\nlearning methods widely used to learn representations of heterogeneous graphs.\nDespite the fast development of HGNNs, they still face some challenges such as\nover-smoothing, and non-robustness. Previous studies have shown that these\nproblems can be reduced by using gradient regularization methods. However, the\nexisting gradient regularization methods focus on either graph topology or node\nfeatures. There is no universal approach to integrate these features, which\nseverely affects the efficiency of regularization. In addition, the inclusion\nof gradient regularization into HGNNs sometimes leads to some problems, such as\nan unstable training process, increased complexity and insufficient coverage\nregularized information. Furthermore, there is still short of a complete\ntheoretical analysis of the effects of gradient regularization on HGNNs. In\nthis paper, we propose a novel gradient regularization method called Grug,\nwhich iteratively applies regularization to the gradients generated by both\npropagated messages and the node features during the message-passing process.\nGrug provides a unified framework integrating graph topology and node features,\nbased on which we conduct a detailed theoretical analysis of their\neffectiveness. Specifically, the theoretical analyses elaborate the advantages\nof Grug: 1) Decreasing sample variance during the training process (Stability);\n2) Enhancing the generalization of the model (Universality); 3) Reducing the\ncomplexity of the model (Simplicity); 4) Improving the integrity and diversity\nof graph information utilization (Diversity). As a result, Grug has the\npotential to surpass the theoretical upper bounds set by DropMessage (AAAI-23\nDistinguished Papers). In addition, we evaluate Grug on five public real-world\ndatasets with two downstream tasks...",
                "Deep Neural Networks in Video Human Action Recognition: A Review\nCurrently, video behavior recognition is one of the most foundational tasks\nof computer vision. The 2D neural networks of deep learning are built for\nrecognizing pixel-level information such as images with RGB, RGB-D, or optical\nflow formats, with the current increasingly wide usage of surveillance video\nand more tasks related to human action recognition. There are increasing tasks\nrequiring temporal information for frames dependency analysis. The researchers\nhave widely studied video-based recognition rather than\nimage-based(pixel-based) only to extract more informative elements from\ngeometry tasks. Our current related research addresses multiple novel proposed\nresearch works and compares their advantages and disadvantages between the\nderived deep learning frameworks rather than machine learning frameworks. The\ncomparison happened between existing frameworks and datasets, which are video\nformat data only. Due to the specific properties of human actions and the\nincreasingly wide usage of deep neural networks, we collected all research\nworks within the last three years between 2020 to 2022. In our article, the\nperformance of deep neural networks surpassed most of the techniques in the\nfeature learning and extraction tasks, especially video action recognition."
            ],
            "interesting paper": 3
        },
        {
            "papers": [
                "Asking Before Acting: Gather Information in Embodied Decision Making\n  with Language Models\nWith strong capabilities of reasoning and a broad understanding of the world,\nLarge Language Models (LLMs) have demonstrated immense potential in building\nversatile embodied decision-making agents capable of executing a wide array of\ntasks. Nevertheless, when deployed in unfamiliar environments, we show that LLM\nagents encounter challenges in efficiently gathering essential information,\nleading to suboptimal performance. Conversely, human individuals often seek\nadditional information from their peers prior to taking action, harnessing\nexternal knowledge to avoid unnecessary trial and error. Drawing inspiration\nfrom this behavior, we propose \\textit{Asking Before Acting} (ABA), a method\nthat empowers the agent to proactively inquire with external sources for\npertinent information using natural language during their interactions within\nthe environment. In this way, the agent is able to enhance its efficiency and\nperformance by circumventing potentially laborious steps and combating the\ndifficulties associated with exploration in unfamiliar environments and\nvagueness of the instructions. We conduct extensive experiments involving a\nspectrum of environments including text-based household everyday tasks, robot\narm manipulation tasks, and real world open domain image based embodied tasks.\nThe experiments involve various models from Vicuna to GPT-4. The results\ndemonstrate that, even with modest prompts modifications, ABA exhibits\nsubstantial advantages on both performance and efficiency over baseline LLM\nagents. Further finetuning ABA with reformulated metadata (ABA-FT) faciliates\nlearning the rationale for asking and allows for additional enhancements\nespecially in tasks that baselines struggle to solve.",
                "From Interactive to Co-Constructive Task Learning\nHumans have developed the capability to teach relevant aspects of new or\nadapted tasks to a social peer with very few task demonstrations by making use\nof scaffolding strategies that leverage prior knowledge and importantly prior\njoint experience to yield a joint understanding and a joint execution of the\nrequired steps to solve the task. This process has been discovered and analyzed\nin parent-infant interaction and constitutes a ``co-construction'' as it allows\nboth, the teacher and the learner, to jointly contribute to the task. We\npropose to focus research in robot interactive learning on this co-construction\nprocess to enable robots to learn from non-expert users in everyday situations.\nIn the following, we will review current proposals for interactive task\nlearning and discuss their main contributions with respect to the entailing\ninteraction. We then discuss our notion of co-construction and summarize\nresearch insights from adult-child and human-robot interactions to elucidate\nits nature in more detail. From this overview we finally derive research\ndesiderata that entail the dimensions architecture, representation, interaction\nand explainability.",
                "Role-Play with Large Language Models\nAs dialogue agents become increasingly human-like in their performance, it is\nimperative that we develop effective ways to describe their behaviour in\nhigh-level terms without falling into the trap of anthropomorphism. In this\npaper, we foreground the concept of role-play. Casting dialogue agent behaviour\nin terms of role-play allows us to draw on familiar folk psychological terms,\nwithout ascribing human characteristics to language models they in fact lack.\nTwo important cases of dialogue agent behaviour are addressed this way, namely\n(apparent) deception and (apparent) self-awareness.",
                "An Experimental Investigation into the Evaluation of Explainability\n  Methods\nEXplainable Artificial Intelligence (XAI) aims to help users to grasp the\nreasoning behind the predictions of an Artificial Intelligence (AI) system.\nMany XAI approaches have emerged in recent years. Consequently, a subfield\nrelated to the evaluation of XAI methods has gained considerable attention,\nwith the aim to determine which methods provide the best explanation using\nvarious approaches and criteria. However, the literature lacks a comparison of\nthe evaluation metrics themselves, that one can use to evaluate XAI methods.\nThis work aims to fill this gap by comparing 14 different metrics when applied\nto nine state-of-the-art XAI methods and three dummy methods (e.g., random\nsaliency maps) used as references. Experimental results show which of these\nmetrics produces highly correlated results, indicating potential redundancy. We\nalso demonstrate the significant impact of varying the baseline hyperparameter\non the evaluation metric values. Finally, we use dummy methods to assess the\nreliability of metrics in terms of ranking, pointing out their limitations.",
                "HuatuoGPT, towards Taming Language Model to Be a Doctor\nIn this paper, we present HuatuoGPT, a large language model (LLM) for medical\nconsultation. The core recipe of HuatuoGPT is to leverage both\n\\textit{distilled data from ChatGPT} and \\textit{real-world data from doctors}\nin the supervised fine-tuned stage. The responses of ChatGPT are usually\ndetailed, well-presented and informative while it cannot perform like a doctor\nin many aspects, e.g. for integrative diagnosis. We argue that real-world data\nfrom doctors would be complementary to distilled data in the sense the former\ncould tame a distilled language model to perform like doctors. To better\nleverage the strengths of both data, we train a reward model to align the\nlanguage model with the merits that both data bring, following an RLAIF\n(reinforced learning from AI feedback) fashion. To evaluate and benchmark the\nmodels, we propose a comprehensive evaluation scheme (including automatic and\nmanual metrics). Experimental results demonstrate that HuatuoGPT achieves\nstate-of-the-art results in performing medical consultation among open-source\nLLMs in GPT-4 evaluation, human evaluation, and medical benchmark datasets. It\nis worth noting that by using additional real-world data and RLAIF, the\ndistilled language model (i.e., HuatuoGPT) outperforms its teacher model\nChatGPT in most cases. Our code, data, and models are publicly available at\n\\url{https://github.com/FreedomIntelligence/HuatuoGPT}. The online demo is\navailable at \\url{https://www.HuatuoGPT.cn/}.",
                "MPE4G: Multimodal Pretrained Encoder for Co-Speech Gesture Generation\nWhen virtual agents interact with humans, gestures are crucial to delivering\ntheir intentions with speech. Previous multimodal co-speech gesture generation\nmodels required encoded features of all modalities to generate gestures. If\nsome input modalities are removed or contain noise, the model may not generate\nthe gestures properly. To acquire robust and generalized encodings, we propose\na novel framework with a multimodal pre-trained encoder for co-speech gesture\ngeneration. In the proposed method, the multi-head-attention-based encoder is\ntrained with self-supervised learning to contain the information on each\nmodality. Moreover, we collect full-body gestures that consist of 3D joint\nrotations to improve visualization and apply gestures to the extensible body\nmodel. Through the series of experiments and human evaluation, the proposed\nmethod renders realistic co-speech gestures not only when all input modalities\nare given but also when the input modalities are missing or noisy."
            ],
            "interesting paper": 2
        },
        {
            "papers": [
                "XGrad: Boosting Gradient-Based Optimizers With Weight Prediction\nIn this paper, we propose a general deep learning training framework XGrad\nwhich introduces weight prediction into the popular gradient-based optimizers\nto boost their convergence and generalization when training the deep neural\nnetwork (DNN) models. In particular, ahead of each mini-batch training, the\nfuture weights are predicted according to the update rule of the used optimizer\nand are then applied to both the forward pass and backward propagation. In this\nway, during the whole training period, the optimizer always utilizes the\ngradients w.r.t. the future weights to update the DNN parameters, making the\ngradient-based optimizer achieve better convergence and generalization compared\nto the original optimizer without weight prediction. XGrad is rather\nstraightforward to implement yet pretty effective in boosting the convergence\nof gradient-based optimizers and the accuracy of DNN models. Empirical results\nconcerning five popular optimizers including SGD with momentum, Adam, AdamW,\nAdaBelief, and AdaM3 demonstrate the effectiveness of our proposal. The\nexperimental results validate that XGrad can attain higher model accuracy than\nthe baseline optimizers when training the DNN models. The code of XGrad will be\navailable at: https://github.com/guanleics/XGrad.",
                "Pruning Distorted Images in MNIST Handwritten Digits\nRecognizing handwritten digits is a challenging task primarily due to the\ndiversity of writing styles and the presence of noisy images. The widely used\nMNIST dataset, which is commonly employed as a benchmark for this task,\nincludes distorted digits with irregular shapes, incomplete strokes, and\nvarying skew in both the training and testing datasets. Consequently, these\nfactors contribute to reduced accuracy in digit recognition. To overcome this\nchallenge, we propose a two-stage deep learning approach. In the first stage,\nwe create a simple neural network to identify distorted digits within the\ntraining set. This model serves to detect and filter out such distorted and\nambiguous images. In the second stage, we exclude these identified images from\nthe training dataset and proceed to retrain the model using the filtered\ndataset. This process aims to improve the classification accuracy and\nconfidence levels while mitigating issues of underfitting and overfitting. Our\nexperimental results demonstrate the effectiveness of the proposed approach,\nachieving an accuracy rate of over 99.5% on the testing dataset. This\nsignificant improvement showcases the potential of our method in enhancing\ndigit classification accuracy. In our future work, we intend to explore the\nscalability of this approach and investigate techniques to further enhance\naccuracy by reducing the size of the training data.",
                "Revisiting Structured Variational Autoencoders\nStructured variational autoencoders (SVAEs) combine probabilistic graphical\nmodel priors on latent variables, deep neural networks to link latent variables\nto observed data, and structure-exploiting algorithms for approximate posterior\ninference. These models are particularly appealing for sequential data, where\nthe prior can capture temporal dependencies. However, despite their conceptual\nelegance, SVAEs have proven difficult to implement, and more general approaches\nhave been favored in practice. Here, we revisit SVAEs using modern machine\nlearning tools and demonstrate their advantages over more general alternatives\nin terms of both accuracy and efficiency. First, we develop a modern\nimplementation for hardware acceleration, parallelization, and automatic\ndifferentiation of the message passing algorithms at the core of the SVAE.\nSecond, we show that by exploiting structure in the prior, the SVAE learns more\naccurate models and posterior distributions, which translate into improved\nperformance on prediction tasks. Third, we show how the SVAE can naturally\nhandle missing data, and we leverage this ability to develop a novel,\nself-supervised training approach. Altogether, these results show that the time\nis ripe to revisit structured variational autoencoders.",
                "SING: A Plug-and-Play DNN Learning Technique\nWe propose SING (StabIlized and Normalized Gradient), a plug-and-play\ntechnique that improves the stability and generalization of the Adam(W)\noptimizer. SING is straightforward to implement and has minimal computational\noverhead, requiring only a layer-wise standardization of the gradients fed to\nAdam(W) without introducing additional hyper-parameters. We support the\neffectiveness and practicality of the proposed approach by showing improved\nresults on a wide range of architectures, problems (such as image\nclassification, depth estimation, and natural language processing), and in\ncombination with other optimizers. We provide a theoretical analysis of the\nconvergence of the method, and we show that by virtue of the standardization,\nSING can escape local minima narrower than a threshold that is inversely\nproportional to the network's depth.",
                "Automated Search-Space Generation Neural Architecture Search\nTo search an optimal sub-network within a general deep neural network (DNN),\nexisting neural architecture search (NAS) methods typically rely on\nhandcrafting a search space beforehand. Such requirements make it challenging\nto extend them onto general scenarios without significant human expertise and\nmanual intervention. To overcome the limitations, we propose Automated\nSearch-Space Generation Neural Architecture Search (ASGNAS), perhaps the first\nautomated system to train general DNNs that cover all candidate connections and\noperations and produce high-performing sub-networks in the one shot manner.\nTechnologically, ASGNAS delivers three noticeable contributions to minimize\nhuman efforts: (i) automated search space generation for general DNNs; (ii) a\nHierarchical Half-Space Projected Gradient (H2SPG) that leverages the hierarchy\nand dependency within generated search space to ensure the network validity\nduring optimization, and reliably produces a solution with both high\nperformance and hierarchical group sparsity; and (iii) automated sub-network\nconstruction upon the H2SPG solution. Numerically, we demonstrate the\neffectiveness of ASGNAS on a variety of general DNNs, including RegNet,\nStackedUnets, SuperResNet, and DARTS, over benchmark datasets such as CIFAR10,\nFashion-MNIST, ImageNet, STL-10 , and SVNH. The sub-networks computed by ASGNAS\nachieve competitive even superior performance compared to the starting full\nDNNs and other state-of-the-arts. The library will be released at\nhttps://github.com/tianyic/only_train_once.",
                "Neural (Tangent Kernel) Collapse\nThis work bridges two important concepts: the Neural Tangent Kernel (NTK),\nwhich captures the evolution of deep neural networks (DNNs) during training,\nand the Neural Collapse (NC) phenomenon, which refers to the emergence of\nsymmetry and structure in the last-layer features of well-trained\nclassification DNNs. We adopt the natural assumption that the empirical NTK\ndevelops a block structure aligned with the class labels, i.e., samples within\nthe same class have stronger correlations than samples from different classes.\nUnder this assumption, we derive the dynamics of DNNs trained with mean squared\n(MSE) loss and break them into interpretable phases. Moreover, we identify an\ninvariant that captures the essence of the dynamics, and use it to prove the\nemergence of NC in DNNs with block-structured NTK. We provide large-scale\nnumerical experiments on three common DNN architectures and three benchmark\ndatasets to support our theory."
            ],
            "interesting paper": 1
        },
        {
            "papers": [
                "Alert of the Second Decision-maker: An Introduction to Human-AI Conflict\nThe collaboration between humans and artificial intelligence (AI) is a\nsignificant feature in this digital age. However, humans and AI may have\nobservation, interpretation, and action conflicts when working synchronously.\nThis phenomenon is often masked by faults and, unfortunately, overlooked. This\npaper systematically introduces the human-AI conflict concept, causes,\nmeasurement methods, and risk assessment. The results highlight that there is a\npotential second decision-maker besides the human, which is the AI; the\nhuman-AI conflict is a unique and emerging risk in digitalized process systems;\nand this is an interdisciplinary field that needs to be distinguished from\ntraditional fault and failure analysis; the conflict risk is significant and\ncannot be ignored.",
                "Enhancing Human Capabilities through Symbiotic Artificial Intelligence\n  with Shared Sensory Experiences\nThe merging of human intelligence and artificial intelligence has long been a\nsubject of interest in both science fiction and academia. In this paper, we\nintroduce a novel concept in Human-AI interaction called Symbiotic Artificial\nIntelligence with Shared Sensory Experiences (SAISSE), which aims to establish\na mutually beneficial relationship between AI systems and human users through\nshared sensory experiences. By integrating multiple sensory input channels and\nprocessing human experiences, SAISSE fosters a strong human-AI bond, enabling\nAI systems to learn from and adapt to individual users, providing personalized\nsupport, assistance, and enhancement. Furthermore, we discuss the incorporation\nof memory storage units for long-term growth and development of both the AI\nsystem and its human user. As we address user privacy and ethical guidelines\nfor responsible AI-human symbiosis, we also explore potential biases and\ninequalities in AI-human symbiosis and propose strategies to mitigate these\nchallenges. Our research aims to provide a comprehensive understanding of the\nSAISSE concept and its potential to effectively support and enhance individual\nhuman users through symbiotic AI systems. This position article aims at\ndiscussing poteintial AI-human interaction related topics within the scientific\ncommunity, rather than providing experimental or theoretical results.",
                "A Hierarchical Approach to Population Training for Human-AI\n  Collaboration\nA major challenge for deep reinforcement learning (DRL) agents is to\ncollaborate with novel partners that were not encountered by them during the\ntraining phase. This is specifically worsened by an increased variance in\naction responses when the DRL agents collaborate with human partners due to the\nlack of consistency in human behaviors. Recent work have shown that training a\nsingle agent as the best response to a diverse population of training partners\nsignificantly increases an agent's robustness to novel partners. We further\nenhance the population-based training approach by introducing a Hierarchical\nReinforcement Learning (HRL) based method for Human-AI Collaboration. Our agent\nis able to learn multiple best-response policies as its low-level policy while\nat the same time, it learns a high-level policy that acts as a manager which\nallows the agent to dynamically switch between the low-level best-response\npolicies based on its current partner. We demonstrate that our method is able\nto dynamically adapt to novel partners of different play styles and skill\nlevels in the 2-player collaborative Overcooked game environment. We also\nconducted a human study in the same environment to test the effectiveness of\nour method when partnering with real human subjects.",
                "Passive learning of active causal strategies in agents and language\n  models\nWhat can be learned about causality and experimentation from passive data?\nThis question is salient given recent successes of passively-trained language\nmodels in interactive domains such as tool use. Passive learning is inherently\nlimited. However, we show that purely passive learning can in fact allow an\nagent to learn generalizable strategies for determining and using causal\nstructures, as long as the agent can intervene at test time. We formally\nillustrate that learning a strategy of first experimenting, then seeking goals,\ncan allow generalization from passive learning in principle. We then show\nempirically that agents trained via imitation on expert data can indeed\ngeneralize at test time to infer and use causal links which are never present\nin the training data; these agents can also generalize experimentation\nstrategies to novel variable sets never observed in training. We then show that\nstrategies for causal intervention and exploitation can be generalized from\npassive data even in a more complex environment with high-dimensional\nobservations, with the support of natural language explanations. Explanations\ncan even allow passive learners to generalize out-of-distribution from\nperfectly-confounded training data. Finally, we show that language models,\ntrained only on passive next-word prediction, can generalize causal\nintervention strategies from a few-shot prompt containing examples of\nexperimentation, together with explanations and reasoning. These results\nhighlight the surprising power of passive learning of active causal strategies,\nand may help to understand the behaviors and capabilities of language models.",
                "Ghost in the Minecraft: Generally Capable Agents for Open-World\n  Environments via Large Language Models with Text-based Knowledge and Memory\nThe captivating realm of Minecraft has attracted substantial research\ninterest in recent years, serving as a rich platform for developing intelligent\nagents capable of functioning in open-world environments. However, the current\nresearch landscape predominantly focuses on specific objectives, such as the\npopular \"ObtainDiamond\" task, and has not yet shown effective generalization to\na broader spectrum of tasks. Furthermore, the current leading success rate for\nthe \"ObtainDiamond\" task stands at around 20%, highlighting the limitations of\nReinforcement Learning (RL) based controllers used in existing methods. To\ntackle these challenges, we introduce Ghost in the Minecraft (GITM), a novel\nframework integrates Large Language Models (LLMs) with text-based knowledge and\nmemory, aiming to create Generally Capable Agents (GCAs) in Minecraft. These\nagents, equipped with the logic and common sense capabilities of LLMs, can\nskillfully navigate complex, sparse-reward environments with text-based\ninteractions. We develop a set of structured actions and leverage LLMs to\ngenerate action plans for the agents to execute. The resulting LLM-based agent\nmarkedly surpasses previous methods, achieving a remarkable improvement of\n+47.5% in success rate on the \"ObtainDiamond\" task, demonstrating superior\nrobustness compared to traditional RL-based controllers. Notably, our agent is\nthe first to procure all items in the Minecraft Overworld technology tree,\ndemonstrating its extensive capabilities. GITM does not need any GPU for\ntraining, but a single CPU node with 32 CPU cores is enough. This research\nshows the potential of LLMs in developing capable agents for handling\nlong-horizon, complex tasks and adapting to uncertainties in open-world\nenvironments. See the project website at https://github.com/OpenGVLab/GITM.",
                "Combining Gamification and Intelligent Tutoring Systems in a Serious\n  Game for Engineering Education\nWe provide ongoing results from the development of a personalized learning\nsystem integrated into a serious game. Given limited instructor resources, the\nuse of computerized systems to help tutor students offers a way to provide\nhigher quality education and to improve educational efficacy. Personalized\nlearning systems like the one proposed in this paper offer an accessible\nsolution. Furthermore, by combining such a system with a serious game, students\nare further engaged in interacting with the system. The proposed learning\nsystem combines expert-driven structure and lesson planning with computational\nintelligence methods and gamification to provide students with a fun and\neducational experience. As the project is ongoing from past years, numerous\ndesign iterations have been made on the system based on feedback from students\nand classroom observations. Using computational intelligence, the system\nadaptively provides support to students based on data collected from both their\nin-game actions and by estimating their emotional state from webcam images. For\nour evaluation, we focus on student data gathered from in-classroom testing in\nrelevant courses, with both educational efficacy, results and student\nobservations. To demonstrate the effect of our proposed system, students in an\nearly electrical engineering course were instructed to interact with the system\nin place of a standard lab assignment. The system would then measure and help\nthem improve their background knowledge before allowing them to complete the\nlab assignment. As they played through the game, we observed their interactions\nwith the system to gather insights for future work. Additionally, we\ndemonstrate the system's educational efficacy through pre-post-test results\nfrom students who played the game with and without the personalized learning\nsystem."
            ],
            "interesting paper": 2
        },
        {
            "papers": [
                "Statistically Significant Concept-based Explanation of Image Classifiers\n  via Model Knockoffs\nA concept-based classifier can explain the decision process of a deep\nlearning model by human-understandable concepts in image classification\nproblems. However, sometimes concept-based explanations may cause false\npositives, which misregards unrelated concepts as important for the prediction\ntask. Our goal is to find the statistically significant concept for\nclassification to prevent misinterpretation. In this study, we propose a method\nusing a deep learning model to learn the image concept and then using the\nKnockoff samples to select the important concepts for prediction by controlling\nthe False Discovery Rate (FDR) under a certain value. We evaluate the proposed\nmethod in our synthetic and real data experiments. Also, it shows that our\nmethod can control the FDR properly while selecting highly interpretable\nconcepts to improve the trustworthiness of the model.",
                "Are Deep Neural Networks Adequate Behavioural Models of Human Visual\n  Perception?\nDeep neural networks (DNNs) are machine learning algorithms that have\nrevolutionised computer vision due to their remarkable successes in tasks like\nobject classification and segmentation. The success of DNNs as computer vision\nalgorithms has led to the suggestion that DNNs may also be good models of human\nvisual perception. We here review evidence regarding current DNNs as adequate\nbehavioural models of human core object recognition. To this end, we argue that\nit is important to distinguish between statistical tools and computational\nmodels, and to understand model quality as a multidimensional concept where\nclarity about modelling goals is key. Reviewing a large number of\npsychophysical and computational explorations of core object recognition\nperformance in humans and DNNs, we argue that DNNs are highly valuable\nscientific tools but that as of today DNNs should only be regarded as promising\n-- but not yet adequate -- computational models of human core object\nrecognition behaviour. On the way we dispel a number of myths surrounding DNNs\nin vision science.",
                "Vision Transformers for Small Histological Datasets Learned through\n  Knowledge Distillation\nComputational Pathology (CPATH) systems have the potential to automate\ndiagnostic tasks. However, the artifacts on the digitized histological glass\nslides, known as Whole Slide Images (WSIs), may hamper the overall performance\nof CPATH systems. Deep Learning (DL) models such as Vision Transformers (ViTs)\nmay detect and exclude artifacts before running the diagnostic algorithm. A\nsimple way to develop robust and generalized ViTs is to train them on massive\ndatasets. Unfortunately, acquiring large medical datasets is expensive and\ninconvenient, prompting the need for a generalized artifact detection method\nfor WSIs. In this paper, we present a student-teacher recipe to improve the\nclassification performance of ViT for the air bubbles detection task. ViT,\ntrained under the student-teacher framework, boosts its performance by\ndistilling existing knowledge from the high-capacity teacher model. Our\nbest-performing ViT yields 0.961 and 0.911 F1-score and MCC, respectively,\nobserving a 7% gain in MCC against stand-alone training. The proposed method\npresents a new perspective of leveraging knowledge distillation over transfer\nlearning to encourage the use of customized transformers for efficient\npreprocessing pipelines in the CPATH systems.",
                "Green Runner: A tool for efficient model selection from model\n  repositories\nDeep learning models have become essential in software engineering, enabling\nintelligent features like image captioning and document generation. However,\ntheir popularity raises concerns about environmental impact and inefficient\nmodel selection. This paper introduces GreenRunnerGPT, a novel tool for\nefficiently selecting deep learning models based on specific use cases. It\nemploys a large language model to suggest weights for quality indicators,\noptimizing resource utilization. The tool utilizes a multi-armed bandit\nframework to evaluate models against target datasets, considering tradeoffs. We\ndemonstrate that GreenRunnerGPT is able to identify a model suited to a target\nuse case without wasteful computations that would occur under a brute-force\napproach to model selection.",
                "Stability of implicit neural networks for long-term forecasting in\n  dynamical systems\nForecasting physical signals in long time range is among the most challenging\ntasks in Partial Differential Equations (PDEs) research. To circumvent\nlimitations of traditional solvers, many different Deep Learning methods have\nbeen proposed. They are all based on auto-regressive methods and exhibit\nstability issues. Drawing inspiration from the stability property of implicit\nnumerical schemes, we introduce a stable auto-regressive implicit neural\nnetwork. We develop a theory based on the stability definition of schemes to\nensure the stability in forecasting of this network. It leads us to introduce\nhard constraints on its weights and propagate the dynamics in the latent space.\nOur experimental results validate our stability property, and show improved\nresults at long-term forecasting for two transports PDEs.",
                "Understanding Sparse Neural Networks from their Topology via\n  Multipartite Graph Representations\nPruning-at-Initialization (PaI) algorithms provide Sparse Neural Networks\n(SNNs) which are computationally more efficient than their dense counterparts,\nand try to avoid performance degradation. While much emphasis has been directed\ntowards \\emph{how} to prune, we still do not know \\emph{what topological\nmetrics} of the SNNs characterize \\emph{good performance}. From prior work, we\nhave layer-wise topological metrics by which SNN performance can be predicted:\nthe Ramanujan-based metrics. To exploit these metrics, proper ways to represent\nnetwork layers via Graph Encodings (GEs) are needed, with Bipartite Graph\nEncodings (BGEs) being the \\emph{de-facto} standard at the current stage.\nNevertheless, existing BGEs neglect the impact of the inputs, and do not\ncharacterize the SNN in an end-to-end manner. Additionally, thanks to a\nthorough study of the Ramanujan-based metrics, we discover that they are only\nas good as the \\emph{layer-wise density} as performance predictors, when paired\nwith BGEs. To close both gaps, we design a comprehensive topological analysis\nfor SNNs with both linear and convolutional layers, via (i) a new input-aware\nMultipartite Graph Encoding (MGE) for SNNs and (ii) the design of new\nend-to-end topological metrics over the MGE. With these novelties, we show the\nfollowing: (a) The proposed MGE allows to extract topological metrics that are\nmuch better predictors of the accuracy drop than metrics computed from current\ninput-agnostic BGEs; (b) Which metrics are important at different sparsity\nlevels and for different architectures; (c) A mixture of our topological\nmetrics can rank PaI algorithms more effectively than Ramanujan-based metrics.\nThe codebase is publicly available at https://github.com/eliacunegatti/mge-snn."
            ],
            "interesting paper": 5
        },
        {
            "papers": [
                "Mindstorms in Natural Language-Based Societies of Mind\nBoth Minsky's \"society of mind\" and Schmidhuber's \"learning to think\" inspire\ndiverse societies of large multimodal neural networks (NNs) that solve problems\nby interviewing each other in a \"mindstorm.\" Recent implementations of NN-based\nsocieties of minds consist of large language models (LLMs) and other NN-based\nexperts communicating through a natural language interface. In doing so, they\novercome the limitations of single LLMs, improving multimodal zero-shot\nreasoning. In these natural language-based societies of mind (NLSOMs), new\nagents -- all communicating through the same universal symbolic language -- are\neasily added in a modular fashion. To demonstrate the power of NLSOMs, we\nassemble and experiment with several of them (having up to 129 members),\nleveraging mindstorms in them to solve some practical AI tasks: visual question\nanswering, image captioning, text-to-image synthesis, 3D generation, egocentric\nretrieval, embodied AI, and general language-based task solving. We view this\nas a starting point towards much larger NLSOMs with billions of agents-some of\nwhich may be humans. And with this emergence of great societies of\nheterogeneous minds, many new research questions have suddenly become paramount\nto the future of artificial intelligence. What should be the social structure\nof an NLSOM? What would be the (dis)advantages of having a monarchical rather\nthan a democratic structure? How can principles of NN economies be used to\nmaximize the total reward of a reinforcement learning NLSOM? In this work, we\nidentify, discuss, and try to answer some of these questions.",
                "How Do UX Practitioners Communicate AI as a Design Material? Artifacts,\n  Conceptions, and Propositions\nUX practitioners (UXPs) face novel challenges when working with and\ncommunicating artificial intelligence (AI) as a design material. We explore how\nUXPs communicate AI concepts when given hands-on experience training and\nexperimenting with AI models. To do so, we conducted a task-based design study\nwith 27 UXPs in which they prototyped and created a design presentation for a\nAI-enabled interface while having access to a simple AI model training tool.\nThrough analyzing UXPs' design presentations and post-activity interviews, we\nfound that although UXPs struggled to clearly communicate some AI concepts,\ntinkering with AI broadened common ground when communicating with technical\nstakeholders. UXPs also identified key risks and benefits of AI in their\ndesigns, and proposed concrete next steps for both UX and AI work. We conclude\nwith a sensitizing concept and recommendations for design and AI tools to\nenhance multi-stakeholder communication and collaboration when crafting\nhuman-centered AI experiences.",
                "SwiftSage: A Generative Agent with Fast and Slow Thinking for Complex\n  Interactive Tasks\nWe introduce SwiftSage, a novel agent framework inspired by the dual-process\ntheory of human cognition, designed to excel in action planning for complex\ninteractive reasoning tasks. SwiftSage integrates the strengths of behavior\ncloning and prompting large language models (LLMs) to enhance task completion\nperformance. The framework comprises two primary modules: the Swift module,\nrepresenting fast and intuitive thinking, and the Sage module, emulating\ndeliberate thought processes. The Swift module is a small encoder-decoder LM\nfine-tuned on the oracle agent's action trajectories, while the Sage module\nemploys LLMs such as GPT-4 for subgoal planning and grounding. We develop a\nheuristic method to harmoniously integrate the two modules, resulting in a more\nefficient and robust problem-solving process. In 30 tasks from the ScienceWorld\nbenchmark, SwiftSage significantly outperforms other methods such as SayCan,\nReAct, and Reflexion, demonstrating its effectiveness in solving complex\ninteractive tasks.",
                "Acting as Inverse Inverse Planning\nGreat storytellers know how to take us on a journey. They direct characters\nto act -- not necessarily in the most rational way -- but rather in a way that\nleads to interesting situations, and ultimately creates an impactful experience\nfor audience members looking on.\n  If audience experience is what matters most, then can we help artists and\nanimators *directly* craft such experiences, independent of the concrete\ncharacter actions needed to evoke those experiences? In this paper, we offer a\nnovel computational framework for such tools. Our key idea is to optimize\nanimations with respect to *simulated* audience members' experiences. To\nsimulate the audience, we borrow an established principle from cognitive\nscience: that human social intuition can be modeled as \"inverse planning,\" the\ntask of inferring an agent's (hidden) goals from its (observed) actions.\nBuilding on this model, we treat storytelling as \"*inverse* inverse planning,\"\nthe task of choosing actions to manipulate an inverse planner's inferences. Our\nframework is grounded in literary theory, naturally capturing many storytelling\nelements from first principles. We give a series of examples to demonstrate\nthis, with supporting evidence from human subject studies.",
                "Attention Schema in Neural Agents\nAttention has become a common ingredient in deep learning architectures. It\nadds a dynamical selection of information on top of the static selection of\ninformation supported by weights. In the same way, we can imagine a\nhigher-order informational filter built on top of attention: an Attention\nSchema (AS), namely, a descriptive and predictive model of attention. In\ncognitive neuroscience, Attention Schema Theory (AST) supports this idea of\ndistinguishing attention from AS. A strong prediction of this theory is that an\nagent can use its own AS to also infer the states of other agents' attention\nand consequently enhance coordination with other agents. As such, multi-agent\nreinforcement learning would be an ideal setting to experimentally test the\nvalidity of AST. We explore different ways in which attention and AS interact\nwith each other. Our preliminary results indicate that agents that implement\nthe AS as a recurrent internal control achieve the best performance. In\ngeneral, these exploratory experiments suggest that equipping artificial agents\nwith a model of attention can enhance their social intelligence.",
                "Towards Cognitive Bots: Architectural Research Challenges\nSoftware bots operating in multiple virtual digital platforms must understand\nthe platforms' affordances and behave like human users. Platform affordances or\nfeatures differ from one application platform to another or through a life\ncycle, requiring such bots to be adaptable. Moreover, bots in such platforms\ncould cooperate with humans or other software agents for work or to learn\nspecific behavior patterns. However, present-day bots, particularly chatbots,\nother than language processing and prediction, are far from reaching a human\nuser's behavior level within complex business information systems. They lack\nthe cognitive capabilities to sense and act in such virtual environments,\nrendering their development a challenge to artificial general intelligence\nresearch. In this study, we problematize and investigate assumptions in\nconceptualizing software bot architecture by directing attention to significant\narchitectural research challenges in developing cognitive bots endowed with\ncomplex behavior for operation on information systems. As an outlook, we\npropose alternate architectural assumptions to consider in future bot design\nand bot development frameworks."
            ],
            "interesting paper": 1
        },
        {
            "papers": [
                "A Comprehensive Overview and Comparative Analysis on Deep Learning\n  Models: CNN, RNN, LSTM, GRU\nDeep learning (DL) has emerged as a powerful subset of machine learning (ML)\nand artificial intelligence (AI), outperforming traditional ML methods,\nespecially in handling unstructured and large datasets. Its impact spans across\nvarious domains, including speech recognition, healthcare, autonomous vehicles,\ncybersecurity, predictive analytics, and more. However, the complexity and\ndynamic nature of real-world problems present challenges in designing effective\ndeep learning models. Consequently, several deep learning models have been\ndeveloped to address different problems and applications. In this article, we\nconduct a comprehensive survey of various deep learning models, including\nConvolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs),\nGenerative Models, Deep Reinforcement Learning (DRL), and Deep Transfer\nLearning. We examine the structure, applications, benefits, and limitations of\neach model. Furthermore, we perform an analysis using three publicly available\ndatasets: IMDB, ARAS, and Fruit-360. We compare the performance of six renowned\ndeep learning models: CNN, Simple RNN, Long Short-Term Memory (LSTM),\nBidirectional LSTM, Gated Recurrent Unit (GRU), and Bidirectional GRU.",
                "USIM-DAL: Uncertainty-aware Statistical Image Modeling-based Dense\n  Active Learning for Super-resolution\nDense regression is a widely used approach in computer vision for tasks such\nas image super-resolution, enhancement, depth estimation, etc. However, the\nhigh cost of annotation and labeling makes it challenging to achieve accurate\nresults. We propose incorporating active learning into dense regression models\nto address this problem. Active learning allows models to select the most\ninformative samples for labeling, reducing the overall annotation cost while\nimproving performance. Despite its potential, active learning has not been\nwidely explored in high-dimensional computer vision regression tasks like\nsuper-resolution. We address this research gap and propose a new framework\ncalled USIM-DAL that leverages the statistical properties of colour images to\nlearn informative priors using probabilistic deep neural networks that model\nthe heteroscedastic predictive distribution allowing uncertainty\nquantification. Moreover, the aleatoric uncertainty from the network serves as\na proxy for error that is used for active learning. Our experiments on a wide\nvariety of datasets spanning applications in natural images (visual genome,\nBSD100), medical imaging (histopathology slides), and remote sensing (satellite\nimages) demonstrate the efficacy of the newly proposed USIM-DAL and superiority\nover several dense regression active learning methods.",
                "One Network, Many Masks: Towards More Parameter-Efficient Transfer\n  Learning\nFine-tuning pre-trained language models for multiple tasks tends to be\nexpensive in terms of storage. To mitigate this, parameter-efficient transfer\nlearning (PETL) methods have been proposed to address this issue, but they\nstill require a significant number of parameters and storage when being applied\nto broader ranges of tasks. To achieve even greater storage reduction, we\npropose PROPETL, a novel method that enables efficient sharing of a single PETL\nmodule which we call prototype network (e.g., adapter, LoRA, and prefix-tuning)\nacross layers and tasks. We then learn binary masks to select different\nsub-networks from the shared prototype network and apply them as PETL modules\ninto different layers. We find that the binary masks can determine crucial\ninformation from the network, which is often ignored in previous studies. Our\nwork can also be seen as a type of pruning method, where we find that\noverparameterization also exists in the seemingly small PETL modules. We\nevaluate PROPETL on various downstream tasks and show that it can outperform\nother PETL methods with approximately 10% of the parameter storage required by\nthe latter.",
                "Distill Gold from Massive Ores: Bi-level Data Pruning towards Efficient\n  Dataset Distillation\nData-efficient learning has garnered significant attention, especially given\nthe current trend of large multi-modal models. Recently, dataset distillation\nhas become an effective approach by synthesizing data samples that are\nessential for network training. However, it remains to be explored which\nsamples are essential for the dataset distillation process itself. In this\nwork, we study the data efficiency and selection for the dataset distillation\ntask. By re-formulating the dynamics of distillation, we provide insight into\nthe inherent redundancy in the real dataset, both theoretically and\nempirically. We propose to use the empirical loss value as a static data\npruning criterion. To further compensate for the variation of the data value in\ntraining, we find the most contributing samples based on their causal effects\non the distillation. The proposed selection strategy can efficiently exploit\nthe training dataset, outperform the previous SOTA distillation algorithms, and\nconsistently enhance the distillation algorithms, even on much larger-scale and\nmore heterogeneous datasets, e.g., full ImageNet-1K and Kinetics-400. We\nbelieve this paradigm will open up new avenues in the dynamics of distillation\nand pave the way for efficient dataset distillation. Our code is available on\nhttps://github.com/silicx/GoldFromOres-BiLP.",
                "Observation Denoising in CYRUS Soccer Simulation 2D Team For RoboCup\n  2023\nThe RoboCup competitions hold various leagues, and the Soccer Simulation 2D\nLeague is a major one among them. Soccer Simulation 2D (SS2D) match involves\ntwo teams, including 11 players and a coach, competing against each other. The\nplayers can only communicate with the Soccer Simulation Server during the game.\nThis paper presents the latest research of the CYRUS soccer simulation 2D team,\nthe champion of RoboCup 2021. We will explain our denoising idea powered by\nlong short-term memory networks (LSTM) and deep neural networks (DNN). The\nCYRUS team uses the CYRUS2D base code that was developed based on the Helios\nand Gliders bases.",
                "PuMer: Pruning and Merging Tokens for Efficient Vision Language Models\nLarge-scale vision language (VL) models use Transformers to perform\ncross-modal interactions between the input text and image. These cross-modal\ninteractions are computationally expensive and memory-intensive due to the\nquadratic complexity of processing the input image and text. We present PuMer:\na token reduction framework that uses text-informed Pruning and modality-aware\nMerging strategies to progressively reduce the tokens of input image and text,\nimproving model inference speed and reducing memory footprint. PuMer learns to\nkeep salient image tokens related to the input text and merges similar textual\nand visual tokens by adding lightweight token reducer modules at several\ncross-modal layers in the VL model. Training PuMer is mostly the same as\nfinetuning the original VL model but faster. Our evaluation for two vision\nlanguage models on four downstream VL tasks shows PuMer increases inference\nthroughput by up to 2x and reduces memory footprint by over 50% while incurring\nless than a 1% accuracy drop."
            ],
            "interesting paper": 2
        },
        {
            "papers": [
                "Integrating Action Knowledge and LLMs for Task Planning and Situation\n  Handling in Open Worlds\nTask planning systems have been developed to help robots use human knowledge\n(about actions) to complete long-horizon tasks. Most of them have been\ndeveloped for \"closed worlds\" while assuming the robot is provided with\ncomplete world knowledge. However, the real world is generally open, and the\nrobots frequently encounter unforeseen situations that can potentially break\nthe planner's completeness. Could we leverage the recent advances on\npre-trained Large Language Models (LLMs) to enable classical planning systems\nto deal with novel situations?\n  This paper introduces a novel framework, called COWP, for open-world task\nplanning and situation handling. COWP dynamically augments the robot's action\nknowledge, including the preconditions and effects of actions, with\ntask-oriented commonsense knowledge. COWP embraces the openness from LLMs, and\nis grounded to specific domains via action knowledge. For systematic\nevaluations, we collected a dataset that includes 1,085 execution-time\nsituations. Each situation corresponds to a state instance wherein a robot is\npotentially unable to complete a task using a solution that normally works.\nExperimental results show that our approach outperforms competitive baselines\nfrom the literature in the success rate of service tasks. Additionally, we have\ndemonstrated COWP using a mobile manipulator. Supplementary materials are\navailable at: https://cowplanning.github.io/",
                "AI Coach Assist: An Automated Approach for Call Recommendation in\n  Contact Centers for Agent Coaching\nIn recent years, the utilization of Artificial Intelligence (AI) in the\ncontact center industry is on the rise. One area where AI can have a\nsignificant impact is in the coaching of contact center agents. By analyzing\ncall transcripts using Natural Language Processing (NLP) techniques, it would\nbe possible to quickly determine which calls are most relevant for coaching\npurposes. In this paper, we present AI Coach Assist, which leverages the\npre-trained transformer-based language models to determine whether a given call\nis coachable or not based on the quality assurance (QA) questions asked by the\ncontact center managers or supervisors. The system was trained and evaluated on\na large dataset collected from real-world contact centers and provides an\neffective way to recommend calls to the contact center managers that are more\nlikely to contain coachable moments. Our experimental findings demonstrate the\npotential of AI Coach Assist to improve the coaching process, resulting in\nenhancing the performance of contact center agents.",
                "On the Value of Myopic Behavior in Policy Reuse\nLeveraging learned strategies in unfamiliar scenarios is fundamental to human\nintelligence. In reinforcement learning, rationally reusing the policies\nacquired from other tasks or human experts is critical for tackling problems\nthat are difficult to learn from scratch. In this work, we present a framework\ncalled Selective Myopic bEhavior Control~(SMEC), which results from the insight\nthat the short-term behaviors of prior policies are sharable across tasks. By\nevaluating the behaviors of prior policies via a hybrid value function\narchitecture, SMEC adaptively aggregates the sharable short-term behaviors of\nprior policies and the long-term behaviors of the task policy, leading to\ncoordinated decisions. Empirical results on a collection of manipulation and\nlocomotion tasks demonstrate that SMEC outperforms existing methods, and\nvalidate the ability of SMEC to leverage related prior policies.",
                "In-Context Analogical Reasoning with Pre-Trained Language Models\nAnalogical reasoning is a fundamental capacity of human cognition that allows\nus to reason abstractly about novel situations by relating them to past\nexperiences. While it is thought to be essential for robust reasoning in AI\nsystems, conventional approaches require significant training and/or\nhard-coding of domain knowledge to be applied to benchmark tasks. Inspired by\ncognitive science research that has found connections between human language\nand analogy-making, we explore the use of intuitive language-based abstractions\nto support analogy in AI systems. Specifically, we apply large pre-trained\nlanguage models (PLMs) to visual Raven's Progressive Matrices (RPM), a common\nrelational reasoning test. By simply encoding the perceptual features of the\nproblem into language form, we find that PLMs exhibit a striking capacity for\nzero-shot relational reasoning, exceeding human performance and nearing\nsupervised vision-based methods. We explore different encodings that vary the\nlevel of abstraction over task features, finding that higher-level abstractions\nfurther strengthen PLMs' analogical reasoning. Our detailed analysis reveals\ninsights on the role of model complexity, in-context learning, and prior\nknowledge in solving RPM tasks.",
                "Modeling Dynamic Environments with Scene Graph Memory\nEmbodied AI agents that search for objects in large environments such as\nhouseholds often need to make efficient decisions by predicting object\nlocations based on partial information. We pose this as a new type of link\nprediction problem: link prediction on partially observable dynamic graphs. Our\ngraph is a representation of a scene in which rooms and objects are nodes, and\ntheir relationships are encoded in the edges; only parts of the changing graph\nare known to the agent at each timestep. This partial observability poses a\nchallenge to existing link prediction approaches, which we address. We propose\na novel state representation -- Scene Graph Memory (SGM) -- with captures the\nagent's accumulated set of observations, as well as a neural net architecture\ncalled a Node Edge Predictor (NEP) that extracts information from the SGM to\nsearch efficiently. We evaluate our method in the Dynamic House Simulator, a\nnew benchmark that creates diverse dynamic graphs following the semantic\npatterns typically seen at homes, and show that NEP can be trained to predict\nthe locations of objects in a variety of environments with diverse object\nmovement dynamics, outperforming baselines both in terms of new scene\nadaptability and overall accuracy. The codebase and more can be found at\nhttps://www.scenegraphmemory.com.",
                "Probing reaction channels via reinforcement learning\nWe propose a reinforcement learning based method to identify important\nconfigurations that connect reactant and product states along chemical reaction\npaths. By shooting multiple trajectories from these configurations, we can\ngenerate an ensemble of configurations that concentrate on the transition path\nensemble. This configuration ensemble can be effectively employed in a neural\nnetwork-based partial differential equation solver to obtain an approximation\nsolution of a restricted Backward Kolmogorov equation, even when the dimension\nof the problem is very high. The resulting solution, known as the committor\nfunction, encodes mechanistic information for the reaction and can in turn be\nused to evaluate reaction rates."
            ],
            "interesting paper": 2
        },
        {
            "papers": [
                "Intelligent gradient amplification for deep neural networks\nDeep learning models offer superior performance compared to other machine\nlearning techniques for a variety of tasks and domains, but pose their own\nchallenges. In particular, deep learning models require larger training times\nas the depth of a model increases, and suffer from vanishing gradients. Several\nsolutions address these problems independently, but there have been minimal\nefforts to identify an integrated solution that improves the performance of a\nmodel by addressing vanishing gradients, as well as accelerates the training\nprocess to achieve higher performance at larger learning rates. In this work,\nwe intelligently determine which layers of a deep learning model to apply\ngradient amplification to, using a formulated approach that analyzes gradient\nfluctuations of layers during training. Detailed experiments are performed for\nsimpler and deeper neural networks using two different intelligent measures and\ntwo different thresholds that determine the amplification layers, and a\ntraining strategy where gradients are amplified only during certain epochs.\nResults show that our amplification offers better performance compared to the\noriginal models, and achieves accuracy improvement of around 2.5% on CIFAR- 10\nand around 4.5% on CIFAR-100 datasets, even when the models are trained with\nhigher learning rates.",
                "Deep Electron Cloud-activity and Field-activity Relationships\nChemists have been pursuing the general mathematical laws to explain and\npredict molecular properties for a long time. However, most of the traditional\nquantitative structure-activity relationship (QSAR) models have limited\napplication domains, e.g., they tend to have poor generalization performance\nwhen applied to molecules with parent structures different from those of the\ntrained molecules. This paper attempts to develop a new QSAR method that is\ntheoretically possible to predict various properties of molecules with diverse\nstructures. The proposed deep electron cloud-activity relationships (DECAR) and\ndeep field-activity relationships (DFAR) methods consist of three essentials:\n(1) A large number of molecule entities with activity data as training objects\nand responses; (2) three-dimensional electron cloud density (ECD) or related\nfield data by the accurate density functional theory methods as input\ndescriptors; (3) a deep learning model that is sufficiently flexible and\npowerful to learn the large data described above. DECAR and DFAR are used to\ndistinguish 977 sweet and 1965 non-sweet molecules (with 6-fold data\naugmentation) and the classification performance is demonstrated to be\nsignificantly better than the traditional least squares support vector machine\n(LS-SVM) models using traditional descriptors. DECAR and DFAR would provide a\npossible way to establish a widely applicable, cumulative, and shareable\nartificial intelligence-driven QSAR system. They are likely to promote the\ndevelopment of an interactive platform to collect and share the accurate ECD\nand field data of millions of molecules with annotated activities. With enough\ninput data, we envision the appearance of several deep networks trained for\nvarious molecular activities. Finally, we could anticipate a single DECAR or\nDFAR network to learn and infer various properties of interest for chemical\nmolecules.",
                "On the impact of activation and normalization in obtaining isometric\n  embeddings at initialization\nIn this paper, we explore the structure of the penultimate Gram matrix in\ndeep neural networks, which contains the pairwise inner products of outputs\ncorresponding to a batch of inputs. In several architectures it has been\nobserved that this Gram matrix becomes degenerate with depth at initialization,\nwhich dramatically slows training. Normalization layers, such as batch or layer\nnormalization, play a pivotal role in preventing the rank collapse issue.\nDespite promising advances, the existing theoretical results do not extend to\nlayer normalization, which is widely used in transformers, and can not\nquantitatively characterize the role of non-linear activations. To bridge this\ngap, we prove that layer normalization, in conjunction with activation layers,\nbiases the Gram matrix of a multilayer perceptron towards the identity matrix\nat an exponential rate with depth at initialization. We quantify this rate\nusing the Hermite expansion of the activation function.",
                "Pedestrian detection with high-resolution event camera\nDespite the dynamic development of computer vision algorithms, the\nimplementation of perception and control systems for autonomous vehicles such\nas drones and self-driving cars still poses many challenges. A video stream\ncaptured by traditional cameras is often prone to problems such as motion blur\nor degraded image quality due to challenging lighting conditions. In addition,\nthe frame rate - typically 30 or 60 frames per second - can be a limiting\nfactor in certain scenarios. Event cameras (DVS -- Dynamic Vision Sensor) are a\npotentially interesting technology to address the above mentioned problems. In\nthis paper, we compare two methods of processing event data by means of deep\nlearning for the task of pedestrian detection. We used a representation in the\nform of video frames, convolutional neural networks and asynchronous sparse\nconvolutional neural networks. The results obtained illustrate the potential of\nevent cameras and allow the evaluation of the accuracy and efficiency of the\nmethods used for high-resolution (1280 x 720 pixels) footage.",
                "Dink-Net: Neural Clustering on Large Graphs\nDeep graph clustering, which aims to group the nodes of a graph into disjoint\nclusters with deep neural networks, has achieved promising progress in recent\nyears. However, the existing methods fail to scale to the large graph with\nmillion nodes. To solve this problem, a scalable deep graph clustering method\n(Dink-Net) is proposed with the idea of dilation and shrink. Firstly, by\ndiscriminating nodes, whether being corrupted by augmentations, representations\nare learned in a self-supervised manner. Meanwhile, the cluster centres are\ninitialized as learnable neural parameters. Subsequently, the clustering\ndistribution is optimized by minimizing the proposed cluster dilation loss and\ncluster shrink loss in an adversarial manner. By these settings, we unify the\ntwo-step clustering, i.e., representation learning and clustering optimization,\ninto an end-to-end framework, guiding the network to learn clustering-friendly\nfeatures. Besides, Dink-Net scales well to large graphs since the designed loss\nfunctions adopt the mini-batch data to optimize the clustering distribution\neven without performance drops. Both experimental results and theoretical\nanalyses demonstrate the superiority of our method. Compared to the runner-up,\nDink-Net achieves 9.62% NMI improvement on the ogbn-papers100M dataset with 111\nmillion nodes and 1.6 billion edges. The source code is released at\nhttps://github.com/yueliu1999/Dink-Net. Besides, a collection (papers, codes,\nand datasets) of deep graph clustering is shared at\nhttps://github.com/yueliu1999/Awesome-Deep-Graph-Clustering.",
                "InDL: A New Dataset and Benchmark for In-Diagram Logic Interpretation\n  based on Visual Illusion\nThis paper introduces a novel approach to evaluating deep learning models'\ncapacity for in-diagram logic interpretation. Leveraging the intriguing realm\nof visual illusions, we establish a unique dataset, InDL, designed to\nrigorously test and benchmark these models. Deep learning has witnessed\nremarkable progress in domains such as computer vision and natural language\nprocessing. However, models often stumble in tasks requiring logical reasoning\ndue to their inherent 'black box' characteristics, which obscure the\ndecision-making process. Our work presents a new lens to understand these\nmodels better by focusing on their handling of visual illusions -- a complex\ninterplay of perception and logic. We utilize six classic geometric optical\nillusions to create a comparative framework between human and machine visual\nperception. This methodology offers a quantifiable measure to rank models,\nelucidating potential weaknesses and providing actionable insights for model\nimprovements. Our experimental results affirm the efficacy of our benchmarking\nstrategy, demonstrating its ability to effectively rank models based on their\nlogic interpretation ability. As part of our commitment to reproducible\nresearch, the source code and datasets will be made publicly available at\nhttps://github.com/rabbit-magic-wh/InDL"
            ],
            "interesting paper": 1
        },
        {
            "papers": [
                "The Digital Divide in Process Safety: Quantitative Risk Analysis of\n  Human-AI Collaboration\nDigital technologies have dramatically accelerated the digital transformation\nin process industries, boosted new industrial applications, upgraded the\nproduction system, and enhanced operational efficiency. In contrast, the\nchallenges and gaps between human and artificial intelligence (AI) have become\nmore and more prominent, whereas the digital divide in process safety is\naggregating. The study attempts to address the following questions: (i)What is\nAI in the process safety context? (ii)What is the difference between AI and\nhumans in process safety? (iii)How do AI and humans collaborate in process\nsafety? (iv)What are the challenges and gaps in human-AI collaboration? (v)How\nto quantify the risk of human-AI collaboration in process safety? Qualitative\nrisk analysis based on brainstorming and literature review, and quantitative\nrisk analysis based on layer of protection analysis (LOPA) and Bayesian network\n(BN), were applied to explore and model. The importance of human reliability\nshould be stressed in the digital age, not usually to increase the reliability\nof AI, and human-centered AI design in process safety needs to be propagated.",
                "IoT based Personal Voice Assistant\nToday, technological advancement is increasing day by day. Earlier, there was\nonly a computer system in which we could only perform a few tasks. But now,\nmachine learning, artificial intelligence, deep learning, and a few more\ntechnologies have made computer systems so advanced that we can perform any\ntype of task. In this era of advancement, if people are still struggling to\ninteract using various input devices, then it's not worth it. For this reason,\nwe developed a voice assistant using Python that allows the user to run any\ntype of command in Linux without interaction with the keyboard. The main task\nof the voice assistant is to minimize the use of input devices like the\nkeyboard and mouse. It will also reduce hardware space and cost.",
                "Visual Affordance Prediction for Guiding Robot Exploration\nMotivated by the intuitive understanding humans have about the space of\npossible interactions, and the ease with which they can generalize this\nunderstanding to previously unseen scenes, we develop an approach for learning\nvisual affordances for guiding robot exploration. Given an input image of a\nscene, we infer a distribution over plausible future states that can be\nachieved via interactions with it. We use a Transformer-based model to learn a\nconditional distribution in the latent embedding space of a VQ-VAE and show\nthat these models can be trained using large-scale and diverse passive data,\nand that the learned models exhibit compositional generalization to diverse\nobjects beyond the training distribution. We show how the trained affordance\nmodel can be used for guiding exploration by acting as a goal-sampling\ndistribution, during visual goal-conditioned policy learning in robotic\nmanipulation.",
                "Taming AI Bots: Controllability of Neural States in Large Language\n  Models\nWe tackle the question of whether an agent can, by suitable choice of\nprompts, control an AI bot to any state. To that end, we first introduce a\nformal definition of ``meaning'' that is amenable to analysis. Then, we\ncharacterize ``meaningful data'' on which large language models (LLMs) are\nostensibly trained, and ``well-trained LLMs'' through conditions that are\nlargely met by today's LLMs. While a well-trained LLM constructs an embedding\nspace of meanings that is Euclidean, meanings themselves do not form a vector\n(linear) subspace, but rather a quotient space within. We then characterize the\nsubset of meanings that can be reached by the state of the LLMs for some input\nprompt, and show that a well-trained bot can reach any meaning albeit with\nsmall probability. We then introduce a stronger notion of controllability as\n{\\em almost certain reachability}, and show that, when restricted to the space\nof meanings, an AI bot is controllable. We do so after introducing a functional\ncharacterization of attentive AI bots, and finally derive necessary and\nsufficient conditions for controllability. The fact that AI bots are\ncontrollable means that an adversary could steer them towards any state.\nHowever, the sampling process can be designed to counteract adverse actions and\navoid reaching undesirable regions of state space before their boundary is\ncrossed.",
                "Re-imagining health and well-being in low resource African settings\n  using an augmented AI system and a 3D digital twin\nThis paper discusses and explores the potential and relevance of recent\ndevelopments in artificial intelligence (AI) and digital twins for health and\nwell-being in low-resource African countries. We use the case of public health\nemergency response to disease outbreaks and epidemic control. There is\npotential to take advantage of the increasing availability of data and\ndigitization to develop advanced AI methods for analysis and prediction. Using\nan AI systems perspective, we review emerging trends in AI systems and digital\ntwins and propose an initial augmented AI system architecture to illustrate how\nan AI system can work with a 3D digital twin to address public health goals. We\nhighlight scientific knowledge discovery, continual learning, pragmatic\ninteroperability, and interactive explanation and decision-making as essential\nresearch challenges for AI systems and digital twins.",
                "Reinforcement Learning with Human Feedback: Learning Dynamic Choices via\n  Pessimism\nIn this paper, we study offline Reinforcement Learning with Human Feedback\n(RLHF) where we aim to learn the human's underlying reward and the MDP's\noptimal policy from a set of trajectories induced by human choices. RLHF is\nchallenging for multiple reasons: large state space but limited human feedback,\nthe bounded rationality of human decisions, and the off-policy distribution\nshift. In this paper, we focus on the Dynamic Discrete Choice (DDC) model for\nmodeling and understanding human choices. DCC, rooted in econometrics and\ndecision theory, is widely used to model a human decision-making process with\nforward-looking and bounded rationality. We propose a\n\\underline{D}ynamic-\\underline{C}hoice-\\underline{P}essimistic-\\underline{P}olicy-\\underline{O}ptimization\n(DCPPO) method. \\ The method involves a three-stage process: The first step is\nto estimate the human behavior policy and the state-action value function via\nmaximum likelihood estimation (MLE); the second step recovers the human reward\nfunction via minimizing Bellman mean squared error using the learned value\nfunctions; the third step is to plug in the learned reward and invoke\npessimistic value iteration for finding a near-optimal policy. With only\nsingle-policy coverage (i.e., optimal policy) of the dataset, we prove that the\nsuboptimality of DCPPO almost matches the classical pessimistic offline RL\nalgorithm in terms of suboptimality's dependency on distribution shift and\ndimension. To the best of our knowledge, this paper presents the first\ntheoretical guarantees for off-policy offline RLHF with dynamic discrete choice\nmodel."
            ],
            "interesting paper": 1
        },
        {
            "papers": [
                "How Does Information Bottleneck Help Deep Learning?\nNumerous deep learning algorithms have been inspired by and understood via\nthe notion of information bottleneck, where unnecessary information is (often\nimplicitly) minimized while task-relevant information is maximized. However, a\nrigorous argument for justifying why it is desirable to control information\nbottlenecks has been elusive. In this paper, we provide the first rigorous\nlearning theory for justifying the benefit of information bottleneck in deep\nlearning by mathematically relating information bottleneck to generalization\nerrors. Our theory proves that controlling information bottleneck is one way to\ncontrol generalization errors in deep learning, although it is not the only or\nnecessary way. We investigate the merit of our new mathematical findings with\nexperiments across a range of architectures and learning settings. In many\ncases, generalization errors are shown to correlate with the degree of\ninformation bottleneck: i.e., the amount of the unnecessary information at\nhidden layers. This paper provides a theoretical foundation for current and\nfuture methods through the lens of information bottleneck. Our new\ngeneralization bounds scale with the degree of information bottleneck, unlike\nthe previous bounds that scale with the number of parameters, VC dimension,\nRademacher complexity, stability or robustness. Our code is publicly available\nat: https://github.com/xu-ji/information-bottleneck",
                "Stochastic Gradient Langevin Dynamics Based on Quantization with\n  Increasing Resolution\nStochastic learning dynamics based on Langevin or Levy stochastic\ndifferential equations (SDEs) in deep neural networks control the variance of\nnoise by varying the size of the mini-batch or directly those of injecting\nnoise. Since the noise variance affects the approximation performance, the\ndesign of the additive noise is significant in SDE-based learning and practical\nimplementation. In this paper, we propose an alternative stochastic descent\nlearning equation based on quantized optimization for non-convex objective\nfunctions, adopting a stochastic analysis perspective. The proposed method\nemploys a quantized optimization approach that utilizes Langevin SDE dynamics,\nallowing for controllable noise with an identical distribution without the need\nfor additive noise or adjusting the mini-batch size. Numerical experiments\ndemonstrate the effectiveness of the proposed algorithm on vanilla convolution\nneural network(CNN) models and the ResNet-50 architecture across various data\nsets. Furthermore, we provide a simple PyTorch implementation of the proposed\nalgorithm.",
                "E-PANNs: Sound Recognition Using Efficient Pre-trained Audio Neural\n  Networks\nSounds carry an abundance of information about activities and events in our\neveryday environment, such as traffic noise, road works, music, or people\ntalking. Recent machine learning methods, such as convolutional neural networks\n(CNNs), have been shown to be able to automatically recognize sound activities,\na task known as audio tagging. One such method, pre-trained audio neural\nnetworks (PANNs), provides a neural network which has been pre-trained on over\n500 sound classes from the publicly available AudioSet dataset, and can be used\nas a baseline or starting point for other tasks. However, the existing PANNs\nmodel has a high computational complexity and large storage requirement. This\ncould limit the potential for deploying PANNs on resource-constrained devices,\nsuch as on-the-edge sound sensors, and could lead to high energy consumption if\nmany such devices were deployed. In this paper, we reduce the computational\ncomplexity and memory requirement of the PANNs model by taking a pruning\napproach to eliminate redundant parameters from the PANNs model. The resulting\nEfficient PANNs (E-PANNs) model, which requires 36\\% less computations and 70\\%\nless memory, also slightly improves the sound recognition (audio tagging)\nperformance. The code for the E-PANNs model has been released under an open\nsource license.",
                "Simulation-Aided Deep Learning for Laser Ultrasonic Visualization\n  Testing\nIn recent years, laser ultrasonic visualization testing (LUVT) has attracted\nmuch attention because of its ability to efficiently perform non-contact\nultrasonic non-destructive testing.Despite many success reports of deep\nlearning based image analysis for widespread areas, attempts to apply deep\nlearning to defect detection in LUVT images face the difficulty of preparing a\nlarge dataset of LUVT images that is too expensive to scale. To compensate for\nthe scarcity of such training data, we propose a data augmentation method that\ngenerates artificial LUVT images by simulation and applies a style transfer to\nsimulated LUVT images.The experimental results showed that the effectiveness of\ndata augmentation based on the style-transformed simulated images improved the\nprediction performance of defects, rather than directly using the raw simulated\nimages for data augmentation.",
                "Vector-based Representation is the Key: A Study on Disentanglement and\n  Compositional Generalization\nRecognizing elementary underlying concepts from observations\n(disentanglement) and generating novel combinations of these concepts\n(compositional generalization) are fundamental abilities for humans to support\nrapid knowledge learning and generalize to new tasks, with which the deep\nlearning models struggle. Towards human-like intelligence, various works on\ndisentangled representation learning have been proposed, and recently some\nstudies on compositional generalization have been presented. However, few works\nstudy the relationship between disentanglement and compositional\ngeneralization, and the observed results are inconsistent. In this paper, we\nstudy several typical disentangled representation learning works in terms of\nboth disentanglement and compositional generalization abilities, and we provide\nan important insight: vector-based representation (using a vector instead of a\nscalar to represent a concept) is the key to empower both good disentanglement\nand strong compositional generalization. This insight also resonates the\nneuroscience research that the brain encodes information in neuron population\nactivity rather than individual neurons. Motivated by this observation, we\nfurther propose a method to reform the scalar-based disentanglement works\n($\\beta$-TCVAE and FactorVAE) to be vector-based to increase both capabilities.\nWe investigate the impact of the dimensions of vector-based representation and\none important question: whether better disentanglement indicates higher\ncompositional generalization. In summary, our study demonstrates that it is\npossible to achieve both good concept recognition and novel concept\ncomposition, contributing an important step towards human-like intelligence.",
                "Towards Machine Learning and Inference for Resource-constrained MCUs\nMachine learning (ML) is moving towards edge devices. However, ML models with\nhigh computational demands and energy consumption pose challenges for ML\ninference in resource-constrained environments, such as the deep sea. To\naddress these challenges, we propose a battery-free ML inference and model\npersonalization pipeline for microcontroller units (MCUs). As an example, we\nperformed fish image recognition in the ocean. We evaluated and compared the\naccuracy, runtime, power, and energy consumption of the model before and after\noptimization. The results demonstrate that, our pipeline can achieve 97.78%\naccuracy with 483.82 KB Flash, 70.32 KB RAM, 118 ms runtime, 4.83 mW power, and\n0.57 mJ energy consumption on MCUs, reducing by 64.17%, 12.31%, 52.42%, 63.74%,\nand 82.67%, compared to the baseline. The results indicate the feasibility of\nbattery-free ML inference on MCUs."
            ],
            "interesting paper": 1
        },
        {
            "papers": [
                "AlphaBlock: Embodied Finetuning for Vision-Language Reasoning in Robot\n  Manipulation\nWe propose a novel framework for learning high-level cognitive capabilities\nin robot manipulation tasks, such as making a smiley face using building\nblocks. These tasks often involve complex multi-step reasoning, presenting\nsignificant challenges due to the limited paired data connecting human\ninstructions (e.g., making a smiley face) and robot actions (e.g., end-effector\nmovement). Existing approaches relieve this challenge by adopting an open-loop\nparadigm decomposing high-level instructions into simple sub-task plans, and\nexecuting them step-by-step using low-level control models. However, these\napproaches are short of instant observations in multi-step reasoning, leading\nto sub-optimal results. To address this issue, we propose to automatically\ncollect a cognitive robot dataset by Large Language Models (LLMs). The\nresulting dataset AlphaBlock consists of 35 comprehensive high-level tasks of\nmulti-step text plans and paired observation sequences. To enable efficient\ndata acquisition, we employ elaborated multi-round prompt designs that\neffectively reduce the burden of extensive human involvement. We further\npropose a closed-loop multi-modal embodied planning model that autoregressively\ngenerates plans by taking image observations as input. To facilitate effective\nlearning, we leverage MiniGPT-4 with a frozen visual encoder and LLM, and\nfinetune additional vision adapter and Q-former to enable fine-grained spatial\nperception for manipulation tasks. We conduct experiments to verify the\nsuperiority over existing open and closed-loop methods, and achieve a\nsignificant increase in success rate by 21.4% and 14.5% over ChatGPT and GPT-4\nbased robot tasks. Real-world demos are shown in\nhttps://www.youtube.com/watch?v=ayAzID1_qQk .",
                "An Emergency Disposal Decision-making Method with Human--Machine\n  Collaboration\nRapid developments in artificial intelligence technology have led to unmanned\nsystems replacing human beings in many fields requiring high-precision\npredictions and decisions. In modern operational environments, all job plans\nare affected by emergency events such as equipment failures and resource\nshortages, making a quick resolution critical. The use of unmanned systems to\nassist decision-making can improve resolution efficiency, but their\ndecision-making is not interpretable and may make the wrong decisions. Current\nunmanned systems require human supervision and control. Based on this, we\npropose a collaborative human--machine method for resolving unplanned events\nusing two phases: task filtering and task scheduling. In the task filtering\nphase, we propose a human--machine collaborative decision-making algorithm for\ndynamic tasks. The GACRNN model is used to predict the state of the job nodes,\nlocate the key nodes, and generate a machine-predicted resolution task list. A\nhuman decision-maker supervises the list in real time and modifies and confirms\nthe machine-predicted list through the human--machine interface. In the task\nscheduling phase, we propose a scheduling algorithm that integrates human\nexperience constraints. The steps to resolve an event are inserted into the\nnormal job sequence to schedule the resolution. We propose several\nhuman--machine collaboration methods in each phase to generate steps to resolve\nan unplanned event while minimizing the impact on the original job plan.",
                "Towards a Unifying Model of Rationality in Multiagent Systems\nMultiagent systems deployed in the real world need to cooperate with other\nagents (including humans) nearly as effectively as these agents cooperate with\none another. To design such AI, and provide guarantees of its effectiveness, we\nneed to clearly specify what types of agents our AI must be able to cooperate\nwith. In this work we propose a generic model of socially intelligent agents,\nwhich are individually rational learners that are also able to cooperate with\none another (in the sense that their joint behavior is Pareto efficient). We\ndefine rationality in terms of the regret incurred by each agent over its\nlifetime, and show how we can construct socially intelligent agents for\ndifferent forms of regret. We then discuss the implications of this model for\nthe development of \"robust\" MAS that can cooperate with a wide variety of\nsocially intelligent agents.",
                "Reason to explain: Interactive contrastive explanations (REASONX)\nMany high-performing machine learning models are not interpretable. As they\nare increasingly used in decision scenarios that can critically affect\nindividuals, it is necessary to develop tools to better understand their\noutputs. Popular explanation methods include contrastive explanations. However,\nthey suffer several shortcomings, among others an insufficient incorporation of\nbackground knowledge, and a lack of interactivity. While (dialogue-like)\ninteractivity is important to better communicate an explanation, background\nknowledge has the potential to significantly improve their quality, e.g., by\nadapting the explanation to the needs of the end-user. To close this gap, we\npresent REASONX, an explanation tool based on Constraint Logic Programming\n(CLP). REASONX provides interactive contrastive explanations that can be\naugmented by background knowledge, and allows to operate under a setting of\nunder-specified information, leading to increased flexibility in the provided\nexplanations. REASONX computes factual and constrative decision rules, as well\nas closest constrative examples. It provides explanations for decision trees,\nwhich can be the ML models under analysis, or global/local surrogate models of\nany ML model. While the core part of REASONX is built on CLP, we also provide a\nprogram layer that allows to compute the explanations via Python, making the\ntool accessible to a wider audience. We illustrate the capability of REASONX on\na synthetic data set, and on a a well-developed example in the credit domain.\nIn both cases, we can show how REASONX can be flexibly used and tailored to the\nneeds of the user.",
                "Contextual Object Detection with Multimodal Large Language Models\nRecent Multimodal Large Language Models (MLLMs) are remarkable in\nvision-language tasks, such as image captioning and question answering, but\nlack the essential perception ability, i.e., object detection. In this work, we\naddress this limitation by introducing a novel research problem of contextual\nobject detection -- understanding visible objects within different human-AI\ninteractive contexts. Three representative scenarios are investigated,\nincluding the language cloze test, visual captioning, and question answering.\nMoreover, we present ContextDET, a unified multimodal model that is capable of\nend-to-end differentiable modeling of visual-language contexts, so as to\nlocate, identify, and associate visual objects with language inputs for\nhuman-AI interaction. Our ContextDET involves three key submodels: (i) a visual\nencoder for extracting visual representations, (ii) a pre-trained LLM for\nmultimodal context decoding, and (iii) a visual decoder for predicting bounding\nboxes given contextual object words. The new generate-then-detect framework\nenables us to detect object words within human vocabulary. Extensive\nexperiments show the advantages of ContextDET on our proposed CODE benchmark,\nopen-vocabulary detection, and referring image segmentation. Github:\nhttps://github.com/yuhangzang/ContextDET.",
                "Code Prompting: a Neural Symbolic Method for Complex Reasoning in Large\n  Language Models\nLarge language models (LLMs) have scaled up to unlock a wide range of complex\nreasoning tasks with the aid of various prompting methods. However, current\nprompting methods generate natural language intermediate steps to help\nreasoning, which can cause imperfect task reduction and confusion. To mitigate\nsuch limitations, we explore code prompting, a neural symbolic prompting method\nwith both zero-shot and few-shot versions which triggers code as intermediate\nsteps. We conduct experiments on 7 widely-used benchmarks involving symbolic\nreasoning and arithmetic reasoning. Code prompting generally outperforms\nchain-of-thought (CoT) prompting. To further understand the performance and\nlimitations of code prompting, we perform extensive ablation studies and error\nanalyses, and identify several exclusive advantages of using symbolic\npromptings compared to natural language. We also consider the ensemble of code\nprompting and CoT prompting to combine the strengths of both. Finally, we show\nthrough experiments how code annotations and their locations affect code\nprompting."
            ],
            "interesting paper": 5
        },
        {
            "papers": [
                "CVSNet: A Computer Implementation for Central Visual System of The Brain\nIn computer vision, different basic blocks are created around different\nmatrix operations, and models based on different basic blocks have achieved\ngood results. Good results achieved in vision tasks grants them rationality.\nHowever, these experimental-based models also make deep learning long\ncriticized for principle and interpretability. Deep learning originated from\nthe concept of neurons in neuroscience, but recent designs detached natural\nneural networks except for some simple concepts. In this paper, we build an\nartificial neural network, CVSNet, which can be seen as a computer\nimplementation for central visual system of the brain. Each block in CVSNet\nrepresents the same vision information as that in brains. In CVSNet, blocks\ndiffers from each other and visual information flows through three independent\npathways and five different blocks. Thus CVSNet is completely different from\nthe design of all previous models, in which basic blocks are repeated to build\nmodel and information between channels is mixed at the outset. In ablation\nexperiment, we show the information extracted by blocks in CVSNet and compare\nwith previous networks, proving effectiveness and rationality of blocks in\nCVSNet from experiment side. And in the experiment of object recognition,\nCVSNet achieves comparable results to ConvNets, Vision Transformers and MLPs.",
                "Are Large Kernels Better Teachers than Transformers for ConvNets?\nThis paper reveals a new appeal of the recently emerged large-kernel\nConvolutional Neural Networks (ConvNets): as the teacher in Knowledge\nDistillation (KD) for small-kernel ConvNets. While Transformers have led\nstate-of-the-art (SOTA) performance in various fields with ever-larger models\nand labeled data, small-kernel ConvNets are considered more suitable for\nresource-limited applications due to the efficient convolution operation and\ncompact weight sharing. KD is widely used to boost the performance of\nsmall-kernel ConvNets. However, previous research shows that it is not quite\neffective to distill knowledge (e.g., global information) from Transformers to\nsmall-kernel ConvNets, presumably due to their disparate architectures. We\nhereby carry out a first-of-its-kind study unveiling that modern large-kernel\nConvNets, a compelling competitor to Vision Transformers, are remarkably more\neffective teachers for small-kernel ConvNets, due to more similar\narchitectures. Our findings are backed up by extensive experiments on both\nlogit-level and feature-level KD ``out of the box\", with no dedicated\narchitectural nor training recipe modifications. Notably, we obtain the\n\\textbf{best-ever pure ConvNet} under 30M parameters with \\textbf{83.1\\%} top-1\naccuracy on ImageNet, outperforming current SOTA methods including ConvNeXt V2\nand Swin V2. We also find that beneficial characteristics of large-kernel\nConvNets, e.g., larger effective receptive fields, can be seamlessly\ntransferred to students through this large-to-small kernel distillation. Code\nis available at: \\url{https://github.com/VITA-Group/SLaK}.",
                "End-to-end Training of Deep Boltzmann Machines by Unbiased Contrastive\n  Divergence with Local Mode Initialization\nWe address the problem of biased gradient estimation in deep Boltzmann\nmachines (DBMs). The existing method to obtain an unbiased estimator uses a\nmaximal coupling based on a Gibbs sampler, but when the state is\nhigh-dimensional, it takes a long time to converge. In this study, we propose\nto use a coupling based on the Metropolis-Hastings (MH) and to initialize the\nstate around a local mode of the target distribution. Because of the propensity\nof MH to reject proposals, the coupling tends to converge in only one step with\na high probability, leading to high efficiency. We find that our method allows\nDBMs to be trained in an end-to-end fashion without greedy pretraining. We also\npropose some practical techniques to further improve the performance of DBMs.\nWe empirically demonstrate that our training algorithm enables DBMs to show\ncomparable generative performance to other deep generative models, achieving\nthe FID score of 10.33 for MNIST.",
                "Adaptation of Tongue Ultrasound-Based Silent Speech Interfaces Using\n  Spatial Transformer Networks\nThanks to the latest deep learning algorithms, silent speech interfaces (SSI)\nare now able to synthesize intelligible speech from articulatory movement data\nunder certain conditions. However, the resulting models are rather\nspeaker-specific, making a quick switch between users troublesome. Even for the\nsame speaker, these models perform poorly cross-session, i.e. after dismounting\nand re-mounting the recording equipment. To aid quick speaker and session\nadaptation of ultrasound tongue imaging-based SSI models, we extend our deep\nnetworks with a spatial transformer network (STN) module, capable of performing\nan affine transformation on the input images. Although the STN part takes up\nonly about 10% of the network, our experiments show that adapting just the STN\nmodule might allow to reduce MSE by 88% on the average, compared to retraining\nthe whole network. The improvement is even larger (around 92%) when adapting\nthe network to different recording sessions from the same speaker.",
                "Diagnosis and Prognosis of Head and Neck Cancer Patients using\n  Artificial Intelligence\nCancer is one of the most life-threatening diseases worldwide, and head and\nneck (H&N) cancer is a prevalent type with hundreds of thousands of new cases\nrecorded each year. Clinicians use medical imaging modalities such as computed\ntomography and positron emission tomography to detect the presence of a tumor,\nand they combine that information with clinical data for patient prognosis. The\nprocess is mostly challenging and time-consuming. Machine learning and deep\nlearning can automate these tasks to help clinicians with highly promising\nresults. This work studies two approaches for H&N tumor segmentation: (i)\nexploration and comparison of vision transformer (ViT)-based and convolutional\nneural network-based models; and (ii) proposal of a novel 2D perspective to\nworking with 3D data. Furthermore, this work proposes two new architectures for\nthe prognosis task. An ensemble of several models predicts patient outcomes\n(which won the HECKTOR 2021 challenge prognosis task), and a ViT-based\nframework concurrently performs patient outcome prediction and tumor\nsegmentation, which outperforms the ensemble model.",
                "KrADagrad: Kronecker Approximation-Domination Gradient Preconditioned\n  Stochastic Optimization\nSecond order stochastic optimizers allow parameter update step size and\ndirection to adapt to loss curvature, but have traditionally required too much\nmemory and compute for deep learning. Recently, Shampoo [Gupta et al., 2018]\nintroduced a Kronecker factored preconditioner to reduce these requirements: it\nis used for large deep models [Anil et al., 2020] and in production [Anil et\nal., 2022]. However, it takes inverse matrix roots of ill-conditioned matrices.\nThis requires 64-bit precision, imposing strong hardware constraints. In this\npaper, we propose a novel factorization, Kronecker Approximation-Domination\n(KrAD). Using KrAD, we update a matrix that directly approximates the inverse\nempirical Fisher matrix (like full matrix AdaGrad), avoiding inversion and\nhence 64-bit precision. We then propose KrADagrad$^\\star$, with similar\ncomputational costs to Shampoo and the same regret. Synthetic ill-conditioned\nexperiments show improved performance over Shampoo for 32-bit precision, while\nfor several real datasets we have comparable or better generalization."
            ],
            "interesting paper": 2
        },
        {
            "papers": [
                "Intent-aligned AI systems deplete human agency: the need for agency\n  foundations research in AI safety\nThe rapid advancement of artificial intelligence (AI) systems suggests that\nartificial general intelligence (AGI) systems may soon arrive. Many researchers\nare concerned that AIs and AGIs will harm humans via intentional misuse\n(AI-misuse) or through accidents (AI-accidents). In respect of AI-accidents,\nthere is an increasing effort focused on developing algorithms and paradigms\nthat ensure AI systems are aligned to what humans intend, e.g. AI systems that\nyield actions or recommendations that humans might judge as consistent with\ntheir intentions and goals. Here we argue that alignment to human intent is\ninsufficient for safe AI systems and that preservation of long-term agency of\nhumans may be a more robust standard, and one that needs to be separated\nexplicitly and a priori during optimization. We argue that AI systems can\nreshape human intention and discuss the lack of biological and psychological\nmechanisms that protect humans from loss of agency. We provide the first formal\ndefinition of agency-preserving AI-human interactions which focuses on\nforward-looking agency evaluations and argue that AI systems - not humans -\nmust be increasingly tasked with making these evaluations. We show how agency\nloss can occur in simple environments containing embedded agents that use\ntemporal-difference learning to make action recommendations. Finally, we\npropose a new area of research called \"agency foundations\" and pose four\ninitial topics designed to improve our understanding of agency in AI-human\ninteractions: benevolent game theory, algorithmic foundations of human rights,\nmechanistic interpretability of agency representation in neural-networks and\nreinforcement learning from internal states.",
                "A Computational Account Of Self-Supervised Visual Learning From\n  Egocentric Object Play\nResearch in child development has shown that embodied experience handling\nphysical objects contributes to many cognitive abilities, including visual\nlearning. One characteristic of such experience is that the learner sees the\nsame object from several different viewpoints. In this paper, we study how\nlearning signals that equate different viewpoints -- e.g., assigning similar\nrepresentations to different views of a single object -- can support robust\nvisual learning. We use the Toybox dataset, which contains egocentric videos of\nhumans manipulating different objects, and conduct experiments using a computer\nvision framework for self-supervised contrastive learning. We find that\nrepresentations learned by equating different physical viewpoints of an object\nbenefit downstream image classification accuracy. Further experiments show that\nthis performance improvement is robust to variations in the gaps between\nviewpoints, and that the benefits transfer to several different image\nclassification tasks.",
                "Strategic Reasoning with Language Models\nStrategic reasoning enables agents to cooperate, communicate, and compete\nwith other agents in diverse situations. Existing approaches to solving\nstrategic games rely on extensive training, yielding strategies that do not\ngeneralize to new scenarios or games without retraining. Large Language Models\n(LLMs), with their ability to comprehend and generate complex, context-rich\nlanguage, could prove powerful as tools for strategic gameplay. This paper\nintroduces an approach that uses pretrained LLMs with few-shot chain-of-thought\nexamples to enable strategic reasoning for AI agents. Our approach uses\nsystematically generated demonstrations of reasoning about states, values, and\nbeliefs to prompt the model. Using extensive variations of simple matrix games,\nwe show that strategies that are derived based on systematically generated\nprompts generalize almost perfectly to new game structures, alternate\nobjectives, and hidden information. Additionally, we demonstrate our approach\ncan lead to human-like negotiation strategies in realistic scenarios without\nany extra training or fine-tuning. Our results highlight the ability of LLMs,\nguided by systematic reasoning demonstrations, to adapt and excel in diverse\nstrategic scenarios.",
                "Less Likely Brainstorming: Using Language Models to Generate Alternative\n  Hypotheses\nA human decision-maker benefits the most from an AI assistant that corrects\nfor their biases. For problems such as generating interpretation of a radiology\nreport given findings, a system predicting only highly likely outcomes may be\nless useful, where such outcomes are already obvious to the user. To alleviate\nbiases in human decision-making, it is worth considering a broad differential\ndiagnosis, going beyond the most likely options. We introduce a new task, \"less\nlikely brainstorming,\" that asks a model to generate outputs that humans think\nare relevant but less likely to happen. We explore the task in two settings: a\nbrain MRI interpretation generation setting and an everyday commonsense\nreasoning setting. We found that a baseline approach of training with less\nlikely hypotheses as targets generates outputs that humans evaluate as either\nlikely or irrelevant nearly half of the time; standard MLE training is not\neffective. To tackle this problem, we propose a controlled text generation\nmethod that uses a novel contrastive learning strategy to encourage models to\ndifferentiate between generating likely and less likely outputs according to\nhumans. We compare our method with several state-of-the-art controlled text\ngeneration models via automatic and human evaluations and show that our models'\ncapability of generating less likely outputs is improved.",
                "Bigger, Better, Faster: Human-level Atari with human-level efficiency\nWe introduce a value-based RL agent, which we call BBF, that achieves\nsuper-human performance in the Atari 100K benchmark. BBF relies on scaling the\nneural networks used for value estimation, as well as a number of other design\nchoices that enable this scaling in a sample-efficient manner. We conduct\nextensive analyses of these design choices and provide insights for future\nwork. We end with a discussion about updating the goalposts for\nsample-efficient RL research on the ALE. We make our code and data publicly\navailable at\nhttps://github.com/google-research/google-research/tree/master/bigger_better_faster.",
                "Language-Conditioned Imitation Learning with Base Skill Priors under\n  Unstructured Data\nThe growing interest in language-conditioned robot manipulation aims to\ndevelop robots capable of understanding and executing complex tasks, with the\nobjective of enabling robots to interpret language commands and manipulate\nobjects accordingly. While language-conditioned approaches demonstrate\nimpressive capabilities for addressing tasks in familiar environments, they\nencounter limitations in adapting to unfamiliar environment settings. In this\nstudy, we propose a general-purpose, language-conditioned approach that\ncombines base skill priors and imitation learning under unstructured data to\nenhance the algorithm's generalization in adapting to unfamiliar environments.\nWe assess our model's performance in both simulated and real-world environments\nusing a zero-shot setting. In the simulated environment, the proposed approach\nsurpasses previously reported scores for CALVIN benchmark, especially in the\nchallenging Zero-Shot Multi-Environment setting. The average completed task\nlength, indicating the average number of tasks the agent can continuously\ncomplete, improves more than 2.5 times compared to the state-of-the-art method\nHULC. In addition, we conduct a zero-shot evaluation of our policy in a\nreal-world setting, following training exclusively in simulated environments\nwithout additional specific adaptations. In this evaluation, we set up ten\ntasks and achieved an average 30% improvement in our approach compared to the\ncurrent state-of-the-art approach, demonstrating a high generalization\ncapability in both simulated environments and the real world. For further\ndetails, including access to our code and videos, please refer to\nhttps://hk-zh.github.io/spil/"
            ],
            "interesting paper": 1
        },
        {
            "papers": [
                "Breast Cancer Detection and Diagnosis: A comparative study of\n  state-of-the-arts deep learning architectures\nBreast cancer is a prevalent form of cancer among women, with over 1.5\nmillion women being diagnosed each year. Unfortunately, the survival rates for\nbreast cancer patients in certain third-world countries, like South Africa, are\nalarmingly low, with only 40% of diagnosed patients surviving beyond five\nyears. The inadequate availability of resources, including qualified\npathologists, delayed diagnoses, and ineffective therapy planning, contribute\nto this low survival rate. To address this pressing issue, medical specialists\nand researchers have turned to domain-specific AI approaches, specifically deep\nlearning models, to develop end-to-end solutions that can be integrated into\ncomputer-aided diagnosis (CAD) systems. By improving the workflow of\npathologists, these AI models have the potential to enhance the detection and\ndiagnosis of breast cancer. This research focuses on evaluating the performance\nof various cutting-edge convolutional neural network (CNN) architectures in\ncomparison to a relatively new model called the Vision Trans-former (ViT). The\nobjective is to determine the superiority of these models in terms of their\naccuracy and effectiveness. The experimental results reveal that the ViT models\noutperform the other selected state-of-the-art CNN architectures, achieving an\nimpressive accuracy rate of 95.15%. This study signifies a significant\nadvancement in the field, as it explores the utilization of data augmentation\nand other relevant preprocessing techniques in conjunction with deep learning\nmodels for the detection and diagnosis of breast cancer using datasets of\nBreast Cancer Histopathological Image Classification.",
                "Sea Ice Extraction via Remote Sensed Imagery: Algorithms, Datasets,\n  Applications and Challenges\nThe deep learning, which is a dominating technique in artificial\nintelligence, has completely changed the image understanding over the past\ndecade. As a consequence, the sea ice extraction (SIE) problem has reached a\nnew era. We present a comprehensive review of four important aspects of SIE,\nincluding algorithms, datasets, applications, and the future trends. Our review\nfocuses on researches published from 2016 to the present, with a specific focus\non deep learning-based approaches in the last five years. We divided all\nrelegated algorithms into 3 categories, including classical image segmentation\napproach, machine learning-based approach and deep learning-based methods. We\nreviewed the accessible ice datasets including SAR-based datasets, the\noptical-based datasets and others. The applications are presented in 4 aspects\nincluding climate research, navigation, geographic information systems (GIS)\nproduction and others. It also provides insightful observations and inspiring\nfuture research directions.",
                "Training-free Neural Architecture Search for RNNs and Transformers\nNeural architecture search (NAS) has allowed for the automatic creation of\nnew and effective neural network architectures, offering an alternative to the\nlaborious process of manually designing complex architectures. However,\ntraditional NAS algorithms are slow and require immense amounts of computing\npower. Recent research has investigated training-free NAS metrics for image\nclassification architectures, drastically speeding up search algorithms. In\nthis paper, we investigate training-free NAS metrics for recurrent neural\nnetwork (RNN) and BERT-based transformer architectures, targeted towards\nlanguage modeling tasks. First, we develop a new training-free metric, named\nhidden covariance, that predicts the trained performance of an RNN architecture\nand significantly outperforms existing training-free metrics. We experimentally\nevaluate the effectiveness of the hidden covariance metric on the NAS-Bench-NLP\nbenchmark. Second, we find that the current search space paradigm for\ntransformer architectures is not optimized for training-free neural\narchitecture search. Instead, a simple qualitative analysis can effectively\nshrink the search space to the best performing architectures. This conclusion\nis based on our investigation of existing training-free metrics and new metrics\ndeveloped from recent transformer pruning literature, evaluated on our own\nbenchmark of trained BERT architectures. Ultimately, our analysis shows that\nthe architecture search space and the training-free metric must be developed\ntogether in order to achieve effective results.",
                "Diffused Redundancy in Pre-trained Representations\nRepresentations learned by pre-training a neural network on a large dataset\nare increasingly used successfully to perform a variety of downstream tasks. In\nthis work, we take a closer look at how features are encoded in such\npre-trained representations. We find that learned representations in a given\nlayer exhibit a degree of diffuse redundancy, ie, any randomly chosen subset of\nneurons in the layer that is larger than a threshold size shares a large degree\nof similarity with the full layer and is able to perform similarly as the whole\nlayer on a variety of downstream tasks. For example, a linear probe trained on\n$20\\%$ of randomly picked neurons from the penultimate layer of a ResNet50\npre-trained on ImageNet1k achieves an accuracy within $5\\%$ of a linear probe\ntrained on the full layer of neurons for downstream CIFAR10 classification. We\nconduct experiments on different neural architectures (including CNNs and\nTransformers) pre-trained on both ImageNet1k and ImageNet21k and evaluate a\nvariety of downstream tasks taken from the VTAB benchmark. We find that the\nloss and dataset used during pre-training largely govern the degree of diffuse\nredundancy and the \"critical mass\" of neurons needed often depends on the\ndownstream task, suggesting that there is a task-inherent\nredundancy-performance Pareto frontier. Our findings shed light on the nature\nof representations learned by pre-trained deep neural networks and suggest that\nentire layers might not be necessary to perform many downstream tasks. We\ninvestigate the potential for exploiting this redundancy to achieve efficient\ngeneralization for downstream tasks and also draw caution to certain possible\nunintended consequences. Our code is available at\n\\url{https://github.com/nvedant07/diffused-redundancy}.",
                "Adam Accumulation to Reduce Memory Footprints of both Activations and\n  Gradients for Large-scale DNN Training\nRunning out of GPU memory has become a main bottleneck for large-scale DNN\ntraining. How to reduce the memory footprint during training has received\nintensive research attention. We find that previous gradient accumulation\nreduces activation memory but fails to be compatible with gradient memory\nreduction due to a contradiction between preserving gradients and releasing\ngradients. To address this issue, we propose a novel optimizer accumulation\nmethod for Adam, named Adam Accumulation (AdamA), which enables reducing both\nactivation and gradient memory. Specifically, AdamA directly integrates\ngradients into optimizer states and accumulates optimizer states over\nmicro-batches, so that gradients can be released immediately after use. We\nmathematically and experimentally demonstrate AdamA yields the same convergence\nproperties as Adam. Evaluated on transformer-based models, AdamA achieves up to\n23% memory reduction compared to gradient accumulation with less than 2%\ndegradation in training throughput. Notably, AdamA can work together with\nmemory reduction methods for optimizer states to fit 1.26x~3.14x larger models\nover PyTorch and DeepSpeed baseline on GPUs with different memory capacities.",
                "Transformers learn to implement preconditioned gradient descent for\n  in-context learning\nSeveral recent works demonstrate that transformers can implement algorithms\nlike gradient descent. By a careful construction of weights, these works show\nthat multiple layers of transformers are expressive enough to simulate\niterations of gradient descent. Going beyond the question of expressivity, we\nask: Can transformers learn to implement such algorithms by training over\nrandom problem instances? To our knowledge, we make the first theoretical\nprogress on this question via an analysis of the loss landscape for linear\ntransformers trained over random instances of linear regression. For a single\nattention layer, we prove the global minimum of the training objective\nimplements a single iteration of preconditioned gradient descent. Notably, the\npreconditioning matrix not only adapts to the input distribution but also to\nthe variance induced by data inadequacy. For a transformer with $L$ attention\nlayers, we prove certain critical points of the training objective implement\n$L$ iterations of preconditioned gradient descent. Our results call for future\ntheoretical studies on learning algorithms by training transformers."
            ],
            "interesting paper": 3
        },
        {
            "papers": [
                "Human Control: Definitions and Algorithms\nHow can humans stay in control of advanced artificial intelligence systems?\nOne proposal is corrigibility, which requires the agent to follow the\ninstructions of a human overseer, without inappropriately influencing them. In\nthis paper, we formally define a variant of corrigibility called shutdown\ninstructability, and show that it implies appropriate shutdown behavior,\nretention of human autonomy, and avoidance of user harm. We also analyse the\nrelated concepts of non-obstruction and shutdown alignment, three previously\nproposed algorithms for human control, and one new algorithm.",
                "Human or Not? A Gamified Approach to the Turing Test\nWe present \"Human or Not?\", an online game inspired by the Turing test, that\nmeasures the capability of AI chatbots to mimic humans in dialog, and of humans\nto tell bots from other humans. Over the course of a month, the game was played\nby over 1.5 million users who engaged in anonymous two-minute chat sessions\nwith either another human or an AI language model which was prompted to behave\nlike humans. The task of the players was to correctly guess whether they spoke\nto a person or to an AI. This largest scale Turing-style test conducted to date\nrevealed some interesting facts. For example, overall users guessed the\nidentity of their partners correctly in only 68% of the games. In the subset of\nthe games in which users faced an AI bot, users had even lower correct guess\nrates of 60% (that is, not much higher than chance). This white paper details\nthe development, deployment, and results of this unique experiment. While this\nexperiment calls for many extensions and refinements, these findings already\nbegin to shed light on the inevitable near future which will commingle humans\nand AI.",
                "Decision-Oriented Dialogue for Human-AI Collaboration\nWe describe a class of tasks called decision-oriented dialogues, in which AI\nassistants such as large language models (LMs) must collaborate with one or\nmore humans via natural language to help them make complex decisions. We\nformalize three domains in which users face everyday decisions: (1) choosing an\nassignment of reviewers to conference papers, (2) planning a multi-step\nitinerary in a city, and (3) negotiating travel plans for a group of friends.\nIn each of these settings, AI assistants and users have disparate abilities\nthat they must combine to arrive at the best decision: assistants can access\nand process large amounts of information, while users have preferences and\nconstraints external to the system. For each task, we build a dialogue\nenvironment where agents receive a reward based on the quality of the final\ndecision they reach. We evaluate LMs in self-play and in collaboration with\nhumans and find that they fall short compared to human assistants, achieving\nmuch lower rewards despite engaging in longer dialogues. We highlight a number\nof challenges models face in decision-oriented dialogues, ranging from\ngoal-directed behavior to reasoning and optimization, and release our\nenvironments as a testbed for future work.",
                "From Human-Centered to Social-Centered Artificial Intelligence:\n  Assessing ChatGPT's Impact through Disruptive Events\nLarge language models (LLMs) and dialogue agents have existed for years, but\nthe release of recent GPT models has been a watershed moment for artificial\nintelligence (AI) research and society at large. Immediately recognized for its\ngenerative capabilities and versatility, ChatGPT's impressive proficiency\nacross technical and creative domains led to its widespread adoption. While\nsociety grapples with the emerging cultural impacts of ChatGPT, critiques of\nChatGPT's impact within the machine learning community have coalesced around\nits performance or other conventional Responsible AI evaluations relating to\nbias, toxicity, and 'hallucination.' We argue that these latter critiques draw\nheavily on a particular conceptualization of the 'human-centered' framework,\nwhich tends to cast atomized individuals as the key recipients of both the\nbenefits and detriments of technology. In this article, we direct attention to\nanother dimension of LLMs and dialogue agents' impact: their effect on social\ngroups, institutions, and accompanying norms and practices. By illustrating\nChatGPT's social impact through three disruptive events, we challenge\nindividualistic approaches in AI development and contribute to ongoing debates\naround the ethical and responsible implementation of AI systems. We hope this\neffort will call attention to more comprehensive and longitudinal evaluation\ntools and compel technologists to go beyond human-centered thinking and ground\ntheir efforts through social-centered AI.",
                "EMOTE: An Explainable architecture for Modelling the Other Through\n  Empathy\nWe can usually assume others have goals analogous to our own. This assumption\ncan also, at times, be applied to multi-agent games - e.g. Agent 1's attraction\nto green pellets is analogous to Agent 2's attraction to red pellets. This\n\"analogy\" assumption is tied closely to the cognitive process known as empathy.\nInspired by empathy, we design a simple and explainable architecture to model\nanother agent's action-value function. This involves learning an \"Imagination\nNetwork\" to transform the other agent's observed state in order to produce a\nhuman-interpretable \"empathetic state\" which, when presented to the learning\nagent, produces behaviours that mimic the other agent. Our approach is\napplicable to multi-agent scenarios consisting of a single learning agent and\nother (independent) agents acting according to fixed policies. This\narchitecture is particularly beneficial for (but not limited to) algorithms\nusing a composite value or reward function. We show our method produces better\nperformance in multi-agent games, where it robustly estimates the other's model\nin different environment configurations. Additionally, we show that the\nempathetic states are human interpretable, and thus verifiable.",
                "Speaking the Language of Your Listener: Audience-Aware Adaptation via\n  Plug-and-Play Theory of Mind\nDialogue participants may have varying levels of knowledge about the topic\nunder discussion. In such cases, it is essential for speakers to adapt their\nutterances by taking their audience into account. Yet, it is an open question\nhow such adaptation can be modelled in computational agents. In this paper, we\nmodel a visually grounded referential game between a knowledgeable speaker and\na listener with more limited visual and linguistic experience. Inspired by\npsycholinguistic theories, we endow our speaker with the ability to adapt its\nreferring expressions via a simulation module that monitors the effectiveness\nof planned utterances from the listener's perspective. We propose an adaptation\nmechanism building on plug-and-play approaches to controlled language\ngeneration, where utterance generation is steered on the fly by the simulator\nwithout finetuning the speaker's underlying language model. Our results and\nanalyses show that our approach is effective: the speaker's utterances become\ncloser to the listener's domain of expertise, which leads to higher\ncommunicative success."
            ],
            "interesting paper": 2
        },
        {
            "papers": [
                "On the Weight Dynamics of Deep Normalized Networks\nRecent studies have shown that high disparities in effective learning rates\n(ELRs) across layers in deep neural networks can negatively affect\ntrainability. We formalize how these disparities evolve over time by modeling\nweight dynamics (evolution of expected gradient and weight norms) of networks\nwith normalization layers, predicting the evolution of layer-wise ELR ratios.\nWe prove that when training with any constant learning rate, ELR ratios\nconverge to 1, despite initial gradient explosion. We identify a ``critical\nlearning rate\" beyond which ELR disparities widen, which only depends on\ncurrent ELRs. To validate our findings, we devise a hyper-parameter-free\nwarm-up method that successfully minimizes ELR spread quickly in theory and\npractice. Our experiments link ELR spread with trainability, a relationship\nthat is most evident in very deep networks with significant gradient magnitude\nexcursions.",
                "Smooth Min-Max Monotonic Networks\nMonotonicity constraints are powerful regularizers in statistical modelling.\nThey can support fairness in computer-aided decision making and increase\nplausibility in data-driven scientific models. The seminal min-max (MM) neural\nnetwork architecture ensures monotonicity, but often gets stuck in undesired\nlocal optima during training because of partial derivatives of the MM\nnonlinearities being zero. We propose a simple modification of the MM network\nusing strictly-increasing smooth minimum and maximum functions that alleviates\nthis problem. The resulting smooth min-max (SMM) network module inherits the\nasymptotic approximation properties from the MM architecture. It can be used\nwithin larger deep learning systems trained end-to-end. The SMM module is\nconceptually simple and computationally less demanding than state-of-the-art\nneural networks for monotonic modelling. Our experiments show that this does\nnot come with a loss in generalization performance compared to alternative\nneural and non-neural approaches.",
                "Adaptation and Optimization of Automatic Speech Recognition (ASR) for\n  the Maritime Domain in the Field of VHF Communication\nThis paper introduces a multilingual automatic speech recognizer (ASR) for\nmaritime radio communi-cation that automatically converts received VHF radio\nsignals into text. The challenges of maritime radio communication are described\nat first, and the deep learning architecture of marFM consisting of audio\nprocessing techniques and machine learning algorithms is presented.\nSubsequently, maritime radio data of interest is analyzed and then used to\nevaluate the transcription performance of our ASR model for various maritime\nradio data.",
                "Rotating Features for Object Discovery\nThe binding problem in human cognition, concerning how the brain represents\nand connects objects within a fixed network of neural connections, remains a\nsubject of intense debate. Most machine learning efforts addressing this issue\nin an unsupervised setting have focused on slot-based methods, which may be\nlimiting due to their discrete nature and difficulty to express uncertainty.\nRecently, the Complex AutoEncoder was proposed as an alternative that learns\ncontinuous and distributed object-centric representations. However, it is only\napplicable to simple toy data. In this paper, we present Rotating Features, a\ngeneralization of complex-valued features to higher dimensions, and a new\nevaluation procedure for extracting objects from distributed representations.\nAdditionally, we show the applicability of our approach to pre-trained\nfeatures. Together, these advancements enable us to scale distributed\nobject-centric representations from simple toy to real-world data. We believe\nthis work advances a new paradigm for addressing the binding problem in machine\nlearning and has the potential to inspire further innovation in the field.",
                "The Galerkin method beats Graph-Based Approaches for Spectral Algorithms\nHistorically, the machine learning community has derived spectral\ndecompositions from graph-based approaches. We break with this approach and\nprove the statistical and computational superiority of the Galerkin method,\nwhich consists in restricting the study to a small set of test functions. In\nparticular, we introduce implementation tricks to deal with differential\noperators in large dimensions with structured kernels. Finally, we extend on\nthe core principles beyond our approach to apply them to non-linear spaces of\nfunctions, such as the ones parameterized by deep neural networks, through\nloss-based optimization procedures.",
                "Microstructure quality control of steels using deep learning\nIn quality control, microstructures are investigated rigorously to ensure\nstructural integrity, exclude the presence of critical volume defects, and\nvalidate the formation of the target microstructure. For quenched,\nhierarchically-structured steels, the morphology of the bainitic and\nmartensitic microstructures are of major concern to guarantee the reliability\nof the material under service conditions. Therefore, industries conduct small\nsample-size inspections of materials cross-sections through metallographers to\nvalidate the needle morphology of such microstructures. We demonstrate\nround-robin test results revealing that this visual grading is afflicted by\npronounced subjectivity despite the thorough training of personnel. Instead, we\npropose a deep learning image classification approach that distinguishes steels\nbased on their microstructure type and classifies their needle length alluding\nto the ISO 643 grain size assessment standard. This classification approach\nfacilitates the reliable, objective, and automated classification of\nhierarchically structured steels. Specifically, an accuracy of 96% and roughly\n91% is attained for the distinction of martensite/bainite subtypes and needle\nlength, respectively. This is achieved on an image dataset that contains\nsignificant variance and labeling noise as it is acquired over more than ten\nyears from multiple plants, alloys, etchant applications, and light optical\nmicroscopes by many metallographers (raters). Interpretability analysis gives\ninsights into the decision-making of these models and allows for estimating\ntheir generalization capability."
            ],
            "interesting paper": 4
        },
        {
            "papers": [
                "Egocentric Planning for Scalable Embodied Task Achievement\nEmbodied agents face significant challenges when tasked with performing\nactions in diverse environments, particularly in generalizing across object\ntypes and executing suitable actions to accomplish tasks. Furthermore, agents\nshould exhibit robustness, minimizing the execution of illegal actions. In this\nwork, we present Egocentric Planning, an innovative approach that combines\nsymbolic planning and Object-oriented POMDPs to solve tasks in complex\nenvironments, harnessing existing models for visual perception and natural\nlanguage processing. We evaluated our approach in ALFRED, a simulated\nenvironment designed for domestic tasks, and demonstrated its high scalability,\nachieving an impressive 36.07% unseen success rate in the ALFRED benchmark and\nwinning the ALFRED challenge at CVPR Embodied AI workshop. Our method requires\nreliable perception and the specification or learning of a symbolic description\nof the preconditions and effects of the agent's actions, as well as what object\ntypes reveal information about others. It is capable of naturally scaling to\nsolve new tasks beyond ALFRED, as long as they can be solved using the\navailable skills. This work offers a solid baseline for studying end-to-end and\nhybrid methods that aim to generalize to new tasks, including recent approaches\nrelying on LLMs, but often struggle to scale to long sequences of actions or\nproduce robust plans for novel tasks.",
                "AI and the creative realm: A short review of current and future\n  applications\nThis study explores the concept of creativity and artificial intelligence\n(AI) and their recent integration. While AI has traditionally been perceived as\nincapable of generating new ideas or creating art, the development of more\nsophisticated AI models and the proliferation of human-computer interaction\ntools have opened up new possibilities for AI in artistic creation. This study\ninvestigates the various applications of AI in a creative context,\ndifferentiating between the type of art, language, and algorithms used. It also\nconsiders the philosophical implications of AI and creativity, questioning\nwhether consciousness can be researched in machines and AI's potential\ninterests and decision-making capabilities. Overall, we aim to stimulate a\nreflection on AI's use and ethical implications in creative contexts.",
                "AI Liability Insurance With an Example in AI-Powered E-diagnosis System\nArtificial Intelligence (AI) has received an increasing amount of attention\nin multiple areas. The uncertainties and risks in AI-powered systems have\ncreated reluctance in their wild adoption. As an economic solution to\ncompensate for potential damages, AI liability insurance is a promising market\nto enhance the integration of AI into daily life. In this work, we use an\nAI-powered E-diagnosis system as an example to study AI liability insurance. We\nprovide a quantitative risk assessment model with evidence-based numerical\nanalysis. We discuss the insurability criteria for AI technologies and suggest\nnecessary adjustments to accommodate the features of AI products. We show that\nAI liability insurance can act as a regulatory mechanism to incentivize\ncompliant behaviors and serve as a certificate of high-quality AI systems.\nFurthermore, we suggest premium adjustment to reflect the dynamic evolution of\nthe inherent uncertainty in AI. Moral hazard problems are discussed and\nsuggestions for AI liability insurance are provided.",
                "STEVE-1: A Generative Model for Text-to-Behavior in Minecraft\nConstructing AI models that respond to text instructions is challenging,\nespecially for sequential decision-making tasks. This work introduces a\nmethodology, inspired by unCLIP, for instruction-tuning generative models of\nbehavior without relying on a large dataset of instruction-labeled\ntrajectories. Using this methodology, we create an instruction-tuned Video\nPretraining (VPT) model called STEVE-1, which can follow short-horizon\nopen-ended text and visual instructions in Minecraft. STEVE-1 is trained in two\nsteps: adapting the pretrained VPT model to follow commands in MineCLIP's\nlatent space, then training a prior to predict latent codes from text. This\nallows us to finetune VPT through self-supervised behavioral cloning and\nhindsight relabeling, reducing the need for costly human text annotations, and\nall for only $60 of compute. By leveraging pretrained models like VPT and\nMineCLIP and employing best practices from text-conditioned image generation,\nSTEVE-1 sets a new bar for open-ended instruction-following in Minecraft with\nlow-level controls (mouse and keyboard) and raw pixel inputs, far outperforming\nprevious baselines and robustly completing 12 of 13 tasks in our early-game\nevaluation suite. We provide experimental evidence highlighting key factors for\ndownstream performance, including pretraining, classifier-free guidance, and\ndata scaling. All resources, including our model weights, training scripts, and\nevaluation tools are made available for further research.",
                "The ethical ambiguity of AI data enrichment: Measuring gaps in research\n  ethics norms and practices\nThe technical progression of artificial intelligence (AI) research has been\nbuilt on breakthroughs in fields such as computer science, statistics, and\nmathematics. However, in the past decade AI researchers have increasingly\nlooked to the social sciences, turning to human interactions to solve the\nchallenges of model development. Paying crowdsourcing workers to generate or\ncurate data, or data enrichment, has become indispensable for many areas of AI\nresearch, from natural language processing to reinforcement learning from human\nfeedback (RLHF). Other fields that routinely interact with crowdsourcing\nworkers, such as Psychology, have developed common governance requirements and\nnorms to ensure research is undertaken ethically. This study explores how, and\nto what extent, comparable research ethics requirements and norms have\ndeveloped for AI research and data enrichment. We focus on the approach taken\nby two leading conferences: ICLR and NeurIPS, and journal publisher Springer.\nIn a longitudinal study of accepted papers, and via a comparison with\nPsychology and CHI papers, this work finds that leading AI venues have begun to\nestablish protocols for human data collection, but these are are inconsistently\nfollowed by authors. Whilst Psychology papers engaging with crowdsourcing\nworkers frequently disclose ethics reviews, payment data, demographic data and\nother information, similar disclosures are far less common in leading AI venues\ndespite similar guidance. The work concludes with hypotheses to explain these\ngaps in research ethics practices and considerations for its implications.",
                "LIV: Language-Image Representations and Rewards for Robotic Control\nWe present Language-Image Value learning (LIV), a unified objective for\nvision-language representation and reward learning from action-free videos with\ntext annotations. Exploiting a novel connection between dual reinforcement\nlearning and mutual information contrastive learning, the LIV objective trains\na multi-modal representation that implicitly encodes a universal value function\nfor tasks specified as language or image goals. We use LIV to pre-train the\nfirst control-centric vision-language representation from large human video\ndatasets such as EpicKitchen. Given only a language or image goal, the\npre-trained LIV model can assign dense rewards to each frame in videos of\nunseen robots or humans attempting that task in unseen environments. Further,\nwhen some target domain-specific data is available, the same objective can be\nused to fine-tune and improve LIV and even other pre-trained representations\nfor robotic control and reward specification in that domain. In our experiments\non several simulated and real-world robot environments, LIV models consistently\noutperform the best prior input state representations for imitation learning,\nas well as reward specification methods for policy synthesis. Our results\nvalidate the advantages of joint vision-language representation and reward\nlearning within the unified, compact LIV framework."
            ],
            "interesting paper": 2
        },
        {
            "papers": [
                "A Novel Correlation-optimized Deep Learning Method for Wind Speed\n  Forecast\nThe increasing installation rate of wind power poses great challenges to the\nglobal power system. In order to ensure the reliable operation of the power\nsystem, it is necessary to accurately forecast the wind speed and power of the\nwind turbines. At present, deep learning is progressively applied to the wind\nspeed prediction. Nevertheless, the recent deep learning methods still reflect\nthe embarrassment for practical applications due to model interpretability and\nhardware limitation. To this end, a novel deep knowledge-based learning method\nis proposed in this paper. The proposed method hybridizes pre-training method\nand auto-encoder structure to improve data representation and modeling of the\ndeep knowledge-based learning framework. In order to form knowledge and\ncorresponding absorbers, the original data is preprocessed by an optimization\nmodel based on correlation to construct multi-layer networks (knowledge) which\nare absorbed by sequence to sequence (Seq2Seq) models. Specifically, new\ncognition and memory units (CMU) are designed to reinforce traditional deep\nlearning framework. Finally, the effectiveness of the proposed method is\nverified by three wind prediction cases from a wind farm in Liaoning, China.\nExperimental results show that the proposed method increases the stability and\ntraining efficiency compared to the traditional LSTM method and LSTM/GRU-based\nSeq2Seq method for applications of wind speed forecasting.",
                "Case Studies on X-Ray Imaging, MRI and Nuclear Imaging\nThe field of medical imaging is an essential aspect of the medical sciences,\ninvolving various forms of radiation to capture images of the internal tissues\nand organs of the body. These images provide vital information for clinical\ndiagnosis, and in this chapter, we will explore the use of X-ray, MRI, and\nnuclear imaging in detecting severe illnesses. However, manual evaluation and\nstorage of these images can be a challenging and time-consuming process. To\naddress this issue, artificial intelligence (AI)-based techniques, particularly\ndeep learning (DL), have become increasingly popular for systematic feature\nextraction and classification from imaging modalities, thereby aiding doctors\nin making rapid and accurate diagnoses. In this review study, we will focus on\nhow AI-based approaches, particularly the use of Convolutional Neural Networks\n(CNN), can assist in disease detection through medical imaging technology. CNN\nis a commonly used approach for image analysis due to its ability to extract\nfeatures from raw input images, and as such, will be the primary area of\ndiscussion in this study. Therefore, we have considered CNN as our discussion\narea in this study to diagnose ailments using medical imaging technology.",
                "Publicly available datasets of breast histopathology H&E whole-slide\n  images: A scoping review\nAdvancements in digital pathology and computing resources have made a\nsignificant impact in the field of computational pathology for breast cancer\ndiagnosis and treatment. However, access to high-quality labeled\nhistopathological images of breast cancer is a big challenge that limits the\ndevelopment of accurate and robust deep learning models. In this scoping\nreview, we identified the publicly available datasets of breast H&E stained\nwhole-slide images (WSI) that can be used to develop deep learning algorithms.\nWe systematically searched nine scientific literature databases and nine\nresearch data repositories and found 17 publicly available datasets containing\n10385 H&E WSIs of breast cancer. Moreover, we reported image metadata and\ncharacteristics for each dataset to assist researchers in selecting proper\ndatasets for specific tasks in breast cancer computational pathology. In\naddition, we compiled two lists of breast H&E patches and private datasets as\nsupplementary resources for researchers. Notably, only 28% of the included\narticles utilized multiple datasets, and only 14% used an external validation\nset, suggesting that the performance of other developed models may be\nsusceptible to overestimation. The TCGA-BRCA was used in 52% of the selected\nstudies. This dataset has a considerable selection bias that can impact the\nrobustness and generalizability of the trained algorithms. There is also a lack\nof consistent metadata reporting of breast WSI datasets that can be an issue in\ndeveloping accurate deep learning models, indicating the necessity of\nestablishing explicit guidelines for documenting breast WSI dataset\ncharacteristics and metadata.",
                "MutateNN: Mutation Testing of Image Recognition Models Deployed on\n  Hardware Accelerators\nThe increased utilization of Artificial Intelligence (AI) solutions brings\nwith it inherent risks, such as misclassification and sub-optimal execution\ntime performance, due to errors introduced in their deployment infrastructure\nbecause of problematic configuration and software faults. On top of that, AI\nmethods such as Deep Neural Networks (DNNs) are utilized to perform demanding,\nresource-intensive and even safety-critical tasks, and in order to effectively\nincrease the performance of the DNN models deployed, a variety of Machine\nLearning (ML) compilers have been developed, allowing compatibility of DNNs\nwith a variety of hardware acceleration devices, such as GPUs and TPUs.\nFurthermore the correctness of the compilation process should be verified. In\norder to allow developers and researchers to explore the robustness of DNN\nmodels deployed on different hardware accelerators via ML compilers, in this\npaper we propose MutateNN, a tool that provides mutation testing and model\nanalysis features in the context of deployment on different hardware\naccelerators. To demonstrate the capabilities of MutateNN, we focus on the\nimage recognition domain by applying mutation testing to 7 well-established\nmodels utilized for image classification. We instruct 21 mutations of 6\ndifferent categories, and deploy our mutants on 4 different hardware\nacceleration devices of varying capabilities. Our results indicate that models\nare proven robust to changes related to layer modifications and arithmetic\noperators, while presenting discrepancies of up to 90.3% in mutants related to\nconditional operators. We also observed unexpectedly severe performance\ndegradation on mutations related to arithmetic types of variables, leading the\nmutants to produce the same classifications for all dataset inputs.",
                "The Information Pathways Hypothesis: Transformers are Dynamic\n  Self-Ensembles\nTransformers use the dense self-attention mechanism which gives a lot of\nflexibility for long-range connectivity. Over multiple layers of a deep\ntransformer, the number of possible connectivity patterns increases\nexponentially. However, very few of these contribute to the performance of the\nnetwork, and even fewer are essential. We hypothesize that there are sparsely\nconnected sub-networks within a transformer, called information pathways which\ncan be trained independently. However, the dynamic (i.e., input-dependent)\nnature of these pathways makes it difficult to prune dense self-attention\nduring training. But the overall distribution of these pathways is often\npredictable. We take advantage of this fact to propose Stochastically\nSubsampled self-Attention (SSA) - a general-purpose training strategy for\ntransformers that can reduce both the memory and computational cost of\nself-attention by 4 to 8 times during training while also serving as a\nregularization method - improving generalization over dense training. We show\nthat an ensemble of sub-models can be formed from the subsampled pathways\nwithin a network, which can achieve better performance than its densely\nattended counterpart. We perform experiments on a variety of NLP, computer\nvision and graph learning tasks in both generative and discriminative settings\nto provide empirical evidence for our claims and show the effectiveness of the\nproposed method.",
                "AlerTiger: Deep Learning for AI Model Health Monitoring at LinkedIn\nData-driven companies use AI models extensively to develop products and\nintelligent business solutions, making the health of these models crucial for\nbusiness success. Model monitoring and alerting in industries pose unique\nchallenges, including a lack of clear model health metrics definition, label\nsparsity, and fast model iterations that result in short-lived models and\nfeatures. As a product, there are also requirements for scalability,\ngeneralizability, and explainability. To tackle these challenges, we propose\nAlerTiger, a deep-learning-based MLOps model monitoring system that helps AI\nteams across the company monitor their AI models' health by detecting anomalies\nin models' input features and output score over time. The system consists of\nfour major steps: model statistics generation, deep-learning-based anomaly\ndetection, anomaly post-processing, and user alerting. Our solution generates\nthree categories of statistics to indicate AI model health, offers a two-stage\ndeep anomaly detection solution to address label sparsity and attain the\ngeneralizability of monitoring new models, and provides holistic reports for\nactionable alerts. This approach has been deployed to most of LinkedIn's\nproduction AI models for over a year and has identified several model issues\nthat later led to significant business metric gains after fixing."
            ],
            "interesting paper": 5
        }
    ],
    "Riley Johnson": [
        {
            "papers": [
                "Towards a Capability Assessment Model for the Comprehension and Adoption\n  of AI in Organisations\nThe comprehension and adoption of Artificial Intelligence (AI) are beset with\npractical and ethical problems. This article presents a 5-level AI Capability\nAssessment Model (AI-CAM) and a related AI Capabilities Matrix (AI-CM) to\nassist practitioners in AI comprehension and adoption. These practical tools\nwere developed with business executives, technologists, and other\norganisational stakeholders in mind. They are founded on a comprehensive\nconception of AI compared to those in other AI adoption models and are also\nopen-source artefacts. Thus, the AI-CAM and AI-CM present an accessible\nresource to help inform organisational decision-makers on the capability\nrequirements for (1) AI-based data analytics use cases based on machine\nlearning technologies; (2) Knowledge representation to engineer and represent\ndata, information and knowledge using semantic technologies; and (3) AI-based\nsolutions that seek to emulate human reasoning and decision-making. The AI-CAM\ncovers the core capability dimensions (business, data, technology,\norganisation, AI skills, risks, and ethical considerations) required at the\nfive capability maturity levels to achieve optimal use of AI in organisations.",
                "Learning-Based Automatic Synthesis of Software Code and Configuration\nIncreasing demands in software industry and scarcity of software engineers\nmotivates researchers and practitioners to automate the process of software\ngeneration and configuration. Large scale automatic software generation and\nconfiguration is a very complex and challenging task. In this proposal, we set\nout to investigate this problem by breaking down automatic software generation\nand configuration into two different tasks. In first task, we propose to\nsynthesize software automatically with input output specifications. This task\nis further broken down into two sub-tasks. The first sub-task is about\nsynthesizing programs with a genetic algorithm which is driven by a neural\nnetwork based fitness function trained with program traces and specifications.\nFor the second sub-task, we formulate program synthesis as a continuous\noptimization problem and synthesize programs with covariance matrix adaption\nevolutionary strategy (a state-of-the-art continuous optimization method).\nFinally, for the second task, we propose to synthesize configurations of large\nscale software from different input files (e.g. software manuals,\nconfigurations files, online blogs, etc.) using a sequence-to-sequence deep\nlearning mechanism.",
                "A Mini Review on the utilization of Reinforcement Learning with OPC UA\nReinforcement Learning (RL) is a powerful machine learning paradigm that has\nbeen applied in various fields such as robotics, natural language processing\nand game playing achieving state-of-the-art results. Targeted to solve\nsequential decision making problems, it is by design able to learn from\nexperience and therefore adapt to changing dynamic environments. These\ncapabilities make it a prime candidate for controlling and optimizing complex\nprocesses in industry. The key to fully exploiting this potential is the\nseamless integration of RL into existing industrial systems. The industrial\ncommunication standard Open Platform Communications UnifiedArchitecture (OPC\nUA) could bridge this gap. However, since RL and OPC UA are from different\nfields,there is a need for researchers to bridge the gap between the two\ntechnologies. This work serves to bridge this gap by providing a brief\ntechnical overview of both technologies and carrying out a semi-exhaustive\nliterature review to gain insights on how RL and OPC UA are applied in\ncombination. With this survey, three main research topics have been identified,\nfollowing the intersection of RL with OPC UA. The results of the literature\nreview show that RL is a promising technology for the control and optimization\nof industrial processes, but does not yet have the necessary standardized\ninterfaces to be deployed in real-world scenarios with reasonably low effort.",
                "Trends and Challenges Towards an Effective Data-Driven Decision Making\n  in UK SMEs: Case Studies and Lessons Learnt from the Analysis of 85 SMEs\nThe adoption of data science brings vast benefits to Small and Medium-sized\nEnterprises (SMEs) including business productivity, economic growth, innovation\nand jobs creation. Data Science can support SMEs to optimise production\nprocesses, anticipate customers' needs, predict machinery failures and deliver\nefficient smart services. Businesses can also harness the power of Artificial\nIntelligence (AI) and Big Data and the smart use of digital technologies to\nenhance productivity and performance, paving the way for innovation. However,\nintegrating data science decisions into an SME requires both skills and IT\ninvestments. In most cases, such expenses are beyond the means of SMEs due to\nlimited resources and restricted access to financing. This paper presents\ntrends and challenges towards an effective data-driven decision making for\norganisations based on a case study of 85 SMEs, mostly from the West Midlands\nregion of England. The work is supported as part of a 3 years ERDF (European\nRegional Development Funded project) in the areas of big data management,\nanalytics and business intelligence. We present two case studies that\ndemonstrates the potential of Digitisation, AI and Machine Learning and use\nthese as examples to unveil challenges and showcase the wealth of current\navailable opportunities for SMEs.",
                "Symbolic Regression via Control Variable Genetic Programming\nLearning symbolic expressions directly from experiment data is a vital step\nin AI-driven scientific discovery. Nevertheless, state-of-the-art approaches\nare limited to learning simple expressions. Regressing expressions involving\nmany independent variables still remain out of reach. Motivated by the control\nvariable experiments widely utilized in science, we propose Control Variable\nGenetic Programming (CVGP) for symbolic regression over many independent\nvariables. CVGP expedites symbolic expression discovery via customized\nexperiment design, rather than learning from a fixed dataset collected a\npriori. CVGP starts by fitting simple expressions involving a small set of\nindependent variables using genetic programming, under controlled experiments\nwhere other variables are held as constants. It then extends expressions\nlearned in previous generations by adding new independent variables, using new\ncontrol variable experiments in which these variables are allowed to vary.\nTheoretically, we show CVGP as an incremental building approach can yield an\nexponential reduction in the search space when learning a class of expressions.\nExperimentally, CVGP outperforms several baselines in learning symbolic\nexpressions involving multiple independent variables.",
                "Learning to Act through Evolution of Neural Diversity in Random Neural\n  Networks\nBiological nervous systems consist of networks of diverse, sophisticated\ninformation processors in the form of neurons of different classes. In most\nartificial neural networks (ANNs), neural computation is abstracted to an\nactivation function that is usually shared between all neurons within a layer\nor even the whole network; training of ANNs focuses on synaptic optimization.\nIn this paper, we propose the optimization of neuro-centric parameters to\nattain a set of diverse neurons that can perform complex computations.\nDemonstrating the promise of the approach, we show that evolving neural\nparameters alone allows agents to solve various reinforcement learning tasks\nwithout optimizing any synaptic weights. While not aiming to be an accurate\nbiological model, parameterizing neurons to a larger degree than the current\ncommon practice, allows us to ask questions about the computational abilities\nafforded by neural diversity in random neural networks. The presented results\nopen up interesting future research directions, such as combining evolved\nneural diversity with activity-dependent plasticity."
            ],
            "interesting paper": 3
        },
        {
            "papers": [
                "Attention Paper: How Generative AI Reshapes Digital Shadow Industry?\nThe rapid development of digital economy has led to the emergence of various\nblack and shadow internet industries, which pose potential risks that can be\nidentified and managed through digital risk management (DRM) that uses\ndifferent techniques such as machine learning and deep learning. The evolution\nof DRM architecture has been driven by changes in data forms. However, the\ndevelopment of AI-generated content (AIGC) technology, such as ChatGPT and\nStable Diffusion, has given black and shadow industries powerful tools to\npersonalize data and generate realistic images and conversations for fraudulent\nactivities. This poses a challenge for DRM systems to control risks from the\nsource of data generation and to respond quickly to the fast-changing risk\nenvironment. This paper aims to provide a technical analysis of the challenges\nand opportunities of AIGC from upstream, midstream, and downstream paths of\nblack/shadow industries and suggest future directions for improving existing\nrisk control systems. The paper will explore the new black and shadow\ntechniques triggered by generative AI technology and provide insights for\nbuilding the next-generation DRM system.",
                "On Computing Universal Plans for Partially Observable Multi-Agent Path\n  Finding\nMulti-agent routing problems have drawn significant attention nowadays due to\ntheir broad industrial applications in, e.g., warehouse robots, logistics\nautomation, and traffic control. Conventionally, they are modelled as classical\nplanning problems. In this paper, we argue that it is beneficial to formulate\nthem as universal planning problems. We therefore propose universal plans, also\nknown as policies, as the solution concepts, and implement a system called\nASP-MAUPF (Answer Set Programming for Multi-Agent Universal Plan Finding) for\ncomputing them. Given an arbitrary two-dimensional map and a profile of goals\nfor the agents, the system finds a feasible universal plan for each agent that\nensures no collision with others. We use the system to conduct some\nexperiments, and make some observations on the types of goal profiles and\nenvironments that will have feasible policies, and how they may depend on\nagents' sensors. We also demonstrate how users can customize action preferences\nto compute more efficient policies, even (near-)optimal ones.",
                "AI Techniques in the Microservices Life-Cycle: A Survey\nMicroservices is a popular architectural style for the development of\ndistributed software, with an emphasis on modularity, scalability, and\nflexibility. Indeed, in microservice systems, functionalities are provided by\nloosely coupled, small services, each focusing on a specific business\ncapability. Building a system according to the microservices architectural\nstyle brings a number of challenges, mainly related to how the different\nmicroservices are deployed and coordinated and how they interact. In this\npaper, we provide a survey about how techniques in the area of Artificial\nIntelligence have been used to tackle these challenges.",
                "Generating Synergistic Formulaic Alpha Collections via Reinforcement\n  Learning\nIn the field of quantitative trading, it is common practice to transform raw\nhistorical stock data into indicative signals for the market trend. Such\nsignals are called alpha factors. Alphas in formula forms are more\ninterpretable and thus favored by practitioners concerned with risk. In\npractice, a set of formulaic alphas is often used together for better modeling\nprecision, so we need to find synergistic formulaic alpha sets that work well\ntogether. However, most traditional alpha generators mine alphas one by one\nseparately, overlooking the fact that the alphas would be combined later. In\nthis paper, we propose a new alpha-mining framework that prioritizes mining a\nsynergistic set of alphas, i.e., it directly uses the performance of the\ndownstream combination model to optimize the alpha generator. Our framework\nalso leverages the strong exploratory capabilities of reinforcement\nlearning~(RL) to better explore the vast search space of formulaic alphas. The\ncontribution to the combination models' performance is assigned to be the\nreturn used in the RL process, driving the alpha generator to find better\nalphas that improve upon the current set. Experimental evaluations on\nreal-world stock market data demonstrate both the effectiveness and the\nefficiency of our framework for stock trend forecasting. The investment\nsimulation results show that our framework is able to achieve higher returns\ncompared to previous approaches.",
                "Alert of the Second Decision-maker: An Introduction to Human-AI Conflict\nThe collaboration between humans and artificial intelligence (AI) is a\nsignificant feature in this digital age. However, humans and AI may have\nobservation, interpretation, and action conflicts when working synchronously.\nThis phenomenon is often masked by faults and, unfortunately, overlooked. This\npaper systematically introduces the human-AI conflict concept, causes,\nmeasurement methods, and risk assessment. The results highlight that there is a\npotential second decision-maker besides the human, which is the AI; the\nhuman-AI conflict is a unique and emerging risk in digitalized process systems;\nand this is an interdisciplinary field that needs to be distinguished from\ntraditional fault and failure analysis; the conflict risk is significant and\ncannot be ignored.",
                "C-MCTS: Safe Planning with Monte Carlo Tree Search\nThe Constrained Markov Decision Process (CMDP) formulation allows to solve\nsafety-critical decision making tasks that are subject to constraints. While\nCMDPs have been extensively studied in the Reinforcement Learning literature,\nlittle attention has been given to sampling-based planning algorithms such as\nMCTS for solving them. Previous approaches perform conservatively with respect\nto costs as they avoid constraint violations by using Monte Carlo cost\nestimates that suffer from high variance. We propose Constrained MCTS (C-MCTS),\nwhich estimates cost using a safety critic that is trained with Temporal\nDifference learning in an offline phase prior to agent deployment. The critic\nlimits exploration by pruning unsafe trajectories within MCTS during\ndeployment. C-MCTS satisfies cost constraints but operates closer to the\nconstraint boundary, achieving higher rewards than previous work. As a nice\nbyproduct, the planner is more efficient w.r.t. planning steps. Most\nimportantly, under model mismatch between the planner and the real world,\nC-MCTS is less susceptible to cost violations than previous work."
            ],
            "interesting paper": 2
        },
        {
            "papers": [
                "Optimization for truss design using Bayesian optimization\nIn this work, geometry optimization of mechanical truss using computer-aided\nfinite element analysis is presented. The shape of the truss is a dominant\nfactor in determining the capacity of load it can bear. At a given parameter\nspace, our goal is to find the parameters of a hull that maximize the\nload-bearing capacity and also don't yield to the induced stress. We rely on\nfinite element analysis, which is a computationally costly design analysis tool\nfor design evaluation. For such expensive to-evaluate functions, we chose\nBayesian optimization as our optimization framework which has empirically\nproven sample efficient than other simulation-based optimization methods.\n  By utilizing Bayesian optimization algorithms, the truss design involves\niteratively evaluating a set of candidate truss designs and updating a\nprobabilistic model of the design space based on the results. The model is used\nto predict the performance of each candidate design, and the next candidate\ndesign is selected based on the prediction and an acquisition function that\nbalances exploration and exploitation of the design space. Our result can be\nused as a baseline for future study on AI-based optimization in expensive\nengineering domains especially in finite element Analysis.",
                "Frontier AI developers need an internal audit function\nThis article argues that frontier artificial intelligence (AI) developers\nneed an internal audit function. First, it describes the role of internal audit\nin corporate governance: internal audit evaluates the adequacy and\neffectiveness of a company's risk management, control, and governance\nprocesses. It is organizationally independent from senior management and\nreports directly to the board of directors, typically its audit committee. In\nthe IIA's Three Lines Model, internal audit serves as the third line and is\nresponsible for providing assurance to the board, while the Combined Assurance\nFramework highlights the need to coordinate the activities of internal and\nexternal assurance providers. Next, the article provides an overview of key\ngovernance challenges in frontier AI development: dangerous capabilities can\narise unpredictably and undetected; it is difficult to prevent a deployed model\nfrom causing harm; frontier models can proliferate rapidly; it is inherently\ndifficult to assess frontier AI risks; and frontier AI developers do not seem\nto follow best practices in risk governance. Finally, the article discusses how\nan internal audit function could address some of these challenges: internal\naudit could identify ineffective risk management practices; it could ensure\nthat the board of directors has a more accurate understanding of the current\nlevel of risk and the adequacy of the developer's risk management practices;\nand it could serve as a contact point for whistleblowers. In light of rapid\nprogress in AI research and development, frontier AI developers need to\nstrengthen their risk governance. Instead of reinventing the wheel, they should\nfollow existing best practices. While this might not be sufficient, they should\nnot skip this obvious first step.",
                "Improved Sales Forecasting using Trend and Seasonality Decomposition\n  with LightGBM\nRetail sales forecasting presents a significant challenge for large retailers\nsuch as Walmart and Amazon, due to the vast assortment of products,\ngeographical location heterogeneity, seasonality, and external factors\nincluding weather, local economic conditions, and geopolitical events. Various\nmethods have been employed to tackle this challenge, including traditional time\nseries models, machine learning models, and neural network mechanisms, but the\ndifficulty persists. Categorizing data into relevant groups has been shown to\nimprove sales forecast accuracy as time series from different categories may\nexhibit distinct patterns. In this paper, we propose a new measure to indicate\nthe unique impacts of the trend and seasonality components on a time series and\nsuggest grouping time series based on this measure. We apply this approach to\nWalmart sales data from 01/29/2011 to 05/22/2016 and generate sales forecasts\nfrom 05/23/2016 to 06/19/2016. Our experiments show that the proposed strategy\ncan achieve improved accuracy. Furthermore, we present a robust pipeline for\nconducting retail sales forecasting.",
                "How Do UX Practitioners Communicate AI as a Design Material? Artifacts,\n  Conceptions, and Propositions\nUX practitioners (UXPs) face novel challenges when working with and\ncommunicating artificial intelligence (AI) as a design material. We explore how\nUXPs communicate AI concepts when given hands-on experience training and\nexperimenting with AI models. To do so, we conducted a task-based design study\nwith 27 UXPs in which they prototyped and created a design presentation for a\nAI-enabled interface while having access to a simple AI model training tool.\nThrough analyzing UXPs' design presentations and post-activity interviews, we\nfound that although UXPs struggled to clearly communicate some AI concepts,\ntinkering with AI broadened common ground when communicating with technical\nstakeholders. UXPs also identified key risks and benefits of AI in their\ndesigns, and proposed concrete next steps for both UX and AI work. We conclude\nwith a sensitizing concept and recommendations for design and AI tools to\nenhance multi-stakeholder communication and collaboration when crafting\nhuman-centered AI experiences.",
                "Let the Flows Tell: Solving Graph Combinatorial Optimization Problems\n  with GFlowNets\nCombinatorial optimization (CO) problems are often NP-hard and thus out of\nreach for exact algorithms, making them a tempting domain to apply machine\nlearning methods. The highly structured constraints in these problems can\nhinder either optimization or sampling directly in the solution space. On the\nother hand, GFlowNets have recently emerged as a powerful machinery to\nefficiently sample from composite unnormalized densities sequentially and have\nthe potential to amortize such solution-searching processes in CO, as well as\ngenerate diverse solution candidates. In this paper, we design Markov decision\nprocesses (MDPs) for different combinatorial problems and propose to train\nconditional GFlowNets to sample from the solution space. Efficient training\ntechniques are also developed to benefit long-range credit assignment. Through\nextensive experiments on a variety of different CO tasks with synthetic and\nrealistic data, we demonstrate that GFlowNet policies can efficiently find\nhigh-quality solutions. Our implementation is open-sourced at\nhttps://github.com/zdhNarsil/GFlowNet-CombOpt.",
                "Learning Interpretable Models of Aircraft Handling Behaviour by\n  Reinforcement Learning from Human Feedback\nWe propose a method to capture the handling abilities of fast jet pilots in a\nsoftware model via reinforcement learning (RL) from human preference feedback.\nWe use pairwise preferences over simulated flight trajectories to learn an\ninterpretable rule-based model called a reward tree, which enables the\nautomated scoring of trajectories alongside an explanatory rationale. We train\nan RL agent to execute high-quality handling behaviour by using the reward tree\nas the objective, and thereby generate data for iterative preference collection\nand further refinement of both tree and agent. Experiments with synthetic\npreferences show reward trees to be competitive with uninterpretable neural\nnetwork reward models on quantitative and qualitative evaluations."
            ],
            "interesting paper": 5
        },
        {
            "papers": [
                "Optimization's Neglected Normative Commitments\nOptimization is offered as an objective approach to resolving complex,\nreal-world decisions involving uncertainty and conflicting interests. It drives\nbusiness strategies as well as public policies and, increasingly, lies at the\nheart of sophisticated machine learning systems. A paradigm used to approach\npotentially high-stakes decisions, optimization relies on abstracting the real\nworld to a set of decision(s), objective(s) and constraint(s). Drawing from the\nmodeling process and a range of actual cases, this paper describes the\nnormative choices and assumptions that are necessarily part of using\noptimization. It then identifies six emergent problems that may be neglected:\n1) Misspecified values can yield optimizations that omit certain imperatives\naltogether or incorporate them incorrectly as a constraint or as part of the\nobjective, 2) Problematic decision boundaries can lead to faulty modularity\nassumptions and feedback loops, 3) Failing to account for multiple agents'\ndivergent goals and decisions can lead to policies that serve only certain\nnarrow interests, 4) Mislabeling and mismeasurement can introduce bias and\nimprecision, 5) Faulty use of relaxation and approximation methods,\nunaccompanied by formal characterizations and guarantees, can severely impede\napplicability, and 6) Treating optimization as a justification for action,\nwithout specifying the necessary contextual information, can lead to ethically\ndubious or faulty decisions. Suggestions are given to further understand and\ncurb the harms that can arise when optimization is used wrongfully.",
                "Probing reaction channels via reinforcement learning\nWe propose a reinforcement learning based method to identify important\nconfigurations that connect reactant and product states along chemical reaction\npaths. By shooting multiple trajectories from these configurations, we can\ngenerate an ensemble of configurations that concentrate on the transition path\nensemble. This configuration ensemble can be effectively employed in a neural\nnetwork-based partial differential equation solver to obtain an approximation\nsolution of a restricted Backward Kolmogorov equation, even when the dimension\nof the problem is very high. The resulting solution, known as the committor\nfunction, encodes mechanistic information for the reaction and can in turn be\nused to evaluate reaction rates.",
                "MLOps: A Step Forward to Enterprise Machine Learning\nMachine Learning Operations (MLOps) is becoming a highly crucial part of\nbusinesses looking to capitalize on the benefits of AI and ML models. This\nresearch presents a detailed review of MLOps, its benefits, difficulties,\nevolutions, and important underlying technologies such as MLOps frameworks,\nDocker, GitHub actions, and Kubernetes. The MLOps workflow, which includes\nmodel design, deployment, and operations, is explained in detail along with the\nvarious tools necessary for both model and data exploration and deployment.\nThis article also puts light on the end-to-end production of ML projects using\nvarious maturity levels of automated pipelines, with the least at no automation\nat all and the highest with complete CI/CD and CT capabilities. Furthermore, a\ndetailed example of an enterprise-level MLOps project for an object detection\nservice is used to explain the workflow of the technology in a real-world\nscenario. For this purpose, a web application hosting a pre-trained model from\nTensorFlow 2 Model Zoo is packaged and deployed to the internet making sure\nthat the system is scalable, reliable, and optimized for deployment at an\nenterprise level.",
                "AI Coach Assist: An Automated Approach for Call Recommendation in\n  Contact Centers for Agent Coaching\nIn recent years, the utilization of Artificial Intelligence (AI) in the\ncontact center industry is on the rise. One area where AI can have a\nsignificant impact is in the coaching of contact center agents. By analyzing\ncall transcripts using Natural Language Processing (NLP) techniques, it would\nbe possible to quickly determine which calls are most relevant for coaching\npurposes. In this paper, we present AI Coach Assist, which leverages the\npre-trained transformer-based language models to determine whether a given call\nis coachable or not based on the quality assurance (QA) questions asked by the\ncontact center managers or supervisors. The system was trained and evaluated on\na large dataset collected from real-world contact centers and provides an\neffective way to recommend calls to the contact center managers that are more\nlikely to contain coachable moments. Our experimental findings demonstrate the\npotential of AI Coach Assist to improve the coaching process, resulting in\nenhancing the performance of contact center agents.",
                "Synthesizing a Progression of Subtasks for Block-Based Visual\n  Programming Tasks\nBlock-based visual programming environments play an increasingly important\nrole in introducing computing concepts to K-12 students. In recent years, they\nhave also gained popularity in neuro-symbolic AI, serving as a benchmark to\nevaluate general problem-solving and logical reasoning skills. The open-ended\nand conceptual nature of these visual programming tasks make them challenging,\nboth for state-of-the-art AI agents as well as for novice programmers. A\nnatural approach to providing assistance for problem-solving is breaking down a\ncomplex task into a progression of simpler subtasks; however, this is not\ntrivial given that the solution codes are typically nested and have non-linear\nexecution behavior. In this paper, we formalize the problem of synthesizing\nsuch a progression for a given reference block-based visual programming task.\nWe propose a novel synthesis algorithm that generates a progression of subtasks\nthat are high-quality, well-spaced in terms of their complexity, and solving\nthis progression leads to solving the reference task. We show the utility of\nour synthesis algorithm in improving the efficacy of AI agents (in this case,\nneural program synthesizers) for solving tasks in the Karel programming\nenvironment. Then, we conduct a user study to demonstrate that our synthesized\nprogression of subtasks can assist a novice programmer in solving tasks in the\nHour of Code: Maze Challenge by Code-dot-org.",
                "PFNs4BO: In-Context Learning for Bayesian Optimization\nIn this paper, we use Prior-data Fitted Networks (PFNs) as a flexible\nsurrogate for Bayesian Optimization (BO). PFNs are neural processes that are\ntrained to approximate the posterior predictive distribution (PPD) through\nin-context learning on any prior distribution that can be efficiently sampled\nfrom. We describe how this flexibility can be exploited for surrogate modeling\nin BO. We use PFNs to mimic a naive Gaussian process (GP), an advanced GP, and\na Bayesian Neural Network (BNN). In addition, we show how to incorporate\nfurther information into the prior, such as allowing hints about the position\nof optima (user priors), ignoring irrelevant dimensions, and performing\nnon-myopic BO by learning the acquisition function. The flexibility underlying\nthese extensions opens up vast possibilities for using PFNs for BO. We\ndemonstrate the usefulness of PFNs for BO in a large-scale evaluation on\nartificial GP samples and three different hyperparameter optimization testbeds:\nHPO-B, Bayesmark, and PD1. We publish code alongside trained models at\ngithub.com/automl/PFNs4BO."
            ],
            "interesting paper": 6
        },
        {
            "papers": [
                "Improving Confidence in Evolutionary Mine Scheduling via Uncertainty\n  Discounting\nMine planning is a complex task that involves many uncertainties. During\nearly stage feasibility, available mineral resources can only be estimated\nbased on limited sampling of ore grades from sparse drilling, leading to large\nuncertainty in under-sampled parts of the deposit. Planning the extraction\nschedule of ore over the life of a mine is crucial for its economic viability.\nWe introduce a new approach for determining an \"optimal schedule under\nuncertainty\" that provides probabilistic bounds on the profits obtained in each\nperiod. This treatment of uncertainty within an economic framework reduces\npreviously difficult-to-use models of variability into actionable insights. The\nnew method discounts profits based on uncertainty within an evolutionary\nalgorithm, sacrificing economic optimality of a single geological model for\nimproving the downside risk over an ensemble of equally likely models. We\nprovide experimental studies using Maptek's mine planning software Evolution.\nOur results show that our new approach is successful for effectively making use\nof uncertainty information in the mine planning process.",
                "Towards Autonomous and Safe Last-mile Deliveries with AI-augmented\n  Self-driving Delivery Robots\nIn addition to its crucial impact on customer satisfaction, last-mile\ndelivery (LMD) is notorious for being the most time-consuming and costly stage\nof the shipping process. Pressing environmental concerns combined with the\nrecent surge of e-commerce sales have sparked renewed interest in automation\nand electrification of last-mile logistics. To address the hurdles faced by\nexisting robotic couriers, this paper introduces a customer-centric and\nsafety-conscious LMD system for small urban communities based on AI-assisted\nautonomous delivery robots. The presented framework enables end-to-end\nautomation and optimization of the logistic process while catering for\nreal-world imposed operational uncertainties, clients' preferred time\nschedules, and safety of pedestrians. To this end, the integrated optimization\ncomponent is modeled as a robust variant of the Cumulative Capacitated Vehicle\nRouting Problem with Time Windows, where routes are constructed under uncertain\ntravel times with an objective to minimize the total latency of deliveries\n(i.e., the overall waiting time of customers, which can negatively affect their\nsatisfaction). We demonstrate the proposed LMD system's utility through\nreal-world trials in a university campus with a single robotic courier.\nImplementation aspects as well as the findings and practical insights gained\nfrom the deployment are discussed in detail. Lastly, we round up the\ncontributions with numerical simulations to investigate the scalability of the\ndeveloped mathematical formulation with respect to the number of robotic\nvehicles and customers.",
                "Employing Explainable Artificial Intelligence (XAI) Methodologies to\n  Analyze the Correlation between Input Variables and Tensile Strength in\n  Additively Manufactured Samples\nThis research paper explores the impact of various input parameters,\nincluding Infill percentage, Layer Height, Extrusion Temperature, and Print\nSpeed, on the resulting Tensile Strength in objects produced through additive\nmanufacturing. The main objective of this study is to enhance our understanding\nof the correlation between the input parameters and Tensile Strength, as well\nas to identify the key factors influencing the performance of the additive\nmanufacturing process. To achieve this objective, we introduced the utilization\nof Explainable Artificial Intelligence (XAI) techniques for the first time,\nwhich allowed us to analyze the data and gain valuable insights into the\nsystem's behavior. Specifically, we employed SHAP (SHapley Additive\nexPlanations), a widely adopted framework for interpreting machine learning\nmodel predictions, to provide explanations for the behavior of a machine\nlearning model trained on the data. Our findings reveal that the Infill\npercentage and Extrusion Temperature have the most significant influence on\nTensile Strength, while the impact of Layer Height and Print Speed is\nrelatively minor. Furthermore, we discovered that the relationship between the\ninput parameters and Tensile Strength is highly intricate and nonlinear, making\nit difficult to accurately describe using simple linear models.",
                "The Digital Divide in Process Safety: Quantitative Risk Analysis of\n  Human-AI Collaboration\nDigital technologies have dramatically accelerated the digital transformation\nin process industries, boosted new industrial applications, upgraded the\nproduction system, and enhanced operational efficiency. In contrast, the\nchallenges and gaps between human and artificial intelligence (AI) have become\nmore and more prominent, whereas the digital divide in process safety is\naggregating. The study attempts to address the following questions: (i)What is\nAI in the process safety context? (ii)What is the difference between AI and\nhumans in process safety? (iii)How do AI and humans collaborate in process\nsafety? (iv)What are the challenges and gaps in human-AI collaboration? (v)How\nto quantify the risk of human-AI collaboration in process safety? Qualitative\nrisk analysis based on brainstorming and literature review, and quantitative\nrisk analysis based on layer of protection analysis (LOPA) and Bayesian network\n(BN), were applied to explore and model. The importance of human reliability\nshould be stressed in the digital age, not usually to increase the reliability\nof AI, and human-centered AI design in process safety needs to be propagated.",
                "ProcessGPT: Transforming Business Process Management with Generative\n  Artificial Intelligence\nGenerative Pre-trained Transformer (GPT) is a state-of-the-art machine\nlearning model capable of generating human-like text through natural language\nprocessing (NLP). GPT is trained on massive amounts of text data and uses deep\nlearning techniques to learn patterns and relationships within the data,\nenabling it to generate coherent and contextually appropriate text. This\nposition paper proposes using GPT technology to generate new process models\nwhen/if needed. We introduce ProcessGPT as a new technology that has the\npotential to enhance decision-making in data-centric and knowledge-intensive\nprocesses. ProcessGPT can be designed by training a generative pre-trained\ntransformer model on a large dataset of business process data. This model can\nthen be fine-tuned on specific process domains and trained to generate process\nflows and make decisions based on context and user input. The model can be\nintegrated with NLP and machine learning techniques to provide insights and\nrecommendations for process improvement. Furthermore, the model can automate\nrepetitive tasks and improve process efficiency while enabling knowledge\nworkers to communicate analysis findings, supporting evidence, and make\ndecisions. ProcessGPT can revolutionize business process management (BPM) by\noffering a powerful tool for process augmentation, automation and improvement.\nFinally, we demonstrate how ProcessGPT can be a powerful tool for augmenting\ndata engineers in maintaining data ecosystem processes within large bank\norganizations. Our scenario highlights the potential of this approach to\nimprove efficiency, reduce costs, and enhance the quality of business\noperations through the automation of data-centric and knowledge-intensive\nprocesses. These results underscore the promise of ProcessGPT as a\ntransformative technology for organizations looking to improve their process\nworkflows.",
                "Optimizing Airbnb Search Journey with Multi-task Learning\nAt Airbnb, an online marketplace for stays and experiences, guests often\nspend weeks exploring and comparing multiple items before making a final\nreservation request. Each reservation request may then potentially be rejected\nor cancelled by the host prior to check-in. The long and exploratory nature of\nthe search journey, as well as the need to balance both guest and host\npreferences, present unique challenges for Airbnb search ranking. In this\npaper, we present Journey Ranker, a new multi-task deep learning model\narchitecture that addresses these challenges. Journey Ranker leverages\nintermediate guest actions as milestones, both positive and negative, to better\nprogress the guest towards a successful booking. It also uses contextual\ninformation such as guest state and search query to balance guest and host\npreferences. Its modular and extensible design, consisting of four modules with\nclear separation of concerns, allows for easy application to use cases beyond\nthe Airbnb search ranking context. We conducted offline and online testing of\nthe Journey Ranker and successfully deployed it in production to four different\nAirbnb products with significant business metrics improvements."
            ],
            "interesting paper": 2
        },
        {
            "papers": [
                "Design of the Reverse Logistics System for Medical Waste Recycling Part\n  II: Route Optimization with Case Study under COVID-19 Pandemic\nMedical waste recycling and treatment has gradually drawn concerns from the\nwhole society, as the amount of medical waste generated is increasing\ndramatically, especially during the pandemic of COVID-19. To tackle the\nemerging challenges, this study designs a reverse logistics system architecture\nwith three modules, i.e., medical waste classification & monitoring module,\ntemporary storage & disposal site (disposal site for short) selection module,\nas well as route optimization module. This overall solution design won the\nGrand Prize of the \"YUNFENG CUP\" China National Contest on Green Supply and\nReverse Logistics Design ranking 1st. This paper focuses on the design of the\nroute optimization module. In this module, a route optimization problem is\ndesigned considering transportation costs and multiple risk costs (e.g.,\nenvironment risk, population risk, property risk, and other accident-related\nrisks). The Analytic Hierarchy Process is employed to determine the weights for\neach risk element, and a customized genetic algorithm is developed to solve the\nroute optimization problem. A case study under the COVID-19 pandemic is further\nprovided to verify the proposed model. Limited by length, detailed descriptions\nof the whole system and the other modules can be found at\nhttps://shorturl.at/cdY59.",
                "Towards a Unifying Model of Rationality in Multiagent Systems\nMultiagent systems deployed in the real world need to cooperate with other\nagents (including humans) nearly as effectively as these agents cooperate with\none another. To design such AI, and provide guarantees of its effectiveness, we\nneed to clearly specify what types of agents our AI must be able to cooperate\nwith. In this work we propose a generic model of socially intelligent agents,\nwhich are individually rational learners that are also able to cooperate with\none another (in the sense that their joint behavior is Pareto efficient). We\ndefine rationality in terms of the regret incurred by each agent over its\nlifetime, and show how we can construct socially intelligent agents for\ndifferent forms of regret. We then discuss the implications of this model for\nthe development of \"robust\" MAS that can cooperate with a wide variety of\nsocially intelligent agents.",
                "Generalized Disparate Impact for Configurable Fairness Solutions in ML\nWe make two contributions in the field of AI fairness over continuous\nprotected attributes. First, we show that the Hirschfeld-Gebelein-Renyi (HGR)\nindicator (the only one currently available for such a case) is valuable but\nsubject to a few crucial limitations regarding semantics, interpretability, and\nrobustness. Second, we introduce a family of indicators that are: 1)\ncomplementary to HGR in terms of semantics; 2) fully interpretable and\ntransparent; 3) robust over finite samples; 4) configurable to suit specific\napplications. Our approach also allows us to define fine-grained constraints to\npermit certain types of dependence and forbid others selectively. By expanding\nthe available options for continuous protected attributes, our approach\nrepresents a significant contribution to the area of fair artificial\nintelligence.",
                "RE-centric Recommendations for the Development of Trustworthy(er)\n  Autonomous Systems\nComplying with the EU AI Act (AIA) guidelines while developing and\nimplementing AI systems will soon be mandatory within the EU. However,\npractitioners lack actionable instructions to operationalise ethics during AI\nsystems development. A literature review of different ethical guidelines\nrevealed inconsistencies in the principles addressed and the terminology used\nto describe them. Furthermore, requirements engineering (RE), which is\nidentified to foster trustworthiness in the AI development process from the\nearly stages was observed to be absent in a lot of frameworks that support the\ndevelopment of ethical and trustworthy AI. This incongruous phrasing combined\nwith a lack of concrete development practices makes trustworthy AI development\nharder. To address this concern, we formulated a comparison table for the\nterminology used and the coverage of the ethical AI principles in major ethical\nAI guidelines. We then examined the applicability of ethical AI development\nframeworks for performing effective RE during the development of trustworthy AI\nsystems. A tertiary review and meta-analysis of literature discussing ethical\nAI frameworks revealed their limitations when developing trustworthy AI. Based\non our findings, we propose recommendations to address such limitations during\nthe development of trustworthy AI.",
                "What is Essential for Unseen Goal Generalization of Offline\n  Goal-conditioned RL?\nOffline goal-conditioned RL (GCRL) offers a way to train general-purpose\nagents from fully offline datasets. In addition to being conservative within\nthe dataset, the generalization ability to achieve unseen goals is another\nfundamental challenge for offline GCRL. However, to the best of our knowledge,\nthis problem has not been well studied yet. In this paper, we study\nout-of-distribution (OOD) generalization of offline GCRL both theoretically and\nempirically to identify factors that are important. In a number of experiments,\nwe observe that weighted imitation learning enjoys better generalization than\npessimism-based offline RL method. Based on this insight, we derive a theory\nfor OOD generalization, which characterizes several important design choices.\nWe then propose a new offline GCRL method, Generalizable Offline\ngoAl-condiTioned RL (GOAT), by combining the findings from our theoretical and\nempirical studies. On a new benchmark containing 9 independent identically\ndistributed (IID) tasks and 17 OOD tasks, GOAT outperforms current\nstate-of-the-art methods by a large margin.",
                "Maximize to Explore: One Objective Function Fusing Estimation, Planning,\n  and Exploration\nIn online reinforcement learning (online RL), balancing exploration and\nexploitation is crucial for finding an optimal policy in a sample-efficient\nway. To achieve this, existing sample-efficient online RL algorithms typically\nconsist of three components: estimation, planning, and exploration. However, in\norder to cope with general function approximators, most of them involve\nimpractical algorithmic components to incentivize exploration, such as\noptimization within data-dependent level-sets or complicated sampling\nprocedures. To address this challenge, we propose an easy-to-implement RL\nframework called \\textit{Maximize to Explore} (\\texttt{MEX}), which only needs\nto optimize \\emph{unconstrainedly} a single objective that integrates the\nestimation and planning components while balancing exploration and exploitation\nautomatically. Theoretically, we prove that \\texttt{MEX} achieves a sublinear\nregret with general function approximations for Markov decision processes (MDP)\nand is further extendable to two-player zero-sum Markov games (MG). Meanwhile,\nwe adapt deep RL baselines to design practical versions of \\texttt{MEX}, in\nboth model-free and model-based manners, which can outperform baselines by a\nstable margin in various MuJoCo environments with sparse rewards. Compared with\nexisting sample-efficient online RL algorithms with general function\napproximations, \\texttt{MEX} achieves similar sample efficiency while enjoying\na lower computational cost and is more compatible with modern deep RL methods."
            ],
            "interesting paper": 5
        },
        {
            "papers": [
                "Towards Omni-generalizable Neural Methods for Vehicle Routing Problems\nLearning heuristics for vehicle routing problems (VRPs) has gained much\nattention due to the less reliance on hand-crafted rules. However, existing\nmethods are typically trained and tested on the same task with a fixed size and\ndistribution (of nodes), and hence suffer from limited generalization\nperformance. This paper studies a challenging yet realistic setting, which\nconsiders generalization across both size and distribution in VRPs. We propose\na generic meta-learning framework, which enables effective training of an\ninitialized model with the capability of fast adaptation to new tasks during\ninference. We further develop a simple yet efficient approximation method to\nreduce the training overhead. Extensive experiments on both synthetic and\nbenchmark instances of the traveling salesman problem (TSP) and capacitated\nvehicle routing problem (CVRP) demonstrate the effectiveness of our method. The\ncode is available at: https://github.com/RoyalSkye/Omni-VRP.",
                "Disentangling and Operationalizing AI Fairness at LinkedIn\nOperationalizing AI fairness at LinkedIn's scale is challenging not only\nbecause there are multiple mutually incompatible definitions of fairness but\nalso because determining what is fair depends on the specifics and context of\nthe product where AI is deployed. Moreover, AI practitioners need clarity on\nwhat fairness expectations need to be addressed at the AI level. In this paper,\nwe present the evolving AI fairness framework used at LinkedIn to address these\nthree challenges. The framework disentangles AI fairness by separating out\nequal treatment and equitable product expectations. Rather than imposing a\ntrade-off between these two commonly opposing interpretations of fairness, the\nframework provides clear guidelines for operationalizing equal AI treatment\ncomplemented with a product equity strategy. This paper focuses on the equal AI\ntreatment component of LinkedIn's AI fairness framework, shares the principles\nthat support it, and illustrates their application through a case study. We\nhope this paper will encourage other big tech companies to join us in sharing\ntheir approach to operationalizing AI fairness at scale, so that together we\ncan keep advancing this constantly evolving field.",
                "Machine Learning Applications in Cascading Failure Analysis in Power\n  Systems: A Review\nCascading failures pose a significant threat to power grids and have garnered\nconsiderable research interest in the power system domain. The inherent\nuncertainty and severe impact associated with cascading failures have raised\nconcerns, prompting the development of various techniques to study these\ncomplex phenomena. In recent years, advancements in monitoring technologies and\nthe availability of large volumes of data from power systems, coupled with the\nemergence of intelligent algorithms, have made machine learning (ML) techniques\nincreasingly attractive for addressing cascading failure problems. This survey\nprovides a comprehensive overview of ML-based techniques for analyzing\ncascading failures in power systems. The survey categorizes these techniques\nbased on the evolutionary phases of the cascade process in power systems, as\nwell as studies focusing on cascade resiliency before the occurrence of\ncascades and problems related to cascades after their termination. By\norganizing and presenting these works into relevant categories, this survey\naims to offer insights and a systematic understanding the role of ML in\nmitigating cascading failures in power systems.",
                "Evaluating GPT's Programming Capability through CodeWars' Katas\nIn the burgeoning field of artificial intelligence (AI), understanding the\ncapabilities and limitations of programming-oriented models is crucial. This\npaper presents a novel evaluation of the programming proficiency of Generative\nPretrained Transformer (GPT) models, specifically GPT-3.5 and GPT-4, against\ncoding problems of varying difficulty levels drawn from Codewars. The\nexperiments reveal a distinct boundary at the 3kyu level, beyond which these\nGPT models struggle to provide solutions. These findings led to the proposal of\na measure for coding problem complexity that incorporates both problem\ndifficulty and the time required for solution. The research emphasizes the need\nfor validation and creative thinking capabilities in AI models to better\nemulate human problem-solving techniques. Future work aims to refine this\nproposed complexity measure, enhance AI models with these suggested\ncapabilities, and develop an objective measure for programming problem\ndifficulty. The results of this research offer invaluable insights for\nimproving AI programming capabilities and advancing the frontier of AI\nproblem-solving abilities.",
                "Necessary and Sufficient Conditions for Optimal Decision Trees using\n  Dynamic Programming\nGlobal optimization of decision trees has shown to be promising in terms of\naccuracy, size, and consequently human comprehensibility. However, many of the\nmethods used rely on general-purpose solvers for which scalability remains an\nissue. Dynamic programming methods have been shown to scale much better because\nthey exploit the tree structure by solving subtrees as independent subproblems.\nHowever, this only works when an objective can be optimized separately for\nsubtrees. We explore this relationship in detail and show the necessary and\nsufficient conditions for such separability and generalize previous dynamic\nprogramming approaches into a framework that can optimize any combination of\nseparable objectives and constraints. Experiments on five application domains\nshow the general applicability of this framework, while outperforming the\nscalability of general-purpose solvers by a large margin.",
                "GPT Models in Construction Industry: Opportunities, Limitations, and a\n  Use Case Validation\nLarge Language Models(LLMs) trained on large data sets came into prominence\nin 2018 after Google introduced BERT. Subsequently, different LLMs such as GPT\nmodels from OpenAI have been released. These models perform well on diverse\ntasks and have been gaining widespread applications in fields such as business\nand education. However, little is known about the opportunities and challenges\nof using LLMs in the construction industry. Thus, this study aims to assess GPT\nmodels in the construction industry. A critical review, expert discussion and\ncase study validation are employed to achieve the study objectives. The\nfindings revealed opportunities for GPT models throughout the project\nlifecycle. The challenges of leveraging GPT models are highlighted and a use\ncase prototype is developed for materials selection and optimization. The\nfindings of the study would be of benefit to researchers, practitioners and\nstakeholders, as it presents research vistas for LLMs in the construction\nindustry."
            ],
            "interesting paper": 1
        },
        {
            "papers": [
                "Responsible Design Patterns for Machine Learning Pipelines\nIntegrating ethical practices into the AI development process for artificial\nintelligence (AI) is essential to ensure safe, fair, and responsible operation.\nAI ethics involves applying ethical principles to the entire life cycle of AI\nsystems. This is essential to mitigate potential risks and harms associated\nwith AI, such as algorithm biases. To achieve this goal, responsible design\npatterns (RDPs) are critical for Machine Learning (ML) pipelines to guarantee\nethical and fair outcomes. In this paper, we propose a comprehensive framework\nincorporating RDPs into ML pipelines to mitigate risks and ensure the ethical\ndevelopment of AI systems. Our framework comprises new responsible AI design\npatterns for ML pipelines identified through a survey of AI ethics and data\nmanagement experts and validated through real-world scenarios with expert\nfeedback. The framework guides AI developers, data scientists, and\npolicy-makers to implement ethical practices in AI development and deploy\nresponsible AI systems in production.",
                "A Virtual-Force Based Swarm Algorithm for Balanced Circular Bin Packing\n  Problems\nBalanced circular bin packing problems consist in positioning a given number\nof weighted circles in order to minimize the radius of a circular container\nwhile satisfying equilibrium constraints. These problems are NP-hard, highly\nconstrained and dimensional. This paper describes a swarm algorithm based on a\nvirtual-force system in order to solve balanced circular bin packing problems.\nIn the proposed approach, a system of forces is applied to each component\nallowing to take into account the constraints and minimizing the objective\nfunction using the fundamental principle of dynamics. The proposed algorithm is\nexperimented and validated on benchmarks of various balanced circular bin\npacking problems with up to 300 circles. The reported results allow to assess\nthe effectiveness of the proposed approach compared to existing results from\nthe literature.",
                "Credit Card Fraud Detection Using Asexual Reproduction Optimization\nAs the number of credit card users has increased, detecting fraud in this\ndomain has become a vital issue. Previous literature has applied various\nsupervised and unsupervised machine learning methods to find an effective fraud\ndetection system. However, some of these methods require an enormous amount of\ntime to achieve reasonable accuracy. In this paper, an Asexual Reproduction\nOptimization (ARO) approach was employed, which is a supervised method to\ndetect credit card fraud. ARO refers to a kind of production in which one\nparent produces some offspring. By applying this method and sampling just from\nthe majority class, the effectiveness of the classification is increased. A\ncomparison to Artificial Immune Systems (AIS), which is one of the best methods\nimplemented on current datasets, has shown that the proposed method is able to\nremarkably reduce the required training time and at the same time increase the\nrecall that is important in fraud detection problems. The obtained results show\nthat ARO achieves the best cost in a short time, and consequently, it can be\nconsidered a real-time fraud detection system.",
                "Constrained Causal Bayesian Optimization\nWe propose constrained causal Bayesian optimization (cCBO), an approach for\nfinding interventions in a known causal graph that optimize a target variable\nunder some constraints. cCBO first reduces the search space by exploiting the\ngraph structure and, if available, an observational dataset; and then solves\nthe restricted optimization problem by modelling target and constraint\nquantities using Gaussian processes and by sequentially selecting interventions\nvia a constrained expected improvement acquisition function. We propose\ndifferent surrogate models that enable to integrate observational and\ninterventional data while capturing correlation among effects with increasing\nlevels of sophistication. We evaluate cCBO on artificial and real-world causal\ngraphs showing successful trade off between fast convergence and percentage of\nfeasible interventions.",
                "BetaZero: Belief-State Planning for Long-Horizon POMDPs using Learned\n  Approximations\nReal-world planning problems, including autonomous driving and sustainable\nenergy applications like carbon storage and resource exploration, have recently\nbeen modeled as partially observable Markov decision processes (POMDPs) and\nsolved using approximate methods. To solve high-dimensional POMDPs in practice,\nstate-of-the-art methods use online planning with problem-specific heuristics\nto reduce planning horizons and make the problems tractable. Algorithms that\nlearn approximations to replace heuristics have recently found success in\nlarge-scale fully observable domains. The key insight is the combination of\nonline Monte Carlo tree search with offline neural network approximations of\nthe optimal policy and value function. In this work, we bring this insight to\npartially observable domains and propose BetaZero, a belief-state planning\nalgorithm for high-dimensional POMDPs. BetaZero learns offline approximations\nthat replace heuristics to enable online decision making in long-horizon\nproblems. We address several challenges inherent in large-scale partially\nobservable domains; namely challenges of transitioning in stochastic\nenvironments, prioritizing action branching with a limited search budget, and\nrepresenting beliefs as input to the network. To formalize the use of all\nlimited search information, we train against a novel $Q$-weighted visit counts\npolicy. We test BetaZero on various well-established POMDP benchmarks found in\nthe literature and a real-world problem of critical mineral exploration.\nExperiments show that BetaZero outperforms state-of-the-art POMDP solvers on a\nvariety of tasks.",
                "Representation-Driven Reinforcement Learning\nWe present a representation-driven framework for reinforcement learning. By\nrepresenting policies as estimates of their expected values, we leverage\ntechniques from contextual bandits to guide exploration and exploitation.\nParticularly, embedding a policy network into a linear feature space allows us\nto reframe the exploration-exploitation problem as a\nrepresentation-exploitation problem, where good policy representations enable\noptimal exploration. We demonstrate the effectiveness of this framework through\nits application to evolutionary and policy gradient-based approaches, leading\nto significantly improved performance compared to traditional methods. Our\nframework provides a new perspective on reinforcement learning, highlighting\nthe importance of policy representation in determining optimal\nexploration-exploitation strategies."
            ],
            "interesting paper": 5
        },
        {
            "papers": [
                "A Modular Test Bed for Reinforcement Learning Incorporation into\n  Industrial Applications\nThis application paper explores the potential of using reinforcement learning\n(RL) to address the demands of Industry 4.0, including shorter time-to-market,\nmass customization, and batch size one production. Specifically, we present a\nuse case in which the task is to transport and assemble goods through a model\nfactory following predefined rules. Each simulation run involves placing a\nspecific number of goods of random color at the entry point. The objective is\nto transport the goods to the assembly station, where two rivets are installed\nin each product, connecting the upper part to the lower part. Following the\ninstallation of rivets, blue products must be transported to the exit, while\ngreen products are to be transported to storage. The study focuses on the\napplication of reinforcement learning techniques to address this problem and\nimprove the efficiency of the production process.",
                "AI Liability Insurance With an Example in AI-Powered E-diagnosis System\nArtificial Intelligence (AI) has received an increasing amount of attention\nin multiple areas. The uncertainties and risks in AI-powered systems have\ncreated reluctance in their wild adoption. As an economic solution to\ncompensate for potential damages, AI liability insurance is a promising market\nto enhance the integration of AI into daily life. In this work, we use an\nAI-powered E-diagnosis system as an example to study AI liability insurance. We\nprovide a quantitative risk assessment model with evidence-based numerical\nanalysis. We discuss the insurability criteria for AI technologies and suggest\nnecessary adjustments to accommodate the features of AI products. We show that\nAI liability insurance can act as a regulatory mechanism to incentivize\ncompliant behaviors and serve as a certificate of high-quality AI systems.\nFurthermore, we suggest premium adjustment to reflect the dynamic evolution of\nthe inherent uncertainty in AI. Moral hazard problems are discussed and\nsuggestions for AI liability insurance are provided.",
                "An Architecture for Deploying Reinforcement Learning in Industrial\n  Environments\nIndustry 4.0 is driven by demands like shorter time-to-market, mass\ncustomization of products, and batch size one production. Reinforcement\nLearning (RL), a machine learning paradigm shown to possess a great potential\nin improving and surpassing human level performance in numerous complex tasks,\nallows coping with the mentioned demands. In this paper, we present an OPC UA\nbased Operational Technology (OT)-aware RL architecture, which extends the\nstandard RL setting, combining it with the setting of digital twins. Moreover,\nwe define an OPC UA information model allowing for a generalized plug-and-play\nlike approach for exchanging the RL agent used. In conclusion, we demonstrate\nand evaluate the architecture, by creating a proof of concept. By means of\nsolving a toy example, we show that this architecture can be used to determine\nthe optimal policy using a real control system.",
                "Deep Q-Learning versus Proximal Policy Optimization: Performance\n  Comparison in a Material Sorting Task\nThis paper presents a comparison between two well-known deep Reinforcement\nLearning (RL) algorithms: Deep Q-Learning (DQN) and Proximal Policy\nOptimization (PPO) in a simulated production system. We utilize a Petri Net\n(PN)-based simulation environment, which was previously proposed in related\nwork. The performance of the two algorithms is compared based on several\nevaluation metrics, including average percentage of correctly assembled and\nsorted products, average episode length, and percentage of successful episodes.\nThe results show that PPO outperforms DQN in terms of all evaluation metrics.\nThe study highlights the advantages of policy-based algorithms in problems with\nhigh-dimensional state and action spaces. The study contributes to the field of\ndeep RL in context of production systems by providing insights into the\neffectiveness of different algorithms and their suitability for different\ntasks.",
                "Large-Batch, Iteration-Efficient Neural Bayesian Design Optimization\nBayesian optimization (BO) provides a powerful framework for optimizing\nblack-box, expensive-to-evaluate functions. It is therefore an attractive tool\nfor engineering design problems, typically involving multiple objectives.\nThanks to the rapid advances in fabrication and measurement methods as well as\nparallel computing infrastructure, querying many design problems can be heavily\nparallelized. This class of problems challenges BO with an unprecedented setup\nwhere it has to deal with very large batches, shifting its focus from sample\nefficiency to iteration efficiency. We present a novel Bayesian optimization\nframework specifically tailored to address these limitations. Our key\ncontribution is a highly scalable, sample-based acquisition function that\nperforms a non-dominated sorting of not only the objectives but also their\nassociated uncertainty. We show that our acquisition function in combination\nwith different Bayesian neural network surrogates is effective in\ndata-intensive environments with a minimal number of iterations. We demonstrate\nthe superiority of our method by comparing it with state-of-the-art\nmulti-objective optimizations. We perform our evaluation on two real-world\nproblems -- airfoil design and 3D printing -- showcasing the applicability and\nefficiency of our approach. Our code is available at:\nhttps://github.com/an-on-ym-ous/lbn_mobo",
                "Federated Learning Games for Reconfigurable Intelligent Surfaces via\n  Causal Representations\nIn this paper, we investigate the problem of robust Reconfigurable\nIntelligent Surface (RIS) phase-shifts configuration over heterogeneous\ncommunication environments. The problem is formulated as a distributed learning\nproblem over different environments in a Federated Learning (FL) setting.\nEquivalently, this corresponds to a game played between multiple RISs, as\nlearning agents, in heterogeneous environments. Using Invariant Risk\nMinimization (IRM) and its FL equivalent, dubbed FL Games, we solve the RIS\nconfiguration problem by learning invariant causal representations across\nmultiple environments and then predicting the phases. The solution corresponds\nto playing according to Best Response Dynamics (BRD) which yields the Nash\nEquilibrium of the FL game. The representation learner and the phase predictor\nare modeled by two neural networks, and their performance is validated via\nsimulations against other benchmarks from the literature. Our results show that\ncausality-based learning yields a predictor that is 15% more accurate in unseen\nOut-of-Distribution (OoD) environments."
            ],
            "interesting paper": 4
        },
        {
            "papers": [
                "Enough With \"Human-AI Collaboration\"\nDescribing our interaction with Artificial Intelligence (AI) systems as\n'collaboration' is well-intentioned, but flawed. Not only is it misleading, but\nit also takes away the credit of AI 'labour' from the humans behind it, and\nerases and obscures an often exploitative arrangement between AI producers and\nconsumers. The AI 'collaboration' metaphor is merely the latest episode in a\nlong history of labour appropriation and credit reassignment that\ndisenfranchises labourers in the Global South. I propose that viewing AI as a\ntool or an instrument, rather than a collaborator, is more accurate, and\nultimately fairer.",
                "AlerTiger: Deep Learning for AI Model Health Monitoring at LinkedIn\nData-driven companies use AI models extensively to develop products and\nintelligent business solutions, making the health of these models crucial for\nbusiness success. Model monitoring and alerting in industries pose unique\nchallenges, including a lack of clear model health metrics definition, label\nsparsity, and fast model iterations that result in short-lived models and\nfeatures. As a product, there are also requirements for scalability,\ngeneralizability, and explainability. To tackle these challenges, we propose\nAlerTiger, a deep-learning-based MLOps model monitoring system that helps AI\nteams across the company monitor their AI models' health by detecting anomalies\nin models' input features and output score over time. The system consists of\nfour major steps: model statistics generation, deep-learning-based anomaly\ndetection, anomaly post-processing, and user alerting. Our solution generates\nthree categories of statistics to indicate AI model health, offers a two-stage\ndeep anomaly detection solution to address label sparsity and attain the\ngeneralizability of monitoring new models, and provides holistic reports for\nactionable alerts. This approach has been deployed to most of LinkedIn's\nproduction AI models for over a year and has identified several model issues\nthat later led to significant business metric gains after fixing.",
                "XAI Renaissance: Redefining Interpretability in Medical Diagnostic\n  Models\nAs machine learning models become increasingly prevalent in medical\ndiagnostics, the need for interpretability and transparency becomes paramount.\nThe XAI Renaissance signifies a significant shift in the field, aiming to\nredefine the interpretability of medical diagnostic models. This paper explores\nthe innovative approaches and methodologies within the realm of Explainable AI\n(XAI) that are revolutionizing the interpretability of medical diagnostic\nmodels. By shedding light on the underlying decision-making process, XAI\ntechniques empower healthcare professionals to understand, trust, and\neffectively utilize these models for accurate and reliable medical diagnoses.\nThis review highlights the key advancements in XAI for medical diagnostics and\ntheir potential to transform the healthcare landscape, ultimately improving\npatient outcomes and fostering trust in AI-driven diagnostic systems.",
                "Painsight: An Extendable Opinion Mining Framework for Detecting Pain\n  Points Based on Online Customer Reviews\nAs the e-commerce market continues to expand and online transactions\nproliferate, customer reviews have emerged as a critical element in shaping the\npurchasing decisions of prospective buyers. Previous studies have endeavored to\nidentify key aspects of customer reviews through the development of sentiment\nanalysis models and topic models. However, extracting specific dissatisfaction\nfactors remains a challenging task. In this study, we delineate the pain point\ndetection problem and propose Painsight, an unsupervised framework for\nautomatically extracting distinct dissatisfaction factors from customer reviews\nwithout relying on ground truth labels. Painsight employs pre-trained language\nmodels to construct sentiment analysis and topic models, leveraging attribution\nscores derived from model gradients to extract dissatisfaction factors. Upon\napplication of the proposed methodology to customer review data spanning five\nproduct categories, we successfully identified and categorized dissatisfaction\nfactors within each group, as well as isolated factors for each type. Notably,\nPainsight outperformed benchmark methods, achieving substantial performance\nenhancements and exceptional results in human evaluations.",
                "Accelerating science with human-aware artificial intelligence\nArtificial intelligence (AI) models trained on published scientific findings\nhave been used to invent valuable materials and targeted therapies, but they\ntypically ignore the human scientists who continually alter the landscape of\ndiscovery. Here we show that incorporating the distribution of human expertise\nby training unsupervised models on simulated inferences cognitively accessible\nto experts dramatically improves (up to 400%) AI prediction of future\ndiscoveries beyond those focused on research content alone, especially when\nrelevant literature is sparse. These models succeed by predicting human\npredictions and the scientists who will make them. By tuning human-aware AI to\navoid the crowd, we can generate scientifically promising \"alien\" hypotheses\nunlikely to be imagined or pursued without intervention until the distant\nfuture, which hold promise to punctuate scientific advance beyond questions\ncurrently pursued. Accelerating human discovery or probing its blind spots,\nhuman-aware AI enables us to move toward and beyond the contemporary scientific\nfrontier.",
                "A Novel Correlation-optimized Deep Learning Method for Wind Speed\n  Forecast\nThe increasing installation rate of wind power poses great challenges to the\nglobal power system. In order to ensure the reliable operation of the power\nsystem, it is necessary to accurately forecast the wind speed and power of the\nwind turbines. At present, deep learning is progressively applied to the wind\nspeed prediction. Nevertheless, the recent deep learning methods still reflect\nthe embarrassment for practical applications due to model interpretability and\nhardware limitation. To this end, a novel deep knowledge-based learning method\nis proposed in this paper. The proposed method hybridizes pre-training method\nand auto-encoder structure to improve data representation and modeling of the\ndeep knowledge-based learning framework. In order to form knowledge and\ncorresponding absorbers, the original data is preprocessed by an optimization\nmodel based on correlation to construct multi-layer networks (knowledge) which\nare absorbed by sequence to sequence (Seq2Seq) models. Specifically, new\ncognition and memory units (CMU) are designed to reinforce traditional deep\nlearning framework. Finally, the effectiveness of the proposed method is\nverified by three wind prediction cases from a wind farm in Liaoning, China.\nExperimental results show that the proposed method increases the stability and\ntraining efficiency compared to the traditional LSTM method and LSTM/GRU-based\nSeq2Seq method for applications of wind speed forecasting."
            ],
            "interesting paper": 2
        }
    ],
    "Taylor Lee": [
        {
            "papers": [
                "Towards a Capability Assessment Model for the Comprehension and Adoption\n  of AI in Organisations\nThe comprehension and adoption of Artificial Intelligence (AI) are beset with\npractical and ethical problems. This article presents a 5-level AI Capability\nAssessment Model (AI-CAM) and a related AI Capabilities Matrix (AI-CM) to\nassist practitioners in AI comprehension and adoption. These practical tools\nwere developed with business executives, technologists, and other\norganisational stakeholders in mind. They are founded on a comprehensive\nconception of AI compared to those in other AI adoption models and are also\nopen-source artefacts. Thus, the AI-CAM and AI-CM present an accessible\nresource to help inform organisational decision-makers on the capability\nrequirements for (1) AI-based data analytics use cases based on machine\nlearning technologies; (2) Knowledge representation to engineer and represent\ndata, information and knowledge using semantic technologies; and (3) AI-based\nsolutions that seek to emulate human reasoning and decision-making. The AI-CAM\ncovers the core capability dimensions (business, data, technology,\norganisation, AI skills, risks, and ethical considerations) required at the\nfive capability maturity levels to achieve optimal use of AI in organisations.",
                "Trends and Challenges Towards an Effective Data-Driven Decision Making\n  in UK SMEs: Case Studies and Lessons Learnt from the Analysis of 85 SMEs\nThe adoption of data science brings vast benefits to Small and Medium-sized\nEnterprises (SMEs) including business productivity, economic growth, innovation\nand jobs creation. Data Science can support SMEs to optimise production\nprocesses, anticipate customers' needs, predict machinery failures and deliver\nefficient smart services. Businesses can also harness the power of Artificial\nIntelligence (AI) and Big Data and the smart use of digital technologies to\nenhance productivity and performance, paving the way for innovation. However,\nintegrating data science decisions into an SME requires both skills and IT\ninvestments. In most cases, such expenses are beyond the means of SMEs due to\nlimited resources and restricted access to financing. This paper presents\ntrends and challenges towards an effective data-driven decision making for\norganisations based on a case study of 85 SMEs, mostly from the West Midlands\nregion of England. The work is supported as part of a 3 years ERDF (European\nRegional Development Funded project) in the areas of big data management,\nanalytics and business intelligence. We present two case studies that\ndemonstrates the potential of Digitisation, AI and Machine Learning and use\nthese as examples to unveil challenges and showcase the wealth of current\navailable opportunities for SMEs.",
                "Deep Learning and Ethics\nThis article appears as chapter 21 of Prince (2023, Understanding Deep\nLearning); a complete draft of the textbook is available here:\nhttp://udlbook.com. This chapter considers potential harms arising from the\ndesign and use of AI systems. These include algorithmic bias, lack of\nexplainability, data privacy violations, militarization, fraud, and\nenvironmental concerns. The aim is not to provide advice on being more ethical.\nInstead, the goal is to express ideas and start conversations in key areas that\nhave received attention in philosophy, political science, and the broader\nsocial sciences.",
                "Symbolic Regression via Control Variable Genetic Programming\nLearning symbolic expressions directly from experiment data is a vital step\nin AI-driven scientific discovery. Nevertheless, state-of-the-art approaches\nare limited to learning simple expressions. Regressing expressions involving\nmany independent variables still remain out of reach. Motivated by the control\nvariable experiments widely utilized in science, we propose Control Variable\nGenetic Programming (CVGP) for symbolic regression over many independent\nvariables. CVGP expedites symbolic expression discovery via customized\nexperiment design, rather than learning from a fixed dataset collected a\npriori. CVGP starts by fitting simple expressions involving a small set of\nindependent variables using genetic programming, under controlled experiments\nwhere other variables are held as constants. It then extends expressions\nlearned in previous generations by adding new independent variables, using new\ncontrol variable experiments in which these variables are allowed to vary.\nTheoretically, we show CVGP as an incremental building approach can yield an\nexponential reduction in the search space when learning a class of expressions.\nExperimentally, CVGP outperforms several baselines in learning symbolic\nexpressions involving multiple independent variables.",
                "Science in the Era of ChatGPT, Large Language Models and Generative AI:\n  Challenges for Research Ethics and How to Respond\nLarge language models of artificial intelligence (AI), such as ChatGPT, find\nremarkable but controversial applicability in science and research. This paper\nreviews epistemological challenges, ethical and integrity risks in science\nconduct in the advent of generative AI. This is with the aim to lay new timely\nfoundations for a high-quality research ethics review. The role of AI language\nmodels as a research instrument and subject is scrutinized along with ethical\nimplications for scientists, participants and reviewers. New emerging practices\nfor research ethics review are discussed, concluding with ten recommendations\nthat shape a response for a more responsible research conduct in the era of AI.",
                "Waiting, Banning, and Embracing: An Empirical Analysis of Adapting\n  Policies for Generative AI in Higher Education\nGenerative AI tools such as ChatGPT have recently gained significant\nattention in higher education. This study aims to understand how universities\nestablish policies regarding the use of AI tools and explore the factors that\ninfluence their decisions. Our study examines ChatGPT policies implemented at\nuniversities around the world, including their existence, content, and issuance\ndates. Specifically, we analyzed the top 500 universities according to the 2022\nQS World University Rankings. Our findings indicate that there is significant\nvariation in university policies. Less than one-third of the universities\nincluded in the study had implemented ChatGPT policies. Of the universities\nwith ChatGPT policies, approximately 67 percent embraced ChatGPT in teaching\nand learning, more than twice the number of universities that banned it. The\nmajority of the universities that ban the use of ChatGPT in assessments allow\nindividual instructors to deviate from this restrictive policy. Our empirical\nanalysis identifies several factors that are significantly and positively\ncorrelated with a university's likelihood of having a ChatGPT policy, including\nthe university's academic reputation score, being in an English-speaking\ncountry, and the general public attitudes toward ChatGPT. In addition, we found\nthat a university's likelihood of having a ban policy is positively associated\nwith faculty student ratio, citations, and the English-speaking country dummy,\nwhile negatively associated with the number of peer universities within the\nsame country that have banned ChatGPT. We discuss the challenges faced by\nuniversities based our empirical findings."
            ],
            "interesting paper": 4
        },
        {
            "papers": [
                "Deep Learning and Ethics\nThis article appears as chapter 21 of Prince (2023, Understanding Deep\nLearning); a complete draft of the textbook is available here:\nhttp://udlbook.com. This chapter considers potential harms arising from the\ndesign and use of AI systems. These include algorithmic bias, lack of\nexplainability, data privacy violations, militarization, fraud, and\nenvironmental concerns. The aim is not to provide advice on being more ethical.\nInstead, the goal is to express ideas and start conversations in key areas that\nhave received attention in philosophy, political science, and the broader\nsocial sciences.",
                "Science in the Era of ChatGPT, Large Language Models and Generative AI:\n  Challenges for Research Ethics and How to Respond\nLarge language models of artificial intelligence (AI), such as ChatGPT, find\nremarkable but controversial applicability in science and research. This paper\nreviews epistemological challenges, ethical and integrity risks in science\nconduct in the advent of generative AI. This is with the aim to lay new timely\nfoundations for a high-quality research ethics review. The role of AI language\nmodels as a research instrument and subject is scrutinized along with ethical\nimplications for scientists, participants and reviewers. New emerging practices\nfor research ethics review are discussed, concluding with ten recommendations\nthat shape a response for a more responsible research conduct in the era of AI.",
                "Lucy-SKG: Learning to Play Rocket League Efficiently Using Deep\n  Reinforcement Learning\nA successful tactic that is followed by the scientific community for\nadvancing AI is to treat games as problems, which has been proven to lead to\nvarious breakthroughs. We adapt this strategy in order to study Rocket League,\na widely popular but rather under-explored 3D multiplayer video game with a\ndistinct physics engine and complex dynamics that pose a significant challenge\nin developing efficient and high-performance game-playing agents. In this\npaper, we present Lucy-SKG, a Reinforcement Learning-based model that learned\nhow to play Rocket League in a sample-efficient manner, outperforming by a\nnotable margin the two highest-ranking bots in this game, namely Necto (2022\nbot champion) and its successor Nexto, thus becoming a state-of-the-art agent.\nOur contributions include: a) the development of a reward analysis and\nvisualization library, b) novel parameterizable reward shape functions that\ncapture the utility of complex reward types via our proposed Kinesthetic Reward\nCombination (KRC) technique, and c) design of auxiliary neural architectures\nfor training on reward prediction and state representation tasks in an\non-policy fashion for enhanced efficiency in learning speed and performance. By\nperforming thorough ablation studies for each component of Lucy-SKG, we showed\ntheir independent effectiveness in overall performance. In doing so, we\ndemonstrate the prospects and challenges of using sample-efficient\nReinforcement Learning techniques for controlling complex dynamical systems\nunder competitive team-based multiplayer conditions.",
                "Model evaluation for extreme risks\nCurrent approaches to building general-purpose AI systems tend to produce\nsystems with both beneficial and harmful capabilities. Further progress in AI\ndevelopment could lead to capabilities that pose extreme risks, such as\noffensive cyber capabilities or strong manipulation skills. We explain why\nmodel evaluation is critical for addressing extreme risks. Developers must be\nable to identify dangerous capabilities (through \"dangerous capability\nevaluations\") and the propensity of models to apply their capabilities for harm\n(through \"alignment evaluations\"). These evaluations will become critical for\nkeeping policymakers and other stakeholders informed, and for making\nresponsible decisions about model training, deployment, and security.",
                "A Systematic Review of Machine Learning Enabled Phishing\nDevelopments in artificial intelligence (AI) are likely to affect social\nengineering and change cyber defense operations. The broad and sweeping nature\nof AI impact means that many aspects of social engineering could be automated,\npotentially giving adversaries an advantage. In this review, we assess the ways\nphishing and spear-phishing might be affected by machine learning techniques.\nBy performing a systematic review of demonstrated ML-enabled phishing\ncampaigns, we take a broad survey the space for current developments. We\ndevelop a detailed approach for evaluation by creating a risk framework for\nanalyzing and contextualizing these developments. The object of this review is\nto answer the research questions: (1) Are there high-risk ML-enabled phishing\nuse cases? (2) Is there a meaningful difference between traditional targeted\nphishing campaigns and ML-enabled phishing campaigns? Practitioners may use\nthis review to inform standards, future research directions, and cyber defense\nstrategies.",
                "Asking Before Acting: Gather Information in Embodied Decision Making\n  with Language Models\nWith strong capabilities of reasoning and a broad understanding of the world,\nLarge Language Models (LLMs) have demonstrated immense potential in building\nversatile embodied decision-making agents capable of executing a wide array of\ntasks. Nevertheless, when deployed in unfamiliar environments, we show that LLM\nagents encounter challenges in efficiently gathering essential information,\nleading to suboptimal performance. Conversely, human individuals often seek\nadditional information from their peers prior to taking action, harnessing\nexternal knowledge to avoid unnecessary trial and error. Drawing inspiration\nfrom this behavior, we propose \\textit{Asking Before Acting} (ABA), a method\nthat empowers the agent to proactively inquire with external sources for\npertinent information using natural language during their interactions within\nthe environment. In this way, the agent is able to enhance its efficiency and\nperformance by circumventing potentially laborious steps and combating the\ndifficulties associated with exploration in unfamiliar environments and\nvagueness of the instructions. We conduct extensive experiments involving a\nspectrum of environments including text-based household everyday tasks, robot\narm manipulation tasks, and real world open domain image based embodied tasks.\nThe experiments involve various models from Vicuna to GPT-4. The results\ndemonstrate that, even with modest prompts modifications, ABA exhibits\nsubstantial advantages on both performance and efficiency over baseline LLM\nagents. Further finetuning ABA with reformulated metadata (ABA-FT) faciliates\nlearning the rationale for asking and allows for additional enhancements\nespecially in tasks that baselines struggle to solve."
            ],
            "interesting paper": 1
        },
        {
            "papers": [
                "Attention Paper: How Generative AI Reshapes Digital Shadow Industry?\nThe rapid development of digital economy has led to the emergence of various\nblack and shadow internet industries, which pose potential risks that can be\nidentified and managed through digital risk management (DRM) that uses\ndifferent techniques such as machine learning and deep learning. The evolution\nof DRM architecture has been driven by changes in data forms. However, the\ndevelopment of AI-generated content (AIGC) technology, such as ChatGPT and\nStable Diffusion, has given black and shadow industries powerful tools to\npersonalize data and generate realistic images and conversations for fraudulent\nactivities. This poses a challenge for DRM systems to control risks from the\nsource of data generation and to respond quickly to the fast-changing risk\nenvironment. This paper aims to provide a technical analysis of the challenges\nand opportunities of AIGC from upstream, midstream, and downstream paths of\nblack/shadow industries and suggest future directions for improving existing\nrisk control systems. The paper will explore the new black and shadow\ntechniques triggered by generative AI technology and provide insights for\nbuilding the next-generation DRM system.",
                "Alert of the Second Decision-maker: An Introduction to Human-AI Conflict\nThe collaboration between humans and artificial intelligence (AI) is a\nsignificant feature in this digital age. However, humans and AI may have\nobservation, interpretation, and action conflicts when working synchronously.\nThis phenomenon is often masked by faults and, unfortunately, overlooked. This\npaper systematically introduces the human-AI conflict concept, causes,\nmeasurement methods, and risk assessment. The results highlight that there is a\npotential second decision-maker besides the human, which is the AI; the\nhuman-AI conflict is a unique and emerging risk in digitalized process systems;\nand this is an interdisciplinary field that needs to be distinguished from\ntraditional fault and failure analysis; the conflict risk is significant and\ncannot be ignored.",
                "Generating Synergistic Formulaic Alpha Collections via Reinforcement\n  Learning\nIn the field of quantitative trading, it is common practice to transform raw\nhistorical stock data into indicative signals for the market trend. Such\nsignals are called alpha factors. Alphas in formula forms are more\ninterpretable and thus favored by practitioners concerned with risk. In\npractice, a set of formulaic alphas is often used together for better modeling\nprecision, so we need to find synergistic formulaic alpha sets that work well\ntogether. However, most traditional alpha generators mine alphas one by one\nseparately, overlooking the fact that the alphas would be combined later. In\nthis paper, we propose a new alpha-mining framework that prioritizes mining a\nsynergistic set of alphas, i.e., it directly uses the performance of the\ndownstream combination model to optimize the alpha generator. Our framework\nalso leverages the strong exploratory capabilities of reinforcement\nlearning~(RL) to better explore the vast search space of formulaic alphas. The\ncontribution to the combination models' performance is assigned to be the\nreturn used in the RL process, driving the alpha generator to find better\nalphas that improve upon the current set. Experimental evaluations on\nreal-world stock market data demonstrate both the effectiveness and the\nefficiency of our framework for stock trend forecasting. The investment\nsimulation results show that our framework is able to achieve higher returns\ncompared to previous approaches.",
                "Applying Interdisciplinary Frameworks to Understand Algorithmic\n  Decision-Making\nWe argue that explanations for \"algorithmic decision-making\" (ADM) systems\ncan profit by adopting practices that are already used in the learning\nsciences. We shortly introduce the importance of explaining ADM systems, give a\nbrief overview of approaches drawing from other disciplines to improve\nexplanations, and present the results of our qualitative task-based study\nincorporating the \"six facets of understanding\" framework. We close with\nquestions guiding the discussion of how future studies can leverage an\ninterdisciplinary approach.",
                "AI Techniques in the Microservices Life-Cycle: A Survey\nMicroservices is a popular architectural style for the development of\ndistributed software, with an emphasis on modularity, scalability, and\nflexibility. Indeed, in microservice systems, functionalities are provided by\nloosely coupled, small services, each focusing on a specific business\ncapability. Building a system according to the microservices architectural\nstyle brings a number of challenges, mainly related to how the different\nmicroservices are deployed and coordinated and how they interact. In this\npaper, we provide a survey about how techniques in the area of Artificial\nIntelligence have been used to tackle these challenges.",
                "Enhancing Human Capabilities through Symbiotic Artificial Intelligence\n  with Shared Sensory Experiences\nThe merging of human intelligence and artificial intelligence has long been a\nsubject of interest in both science fiction and academia. In this paper, we\nintroduce a novel concept in Human-AI interaction called Symbiotic Artificial\nIntelligence with Shared Sensory Experiences (SAISSE), which aims to establish\na mutually beneficial relationship between AI systems and human users through\nshared sensory experiences. By integrating multiple sensory input channels and\nprocessing human experiences, SAISSE fosters a strong human-AI bond, enabling\nAI systems to learn from and adapt to individual users, providing personalized\nsupport, assistance, and enhancement. Furthermore, we discuss the incorporation\nof memory storage units for long-term growth and development of both the AI\nsystem and its human user. As we address user privacy and ethical guidelines\nfor responsible AI-human symbiosis, we also explore potential biases and\ninequalities in AI-human symbiosis and propose strategies to mitigate these\nchallenges. Our research aims to provide a comprehensive understanding of the\nSAISSE concept and its potential to effectively support and enhance individual\nhuman users through symbiotic AI systems. This position article aims at\ndiscussing poteintial AI-human interaction related topics within the scientific\ncommunity, rather than providing experimental or theoretical results."
            ],
            "interesting paper": 3
        },
        {
            "papers": [
                "Enhancing Human Capabilities through Symbiotic Artificial Intelligence\n  with Shared Sensory Experiences\nThe merging of human intelligence and artificial intelligence has long been a\nsubject of interest in both science fiction and academia. In this paper, we\nintroduce a novel concept in Human-AI interaction called Symbiotic Artificial\nIntelligence with Shared Sensory Experiences (SAISSE), which aims to establish\na mutually beneficial relationship between AI systems and human users through\nshared sensory experiences. By integrating multiple sensory input channels and\nprocessing human experiences, SAISSE fosters a strong human-AI bond, enabling\nAI systems to learn from and adapt to individual users, providing personalized\nsupport, assistance, and enhancement. Furthermore, we discuss the incorporation\nof memory storage units for long-term growth and development of both the AI\nsystem and its human user. As we address user privacy and ethical guidelines\nfor responsible AI-human symbiosis, we also explore potential biases and\ninequalities in AI-human symbiosis and propose strategies to mitigate these\nchallenges. Our research aims to provide a comprehensive understanding of the\nSAISSE concept and its potential to effectively support and enhance individual\nhuman users through symbiotic AI systems. This position article aims at\ndiscussing poteintial AI-human interaction related topics within the scientific\ncommunity, rather than providing experimental or theoretical results.",
                "Alert of the Second Decision-maker: An Introduction to Human-AI Conflict\nThe collaboration between humans and artificial intelligence (AI) is a\nsignificant feature in this digital age. However, humans and AI may have\nobservation, interpretation, and action conflicts when working synchronously.\nThis phenomenon is often masked by faults and, unfortunately, overlooked. This\npaper systematically introduces the human-AI conflict concept, causes,\nmeasurement methods, and risk assessment. The results highlight that there is a\npotential second decision-maker besides the human, which is the AI; the\nhuman-AI conflict is a unique and emerging risk in digitalized process systems;\nand this is an interdisciplinary field that needs to be distinguished from\ntraditional fault and failure analysis; the conflict risk is significant and\ncannot be ignored.",
                "Attention Paper: How Generative AI Reshapes Digital Shadow Industry?\nThe rapid development of digital economy has led to the emergence of various\nblack and shadow internet industries, which pose potential risks that can be\nidentified and managed through digital risk management (DRM) that uses\ndifferent techniques such as machine learning and deep learning. The evolution\nof DRM architecture has been driven by changes in data forms. However, the\ndevelopment of AI-generated content (AIGC) technology, such as ChatGPT and\nStable Diffusion, has given black and shadow industries powerful tools to\npersonalize data and generate realistic images and conversations for fraudulent\nactivities. This poses a challenge for DRM systems to control risks from the\nsource of data generation and to respond quickly to the fast-changing risk\nenvironment. This paper aims to provide a technical analysis of the challenges\nand opportunities of AIGC from upstream, midstream, and downstream paths of\nblack/shadow industries and suggest future directions for improving existing\nrisk control systems. The paper will explore the new black and shadow\ntechniques triggered by generative AI technology and provide insights for\nbuilding the next-generation DRM system.",
                "A Hierarchical Approach to Population Training for Human-AI\n  Collaboration\nA major challenge for deep reinforcement learning (DRL) agents is to\ncollaborate with novel partners that were not encountered by them during the\ntraining phase. This is specifically worsened by an increased variance in\naction responses when the DRL agents collaborate with human partners due to the\nlack of consistency in human behaviors. Recent work have shown that training a\nsingle agent as the best response to a diverse population of training partners\nsignificantly increases an agent's robustness to novel partners. We further\nenhance the population-based training approach by introducing a Hierarchical\nReinforcement Learning (HRL) based method for Human-AI Collaboration. Our agent\nis able to learn multiple best-response policies as its low-level policy while\nat the same time, it learns a high-level policy that acts as a manager which\nallows the agent to dynamically switch between the low-level best-response\npolicies based on its current partner. We demonstrate that our method is able\nto dynamically adapt to novel partners of different play styles and skill\nlevels in the 2-player collaborative Overcooked game environment. We also\nconducted a human study in the same environment to test the effectiveness of\nour method when partnering with real human subjects.",
                "Quantifying the Intrinsic Usefulness of Attributional Explanations for\n  Graph Neural Networks with Artificial Simulatability Studies\nDespite the increasing relevance of explainable AI, assessing the quality of\nexplanations remains a challenging issue. Due to the high costs associated with\nhuman-subject experiments, various proxy metrics are often used to\napproximately quantify explanation quality. Generally, one possible\ninterpretation of the quality of an explanation is its inherent value for\nteaching a related concept to a student. In this work, we extend artificial\nsimulatability studies to the domain of graph neural networks. Instead of\ncostly human trials, we use explanation-supervisable graph neural networks to\nperform simulatability studies to quantify the inherent usefulness of\nattributional graph explanations. We perform an extensive ablation study to\ninvestigate the conditions under which the proposed analyses are most\nmeaningful. We additionally validate our methods applicability on real-world\ngraph classification and regression datasets. We find that relevant\nexplanations can significantly boost the sample efficiency of graph neural\nnetworks and analyze the robustness towards noise and bias in the explanations.\nWe believe that the notion of usefulness obtained from our proposed\nsimulatability analysis provides a dimension of explanation quality that is\nlargely orthogonal to the common practice of faithfulness and has great\npotential to expand the toolbox of explanation quality assessments,\nspecifically for graph explanations.",
                "Mapping ChatGPT in Mainstream Media to Unravel Jobs and Diversity\n  Challenges: Early Quantitative Insights through Sentiment Analysis and Word\n  Frequency Analysis\nThe exponential growth in user acquisition and popularity of OpenAIs ChatGPT,\nan artificial intelligence(AI) powered chatbot, was accompanied by widespread\nmainstream media coverage. This article presents a quantitative data analysis\nof the early trends and sentiments revealed by conducting text mining and NLP\nmethods onto a corpus of 10,902 mainstream news headlines related to the\nsubject of ChatGPT and artificial intelligence, from the launch of ChatGPT in\nNovember 2022 to March 2023. The findings revealed in sentiment analysis,\nChatGPT and artificial intelligence, were perceived more positively than\nnegatively in the mainstream media. In regards to word frequency results, over\nsixty-five percent of the top frequency words were focused on Big Tech issues\nand actors while topics such as jobs, diversity, ethics, copyright, gender and\nwomen were poorly represented or completely absent and only accounted for six\npercent of the total corpus. This article is a critical analysis into the power\nstructures and collusions between Big Tech and Big Media in their hegemonic\nexclusion of diversity and job challenges from mainstream media."
            ],
            "interesting paper": 1
        },
        {
            "papers": [
                "Frontier AI developers need an internal audit function\nThis article argues that frontier artificial intelligence (AI) developers\nneed an internal audit function. First, it describes the role of internal audit\nin corporate governance: internal audit evaluates the adequacy and\neffectiveness of a company's risk management, control, and governance\nprocesses. It is organizationally independent from senior management and\nreports directly to the board of directors, typically its audit committee. In\nthe IIA's Three Lines Model, internal audit serves as the third line and is\nresponsible for providing assurance to the board, while the Combined Assurance\nFramework highlights the need to coordinate the activities of internal and\nexternal assurance providers. Next, the article provides an overview of key\ngovernance challenges in frontier AI development: dangerous capabilities can\narise unpredictably and undetected; it is difficult to prevent a deployed model\nfrom causing harm; frontier models can proliferate rapidly; it is inherently\ndifficult to assess frontier AI risks; and frontier AI developers do not seem\nto follow best practices in risk governance. Finally, the article discusses how\nan internal audit function could address some of these challenges: internal\naudit could identify ineffective risk management practices; it could ensure\nthat the board of directors has a more accurate understanding of the current\nlevel of risk and the adequacy of the developer's risk management practices;\nand it could serve as a contact point for whistleblowers. In light of rapid\nprogress in AI research and development, frontier AI developers need to\nstrengthen their risk governance. Instead of reinventing the wheel, they should\nfollow existing best practices. While this might not be sufficient, they should\nnot skip this obvious first step.",
                "How Do UX Practitioners Communicate AI as a Design Material? Artifacts,\n  Conceptions, and Propositions\nUX practitioners (UXPs) face novel challenges when working with and\ncommunicating artificial intelligence (AI) as a design material. We explore how\nUXPs communicate AI concepts when given hands-on experience training and\nexperimenting with AI models. To do so, we conducted a task-based design study\nwith 27 UXPs in which they prototyped and created a design presentation for a\nAI-enabled interface while having access to a simple AI model training tool.\nThrough analyzing UXPs' design presentations and post-activity interviews, we\nfound that although UXPs struggled to clearly communicate some AI concepts,\ntinkering with AI broadened common ground when communicating with technical\nstakeholders. UXPs also identified key risks and benefits of AI in their\ndesigns, and proposed concrete next steps for both UX and AI work. We conclude\nwith a sensitizing concept and recommendations for design and AI tools to\nenhance multi-stakeholder communication and collaboration when crafting\nhuman-centered AI experiences.",
                "A Framework for Incentivized Collaborative Learning\nCollaborations among various entities, such as companies, research labs, AI\nagents, and edge devices, have become increasingly crucial for achieving\nmachine learning tasks that cannot be accomplished by a single entity alone.\nThis is likely due to factors such as security constraints, privacy concerns,\nand limitations in computation resources. As a result, collaborative learning\n(CL) research has been gaining momentum. However, a significant challenge in\npractical applications of CL is how to effectively incentivize multiple\nentities to collaborate before any collaboration occurs. In this study, we\npropose ICL, a general framework for incentivized collaborative learning, and\nprovide insights into the critical issue of when and why incentives can improve\ncollaboration performance. Furthermore, we show the broad applicability of ICL\nto specific cases in federated learning, assisted learning, and multi-armed\nbandit with both theory and experimental results.",
                "Learning Interpretable Models of Aircraft Handling Behaviour by\n  Reinforcement Learning from Human Feedback\nWe propose a method to capture the handling abilities of fast jet pilots in a\nsoftware model via reinforcement learning (RL) from human preference feedback.\nWe use pairwise preferences over simulated flight trajectories to learn an\ninterpretable rule-based model called a reward tree, which enables the\nautomated scoring of trajectories alongside an explanatory rationale. We train\nan RL agent to execute high-quality handling behaviour by using the reward tree\nas the objective, and thereby generate data for iterative preference collection\nand further refinement of both tree and agent. Experiments with synthetic\npreferences show reward trees to be competitive with uninterpretable neural\nnetwork reward models on quantitative and qualitative evaluations.",
                "Towards Cognitive Bots: Architectural Research Challenges\nSoftware bots operating in multiple virtual digital platforms must understand\nthe platforms' affordances and behave like human users. Platform affordances or\nfeatures differ from one application platform to another or through a life\ncycle, requiring such bots to be adaptable. Moreover, bots in such platforms\ncould cooperate with humans or other software agents for work or to learn\nspecific behavior patterns. However, present-day bots, particularly chatbots,\nother than language processing and prediction, are far from reaching a human\nuser's behavior level within complex business information systems. They lack\nthe cognitive capabilities to sense and act in such virtual environments,\nrendering their development a challenge to artificial general intelligence\nresearch. In this study, we problematize and investigate assumptions in\nconceptualizing software bot architecture by directing attention to significant\narchitectural research challenges in developing cognitive bots endowed with\ncomplex behavior for operation on information systems. As an outlook, we\npropose alternate architectural assumptions to consider in future bot design\nand bot development frameworks.",
                "Moral Machine or Tyranny of the Majority?\nWith Artificial Intelligence systems increasingly applied in consequential\ndomains, researchers have begun to ask how these systems ought to act in\nethically charged situations where even humans lack consensus. In the Moral\nMachine project, researchers crowdsourced answers to \"Trolley Problems\"\nconcerning autonomous vehicles. Subsequently, Noothigattu et al. (2018)\nproposed inferring linear functions that approximate each individual's\npreferences and aggregating these linear models by averaging parameters across\nthe population. In this paper, we examine this averaging mechanism, focusing on\nfairness concerns in the presence of strategic effects. We investigate a simple\nsetting where the population consists of two groups, with the minority\nconstituting an {\\alpha} < 0.5 share of the population. To simplify the\nanalysis, we consider the extreme case in which within-group preferences are\nhomogeneous. Focusing on the fraction of contested cases where the minority\ngroup prevails, we make the following observations: (a) even when all parties\nreport their preferences truthfully, the fraction of disputes where the\nminority prevails is less than proportionate in {\\alpha}; (b) the degree of\nsub-proportionality grows more severe as the level of disagreement between the\ngroups increases; (c) when parties report preferences strategically, pure\nstrategy equilibria do not always exist; and (d) whenever a pure strategy\nequilibrium exists, the majority group prevails 100% of the time. These\nfindings raise concerns about stability and fairness of preference vector\naveraging as a mechanism for aggregating diverging voices. Finally, we discuss\nalternatives, including randomized dictatorship and median-based mechanisms."
            ],
            "interesting paper": 3
        },
        {
            "papers": [
                "Training Socially Aligned Language Models on Simulated Social\n  Interactions\nSocial alignment in AI systems aims to ensure that these models behave\naccording to established societal values. However, unlike humans, who derive\nconsensus on value judgments through social interaction, current language\nmodels (LMs) are trained to rigidly replicate their training corpus in\nisolation, leading to subpar generalization in unfamiliar scenarios and\nvulnerability to adversarial attacks. This work presents a novel training\nparadigm that permits LMs to learn from simulated social interactions. In\ncomparison to existing methodologies, our approach is considerably more\nscalable and efficient, demonstrating superior performance in alignment\nbenchmarks and human evaluations. This paradigm shift in the training of LMs\nbrings us a step closer to developing AI systems that can robustly and\naccurately reflect societal norms and values.",
                "Mindstorms in Natural Language-Based Societies of Mind\nBoth Minsky's \"society of mind\" and Schmidhuber's \"learning to think\" inspire\ndiverse societies of large multimodal neural networks (NNs) that solve problems\nby interviewing each other in a \"mindstorm.\" Recent implementations of NN-based\nsocieties of minds consist of large language models (LLMs) and other NN-based\nexperts communicating through a natural language interface. In doing so, they\novercome the limitations of single LLMs, improving multimodal zero-shot\nreasoning. In these natural language-based societies of mind (NLSOMs), new\nagents -- all communicating through the same universal symbolic language -- are\neasily added in a modular fashion. To demonstrate the power of NLSOMs, we\nassemble and experiment with several of them (having up to 129 members),\nleveraging mindstorms in them to solve some practical AI tasks: visual question\nanswering, image captioning, text-to-image synthesis, 3D generation, egocentric\nretrieval, embodied AI, and general language-based task solving. We view this\nas a starting point towards much larger NLSOMs with billions of agents-some of\nwhich may be humans. And with this emergence of great societies of\nheterogeneous minds, many new research questions have suddenly become paramount\nto the future of artificial intelligence. What should be the social structure\nof an NLSOM? What would be the (dis)advantages of having a monarchical rather\nthan a democratic structure? How can principles of NN economies be used to\nmaximize the total reward of a reinforcement learning NLSOM? In this work, we\nidentify, discuss, and try to answer some of these questions.",
                "Attention Schema in Neural Agents\nAttention has become a common ingredient in deep learning architectures. It\nadds a dynamical selection of information on top of the static selection of\ninformation supported by weights. In the same way, we can imagine a\nhigher-order informational filter built on top of attention: an Attention\nSchema (AS), namely, a descriptive and predictive model of attention. In\ncognitive neuroscience, Attention Schema Theory (AST) supports this idea of\ndistinguishing attention from AS. A strong prediction of this theory is that an\nagent can use its own AS to also infer the states of other agents' attention\nand consequently enhance coordination with other agents. As such, multi-agent\nreinforcement learning would be an ideal setting to experimentally test the\nvalidity of AST. We explore different ways in which attention and AS interact\nwith each other. Our preliminary results indicate that agents that implement\nthe AS as a recurrent internal control achieve the best performance. In\ngeneral, these exploratory experiments suggest that equipping artificial agents\nwith a model of attention can enhance their social intelligence.",
                "A Framework for Incentivized Collaborative Learning\nCollaborations among various entities, such as companies, research labs, AI\nagents, and edge devices, have become increasingly crucial for achieving\nmachine learning tasks that cannot be accomplished by a single entity alone.\nThis is likely due to factors such as security constraints, privacy concerns,\nand limitations in computation resources. As a result, collaborative learning\n(CL) research has been gaining momentum. However, a significant challenge in\npractical applications of CL is how to effectively incentivize multiple\nentities to collaborate before any collaboration occurs. In this study, we\npropose ICL, a general framework for incentivized collaborative learning, and\nprovide insights into the critical issue of when and why incentives can improve\ncollaboration performance. Furthermore, we show the broad applicability of ICL\nto specific cases in federated learning, assisted learning, and multi-armed\nbandit with both theory and experimental results.",
                "Moral Machine or Tyranny of the Majority?\nWith Artificial Intelligence systems increasingly applied in consequential\ndomains, researchers have begun to ask how these systems ought to act in\nethically charged situations where even humans lack consensus. In the Moral\nMachine project, researchers crowdsourced answers to \"Trolley Problems\"\nconcerning autonomous vehicles. Subsequently, Noothigattu et al. (2018)\nproposed inferring linear functions that approximate each individual's\npreferences and aggregating these linear models by averaging parameters across\nthe population. In this paper, we examine this averaging mechanism, focusing on\nfairness concerns in the presence of strategic effects. We investigate a simple\nsetting where the population consists of two groups, with the minority\nconstituting an {\\alpha} < 0.5 share of the population. To simplify the\nanalysis, we consider the extreme case in which within-group preferences are\nhomogeneous. Focusing on the fraction of contested cases where the minority\ngroup prevails, we make the following observations: (a) even when all parties\nreport their preferences truthfully, the fraction of disputes where the\nminority prevails is less than proportionate in {\\alpha}; (b) the degree of\nsub-proportionality grows more severe as the level of disagreement between the\ngroups increases; (c) when parties report preferences strategically, pure\nstrategy equilibria do not always exist; and (d) whenever a pure strategy\nequilibrium exists, the majority group prevails 100% of the time. These\nfindings raise concerns about stability and fairness of preference vector\naveraging as a mechanism for aggregating diverging voices. Finally, we discuss\nalternatives, including randomized dictatorship and median-based mechanisms.",
                "Towards Cognitive Bots: Architectural Research Challenges\nSoftware bots operating in multiple virtual digital platforms must understand\nthe platforms' affordances and behave like human users. Platform affordances or\nfeatures differ from one application platform to another or through a life\ncycle, requiring such bots to be adaptable. Moreover, bots in such platforms\ncould cooperate with humans or other software agents for work or to learn\nspecific behavior patterns. However, present-day bots, particularly chatbots,\nother than language processing and prediction, are far from reaching a human\nuser's behavior level within complex business information systems. They lack\nthe cognitive capabilities to sense and act in such virtual environments,\nrendering their development a challenge to artificial general intelligence\nresearch. In this study, we problematize and investigate assumptions in\nconceptualizing software bot architecture by directing attention to significant\narchitectural research challenges in developing cognitive bots endowed with\ncomplex behavior for operation on information systems. As an outlook, we\npropose alternate architectural assumptions to consider in future bot design\nand bot development frameworks."
            ],
            "interesting paper": 1
        },
        {
            "papers": [
                "Incentivizing honest performative predictions with proper scoring rules\nProper scoring rules incentivize experts to accurately report beliefs,\nassuming predictions cannot influence outcomes. We relax this assumption and\ninvestigate incentives when predictions are performative, i.e., when they can\ninfluence the outcome of the prediction, such as when making public predictions\nabout the stock market. We say a prediction is a fixed point if it accurately\nreflects the expert's beliefs after that prediction has been made. We show that\nin this setting, reports maximizing expected score generally do not reflect an\nexpert's beliefs, and we give bounds on the inaccuracy of such reports. We show\nthat, for binary predictions, if the influence of the expert's prediction on\noutcomes is bounded, it is possible to define scoring rules under which optimal\nreports are arbitrarily close to fixed points. However, this is impossible for\npredictions over more than two outcomes. We also perform numerical simulations\nin a toy setting, showing that our bounds are tight in some situations and that\nprediction error is often substantial (greater than 5-10%). Lastly, we discuss\nalternative notions of optimality, including performative stability, and show\nthat they incentivize reporting fixed points.",
                "Optimization's Neglected Normative Commitments\nOptimization is offered as an objective approach to resolving complex,\nreal-world decisions involving uncertainty and conflicting interests. It drives\nbusiness strategies as well as public policies and, increasingly, lies at the\nheart of sophisticated machine learning systems. A paradigm used to approach\npotentially high-stakes decisions, optimization relies on abstracting the real\nworld to a set of decision(s), objective(s) and constraint(s). Drawing from the\nmodeling process and a range of actual cases, this paper describes the\nnormative choices and assumptions that are necessarily part of using\noptimization. It then identifies six emergent problems that may be neglected:\n1) Misspecified values can yield optimizations that omit certain imperatives\naltogether or incorporate them incorrectly as a constraint or as part of the\nobjective, 2) Problematic decision boundaries can lead to faulty modularity\nassumptions and feedback loops, 3) Failing to account for multiple agents'\ndivergent goals and decisions can lead to policies that serve only certain\nnarrow interests, 4) Mislabeling and mismeasurement can introduce bias and\nimprecision, 5) Faulty use of relaxation and approximation methods,\nunaccompanied by formal characterizations and guarantees, can severely impede\napplicability, and 6) Treating optimization as a justification for action,\nwithout specifying the necessary contextual information, can lead to ethically\ndubious or faulty decisions. Suggestions are given to further understand and\ncurb the harms that can arise when optimization is used wrongfully.",
                "AI Coach Assist: An Automated Approach for Call Recommendation in\n  Contact Centers for Agent Coaching\nIn recent years, the utilization of Artificial Intelligence (AI) in the\ncontact center industry is on the rise. One area where AI can have a\nsignificant impact is in the coaching of contact center agents. By analyzing\ncall transcripts using Natural Language Processing (NLP) techniques, it would\nbe possible to quickly determine which calls are most relevant for coaching\npurposes. In this paper, we present AI Coach Assist, which leverages the\npre-trained transformer-based language models to determine whether a given call\nis coachable or not based on the quality assurance (QA) questions asked by the\ncontact center managers or supervisors. The system was trained and evaluated on\na large dataset collected from real-world contact centers and provides an\neffective way to recommend calls to the contact center managers that are more\nlikely to contain coachable moments. Our experimental findings demonstrate the\npotential of AI Coach Assist to improve the coaching process, resulting in\nenhancing the performance of contact center agents.",
                "A Comprehensive Overview and Comparative Analysis on Deep Learning\n  Models: CNN, RNN, LSTM, GRU\nDeep learning (DL) has emerged as a powerful subset of machine learning (ML)\nand artificial intelligence (AI), outperforming traditional ML methods,\nespecially in handling unstructured and large datasets. Its impact spans across\nvarious domains, including speech recognition, healthcare, autonomous vehicles,\ncybersecurity, predictive analytics, and more. However, the complexity and\ndynamic nature of real-world problems present challenges in designing effective\ndeep learning models. Consequently, several deep learning models have been\ndeveloped to address different problems and applications. In this article, we\nconduct a comprehensive survey of various deep learning models, including\nConvolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs),\nGenerative Models, Deep Reinforcement Learning (DRL), and Deep Transfer\nLearning. We examine the structure, applications, benefits, and limitations of\neach model. Furthermore, we perform an analysis using three publicly available\ndatasets: IMDB, ARAS, and Fruit-360. We compare the performance of six renowned\ndeep learning models: CNN, Simple RNN, Long Short-Term Memory (LSTM),\nBidirectional LSTM, Gated Recurrent Unit (GRU), and Bidirectional GRU.",
                "Counterfactual Formulation of Patient-Specific Root Causes of Disease\nRoot causes of disease intuitively correspond to root vertices that increase\nthe likelihood of a diagnosis. This description of a root cause nevertheless\nlacks the rigorous mathematical formulation needed for the development of\ncomputer algorithms designed to automatically detect root causes from data.\nPrior work defined patient-specific root causes of disease using an\ninterventionalist account that only climbs to the second rung of Pearl's Ladder\nof Causation. In this theoretical piece, we climb to the third rung by\nproposing a counterfactual definition matching clinical intuition based on\nfixed factual data alone. We then show how to assign a root causal contribution\nscore to each variable using Shapley values from explainable artificial\nintelligence. The proposed counterfactual formulation of patient-specific root\ncauses of disease accounts for noisy labels, adapts to disease prevalence and\nadmits fast computation without the need for counterfactual simulation.",
                "Backdooring Neural Code Search\nReusing off-the-shelf code snippets from online repositories is a common\npractice, which significantly enhances the productivity of software developers.\nTo find desired code snippets, developers resort to code search engines through\nnatural language queries. Neural code search models are hence behind many such\nengines. These models are based on deep learning and gain substantial attention\ndue to their impressive performance. However, the security aspect of these\nmodels is rarely studied. Particularly, an adversary can inject a backdoor in\nneural code search models, which return buggy or even vulnerable code with\nsecurity/privacy issues. This may impact the downstream software (e.g., stock\ntrading systems and autonomous driving) and cause financial loss and/or\nlife-threatening incidents. In this paper, we demonstrate such attacks are\nfeasible and can be quite stealthy. By simply modifying one variable/function\nname, the attacker can make buggy/vulnerable code rank in the top 11%. Our\nattack BADCODE features a special trigger generation and injection procedure,\nmaking the attack more effective and stealthy. The evaluation is conducted on\ntwo neural code search models and the results show our attack outperforms\nbaselines by 60%. Our user study demonstrates that our attack is more stealthy\nthan the baseline by two times based on the F1 score."
            ],
            "interesting paper": 6
        },
        {
            "papers": [
                "AI Coach Assist: An Automated Approach for Call Recommendation in\n  Contact Centers for Agent Coaching\nIn recent years, the utilization of Artificial Intelligence (AI) in the\ncontact center industry is on the rise. One area where AI can have a\nsignificant impact is in the coaching of contact center agents. By analyzing\ncall transcripts using Natural Language Processing (NLP) techniques, it would\nbe possible to quickly determine which calls are most relevant for coaching\npurposes. In this paper, we present AI Coach Assist, which leverages the\npre-trained transformer-based language models to determine whether a given call\nis coachable or not based on the quality assurance (QA) questions asked by the\ncontact center managers or supervisors. The system was trained and evaluated on\na large dataset collected from real-world contact centers and provides an\neffective way to recommend calls to the contact center managers that are more\nlikely to contain coachable moments. Our experimental findings demonstrate the\npotential of AI Coach Assist to improve the coaching process, resulting in\nenhancing the performance of contact center agents.",
                "Modeling Dynamic Environments with Scene Graph Memory\nEmbodied AI agents that search for objects in large environments such as\nhouseholds often need to make efficient decisions by predicting object\nlocations based on partial information. We pose this as a new type of link\nprediction problem: link prediction on partially observable dynamic graphs. Our\ngraph is a representation of a scene in which rooms and objects are nodes, and\ntheir relationships are encoded in the edges; only parts of the changing graph\nare known to the agent at each timestep. This partial observability poses a\nchallenge to existing link prediction approaches, which we address. We propose\na novel state representation -- Scene Graph Memory (SGM) -- with captures the\nagent's accumulated set of observations, as well as a neural net architecture\ncalled a Node Edge Predictor (NEP) that extracts information from the SGM to\nsearch efficiently. We evaluate our method in the Dynamic House Simulator, a\nnew benchmark that creates diverse dynamic graphs following the semantic\npatterns typically seen at homes, and show that NEP can be trained to predict\nthe locations of objects in a variety of environments with diverse object\nmovement dynamics, outperforming baselines both in terms of new scene\nadaptability and overall accuracy. The codebase and more can be found at\nhttps://www.scenegraphmemory.com.",
                "Integrating Action Knowledge and LLMs for Task Planning and Situation\n  Handling in Open Worlds\nTask planning systems have been developed to help robots use human knowledge\n(about actions) to complete long-horizon tasks. Most of them have been\ndeveloped for \"closed worlds\" while assuming the robot is provided with\ncomplete world knowledge. However, the real world is generally open, and the\nrobots frequently encounter unforeseen situations that can potentially break\nthe planner's completeness. Could we leverage the recent advances on\npre-trained Large Language Models (LLMs) to enable classical planning systems\nto deal with novel situations?\n  This paper introduces a novel framework, called COWP, for open-world task\nplanning and situation handling. COWP dynamically augments the robot's action\nknowledge, including the preconditions and effects of actions, with\ntask-oriented commonsense knowledge. COWP embraces the openness from LLMs, and\nis grounded to specific domains via action knowledge. For systematic\nevaluations, we collected a dataset that includes 1,085 execution-time\nsituations. Each situation corresponds to a state instance wherein a robot is\npotentially unable to complete a task using a solution that normally works.\nExperimental results show that our approach outperforms competitive baselines\nfrom the literature in the success rate of service tasks. Additionally, we have\ndemonstrated COWP using a mobile manipulator. Supplementary materials are\navailable at: https://cowplanning.github.io/",
                "Synthesizing a Progression of Subtasks for Block-Based Visual\n  Programming Tasks\nBlock-based visual programming environments play an increasingly important\nrole in introducing computing concepts to K-12 students. In recent years, they\nhave also gained popularity in neuro-symbolic AI, serving as a benchmark to\nevaluate general problem-solving and logical reasoning skills. The open-ended\nand conceptual nature of these visual programming tasks make them challenging,\nboth for state-of-the-art AI agents as well as for novice programmers. A\nnatural approach to providing assistance for problem-solving is breaking down a\ncomplex task into a progression of simpler subtasks; however, this is not\ntrivial given that the solution codes are typically nested and have non-linear\nexecution behavior. In this paper, we formalize the problem of synthesizing\nsuch a progression for a given reference block-based visual programming task.\nWe propose a novel synthesis algorithm that generates a progression of subtasks\nthat are high-quality, well-spaced in terms of their complexity, and solving\nthis progression leads to solving the reference task. We show the utility of\nour synthesis algorithm in improving the efficacy of AI agents (in this case,\nneural program synthesizers) for solving tasks in the Karel programming\nenvironment. Then, we conduct a user study to demonstrate that our synthesized\nprogression of subtasks can assist a novice programmer in solving tasks in the\nHour of Code: Maze Challenge by Code-dot-org.",
                "Observation Denoising in CYRUS Soccer Simulation 2D Team For RoboCup\n  2023\nThe RoboCup competitions hold various leagues, and the Soccer Simulation 2D\nLeague is a major one among them. Soccer Simulation 2D (SS2D) match involves\ntwo teams, including 11 players and a coach, competing against each other. The\nplayers can only communicate with the Soccer Simulation Server during the game.\nThis paper presents the latest research of the CYRUS soccer simulation 2D team,\nthe champion of RoboCup 2021. We will explain our denoising idea powered by\nlong short-term memory networks (LSTM) and deep neural networks (DNN). The\nCYRUS team uses the CYRUS2D base code that was developed based on the Helios\nand Gliders bases.",
                "On the Value of Myopic Behavior in Policy Reuse\nLeveraging learned strategies in unfamiliar scenarios is fundamental to human\nintelligence. In reinforcement learning, rationally reusing the policies\nacquired from other tasks or human experts is critical for tackling problems\nthat are difficult to learn from scratch. In this work, we present a framework\ncalled Selective Myopic bEhavior Control~(SMEC), which results from the insight\nthat the short-term behaviors of prior policies are sharable across tasks. By\nevaluating the behaviors of prior policies via a hybrid value function\narchitecture, SMEC adaptively aggregates the sharable short-term behaviors of\nprior policies and the long-term behaviors of the task policy, leading to\ncoordinated decisions. Empirical results on a collection of manipulation and\nlocomotion tasks demonstrate that SMEC outperforms existing methods, and\nvalidate the ability of SMEC to leverage related prior policies."
            ],
            "interesting paper": 3
        },
        {
            "papers": [
                "The Digital Divide in Process Safety: Quantitative Risk Analysis of\n  Human-AI Collaboration\nDigital technologies have dramatically accelerated the digital transformation\nin process industries, boosted new industrial applications, upgraded the\nproduction system, and enhanced operational efficiency. In contrast, the\nchallenges and gaps between human and artificial intelligence (AI) have become\nmore and more prominent, whereas the digital divide in process safety is\naggregating. The study attempts to address the following questions: (i)What is\nAI in the process safety context? (ii)What is the difference between AI and\nhumans in process safety? (iii)How do AI and humans collaborate in process\nsafety? (iv)What are the challenges and gaps in human-AI collaboration? (v)How\nto quantify the risk of human-AI collaboration in process safety? Qualitative\nrisk analysis based on brainstorming and literature review, and quantitative\nrisk analysis based on layer of protection analysis (LOPA) and Bayesian network\n(BN), were applied to explore and model. The importance of human reliability\nshould be stressed in the digital age, not usually to increase the reliability\nof AI, and human-centered AI design in process safety needs to be propagated.",
                "ChatGPT Informed Graph Neural Network for Stock Movement Prediction\nChatGPT has demonstrated remarkable capabilities across various natural\nlanguage processing (NLP) tasks. However, its potential for inferring dynamic\nnetwork structures from temporal textual data, specifically financial news,\nremains an unexplored frontier. In this research, we introduce a novel\nframework that leverages ChatGPT's graph inference capabilities to enhance\nGraph Neural Networks (GNN). Our framework adeptly extracts evolving network\nstructures from textual data, and incorporates these networks into graph neural\nnetworks for subsequent predictive tasks. The experimental results from stock\nmovement forecasting indicate our model has consistently outperformed the\nstate-of-the-art Deep Learning-based benchmarks. Furthermore, the portfolios\nconstructed based on our model's outputs demonstrate higher annualized\ncumulative returns, alongside reduced volatility and maximum drawdown. This\nsuperior performance highlights the potential of ChatGPT for text-based network\ninferences and underscores its promising implications for the financial sector.",
                "The Social Value of Dark Energy\nAstrophysics is a social enterprise exemplified here by the Dark Energy\nSurvey (DES) which completed its fieldwork in 2019 after 16 years of\npreparation and observation, while data analysis continues. Society funds\nastrophysics on a grand scale. For human capital and for governance the\ndiscipline draws on a self-governing \"republic of science\", while the funds\nwere provided by philanthropists in the past, and by governments today. The\nbenefits accrue initially to scientists themselves, in the form of a rewarding\nvocation. For the social benefit it is tempting to apply formal cost benefit\nanalysis, but that approach ignores the option value of science and imposes\nquestionable assumptions from welfare economics. Astrophysics generates some\nuseful spinoffs, offers attractive careers, appeals to the popular imagination,\nspeaks to metaphysical cravings and constitutes a good in itself. The rise of\nAI also suggests a role in exploring future habitats for intelligence and\ncognition.",
                "Employing Explainable Artificial Intelligence (XAI) Methodologies to\n  Analyze the Correlation between Input Variables and Tensile Strength in\n  Additively Manufactured Samples\nThis research paper explores the impact of various input parameters,\nincluding Infill percentage, Layer Height, Extrusion Temperature, and Print\nSpeed, on the resulting Tensile Strength in objects produced through additive\nmanufacturing. The main objective of this study is to enhance our understanding\nof the correlation between the input parameters and Tensile Strength, as well\nas to identify the key factors influencing the performance of the additive\nmanufacturing process. To achieve this objective, we introduced the utilization\nof Explainable Artificial Intelligence (XAI) techniques for the first time,\nwhich allowed us to analyze the data and gain valuable insights into the\nsystem's behavior. Specifically, we employed SHAP (SHapley Additive\nexPlanations), a widely adopted framework for interpreting machine learning\nmodel predictions, to provide explanations for the behavior of a machine\nlearning model trained on the data. Our findings reveal that the Infill\npercentage and Extrusion Temperature have the most significant influence on\nTensile Strength, while the impact of Layer Height and Print Speed is\nrelatively minor. Furthermore, we discovered that the relationship between the\ninput parameters and Tensile Strength is highly intricate and nonlinear, making\nit difficult to accurately describe using simple linear models.",
                "AI Audit: A Card Game to Reflect on Everyday AI Systems\nAn essential element of K-12 AI literacy is educating learners about the\nethical and societal implications of AI systems. Previous work in AI ethics\nliteracy have developed curriculum and classroom activities that engage\nlearners in reflecting on the ethical implications of AI systems and developing\nresponsible AI. There is little work in using game-based learning methods in AI\nliteracy. Games are known to be compelling media to teach children about\ncomplex STEM concepts. In this work, we developed a competitive card game for\nmiddle and high school students called \"AI Audit\" where they play as AI\nstart-up founders building novel AI-powered technology. Players can challenge\nother players with potential harms of their technology or defend their own\nbusinesses by features that mitigate these harms. The game mechanics reward\nsystems that are ethically developed or that take steps to mitigate potential\nharms. In this paper, we present the game design, teacher resources for\nclassroom deployment and early playtesting results. We discuss our reflections\nabout using games as teaching tools for AI literacy in K-12 classrooms.",
                "Safety of autonomous vehicles: A survey on Model-based vs. AI-based\n  approaches\nThe growing advancements in Autonomous Vehicles (AVs) have emphasized the\ncritical need to prioritize the absolute safety of AV maneuvers, especially in\ndynamic and unpredictable environments or situations. This objective becomes\neven more challenging due to the uniqueness of every traffic\nsituation/condition. To cope with all these very constrained and complex\nconfigurations, AVs must have appropriate control architectures with reliable\nand real-time Risk Assessment and Management Strategies (RAMS). These targeted\nRAMS must lead to reduce drastically the navigation risks. However, the lack of\nsafety guarantees proves, which is one of the key challenges to be addressed,\nlimit drastically the ambition to introduce more broadly AVs on our roads and\nrestrict the use of AVs to very limited use cases. Therefore, the focus and the\nambition of this paper is to survey research on autonomous vehicles while\nfocusing on the important topic of safety guarantee of AVs. For this purpose,\nit is proposed to review research on relevant methods and concepts defining an\noverall control architecture for AVs, with an emphasis on the safety assessment\nand decision-making systems composing these architectures. Moreover, it is\nintended through this reviewing process to highlight researches that use either\nmodel-based methods or AI-based approaches. This is performed while emphasizing\nthe strengths and weaknesses of each methodology and investigating the research\nthat proposes a comprehensive multi-modal design that combines model-based and\nAI approaches. This paper ends with discussions on the methods used to\nguarantee the safety of AVs namely: safety verification techniques and the\nstandardization/generalization of safety frameworks."
            ],
            "interesting paper": 2
        },
        {
            "papers": [
                "Re-imagining health and well-being in low resource African settings\n  using an augmented AI system and a 3D digital twin\nThis paper discusses and explores the potential and relevance of recent\ndevelopments in artificial intelligence (AI) and digital twins for health and\nwell-being in low-resource African countries. We use the case of public health\nemergency response to disease outbreaks and epidemic control. There is\npotential to take advantage of the increasing availability of data and\ndigitization to develop advanced AI methods for analysis and prediction. Using\nan AI systems perspective, we review emerging trends in AI systems and digital\ntwins and propose an initial augmented AI system architecture to illustrate how\nan AI system can work with a 3D digital twin to address public health goals. We\nhighlight scientific knowledge discovery, continual learning, pragmatic\ninteroperability, and interactive explanation and decision-making as essential\nresearch challenges for AI systems and digital twins.",
                "AI Audit: A Card Game to Reflect on Everyday AI Systems\nAn essential element of K-12 AI literacy is educating learners about the\nethical and societal implications of AI systems. Previous work in AI ethics\nliteracy have developed curriculum and classroom activities that engage\nlearners in reflecting on the ethical implications of AI systems and developing\nresponsible AI. There is little work in using game-based learning methods in AI\nliteracy. Games are known to be compelling media to teach children about\ncomplex STEM concepts. In this work, we developed a competitive card game for\nmiddle and high school students called \"AI Audit\" where they play as AI\nstart-up founders building novel AI-powered technology. Players can challenge\nother players with potential harms of their technology or defend their own\nbusinesses by features that mitigate these harms. The game mechanics reward\nsystems that are ethically developed or that take steps to mitigate potential\nharms. In this paper, we present the game design, teacher resources for\nclassroom deployment and early playtesting results. We discuss our reflections\nabout using games as teaching tools for AI literacy in K-12 classrooms.",
                "The Social Value of Dark Energy\nAstrophysics is a social enterprise exemplified here by the Dark Energy\nSurvey (DES) which completed its fieldwork in 2019 after 16 years of\npreparation and observation, while data analysis continues. Society funds\nastrophysics on a grand scale. For human capital and for governance the\ndiscipline draws on a self-governing \"republic of science\", while the funds\nwere provided by philanthropists in the past, and by governments today. The\nbenefits accrue initially to scientists themselves, in the form of a rewarding\nvocation. For the social benefit it is tempting to apply formal cost benefit\nanalysis, but that approach ignores the option value of science and imposes\nquestionable assumptions from welfare economics. Astrophysics generates some\nuseful spinoffs, offers attractive careers, appeals to the popular imagination,\nspeaks to metaphysical cravings and constitutes a good in itself. The rise of\nAI also suggests a role in exploring future habitats for intelligence and\ncognition.",
                "Voluminous yet Vacuous? Semantic Capital in an Age of Large Language\n  Models\nLarge Language Models (LLMs) have emerged as transformative forces in the\nrealm of natural language processing, wielding the power to generate human-like\ntext. However, despite their potential for content creation, they carry the\nrisk of eroding our Semantic Capital (SC) - the collective knowledge within our\ndigital ecosystem - thereby posing diverse social epistemic challenges. This\npaper explores the evolution, capabilities, and limitations of these models,\nwhile highlighting ethical concerns they raise. The study contribution is\ntwo-fold: first, it is acknowledged that, withstanding the challenges of\ntracking and controlling LLM impacts, it is necessary to reconsider our\ninteraction with these AI technologies and the narratives that form public\nperception of them. It is argued that before achieving this goal, it is\nessential to confront a potential deontological tipping point in an increasing\nAI-driven infosphere. This goes beyond just adhering to AI ethical norms or\nregulations and requires understanding the spectrum of social epistemic risks\nLLMs might bring to our collective SC. Secondly, building on Luciano Floridi's\ntaxonomy for SC risks, those are mapped within the functionality and\nconstraints of LLMs. By this outlook, we aim to protect and enrich our SC while\nfostering a collaborative environment between humans and AI that augments human\nintelligence rather than replacing it.",
                "Taming AI Bots: Controllability of Neural States in Large Language\n  Models\nWe tackle the question of whether an agent can, by suitable choice of\nprompts, control an AI bot to any state. To that end, we first introduce a\nformal definition of ``meaning'' that is amenable to analysis. Then, we\ncharacterize ``meaningful data'' on which large language models (LLMs) are\nostensibly trained, and ``well-trained LLMs'' through conditions that are\nlargely met by today's LLMs. While a well-trained LLM constructs an embedding\nspace of meanings that is Euclidean, meanings themselves do not form a vector\n(linear) subspace, but rather a quotient space within. We then characterize the\nsubset of meanings that can be reached by the state of the LLMs for some input\nprompt, and show that a well-trained bot can reach any meaning albeit with\nsmall probability. We then introduce a stronger notion of controllability as\n{\\em almost certain reachability}, and show that, when restricted to the space\nof meanings, an AI bot is controllable. We do so after introducing a functional\ncharacterization of attentive AI bots, and finally derive necessary and\nsufficient conditions for controllability. The fact that AI bots are\ncontrollable means that an adversary could steer them towards any state.\nHowever, the sampling process can be designed to counteract adverse actions and\navoid reaching undesirable regions of state space before their boundary is\ncrossed.",
                "The Digital Divide in Process Safety: Quantitative Risk Analysis of\n  Human-AI Collaboration\nDigital technologies have dramatically accelerated the digital transformation\nin process industries, boosted new industrial applications, upgraded the\nproduction system, and enhanced operational efficiency. In contrast, the\nchallenges and gaps between human and artificial intelligence (AI) have become\nmore and more prominent, whereas the digital divide in process safety is\naggregating. The study attempts to address the following questions: (i)What is\nAI in the process safety context? (ii)What is the difference between AI and\nhumans in process safety? (iii)How do AI and humans collaborate in process\nsafety? (iv)What are the challenges and gaps in human-AI collaboration? (v)How\nto quantify the risk of human-AI collaboration in process safety? Qualitative\nrisk analysis based on brainstorming and literature review, and quantitative\nrisk analysis based on layer of protection analysis (LOPA) and Bayesian network\n(BN), were applied to explore and model. The importance of human reliability\nshould be stressed in the digital age, not usually to increase the reliability\nof AI, and human-centered AI design in process safety needs to be propagated."
            ],
            "interesting paper": 1
        },
        {
            "papers": [
                "RE-centric Recommendations for the Development of Trustworthy(er)\n  Autonomous Systems\nComplying with the EU AI Act (AIA) guidelines while developing and\nimplementing AI systems will soon be mandatory within the EU. However,\npractitioners lack actionable instructions to operationalise ethics during AI\nsystems development. A literature review of different ethical guidelines\nrevealed inconsistencies in the principles addressed and the terminology used\nto describe them. Furthermore, requirements engineering (RE), which is\nidentified to foster trustworthiness in the AI development process from the\nearly stages was observed to be absent in a lot of frameworks that support the\ndevelopment of ethical and trustworthy AI. This incongruous phrasing combined\nwith a lack of concrete development practices makes trustworthy AI development\nharder. To address this concern, we formulated a comparison table for the\nterminology used and the coverage of the ethical AI principles in major ethical\nAI guidelines. We then examined the applicability of ethical AI development\nframeworks for performing effective RE during the development of trustworthy AI\nsystems. A tertiary review and meta-analysis of literature discussing ethical\nAI frameworks revealed their limitations when developing trustworthy AI. Based\non our findings, we propose recommendations to address such limitations during\nthe development of trustworthy AI.",
                "Shapley Based Residual Decomposition for Instance Analysis\nIn this paper, we introduce the idea of decomposing the residuals of\nregression with respect to the data instances instead of features. This allows\nus to determine the effects of each individual instance on the model and each\nother, and in doing so makes for a model-agnostic method of identifying\ninstances of interest. In doing so, we can also determine the appropriateness\nof the model and data in the wider context of a given study. The paper focuses\non the possible applications that such a framework brings to the relatively\nunexplored field of instance analysis in the context of Explainable AI tasks.",
                "Towards a Unifying Model of Rationality in Multiagent Systems\nMultiagent systems deployed in the real world need to cooperate with other\nagents (including humans) nearly as effectively as these agents cooperate with\none another. To design such AI, and provide guarantees of its effectiveness, we\nneed to clearly specify what types of agents our AI must be able to cooperate\nwith. In this work we propose a generic model of socially intelligent agents,\nwhich are individually rational learners that are also able to cooperate with\none another (in the sense that their joint behavior is Pareto efficient). We\ndefine rationality in terms of the regret incurred by each agent over its\nlifetime, and show how we can construct socially intelligent agents for\ndifferent forms of regret. We then discuss the implications of this model for\nthe development of \"robust\" MAS that can cooperate with a wide variety of\nsocially intelligent agents.",
                "Doing the right thing for the right reason: Evaluating artificial moral\n  cognition by probing cost insensitivity\nIs it possible to evaluate the moral cognition of complex artificial agents?\nIn this work, we take a look at one aspect of morality: `doing the right thing\nfor the right reasons.' We propose a behavior-based analysis of artificial\nmoral cognition which could also be applied to humans to facilitate\nlike-for-like comparison. Morally-motivated behavior should persist despite\nmounting cost; by measuring an agent's sensitivity to this cost, we gain deeper\ninsight into underlying motivations. We apply this evaluation to a particular\nset of deep reinforcement learning agents, trained by memory-based\nmeta-reinforcement learning. Our results indicate that agents trained with a\nreward function that includes other-regarding preferences perform helping\nbehavior in a way that is less sensitive to increasing cost than agents trained\nwith more self-interested preferences.",
                "Generalized Disparate Impact for Configurable Fairness Solutions in ML\nWe make two contributions in the field of AI fairness over continuous\nprotected attributes. First, we show that the Hirschfeld-Gebelein-Renyi (HGR)\nindicator (the only one currently available for such a case) is valuable but\nsubject to a few crucial limitations regarding semantics, interpretability, and\nrobustness. Second, we introduce a family of indicators that are: 1)\ncomplementary to HGR in terms of semantics; 2) fully interpretable and\ntransparent; 3) robust over finite samples; 4) configurable to suit specific\napplications. Our approach also allows us to define fine-grained constraints to\npermit certain types of dependence and forbid others selectively. By expanding\nthe available options for continuous protected attributes, our approach\nrepresents a significant contribution to the area of fair artificial\nintelligence.",
                "Steering control of payoff-maximizing players in adaptive learning\n  dynamics\nEvolutionary game theory provides a mathematical foundation for\ncross-disciplinary fertilization, especially for integrating ideas from\nartificial intelligence and game theory. Such integration offers a transparent\nand rigorous approach to complex decision-making problems in a variety of\nimportant contexts, ranging from evolutionary computation to machine behavior.\nDespite the astronomically huge individual behavioral strategy space for\ninteractions in the iterated Prisoner's Dilemma (IPD) games, the so-called\nZero-Determinant (ZD) strategies is a set of rather simple memory-one\nstrategies yet can unilaterally set a linear payoff relationship between\nthemselves and their opponent. Although the witting of ZD strategies gives\nplayers an upper hand in the IPD games, we find and characterize unbending\nstrategies that can force ZD players to be fair in their own interest.\nMoreover, our analysis reveals the ubiquity of unbending properties in common\nIPD strategies which are previously overlooked. In this work, we demonstrate\nthe important steering role of unbending strategies in fostering fairness and\ncooperation in pairwise interactions. Our results will help bring a new\nperspective by means of combining game theory and multi-agent learning systems\nfor optimizing winning strategies that are robust to noises, errors, and\ndeceptions in non-zero-sum games."
            ],
            "interesting paper": 2
        },
        {
            "papers": [
                "Towards a Unifying Model of Rationality in Multiagent Systems\nMultiagent systems deployed in the real world need to cooperate with other\nagents (including humans) nearly as effectively as these agents cooperate with\none another. To design such AI, and provide guarantees of its effectiveness, we\nneed to clearly specify what types of agents our AI must be able to cooperate\nwith. In this work we propose a generic model of socially intelligent agents,\nwhich are individually rational learners that are also able to cooperate with\none another (in the sense that their joint behavior is Pareto efficient). We\ndefine rationality in terms of the regret incurred by each agent over its\nlifetime, and show how we can construct socially intelligent agents for\ndifferent forms of regret. We then discuss the implications of this model for\nthe development of \"robust\" MAS that can cooperate with a wide variety of\nsocially intelligent agents.",
                "RE-centric Recommendations for the Development of Trustworthy(er)\n  Autonomous Systems\nComplying with the EU AI Act (AIA) guidelines while developing and\nimplementing AI systems will soon be mandatory within the EU. However,\npractitioners lack actionable instructions to operationalise ethics during AI\nsystems development. A literature review of different ethical guidelines\nrevealed inconsistencies in the principles addressed and the terminology used\nto describe them. Furthermore, requirements engineering (RE), which is\nidentified to foster trustworthiness in the AI development process from the\nearly stages was observed to be absent in a lot of frameworks that support the\ndevelopment of ethical and trustworthy AI. This incongruous phrasing combined\nwith a lack of concrete development practices makes trustworthy AI development\nharder. To address this concern, we formulated a comparison table for the\nterminology used and the coverage of the ethical AI principles in major ethical\nAI guidelines. We then examined the applicability of ethical AI development\nframeworks for performing effective RE during the development of trustworthy AI\nsystems. A tertiary review and meta-analysis of literature discussing ethical\nAI frameworks revealed their limitations when developing trustworthy AI. Based\non our findings, we propose recommendations to address such limitations during\nthe development of trustworthy AI.",
                "Doing the right thing for the right reason: Evaluating artificial moral\n  cognition by probing cost insensitivity\nIs it possible to evaluate the moral cognition of complex artificial agents?\nIn this work, we take a look at one aspect of morality: `doing the right thing\nfor the right reasons.' We propose a behavior-based analysis of artificial\nmoral cognition which could also be applied to humans to facilitate\nlike-for-like comparison. Morally-motivated behavior should persist despite\nmounting cost; by measuring an agent's sensitivity to this cost, we gain deeper\ninsight into underlying motivations. We apply this evaluation to a particular\nset of deep reinforcement learning agents, trained by memory-based\nmeta-reinforcement learning. Our results indicate that agents trained with a\nreward function that includes other-regarding preferences perform helping\nbehavior in a way that is less sensitive to increasing cost than agents trained\nwith more self-interested preferences.",
                "An Annotated Dataset for Explainable Interpersonal Risk Factors of\n  Mental Disturbance in Social Media Posts\nWith a surge in identifying suicidal risk and its severity in social media\nposts, we argue that a more consequential and explainable research is required\nfor optimal impact on clinical psychology practice and personalized mental\nhealthcare. The success of computational intelligence techniques for inferring\nmental illness from social media resources, points to natural language\nprocessing as a lens for determining Interpersonal Risk Factors (IRF) in human\nwritings. Motivated with limited availability of datasets for social NLP\nresearch community, we construct and release a new annotated dataset with\nhuman-labelled explanations and classification of IRF affecting mental\ndisturbance on social media: (i) Thwarted Belongingness (TBe), and (ii)\nPerceived Burdensomeness (PBu). We establish baseline models on our dataset\nfacilitating future research directions to develop real-time personalized AI\nmodels by detecting patterns of TBe and PBu in emotional spectrum of user's\nhistorical social media profile.",
                "Generalized Disparate Impact for Configurable Fairness Solutions in ML\nWe make two contributions in the field of AI fairness over continuous\nprotected attributes. First, we show that the Hirschfeld-Gebelein-Renyi (HGR)\nindicator (the only one currently available for such a case) is valuable but\nsubject to a few crucial limitations regarding semantics, interpretability, and\nrobustness. Second, we introduce a family of indicators that are: 1)\ncomplementary to HGR in terms of semantics; 2) fully interpretable and\ntransparent; 3) robust over finite samples; 4) configurable to suit specific\napplications. Our approach also allows us to define fine-grained constraints to\npermit certain types of dependence and forbid others selectively. By expanding\nthe available options for continuous protected attributes, our approach\nrepresents a significant contribution to the area of fair artificial\nintelligence.",
                "Perceived Trustworthiness of Natural Language Generators\nNatural Language Generation tools, such as chatbots that can generate\nhuman-like conversational text, are becoming more common both for personal and\nprofessional use. However, there are concerns about their trustworthiness and\nethical implications. The paper addresses the problem of understanding how\ndifferent users (e.g., linguists, engineers) perceive and adopt these tools and\ntheir perception of machine-generated text quality. It also discusses the\nperceived advantages and limitations of Natural Language Generation tools, as\nwell as users' beliefs on governance strategies. The main findings of this\nstudy include the impact of users' field and level of expertise on the\nperceived trust and adoption of Natural Language Generation tools, the users'\nassessment of the accuracy, fluency, and potential biases of machine-generated\ntext in comparison to human-written text, and an analysis of the advantages and\nethical risks associated with these tools as identified by the participants.\nMoreover, this paper discusses the potential implications of these findings for\nenhancing the AI development process. The paper sheds light on how different\nuser characteristics shape their beliefs on the quality and overall\ntrustworthiness of machine-generated text. Furthermore, it examines the\nbenefits and risks of these tools from the perspectives of different users."
            ],
            "interesting paper": 2
        },
        {
            "papers": [
                "Intent-aligned AI systems deplete human agency: the need for agency\n  foundations research in AI safety\nThe rapid advancement of artificial intelligence (AI) systems suggests that\nartificial general intelligence (AGI) systems may soon arrive. Many researchers\nare concerned that AIs and AGIs will harm humans via intentional misuse\n(AI-misuse) or through accidents (AI-accidents). In respect of AI-accidents,\nthere is an increasing effort focused on developing algorithms and paradigms\nthat ensure AI systems are aligned to what humans intend, e.g. AI systems that\nyield actions or recommendations that humans might judge as consistent with\ntheir intentions and goals. Here we argue that alignment to human intent is\ninsufficient for safe AI systems and that preservation of long-term agency of\nhumans may be a more robust standard, and one that needs to be separated\nexplicitly and a priori during optimization. We argue that AI systems can\nreshape human intention and discuss the lack of biological and psychological\nmechanisms that protect humans from loss of agency. We provide the first formal\ndefinition of agency-preserving AI-human interactions which focuses on\nforward-looking agency evaluations and argue that AI systems - not humans -\nmust be increasingly tasked with making these evaluations. We show how agency\nloss can occur in simple environments containing embedded agents that use\ntemporal-difference learning to make action recommendations. Finally, we\npropose a new area of research called \"agency foundations\" and pose four\ninitial topics designed to improve our understanding of agency in AI-human\ninteractions: benevolent game theory, algorithmic foundations of human rights,\nmechanistic interpretability of agency representation in neural-networks and\nreinforcement learning from internal states.",
                "Disentangling and Operationalizing AI Fairness at LinkedIn\nOperationalizing AI fairness at LinkedIn's scale is challenging not only\nbecause there are multiple mutually incompatible definitions of fairness but\nalso because determining what is fair depends on the specifics and context of\nthe product where AI is deployed. Moreover, AI practitioners need clarity on\nwhat fairness expectations need to be addressed at the AI level. In this paper,\nwe present the evolving AI fairness framework used at LinkedIn to address these\nthree challenges. The framework disentangles AI fairness by separating out\nequal treatment and equitable product expectations. Rather than imposing a\ntrade-off between these two commonly opposing interpretations of fairness, the\nframework provides clear guidelines for operationalizing equal AI treatment\ncomplemented with a product equity strategy. This paper focuses on the equal AI\ntreatment component of LinkedIn's AI fairness framework, shares the principles\nthat support it, and illustrates their application through a case study. We\nhope this paper will encourage other big tech companies to join us in sharing\ntheir approach to operationalizing AI fairness at scale, so that together we\ncan keep advancing this constantly evolving field.",
                "Bigger, Better, Faster: Human-level Atari with human-level efficiency\nWe introduce a value-based RL agent, which we call BBF, that achieves\nsuper-human performance in the Atari 100K benchmark. BBF relies on scaling the\nneural networks used for value estimation, as well as a number of other design\nchoices that enable this scaling in a sample-efficient manner. We conduct\nextensive analyses of these design choices and provide insights for future\nwork. We end with a discussion about updating the goalposts for\nsample-efficient RL research on the ALE. We make our code and data publicly\navailable at\nhttps://github.com/google-research/google-research/tree/master/bigger_better_faster.",
                "Evaluating GPT's Programming Capability through CodeWars' Katas\nIn the burgeoning field of artificial intelligence (AI), understanding the\ncapabilities and limitations of programming-oriented models is crucial. This\npaper presents a novel evaluation of the programming proficiency of Generative\nPretrained Transformer (GPT) models, specifically GPT-3.5 and GPT-4, against\ncoding problems of varying difficulty levels drawn from Codewars. The\nexperiments reveal a distinct boundary at the 3kyu level, beyond which these\nGPT models struggle to provide solutions. These findings led to the proposal of\na measure for coding problem complexity that incorporates both problem\ndifficulty and the time required for solution. The research emphasizes the need\nfor validation and creative thinking capabilities in AI models to better\nemulate human problem-solving techniques. Future work aims to refine this\nproposed complexity measure, enhance AI models with these suggested\ncapabilities, and develop an objective measure for programming problem\ndifficulty. The results of this research offer invaluable insights for\nimproving AI programming capabilities and advancing the frontier of AI\nproblem-solving abilities.",
                "Using Data Analytics to Derive Business Intelligence: A Case Study\nThe data revolution experienced in recent times has thrown up new challenges\nand opportunities for businesses of all sizes in diverse industries. Big data\nanalytics is already at the forefront of innovations to help make meaningful\nbusiness decisions from the abundance of raw data available today. Business\nintelligence and analytics has become a huge trend in todays IT world as\ncompanies of all sizes are looking to improve their business processes and\nscale up using data driven solutions. This paper aims to demonstrate the data\nanalytical process of deriving business intelligence via the historical data of\na fictional bike share company seeking to find innovative ways to convert their\ncasual riders to annual paying registered members. The dataset used is freely\navailable as Chicago Divvy Bicycle Sharing Data on Kaggle. The authors used the\nRTidyverse library in RStudio to analyse the data and followed the six data\nanalysis steps of ask, prepare, process, analyse, share, and act to recommend\nsome actionable approaches the company could adopt to convert casual riders to\npaying annual members. The findings from this research serve as a valuable case\nexample, of a real world deployment of BIA technologies in the industry, and a\ndemonstration of the data analysis cycle for data practitioners, researchers,\nand other potential users.",
                "Data and Knowledge for Overtaking Scenarios in Autonomous Driving\nAutonomous driving has become one of the most popular research topics within\nArtificial Intelligence. An autonomous vehicle is understood as a system that\ncombines perception, decision-making, planning, and control. All of those tasks\nrequire that the vehicle collects surrounding data in order to make a good\ndecision and action. In particular, the overtaking maneuver is one of the most\ncritical actions of driving. The process involves lane changes, acceleration\nand deceleration actions, and estimation of the speed and distance of the\nvehicle in front or in the lane in which it is moving. Despite the amount of\nwork available in the literature, just a few handle overtaking maneuvers and,\nbecause overtaking can be risky, no real-world dataset is available. This work\ncontributes in this area by presenting a new synthetic dataset whose focus is\nthe overtaking maneuver. We start by performing a thorough review of the state\nof the art in autonomous driving and then explore the main datasets found in\nthe literature (public and private, synthetic and real), highlighting their\nlimitations, and suggesting a new set of features whose focus is the overtaking\nmaneuver."
            ],
            "interesting paper": 2
        },
        {
            "papers": [
                "Intent-aligned AI systems deplete human agency: the need for agency\n  foundations research in AI safety\nThe rapid advancement of artificial intelligence (AI) systems suggests that\nartificial general intelligence (AGI) systems may soon arrive. Many researchers\nare concerned that AIs and AGIs will harm humans via intentional misuse\n(AI-misuse) or through accidents (AI-accidents). In respect of AI-accidents,\nthere is an increasing effort focused on developing algorithms and paradigms\nthat ensure AI systems are aligned to what humans intend, e.g. AI systems that\nyield actions or recommendations that humans might judge as consistent with\ntheir intentions and goals. Here we argue that alignment to human intent is\ninsufficient for safe AI systems and that preservation of long-term agency of\nhumans may be a more robust standard, and one that needs to be separated\nexplicitly and a priori during optimization. We argue that AI systems can\nreshape human intention and discuss the lack of biological and psychological\nmechanisms that protect humans from loss of agency. We provide the first formal\ndefinition of agency-preserving AI-human interactions which focuses on\nforward-looking agency evaluations and argue that AI systems - not humans -\nmust be increasingly tasked with making these evaluations. We show how agency\nloss can occur in simple environments containing embedded agents that use\ntemporal-difference learning to make action recommendations. Finally, we\npropose a new area of research called \"agency foundations\" and pose four\ninitial topics designed to improve our understanding of agency in AI-human\ninteractions: benevolent game theory, algorithmic foundations of human rights,\nmechanistic interpretability of agency representation in neural-networks and\nreinforcement learning from internal states.",
                "Disentangling and Operationalizing AI Fairness at LinkedIn\nOperationalizing AI fairness at LinkedIn's scale is challenging not only\nbecause there are multiple mutually incompatible definitions of fairness but\nalso because determining what is fair depends on the specifics and context of\nthe product where AI is deployed. Moreover, AI practitioners need clarity on\nwhat fairness expectations need to be addressed at the AI level. In this paper,\nwe present the evolving AI fairness framework used at LinkedIn to address these\nthree challenges. The framework disentangles AI fairness by separating out\nequal treatment and equitable product expectations. Rather than imposing a\ntrade-off between these two commonly opposing interpretations of fairness, the\nframework provides clear guidelines for operationalizing equal AI treatment\ncomplemented with a product equity strategy. This paper focuses on the equal AI\ntreatment component of LinkedIn's AI fairness framework, shares the principles\nthat support it, and illustrates their application through a case study. We\nhope this paper will encourage other big tech companies to join us in sharing\ntheir approach to operationalizing AI fairness at scale, so that together we\ncan keep advancing this constantly evolving field.",
                "Seeing Seeds Beyond Weeds: Green Teaming Generative AI for Beneficial\n  Uses\nLarge generative AI models (GMs) like GPT and DALL-E are trained to generate\ncontent for general, wide-ranging purposes. GM content filters are generalized\nto filter out content which has a risk of harm in many cases, e.g., hate\nspeech. However, prohibited content is not always harmful -- there are\ninstances where generating prohibited content can be beneficial. So, when GMs\nfilter out content, they preclude beneficial use cases along with harmful ones.\nWhich use cases are precluded reflects the values embedded in GM content\nfiltering. Recent work on red teaming proposes methods to bypass GM content\nfilters to generate harmful content. We coin the term green teaming to describe\nmethods of bypassing GM content filters to design for beneficial use cases. We\nshowcase green teaming by: 1) Using ChatGPT as a virtual patient to simulate a\nperson experiencing suicidal ideation, for suicide support training; 2) Using\nCodex to intentionally generate buggy solutions to train students on debugging;\nand 3) Examining an Instagram page using Midjourney to generate images of\nanti-LGBTQ+ politicians in drag. Finally, we discuss how our use cases\ndemonstrate green teaming as both a practical design method and a mode of\ncritique, which problematizes and subverts current understandings of harms and\nvalues in generative AI.",
                "Building Extractive Question Answering System to Support Human-AI Health\n  Coaching Model for Sleep Domain\nNon-communicable diseases (NCDs) are a leading cause of global deaths,\nnecessitating a focus on primary prevention and lifestyle behavior change.\nHealth coaching, coupled with Question Answering (QA) systems, has the\npotential to transform preventive healthcare. This paper presents a\nhuman-Artificial Intelligence (AI) health coaching model incorporating a\ndomain-specific extractive QA system. A sleep-focused dataset, SleepQA, was\nmanually assembled and used to fine-tune domain-specific BERT models. The QA\nsystem was evaluated using automatic and human methods. A data-centric\nframework enhanced the system's performance by improving passage retrieval and\nquestion reformulation. Although the system did not outperform the baseline in\nautomatic evaluation, it excelled in the human evaluation of real-world\nquestions. Integration into a Human-AI health coaching model was tested in a\npilot Randomized Controlled Trial (RCT).",
                "Bigger, Better, Faster: Human-level Atari with human-level efficiency\nWe introduce a value-based RL agent, which we call BBF, that achieves\nsuper-human performance in the Atari 100K benchmark. BBF relies on scaling the\nneural networks used for value estimation, as well as a number of other design\nchoices that enable this scaling in a sample-efficient manner. We conduct\nextensive analyses of these design choices and provide insights for future\nwork. We end with a discussion about updating the goalposts for\nsample-efficient RL research on the ALE. We make our code and data publicly\navailable at\nhttps://github.com/google-research/google-research/tree/master/bigger_better_faster.",
                "NetHack is Hard to Hack\nNeural policy learning methods have achieved remarkable results in various\ncontrol problems, ranging from Atari games to simulated locomotion. However,\nthese methods struggle in long-horizon tasks, especially in open-ended\nenvironments with multi-modal observations, such as the popular dungeon-crawler\ngame, NetHack. Intriguingly, the NeurIPS 2021 NetHack Challenge revealed that\nsymbolic agents outperformed neural approaches by over four times in median\ngame score. In this paper, we delve into the reasons behind this performance\ngap and present an extensive study on neural policy learning for NetHack. To\nconduct this study, we analyze the winning symbolic agent, extending its\ncodebase to track internal strategy selection in order to generate one of the\nlargest available demonstration datasets. Utilizing this dataset, we examine\n(i) the advantages of an action hierarchy; (ii) enhancements in neural\narchitecture; and (iii) the integration of reinforcement learning with\nimitation learning. Our investigations produce a state-of-the-art neural agent\nthat surpasses previous fully neural policies by 127% in offline settings and\n25% in online settings on median game score. However, we also demonstrate that\nmere scaling is insufficient to bridge the performance gap with the best\nsymbolic models or even the top human players."
            ],
            "interesting paper": 3
        },
        {
            "papers": [
                "Improved Financial Forecasting via Quantum Machine Learning\nQuantum algorithms have the potential to enhance machine learning across a\nvariety of domains and applications. In this work, we show how quantum machine\nlearning can be used to improve financial forecasting. First, we use classical\nand quantum Determinantal Point Processes to enhance Random Forest models for\nchurn prediction, improving precision by almost 6%. Second, we design quantum\nneural network architectures with orthogonal and compound layers for credit\nrisk assessment, which match classical performance with significantly fewer\nparameters. Our results demonstrate that leveraging quantum ideas can\neffectively enhance the performance of machine learning, both today as\nquantum-inspired classical ML solutions, and even more in the future, with the\nadvent of better quantum hardware.",
                "Survey of Trustworthy AI: A Meta Decision of AI\nWhen making strategic decisions, we are often confronted with overwhelming\ninformation to process. The situation can be further complicated when some\npieces of evidence are contradicted each other or paradoxical. The challenge\nthen becomes how to determine which information is useful and which ones should\nbe eliminated. This process is known as meta-decision. Likewise, when it comes\nto using Artificial Intelligence (AI) systems for strategic decision-making,\nplacing trust in the AI itself becomes a meta-decision, given that many AI\nsystems are viewed as opaque \"black boxes\" that process large amounts of data.\nTrusting an opaque system involves deciding on the level of Trustworthy AI\n(TAI). We propose a new approach to address this issue by introducing a novel\ntaxonomy or framework of TAI, which encompasses three crucial domains:\narticulate, authentic, and basic for different levels of trust. To underpin\nthese domains, we create ten dimensions to measure trust:\nexplainability/transparency, fairness/diversity, generalizability, privacy,\ndata governance, safety/robustness, accountability, reproducibility,\nreliability, and sustainability. We aim to use this taxonomy to conduct a\ncomprehensive survey and explore different TAI approaches from a strategic\ndecision-making perspective.",
                "Credit Card Fraud Detection Using Asexual Reproduction Optimization\nAs the number of credit card users has increased, detecting fraud in this\ndomain has become a vital issue. Previous literature has applied various\nsupervised and unsupervised machine learning methods to find an effective fraud\ndetection system. However, some of these methods require an enormous amount of\ntime to achieve reasonable accuracy. In this paper, an Asexual Reproduction\nOptimization (ARO) approach was employed, which is a supervised method to\ndetect credit card fraud. ARO refers to a kind of production in which one\nparent produces some offspring. By applying this method and sampling just from\nthe majority class, the effectiveness of the classification is increased. A\ncomparison to Artificial Immune Systems (AIS), which is one of the best methods\nimplemented on current datasets, has shown that the proposed method is able to\nremarkably reduce the required training time and at the same time increase the\nrecall that is important in fraud detection problems. The obtained results show\nthat ARO achieves the best cost in a short time, and consequently, it can be\nconsidered a real-time fraud detection system.",
                "Citizen Perspectives on Necessary Safeguards to the Use of AI by Law\n  Enforcement Agencies\nIn the light of modern technological advances, Artificial Intelligence (AI)\nis relied upon to enhance performance, increase efficiency, and maximize gains.\nFor Law Enforcement Agencies (LEAs), it can prove valuable in optimizing\nevidence analysis and establishing proactive prevention measures. Nevertheless,\ncitizens raise legitimate concerns around privacy invasions, biases,\ninequalities, and inaccurate decisions. This study explores the views of 111\ncitizens towards AI use by police through interviews, and integrates societal\nconcerns along with propositions of safeguards from negative effects of AI use\nby LEAs in the context of cybercrime and terrorism.",
                "Responsible Design Patterns for Machine Learning Pipelines\nIntegrating ethical practices into the AI development process for artificial\nintelligence (AI) is essential to ensure safe, fair, and responsible operation.\nAI ethics involves applying ethical principles to the entire life cycle of AI\nsystems. This is essential to mitigate potential risks and harms associated\nwith AI, such as algorithm biases. To achieve this goal, responsible design\npatterns (RDPs) are critical for Machine Learning (ML) pipelines to guarantee\nethical and fair outcomes. In this paper, we propose a comprehensive framework\nincorporating RDPs into ML pipelines to mitigate risks and ensure the ethical\ndevelopment of AI systems. Our framework comprises new responsible AI design\npatterns for ML pipelines identified through a survey of AI ethics and data\nmanagement experts and validated through real-world scenarios with expert\nfeedback. The framework guides AI developers, data scientists, and\npolicy-makers to implement ethical practices in AI development and deploy\nresponsible AI systems in production.",
                "Human Control: Definitions and Algorithms\nHow can humans stay in control of advanced artificial intelligence systems?\nOne proposal is corrigibility, which requires the agent to follow the\ninstructions of a human overseer, without inappropriately influencing them. In\nthis paper, we formally define a variant of corrigibility called shutdown\ninstructability, and show that it implies appropriate shutdown behavior,\nretention of human autonomy, and avoidance of user harm. We also analyse the\nrelated concepts of non-obstruction and shutdown alignment, three previously\nproposed algorithms for human control, and one new algorithm."
            ],
            "interesting paper": 1
        },
        {
            "papers": [
                "From Human-Centered to Social-Centered Artificial Intelligence:\n  Assessing ChatGPT's Impact through Disruptive Events\nLarge language models (LLMs) and dialogue agents have existed for years, but\nthe release of recent GPT models has been a watershed moment for artificial\nintelligence (AI) research and society at large. Immediately recognized for its\ngenerative capabilities and versatility, ChatGPT's impressive proficiency\nacross technical and creative domains led to its widespread adoption. While\nsociety grapples with the emerging cultural impacts of ChatGPT, critiques of\nChatGPT's impact within the machine learning community have coalesced around\nits performance or other conventional Responsible AI evaluations relating to\nbias, toxicity, and 'hallucination.' We argue that these latter critiques draw\nheavily on a particular conceptualization of the 'human-centered' framework,\nwhich tends to cast atomized individuals as the key recipients of both the\nbenefits and detriments of technology. In this article, we direct attention to\nanother dimension of LLMs and dialogue agents' impact: their effect on social\ngroups, institutions, and accompanying norms and practices. By illustrating\nChatGPT's social impact through three disruptive events, we challenge\nindividualistic approaches in AI development and contribute to ongoing debates\naround the ethical and responsible implementation of AI systems. We hope this\neffort will call attention to more comprehensive and longitudinal evaluation\ntools and compel technologists to go beyond human-centered thinking and ground\ntheir efforts through social-centered AI.",
                "Human or Not? A Gamified Approach to the Turing Test\nWe present \"Human or Not?\", an online game inspired by the Turing test, that\nmeasures the capability of AI chatbots to mimic humans in dialog, and of humans\nto tell bots from other humans. Over the course of a month, the game was played\nby over 1.5 million users who engaged in anonymous two-minute chat sessions\nwith either another human or an AI language model which was prompted to behave\nlike humans. The task of the players was to correctly guess whether they spoke\nto a person or to an AI. This largest scale Turing-style test conducted to date\nrevealed some interesting facts. For example, overall users guessed the\nidentity of their partners correctly in only 68% of the games. In the subset of\nthe games in which users faced an AI bot, users had even lower correct guess\nrates of 60% (that is, not much higher than chance). This white paper details\nthe development, deployment, and results of this unique experiment. While this\nexperiment calls for many extensions and refinements, these findings already\nbegin to shed light on the inevitable near future which will commingle humans\nand AI.",
                "EMOTE: An Explainable architecture for Modelling the Other Through\n  Empathy\nWe can usually assume others have goals analogous to our own. This assumption\ncan also, at times, be applied to multi-agent games - e.g. Agent 1's attraction\nto green pellets is analogous to Agent 2's attraction to red pellets. This\n\"analogy\" assumption is tied closely to the cognitive process known as empathy.\nInspired by empathy, we design a simple and explainable architecture to model\nanother agent's action-value function. This involves learning an \"Imagination\nNetwork\" to transform the other agent's observed state in order to produce a\nhuman-interpretable \"empathetic state\" which, when presented to the learning\nagent, produces behaviours that mimic the other agent. Our approach is\napplicable to multi-agent scenarios consisting of a single learning agent and\nother (independent) agents acting according to fixed policies. This\narchitecture is particularly beneficial for (but not limited to) algorithms\nusing a composite value or reward function. We show our method produces better\nperformance in multi-agent games, where it robustly estimates the other's model\nin different environment configurations. Additionally, we show that the\nempathetic states are human interpretable, and thus verifiable.",
                "Citizen Perspectives on Necessary Safeguards to the Use of AI by Law\n  Enforcement Agencies\nIn the light of modern technological advances, Artificial Intelligence (AI)\nis relied upon to enhance performance, increase efficiency, and maximize gains.\nFor Law Enforcement Agencies (LEAs), it can prove valuable in optimizing\nevidence analysis and establishing proactive prevention measures. Nevertheless,\ncitizens raise legitimate concerns around privacy invasions, biases,\ninequalities, and inaccurate decisions. This study explores the views of 111\ncitizens towards AI use by police through interviews, and integrates societal\nconcerns along with propositions of safeguards from negative effects of AI use\nby LEAs in the context of cybercrime and terrorism.",
                "Responsible Design Patterns for Machine Learning Pipelines\nIntegrating ethical practices into the AI development process for artificial\nintelligence (AI) is essential to ensure safe, fair, and responsible operation.\nAI ethics involves applying ethical principles to the entire life cycle of AI\nsystems. This is essential to mitigate potential risks and harms associated\nwith AI, such as algorithm biases. To achieve this goal, responsible design\npatterns (RDPs) are critical for Machine Learning (ML) pipelines to guarantee\nethical and fair outcomes. In this paper, we propose a comprehensive framework\nincorporating RDPs into ML pipelines to mitigate risks and ensure the ethical\ndevelopment of AI systems. Our framework comprises new responsible AI design\npatterns for ML pipelines identified through a survey of AI ethics and data\nmanagement experts and validated through real-world scenarios with expert\nfeedback. The framework guides AI developers, data scientists, and\npolicy-makers to implement ethical practices in AI development and deploy\nresponsible AI systems in production.",
                "Environmental Memory Boosts Group Formation of Clueless Individuals\nThe formation of groups of interacting individuals improves performance and\nfitness in many decentralised systems, from micro-organisms to social insects,\nfrom robotic swarms to artificial intelligence algorithms. Often, group\nformation and high-level coordination in these systems emerge from individuals\nwith limited information-processing capabilities implementing low-level rules\nof communication to signal to each other. Here, we show that, even in a\ncommunity of clueless individuals incapable of processing information and\ncommunicating, a dynamic environment can coordinate group formation by\ntransiently storing memory of the earlier passage of individuals. Our results\nidentify a new mechanism of indirect coordination via shared memory that is\nprimarily promoted and reinforced by dynamic environmental factors, thus\novershadowing the need for any form of explicit signalling between individuals.\nWe expect this pathway to group formation to be relevant for understanding and\ncontrolling self-organisation and collective decision making in both living and\nartificial active matter in real-life environments."
            ],
            "interesting paper": 1
        },
        {
            "papers": [
                "AI Liability Insurance With an Example in AI-Powered E-diagnosis System\nArtificial Intelligence (AI) has received an increasing amount of attention\nin multiple areas. The uncertainties and risks in AI-powered systems have\ncreated reluctance in their wild adoption. As an economic solution to\ncompensate for potential damages, AI liability insurance is a promising market\nto enhance the integration of AI into daily life. In this work, we use an\nAI-powered E-diagnosis system as an example to study AI liability insurance. We\nprovide a quantitative risk assessment model with evidence-based numerical\nanalysis. We discuss the insurability criteria for AI technologies and suggest\nnecessary adjustments to accommodate the features of AI products. We show that\nAI liability insurance can act as a regulatory mechanism to incentivize\ncompliant behaviors and serve as a certificate of high-quality AI systems.\nFurthermore, we suggest premium adjustment to reflect the dynamic evolution of\nthe inherent uncertainty in AI. Moral hazard problems are discussed and\nsuggestions for AI liability insurance are provided.",
                "AI and the creative realm: A short review of current and future\n  applications\nThis study explores the concept of creativity and artificial intelligence\n(AI) and their recent integration. While AI has traditionally been perceived as\nincapable of generating new ideas or creating art, the development of more\nsophisticated AI models and the proliferation of human-computer interaction\ntools have opened up new possibilities for AI in artistic creation. This study\ninvestigates the various applications of AI in a creative context,\ndifferentiating between the type of art, language, and algorithms used. It also\nconsiders the philosophical implications of AI and creativity, questioning\nwhether consciousness can be researched in machines and AI's potential\ninterests and decision-making capabilities. Overall, we aim to stimulate a\nreflection on AI's use and ethical implications in creative contexts.",
                "The ethical ambiguity of AI data enrichment: Measuring gaps in research\n  ethics norms and practices\nThe technical progression of artificial intelligence (AI) research has been\nbuilt on breakthroughs in fields such as computer science, statistics, and\nmathematics. However, in the past decade AI researchers have increasingly\nlooked to the social sciences, turning to human interactions to solve the\nchallenges of model development. Paying crowdsourcing workers to generate or\ncurate data, or data enrichment, has become indispensable for many areas of AI\nresearch, from natural language processing to reinforcement learning from human\nfeedback (RLHF). Other fields that routinely interact with crowdsourcing\nworkers, such as Psychology, have developed common governance requirements and\nnorms to ensure research is undertaken ethically. This study explores how, and\nto what extent, comparable research ethics requirements and norms have\ndeveloped for AI research and data enrichment. We focus on the approach taken\nby two leading conferences: ICLR and NeurIPS, and journal publisher Springer.\nIn a longitudinal study of accepted papers, and via a comparison with\nPsychology and CHI papers, this work finds that leading AI venues have begun to\nestablish protocols for human data collection, but these are are inconsistently\nfollowed by authors. Whilst Psychology papers engaging with crowdsourcing\nworkers frequently disclose ethics reviews, payment data, demographic data and\nother information, similar disclosures are far less common in leading AI venues\ndespite similar guidance. The work concludes with hypotheses to explain these\ngaps in research ethics practices and considerations for its implications.",
                "What model does MuZero learn?\nModel-based reinforcement learning (MBRL) has drawn considerable interest in\nrecent years, given its promise to improve sample efficiency. Moreover, when\nusing deep-learned models, it is possible to learn compact and generalizable\nmodels from data. In this work, we study MuZero, a state-of-the-art deep\nmodel-based reinforcement learning algorithm that distinguishes itself from\nexisting algorithms by learning a value-equivalent model. Despite MuZero's\nsuccess and impact in the field of MBRL, existing literature has not thoroughly\naddressed why MuZero performs so well in practice. Specifically, there is a\nlack of in-depth investigation into the value-equivalent model learned by\nMuZero and its effectiveness in model-based credit assignment and policy\nimprovement, which is vital for achieving sample efficiency in MBRL. To fill\nthis gap, we explore two fundamental questions through our empirical analysis:\n1) to what extent does MuZero achieve its learning objective of a\nvalue-equivalent model, and 2) how useful are these models for policy\nimprovement? Our findings reveal that MuZero's model struggles to generalize\nwhen evaluating unseen policies, which limits its capacity for additional\npolicy improvement. However, MuZero's incorporation of the policy prior in MCTS\nalleviates this problem, which biases the search towards actions where the\nmodel is more accurate.",
                "Interpretable and Explainable Logical Policies via Neurally Guided\n  Symbolic Abstraction\nThe limited priors required by neural networks make them the dominating\nchoice to encode and learn policies using reinforcement learning (RL). However,\nthey are also black-boxes, making it hard to understand the agent's behaviour,\nespecially when working on the image level. Therefore, neuro-symbolic RL aims\nat creating policies that are interpretable in the first place. Unfortunately,\ninterpretability is not explainability. To achieve both, we introduce Neurally\ngUided Differentiable loGic policiEs (NUDGE). NUDGE exploits trained neural\nnetwork-based agents to guide the search of candidate-weighted logic rules,\nthen uses differentiable logic to train the logic agents. Our experimental\nevaluation demonstrates that NUDGE agents can induce interpretable and\nexplainable policies while outperforming purely neural ones and showing good\nflexibility to environments of different initial states and problem sizes.",
                "Theoretical Behavior of XAI Methods in the Presence of Suppressor\n  Variables\nIn recent years, the community of 'explainable artificial intelligence' (XAI)\nhas created a vast body of methods to bridge a perceived gap between model\n'complexity' and 'interpretability'. However, a concrete problem to be solved\nby XAI methods has not yet been formally stated. As a result, XAI methods are\nlacking theoretical and empirical evidence for the 'correctness' of their\nexplanations, limiting their potential use for quality-control and transparency\npurposes. At the same time, Haufe et al. (2014) showed, using simple toy\nexamples, that even standard interpretations of linear models can be highly\nmisleading. Specifically, high importance may be attributed to so-called\nsuppressor variables lacking any statistical relation to the prediction target.\nThis behavior has been confirmed empirically for a large array of XAI methods\nin Wilming et al. (2022). Here, we go one step further by deriving analytical\nexpressions for the behavior of a variety of popular XAI methods on a simple\ntwo-dimensional binary classification problem involving Gaussian\nclass-conditional distributions. We show that the majority of the studied\napproaches will attribute non-zero importance to a non-class-related suppressor\nfeature in the presence of correlated noise. This poses important limitations\non the interpretations and conclusions that the outputs of these XAI methods\ncan afford."
            ],
            "interesting paper": 1
        },
        {
            "papers": [
                "The ethical ambiguity of AI data enrichment: Measuring gaps in research\n  ethics norms and practices\nThe technical progression of artificial intelligence (AI) research has been\nbuilt on breakthroughs in fields such as computer science, statistics, and\nmathematics. However, in the past decade AI researchers have increasingly\nlooked to the social sciences, turning to human interactions to solve the\nchallenges of model development. Paying crowdsourcing workers to generate or\ncurate data, or data enrichment, has become indispensable for many areas of AI\nresearch, from natural language processing to reinforcement learning from human\nfeedback (RLHF). Other fields that routinely interact with crowdsourcing\nworkers, such as Psychology, have developed common governance requirements and\nnorms to ensure research is undertaken ethically. This study explores how, and\nto what extent, comparable research ethics requirements and norms have\ndeveloped for AI research and data enrichment. We focus on the approach taken\nby two leading conferences: ICLR and NeurIPS, and journal publisher Springer.\nIn a longitudinal study of accepted papers, and via a comparison with\nPsychology and CHI papers, this work finds that leading AI venues have begun to\nestablish protocols for human data collection, but these are are inconsistently\nfollowed by authors. Whilst Psychology papers engaging with crowdsourcing\nworkers frequently disclose ethics reviews, payment data, demographic data and\nother information, similar disclosures are far less common in leading AI venues\ndespite similar guidance. The work concludes with hypotheses to explain these\ngaps in research ethics practices and considerations for its implications.",
                "AI and the creative realm: A short review of current and future\n  applications\nThis study explores the concept of creativity and artificial intelligence\n(AI) and their recent integration. While AI has traditionally been perceived as\nincapable of generating new ideas or creating art, the development of more\nsophisticated AI models and the proliferation of human-computer interaction\ntools have opened up new possibilities for AI in artistic creation. This study\ninvestigates the various applications of AI in a creative context,\ndifferentiating between the type of art, language, and algorithms used. It also\nconsiders the philosophical implications of AI and creativity, questioning\nwhether consciousness can be researched in machines and AI's potential\ninterests and decision-making capabilities. Overall, we aim to stimulate a\nreflection on AI's use and ethical implications in creative contexts.",
                "AI Liability Insurance With an Example in AI-Powered E-diagnosis System\nArtificial Intelligence (AI) has received an increasing amount of attention\nin multiple areas. The uncertainties and risks in AI-powered systems have\ncreated reluctance in their wild adoption. As an economic solution to\ncompensate for potential damages, AI liability insurance is a promising market\nto enhance the integration of AI into daily life. In this work, we use an\nAI-powered E-diagnosis system as an example to study AI liability insurance. We\nprovide a quantitative risk assessment model with evidence-based numerical\nanalysis. We discuss the insurability criteria for AI technologies and suggest\nnecessary adjustments to accommodate the features of AI products. We show that\nAI liability insurance can act as a regulatory mechanism to incentivize\ncompliant behaviors and serve as a certificate of high-quality AI systems.\nFurthermore, we suggest premium adjustment to reflect the dynamic evolution of\nthe inherent uncertainty in AI. Moral hazard problems are discussed and\nsuggestions for AI liability insurance are provided.",
                "Minding Language Models' (Lack of) Theory of Mind: A Plug-and-Play\n  Multi-Character Belief Tracker\nTheory of Mind (ToM)$\\unicode{x2014}$the ability to reason about the mental\nstates of other people$\\unicode{x2014}$is a key element of our social\nintelligence. Yet, despite their ever more impressive performance, large-scale\nneural language models still lack basic theory of mind capabilities\nout-of-the-box. We posit that simply scaling up models will not imbue them with\ntheory of mind due to the inherently symbolic and implicit nature of the\nphenomenon, and instead investigate an alternative: can we design a\ndecoding-time algorithm that enhances theory of mind of off-the-shelf neural\nlanguage models without explicit supervision? We present SymbolicToM, a\nplug-and-play approach to reason about the belief states of multiple characters\nin reading comprehension tasks via explicit symbolic representation. More\nconcretely, our approach tracks each entity's beliefs, their estimation of\nother entities' beliefs, and higher-order levels of reasoning, all through\ngraphical representations, allowing for more precise and interpretable\nreasoning than previous approaches. Empirical results on the well-known ToMi\nbenchmark (Le et al., 2019) demonstrate that SymbolicToM dramatically enhances\noff-the-shelf neural networks' theory of mind in a zero-shot setting while\nshowing robust out-of-distribution performance compared to supervised\nbaselines. Our work also reveals spurious patterns in existing theory of mind\nbenchmarks, emphasizing the importance of out-of-distribution evaluation and\nmethods that do not overfit a particular dataset.",
                "Egocentric Planning for Scalable Embodied Task Achievement\nEmbodied agents face significant challenges when tasked with performing\nactions in diverse environments, particularly in generalizing across object\ntypes and executing suitable actions to accomplish tasks. Furthermore, agents\nshould exhibit robustness, minimizing the execution of illegal actions. In this\nwork, we present Egocentric Planning, an innovative approach that combines\nsymbolic planning and Object-oriented POMDPs to solve tasks in complex\nenvironments, harnessing existing models for visual perception and natural\nlanguage processing. We evaluated our approach in ALFRED, a simulated\nenvironment designed for domestic tasks, and demonstrated its high scalability,\nachieving an impressive 36.07% unseen success rate in the ALFRED benchmark and\nwinning the ALFRED challenge at CVPR Embodied AI workshop. Our method requires\nreliable perception and the specification or learning of a symbolic description\nof the preconditions and effects of the agent's actions, as well as what object\ntypes reveal information about others. It is capable of naturally scaling to\nsolve new tasks beyond ALFRED, as long as they can be solved using the\navailable skills. This work offers a solid baseline for studying end-to-end and\nhybrid methods that aim to generalize to new tasks, including recent approaches\nrelying on LLMs, but often struggle to scale to long sequences of actions or\nproduce robust plans for novel tasks.",
                "LIV: Language-Image Representations and Rewards for Robotic Control\nWe present Language-Image Value learning (LIV), a unified objective for\nvision-language representation and reward learning from action-free videos with\ntext annotations. Exploiting a novel connection between dual reinforcement\nlearning and mutual information contrastive learning, the LIV objective trains\na multi-modal representation that implicitly encodes a universal value function\nfor tasks specified as language or image goals. We use LIV to pre-train the\nfirst control-centric vision-language representation from large human video\ndatasets such as EpicKitchen. Given only a language or image goal, the\npre-trained LIV model can assign dense rewards to each frame in videos of\nunseen robots or humans attempting that task in unseen environments. Further,\nwhen some target domain-specific data is available, the same objective can be\nused to fine-tune and improve LIV and even other pre-trained representations\nfor robotic control and reward specification in that domain. In our experiments\non several simulated and real-world robot environments, LIV models consistently\noutperform the best prior input state representations for imitation learning,\nas well as reward specification methods for policy synthesis. Our results\nvalidate the advantages of joint vision-language representation and reward\nlearning within the unified, compact LIV framework."
            ],
            "interesting paper": 1
        },
        {
            "papers": [
                "Enough With \"Human-AI Collaboration\"\nDescribing our interaction with Artificial Intelligence (AI) systems as\n'collaboration' is well-intentioned, but flawed. Not only is it misleading, but\nit also takes away the credit of AI 'labour' from the humans behind it, and\nerases and obscures an often exploitative arrangement between AI producers and\nconsumers. The AI 'collaboration' metaphor is merely the latest episode in a\nlong history of labour appropriation and credit reassignment that\ndisenfranchises labourers in the Global South. I propose that viewing AI as a\ntool or an instrument, rather than a collaborator, is more accurate, and\nultimately fairer.",
                "XAI Renaissance: Redefining Interpretability in Medical Diagnostic\n  Models\nAs machine learning models become increasingly prevalent in medical\ndiagnostics, the need for interpretability and transparency becomes paramount.\nThe XAI Renaissance signifies a significant shift in the field, aiming to\nredefine the interpretability of medical diagnostic models. This paper explores\nthe innovative approaches and methodologies within the realm of Explainable AI\n(XAI) that are revolutionizing the interpretability of medical diagnostic\nmodels. By shedding light on the underlying decision-making process, XAI\ntechniques empower healthcare professionals to understand, trust, and\neffectively utilize these models for accurate and reliable medical diagnoses.\nThis review highlights the key advancements in XAI for medical diagnostics and\ntheir potential to transform the healthcare landscape, ultimately improving\npatient outcomes and fostering trust in AI-driven diagnostic systems.",
                "Accelerating science with human-aware artificial intelligence\nArtificial intelligence (AI) models trained on published scientific findings\nhave been used to invent valuable materials and targeted therapies, but they\ntypically ignore the human scientists who continually alter the landscape of\ndiscovery. Here we show that incorporating the distribution of human expertise\nby training unsupervised models on simulated inferences cognitively accessible\nto experts dramatically improves (up to 400%) AI prediction of future\ndiscoveries beyond those focused on research content alone, especially when\nrelevant literature is sparse. These models succeed by predicting human\npredictions and the scientists who will make them. By tuning human-aware AI to\navoid the crowd, we can generate scientifically promising \"alien\" hypotheses\nunlikely to be imagined or pursued without intervention until the distant\nfuture, which hold promise to punctuate scientific advance beyond questions\ncurrently pursued. Accelerating human discovery or probing its blind spots,\nhuman-aware AI enables us to move toward and beyond the contemporary scientific\nfrontier.",
                "SourceP: Detecting Ponzi Schemes on Ethereum with Source Code\nAs blockchain technology becomes more and more popular, a typical financial\nscam, the Ponzi scheme, has also emerged in the blockchain platform Ethereum.\nThis Ponzi scheme deployed through smart contracts, also known as the smart\nPonzi scheme, has caused a lot of economic losses and negative impacts.\nExisting methods for detecting smart Ponzi schemes on Ethereum mainly rely on\nbytecode features, opcode features, account features, and transaction behavior\nfeatures of smart contracts, which are unable to truly characterize the\nbehavioral features of Ponzi schemes, and thus generally perform poorly in\nterms of detection accuracy and false alarm rates. In this paper, we propose\nSourceP, a method to detect smart Ponzi schemes on the Ethereum platform using\npre-trained models and data flow, which only requires using the source code of\nsmart contracts as features. SourceP reduces the difficulty of data acquisition\nand feature extraction of existing detection methods. Specifically, we first\nconvert the source code of a smart contract into a data flow graph and then\nintroduce a pre-trained model based on learning code representations to build a\nclassification model to identify Ponzi schemes in smart contracts. The\nexperimental results show that SourceP achieves 87.2% recall and 90.7% F-score\nfor detecting smart Ponzi schemes within Ethereum's smart contract dataset,\noutperforming state-of-the-art methods in terms of performance and\nsustainability. We also demonstrate through additional experiments that\npre-trained models and data flow play an important contribution to SourceP, as\nwell as proving that SourceP has a good generalization ability.",
                "No Bidding, No Regret: Pairwise-Feedback Mechanisms for Digital Goods\n  and Data Auctions\nThe growing demand for data and AI-generated digital goods, such as\npersonalized written content and artwork, necessitates effective pricing and\nfeedback mechanisms that account for uncertain utility and costly production.\nMotivated by these developments, this study presents a novel mechanism design\naddressing a general repeated-auction setting where the utility derived from a\nsold good is revealed post-sale. The mechanism's novelty lies in using pairwise\ncomparisons for eliciting information from the bidder, arguably easier for\nhumans than assigning a numerical value. Our mechanism chooses allocations\nusing an epsilon-greedy strategy and relies on pairwise comparisons between\nrealized utility from allocated goods and an arbitrary value, avoiding the\nlearning-to-bid problem explored in previous work. We prove this mechanism to\nbe asymptotically truthful, individually rational, and welfare and revenue\nmaximizing. The mechanism's relevance is broad, applying to any setting with\nmade-to-order goods of variable quality. Experimental results on multi-label\ntoxicity annotation data, an example of negative utilities, highlight how our\nproposed mechanism could enhance social welfare in data auctions. Overall, our\nfocus on human factors contributes to the development of more human-aware and\nefficient mechanism design.",
                "AI Transparency in the Age of LLMs: A Human-Centered Research Roadmap\nThe rise of powerful large language models (LLMs) brings about tremendous\nopportunities for innovation but also looming risks for individuals and society\nat large. We have reached a pivotal moment for ensuring that LLMs and\nLLM-infused applications are developed and deployed responsibly. However, a\ncentral pillar of responsible AI -- transparency -- is largely missing from the\ncurrent discourse around LLMs. It is paramount to pursue new approaches to\nprovide transparency for LLMs, and years of research at the intersection of AI\nand human-computer interaction (HCI) highlight that we must do so with a\nhuman-centered perspective: Transparency is fundamentally about supporting\nappropriate human understanding, and this understanding is sought by different\nstakeholders with different goals in different contexts. In this new era of\nLLMs, we must develop and design approaches to transparency by considering the\nneeds of stakeholders in the emerging LLM ecosystem, the novel types of\nLLM-infused applications being built, and the new usage patterns and challenges\naround LLMs, all while building on lessons learned about how people process,\ninteract with, and make use of information. We reflect on the unique challenges\nthat arise in providing transparency for LLMs, along with lessons learned from\nHCI and responsible AI research that has taken a human-centered perspective on\nAI transparency. We then lay out four common approaches that the community has\ntaken to achieve transparency -- model reporting, publishing evaluation\nresults, providing explanations, and communicating uncertainty -- and call out\nopen questions around how these approaches may or may not be applied to LLMs.\nWe hope this provides a starting point for discussion and a useful roadmap for\nfuture research."
            ],
            "interesting paper": 2
        }
    ],
    "Drew Johnson": [
        {
            "papers": [
                "Deep Learning and Ethics\nThis article appears as chapter 21 of Prince (2023, Understanding Deep\nLearning); a complete draft of the textbook is available here:\nhttp://udlbook.com. This chapter considers potential harms arising from the\ndesign and use of AI systems. These include algorithmic bias, lack of\nexplainability, data privacy violations, militarization, fraud, and\nenvironmental concerns. The aim is not to provide advice on being more ethical.\nInstead, the goal is to express ideas and start conversations in key areas that\nhave received attention in philosophy, political science, and the broader\nsocial sciences.",
                "Towards a Capability Assessment Model for the Comprehension and Adoption\n  of AI in Organisations\nThe comprehension and adoption of Artificial Intelligence (AI) are beset with\npractical and ethical problems. This article presents a 5-level AI Capability\nAssessment Model (AI-CAM) and a related AI Capabilities Matrix (AI-CM) to\nassist practitioners in AI comprehension and adoption. These practical tools\nwere developed with business executives, technologists, and other\norganisational stakeholders in mind. They are founded on a comprehensive\nconception of AI compared to those in other AI adoption models and are also\nopen-source artefacts. Thus, the AI-CAM and AI-CM present an accessible\nresource to help inform organisational decision-makers on the capability\nrequirements for (1) AI-based data analytics use cases based on machine\nlearning technologies; (2) Knowledge representation to engineer and represent\ndata, information and knowledge using semantic technologies; and (3) AI-based\nsolutions that seek to emulate human reasoning and decision-making. The AI-CAM\ncovers the core capability dimensions (business, data, technology,\norganisation, AI skills, risks, and ethical considerations) required at the\nfive capability maturity levels to achieve optimal use of AI in organisations.",
                "Trends and Challenges Towards an Effective Data-Driven Decision Making\n  in UK SMEs: Case Studies and Lessons Learnt from the Analysis of 85 SMEs\nThe adoption of data science brings vast benefits to Small and Medium-sized\nEnterprises (SMEs) including business productivity, economic growth, innovation\nand jobs creation. Data Science can support SMEs to optimise production\nprocesses, anticipate customers' needs, predict machinery failures and deliver\nefficient smart services. Businesses can also harness the power of Artificial\nIntelligence (AI) and Big Data and the smart use of digital technologies to\nenhance productivity and performance, paving the way for innovation. However,\nintegrating data science decisions into an SME requires both skills and IT\ninvestments. In most cases, such expenses are beyond the means of SMEs due to\nlimited resources and restricted access to financing. This paper presents\ntrends and challenges towards an effective data-driven decision making for\norganisations based on a case study of 85 SMEs, mostly from the West Midlands\nregion of England. The work is supported as part of a 3 years ERDF (European\nRegional Development Funded project) in the areas of big data management,\nanalytics and business intelligence. We present two case studies that\ndemonstrates the potential of Digitisation, AI and Machine Learning and use\nthese as examples to unveil challenges and showcase the wealth of current\navailable opportunities for SMEs.",
                "Applications of Intelligent Systems in Green Technology\nIntelligent Systems (ISs) are technologically advanced machines, which\nperceive and respond to the environment around them. They are usually of\nvarious forms ranging from software to hardware. ISs are generally the fusion\nof Artificial Intelligence (AI), robotics and Internet of things (IoT). In\norder to strengthen ISs, one of the key technologies is green technology (GT).\nIt refers to the continuously advancing methods and materials, which cover\ntechniques for producing energy to non-toxic cleaning products. It may also be\nbroadened to saving energy, and reducing toxic and waste materials in the\nenvironment. The motto of GT can be achieved by using the ISs. In this paper,\nwe present various applications of ISs in GT. Moreover, we discuss various\npossible solutions using ISs in order to overcome the on-going real-life\nproblems.",
                "Model evaluation for extreme risks\nCurrent approaches to building general-purpose AI systems tend to produce\nsystems with both beneficial and harmful capabilities. Further progress in AI\ndevelopment could lead to capabilities that pose extreme risks, such as\noffensive cyber capabilities or strong manipulation skills. We explain why\nmodel evaluation is critical for addressing extreme risks. Developers must be\nable to identify dangerous capabilities (through \"dangerous capability\nevaluations\") and the propensity of models to apply their capabilities for harm\n(through \"alignment evaluations\"). These evaluations will become critical for\nkeeping policymakers and other stakeholders informed, and for making\nresponsible decisions about model training, deployment, and security.",
                "Distributed Online Rollout for Multivehicle Routing in Unmapped\n  Environments\nIn this work we consider a generalization of the well-known multivehicle\nrouting problem: given a network, a set of agents occupying a subset of its\nnodes, and a set of tasks, we seek a minimum cost sequence of movements subject\nto the constraint that each task is visited by some agent at least once. The\nclassical version of this problem assumes a central computational server that\nobserves the entire state of the system perfectly and directs individual agents\naccording to a centralized control scheme. In contrast, we assume that there is\nno centralized server and that each agent is an individual processor with no a\npriori knowledge of the underlying network (including task and agent\nlocations). Moreover, our agents possess strictly local communication and\nsensing capabilities (restricted to a fixed radius around their respective\nlocations), aligning more closely with several real-world multiagent\napplications. These restrictions introduce many challenges that are overcome\nthrough local information sharing and direct coordination between agents. We\npresent a fully distributed, online, and scalable reinforcement learning\nalgorithm for this problem whereby agents self-organize into local clusters and\nindependently apply a multiagent rollout scheme locally to each cluster. We\ndemonstrate empirically via extensive simulations that there exists a critical\nsensing radius beyond which the distributed rollout algorithm begins to improve\nover a greedy base policy. This critical sensing radius grows proportionally to\nthe $\\log^*$ function of the size of the network, and is, therefore, a small\nconstant for any relevant network. Our decentralized reinforcement learning\nalgorithm achieves approximately a factor of two cost improvement over the base\npolicy for a range of radii bounded from below and above by two and three times\nthe critical sensing radius, respectively."
            ],
            "interesting paper": 4
        },
        {
            "papers": [
                "Prompt Evolution for Generative AI: A Classifier-Guided Approach\nSynthesis of digital artifacts conditioned on user prompts has become an\nimportant paradigm facilitating an explosion of use cases with generative AI.\nHowever, such models often fail to connect the generated outputs and desired\ntarget concepts/preferences implied by the prompts. Current research addressing\nthis limitation has largely focused on enhancing the prompts before output\ngeneration or improving the model's performance up front. In contrast, this\npaper conceptualizes prompt evolution, imparting evolutionary selection\npressure and variation during the generative process to produce multiple\noutputs that satisfy the target concepts/preferences better. We propose a\nmulti-objective instantiation of this broader idea that uses a multi-label\nimage classifier-guided approach. The predicted labels from the classifiers\nserve as multiple objectives to optimize, with the aim of producing diversified\nimages that meet user preferences. A novelty of our evolutionary algorithm is\nthat the pre-trained generative model gives us implicit mutation operations,\nleveraging the model's stochastic generative capability to automate the\ncreation of Pareto-optimized images more faithful to user preferences.",
                "Large Language Models for User Interest Journeys\nLarge language models (LLMs) have shown impressive capabilities in natural\nlanguage understanding and generation. Their potential for deeper user\nunderstanding and improved personalized user experience on recommendation\nplatforms is, however, largely untapped. This paper aims to address this gap.\nRecommender systems today capture users' interests through encoding their\nhistorical activities on the platforms. The generated user representations are\nhard to examine or interpret. On the other hand, if we were to ask people about\ninterests they pursue in their life, they might talk about their hobbies, like\nI just started learning the ukulele, or their relaxation routines, e.g., I like\nto watch Saturday Night Live, or I want to plant a vertical garden. We argue,\nand demonstrate through extensive experiments, that LLMs as foundation models\ncan reason through user activities, and describe their interests in nuanced and\ninteresting ways, similar to how a human would.\n  We define interest journeys as the persistent and overarching user interests,\nin other words, the non-transient ones. These are the interests that we believe\nwill benefit most from the nuanced and personalized descriptions. We introduce\na framework in which we first perform personalized extraction of interest\njourneys, and then summarize the extracted journeys via LLMs, using techniques\nlike few-shot prompting, prompt-tuning and fine-tuning. Together, our results\nin prompting LLMs to name extracted user journeys in a large-scale industrial\nplatform demonstrate great potential of these models in providing deeper, more\ninterpretable, and controllable user understanding. We believe LLM powered user\nunderstanding can be a stepping stone to entirely new user experiences on\nrecommendation platforms that are journey-aware, assistive, and enabling\nfrictionless conversation down the line.",
                "HuatuoGPT, towards Taming Language Model to Be a Doctor\nIn this paper, we present HuatuoGPT, a large language model (LLM) for medical\nconsultation. The core recipe of HuatuoGPT is to leverage both\n\\textit{distilled data from ChatGPT} and \\textit{real-world data from doctors}\nin the supervised fine-tuned stage. The responses of ChatGPT are usually\ndetailed, well-presented and informative while it cannot perform like a doctor\nin many aspects, e.g. for integrative diagnosis. We argue that real-world data\nfrom doctors would be complementary to distilled data in the sense the former\ncould tame a distilled language model to perform like doctors. To better\nleverage the strengths of both data, we train a reward model to align the\nlanguage model with the merits that both data bring, following an RLAIF\n(reinforced learning from AI feedback) fashion. To evaluate and benchmark the\nmodels, we propose a comprehensive evaluation scheme (including automatic and\nmanual metrics). Experimental results demonstrate that HuatuoGPT achieves\nstate-of-the-art results in performing medical consultation among open-source\nLLMs in GPT-4 evaluation, human evaluation, and medical benchmark datasets. It\nis worth noting that by using additional real-world data and RLAIF, the\ndistilled language model (i.e., HuatuoGPT) outperforms its teacher model\nChatGPT in most cases. Our code, data, and models are publicly available at\n\\url{https://github.com/FreedomIntelligence/HuatuoGPT}. The online demo is\navailable at \\url{https://www.HuatuoGPT.cn/}.",
                "An Experimental Investigation into the Evaluation of Explainability\n  Methods\nEXplainable Artificial Intelligence (XAI) aims to help users to grasp the\nreasoning behind the predictions of an Artificial Intelligence (AI) system.\nMany XAI approaches have emerged in recent years. Consequently, a subfield\nrelated to the evaluation of XAI methods has gained considerable attention,\nwith the aim to determine which methods provide the best explanation using\nvarious approaches and criteria. However, the literature lacks a comparison of\nthe evaluation metrics themselves, that one can use to evaluate XAI methods.\nThis work aims to fill this gap by comparing 14 different metrics when applied\nto nine state-of-the-art XAI methods and three dummy methods (e.g., random\nsaliency maps) used as references. Experimental results show which of these\nmetrics produces highly correlated results, indicating potential redundancy. We\nalso demonstrate the significant impact of varying the baseline hyperparameter\non the evaluation metric values. Finally, we use dummy methods to assess the\nreliability of metrics in terms of ranking, pointing out their limitations.",
                "Topic-Guided Self-Introduction Generation for Social Media Users\nMillions of users are active on social media. To allow users to better\nshowcase themselves and network with others, we explore the auto-generation of\nsocial media self-introduction, a short sentence outlining a user's personal\ninterests. While most prior work profiles users with tags (e.g., ages), we\ninvestigate sentence-level self-introductions to provide a more natural and\nengaging way for users to know each other. Here we exploit a user's tweeting\nhistory to generate their self-introduction. The task is non-trivial because\nthe history content may be lengthy, noisy, and exhibit various personal\ninterests. To address this challenge, we propose a novel unified topic-guided\nencoder-decoder (UTGED) framework; it models latent topics to reflect salient\nuser interest, whose topic mixture then guides encoding a user's history and\ntopic words control decoding their self-introduction. For experiments, we\ncollect a large-scale Twitter dataset, and extensive results show the\nsuperiority of our UTGED to the advanced encoder-decoder models without topic\nmodeling.",
                "Visually-Situated Natural Language Understanding with Contrastive\n  Reading Model and Frozen Large Language Models\nRecent advances in Large Language Models (LLMs) have stimulated a surge of\nresearch aimed at extending their applications to the visual domain. While\nthese models exhibit promise in generating abstract image captions and\nfacilitating natural conversations, their performance on text-rich images still\nrequires improvement. In this paper, we introduce Contrastive Reading Model\n(Cream), a novel neural architecture designed to enhance the language-image\nunderstanding capability of LLMs by capturing intricate details that are often\noverlooked in existing methods. Cream combines vision and auxiliary encoders,\nfortified by a contrastive feature alignment technique, to achieve a more\neffective comprehension of language information in visually situated contexts\nwithin the images. Our approach bridges the gap between vision and language\nunderstanding, paving the way for the development of more sophisticated\nDocument Intelligence Assistants. Through rigorous evaluations across diverse\nvisually-situated language understanding tasks that demand reasoning\ncapabilities, we demonstrate the compelling performance of Cream, positioning\nit as a prominent model in the field of visual document understanding. We\nprovide our codebase and newly-generated datasets at\nhttps://github.com/naver-ai/cream ."
            ],
            "interesting paper": 2
        },
        {
            "papers": [
                "Alert of the Second Decision-maker: An Introduction to Human-AI Conflict\nThe collaboration between humans and artificial intelligence (AI) is a\nsignificant feature in this digital age. However, humans and AI may have\nobservation, interpretation, and action conflicts when working synchronously.\nThis phenomenon is often masked by faults and, unfortunately, overlooked. This\npaper systematically introduces the human-AI conflict concept, causes,\nmeasurement methods, and risk assessment. The results highlight that there is a\npotential second decision-maker besides the human, which is the AI; the\nhuman-AI conflict is a unique and emerging risk in digitalized process systems;\nand this is an interdisciplinary field that needs to be distinguished from\ntraditional fault and failure analysis; the conflict risk is significant and\ncannot be ignored.",
                "AI Techniques in the Microservices Life-Cycle: A Survey\nMicroservices is a popular architectural style for the development of\ndistributed software, with an emphasis on modularity, scalability, and\nflexibility. Indeed, in microservice systems, functionalities are provided by\nloosely coupled, small services, each focusing on a specific business\ncapability. Building a system according to the microservices architectural\nstyle brings a number of challenges, mainly related to how the different\nmicroservices are deployed and coordinated and how they interact. In this\npaper, we provide a survey about how techniques in the area of Artificial\nIntelligence have been used to tackle these challenges.",
                "Attention Paper: How Generative AI Reshapes Digital Shadow Industry?\nThe rapid development of digital economy has led to the emergence of various\nblack and shadow internet industries, which pose potential risks that can be\nidentified and managed through digital risk management (DRM) that uses\ndifferent techniques such as machine learning and deep learning. The evolution\nof DRM architecture has been driven by changes in data forms. However, the\ndevelopment of AI-generated content (AIGC) technology, such as ChatGPT and\nStable Diffusion, has given black and shadow industries powerful tools to\npersonalize data and generate realistic images and conversations for fraudulent\nactivities. This poses a challenge for DRM systems to control risks from the\nsource of data generation and to respond quickly to the fast-changing risk\nenvironment. This paper aims to provide a technical analysis of the challenges\nand opportunities of AIGC from upstream, midstream, and downstream paths of\nblack/shadow industries and suggest future directions for improving existing\nrisk control systems. The paper will explore the new black and shadow\ntechniques triggered by generative AI technology and provide insights for\nbuilding the next-generation DRM system.",
                "On Computing Universal Plans for Partially Observable Multi-Agent Path\n  Finding\nMulti-agent routing problems have drawn significant attention nowadays due to\ntheir broad industrial applications in, e.g., warehouse robots, logistics\nautomation, and traffic control. Conventionally, they are modelled as classical\nplanning problems. In this paper, we argue that it is beneficial to formulate\nthem as universal planning problems. We therefore propose universal plans, also\nknown as policies, as the solution concepts, and implement a system called\nASP-MAUPF (Answer Set Programming for Multi-Agent Universal Plan Finding) for\ncomputing them. Given an arbitrary two-dimensional map and a profile of goals\nfor the agents, the system finds a feasible universal plan for each agent that\nensures no collision with others. We use the system to conduct some\nexperiments, and make some observations on the types of goal profiles and\nenvironments that will have feasible policies, and how they may depend on\nagents' sensors. We also demonstrate how users can customize action preferences\nto compute more efficient policies, even (near-)optimal ones.",
                "Enhancing Human Capabilities through Symbiotic Artificial Intelligence\n  with Shared Sensory Experiences\nThe merging of human intelligence and artificial intelligence has long been a\nsubject of interest in both science fiction and academia. In this paper, we\nintroduce a novel concept in Human-AI interaction called Symbiotic Artificial\nIntelligence with Shared Sensory Experiences (SAISSE), which aims to establish\na mutually beneficial relationship between AI systems and human users through\nshared sensory experiences. By integrating multiple sensory input channels and\nprocessing human experiences, SAISSE fosters a strong human-AI bond, enabling\nAI systems to learn from and adapt to individual users, providing personalized\nsupport, assistance, and enhancement. Furthermore, we discuss the incorporation\nof memory storage units for long-term growth and development of both the AI\nsystem and its human user. As we address user privacy and ethical guidelines\nfor responsible AI-human symbiosis, we also explore potential biases and\ninequalities in AI-human symbiosis and propose strategies to mitigate these\nchallenges. Our research aims to provide a comprehensive understanding of the\nSAISSE concept and its potential to effectively support and enhance individual\nhuman users through symbiotic AI systems. This position article aims at\ndiscussing poteintial AI-human interaction related topics within the scientific\ncommunity, rather than providing experimental or theoretical results.",
                "Applying Interdisciplinary Frameworks to Understand Algorithmic\n  Decision-Making\nWe argue that explanations for \"algorithmic decision-making\" (ADM) systems\ncan profit by adopting practices that are already used in the learning\nsciences. We shortly introduce the importance of explaining ADM systems, give a\nbrief overview of approaches drawing from other disciplines to improve\nexplanations, and present the results of our qualitative task-based study\nincorporating the \"six facets of understanding\" framework. We close with\nquestions guiding the discussion of how future studies can leverage an\ninterdisciplinary approach."
            ],
            "interesting paper": 3
        },
        {
            "papers": [
                "Multimodal Recommendation Dialog with Subjective Preference: A New\n  Challenge and Benchmark\nExisting multimodal task-oriented dialog data fails to demonstrate the\ndiverse expressions of user subjective preferences and recommendation acts in\nthe real-life shopping scenario. This paper introduces a new dataset SURE\n(Multimodal Recommendation Dialog with SUbjective PREference), which contains\n12K shopping dialogs in complex store scenes. The data is built in two phases\nwith human annotations to ensure quality and diversity. SURE is well-annotated\nwith subjective preferences and recommendation acts proposed by sales experts.\nA comprehensive analysis is given to reveal the distinguishing features of\nSURE. Three benchmark tasks are then proposed on the data to evaluate the\ncapability of multimodal recommendation agents. Based on the SURE, we propose a\nbaseline model, powered by a state-of-the-art multimodal model, for these\ntasks.",
                "Enhancing Human Capabilities through Symbiotic Artificial Intelligence\n  with Shared Sensory Experiences\nThe merging of human intelligence and artificial intelligence has long been a\nsubject of interest in both science fiction and academia. In this paper, we\nintroduce a novel concept in Human-AI interaction called Symbiotic Artificial\nIntelligence with Shared Sensory Experiences (SAISSE), which aims to establish\na mutually beneficial relationship between AI systems and human users through\nshared sensory experiences. By integrating multiple sensory input channels and\nprocessing human experiences, SAISSE fosters a strong human-AI bond, enabling\nAI systems to learn from and adapt to individual users, providing personalized\nsupport, assistance, and enhancement. Furthermore, we discuss the incorporation\nof memory storage units for long-term growth and development of both the AI\nsystem and its human user. As we address user privacy and ethical guidelines\nfor responsible AI-human symbiosis, we also explore potential biases and\ninequalities in AI-human symbiosis and propose strategies to mitigate these\nchallenges. Our research aims to provide a comprehensive understanding of the\nSAISSE concept and its potential to effectively support and enhance individual\nhuman users through symbiotic AI systems. This position article aims at\ndiscussing poteintial AI-human interaction related topics within the scientific\ncommunity, rather than providing experimental or theoretical results.",
                "Explainability Techniques for Chemical Language Models\nExplainability techniques are crucial in gaining insights into the reasons\nbehind the predictions of deep learning models, which have not yet been applied\nto chemical language models. We propose an explainable AI technique that\nattributes the importance of individual atoms towards the predictions made by\nthese models. Our method backpropagates the relevance information towards the\nchemical input string and visualizes the importance of individual atoms. We\nfocus on self-attention Transformers operating on molecular string\nrepresentations and leverage a pretrained encoder for finetuning. We showcase\nthe method by predicting and visualizing solubility in water and organic\nsolvents. We achieve competitive model performance while obtaining\ninterpretable predictions, which we use to inspect the pretrained model.",
                "Mitigating Exploitation Bias in Learning to Rank with an\n  Uncertainty-aware Empirical Bayes Approach\nRanking is at the core of many artificial intelligence (AI) applications,\nincluding search engines, recommender systems, etc. Modern ranking systems are\noften constructed with learning-to-rank (LTR) models built from user behavior\nsignals. While previous studies have demonstrated the effectiveness of using\nuser behavior signals (e.g., clicks) as both features and labels of LTR\nalgorithms, we argue that existing LTR algorithms that indiscriminately treat\nbehavior and non-behavior signals in input features could lead to suboptimal\nperformance in practice. Particularly because user behavior signals often have\nstrong correlations with the ranking objective and can only be collected on\nitems that have already been shown to users, directly using behavior signals in\nLTR could create an exploitation bias that hurts the system performance in the\nlong run.\n  To address the exploitation bias, we propose EBRank, an empirical Bayes-based\nuncertainty-aware ranking algorithm. Specifically, to overcome exploitation\nbias brought by behavior features in ranking models, EBRank uses a sole\nnon-behavior feature based prior model to get a prior estimation of relevance.\nIn the dynamic training and serving of ranking systems, EBRank uses the\nobserved user behaviors to update posterior relevance estimation instead of\nconcatenating behaviors as features in ranking models. Besides, EBRank\nadditionally applies an uncertainty-aware exploration strategy to explore\nactively, collect user behaviors for empirical Bayesian modeling and improve\nranking performance. Experiments on three public datasets show that EBRank is\neffective, practical and significantly outperforms state-of-the-art ranking\nalgorithms.",
                "Multiview Identifiers Enhanced Generative Retrieval\nInstead of simply matching a query to pre-existing passages, generative\nretrieval generates identifier strings of passages as the retrieval target. At\na cost, the identifier must be distinctive enough to represent a passage.\nCurrent approaches use either a numeric ID or a text piece (such as a title or\nsubstrings) as the identifier. However, these identifiers cannot cover a\npassage's content well. As such, we are motivated to propose a new type of\nidentifier, synthetic identifiers, that are generated based on the content of a\npassage and could integrate contextualized information that text pieces lack.\nFurthermore, we simultaneously consider multiview identifiers, including\nsynthetic identifiers, titles, and substrings. These views of identifiers\ncomplement each other and facilitate the holistic ranking of passages from\nmultiple perspectives. We conduct a series of experiments on three public\ndatasets, and the results indicate that our proposed approach performs the best\nin generative retrieval, demonstrating its effectiveness and robustness.",
                "GenQ: Automated Question Generation to Support Caregivers While Reading\n  Stories with Children\nWhen caregivers ask open--ended questions to motivate dialogue with children,\nit facilitates the child's reading comprehension skills.Although there is scope\nfor use of technological tools, referred here as \"intelligent tutoring\nsystems\", to scaffold this process, it is currently unclear whether existing\nintelligent systems that generate human--language like questions is beneficial.\nAdditionally, training data used in the development of these automated question\ngeneration systems is typically sourced without attention to demographics, but\npeople with different cultural backgrounds may ask different questions. As a\npart of a broader project to design an intelligent reading support app for\nLatinx children, we crowdsourced questions from Latinx caregivers and\nnoncaregivers as well as caregivers and noncaregivers from other demographics.\nWe examine variations in question--asking within this dataset mediated by\nindividual, cultural, and contextual factors. We then design a system that\nautomatically extracts templates from this data to generate open--ended\nquestions that are representative of those asked by Latinx caregivers."
            ],
            "interesting paper": 1
        },
        {
            "papers": [
                "Towards Cognitive Bots: Architectural Research Challenges\nSoftware bots operating in multiple virtual digital platforms must understand\nthe platforms' affordances and behave like human users. Platform affordances or\nfeatures differ from one application platform to another or through a life\ncycle, requiring such bots to be adaptable. Moreover, bots in such platforms\ncould cooperate with humans or other software agents for work or to learn\nspecific behavior patterns. However, present-day bots, particularly chatbots,\nother than language processing and prediction, are far from reaching a human\nuser's behavior level within complex business information systems. They lack\nthe cognitive capabilities to sense and act in such virtual environments,\nrendering their development a challenge to artificial general intelligence\nresearch. In this study, we problematize and investigate assumptions in\nconceptualizing software bot architecture by directing attention to significant\narchitectural research challenges in developing cognitive bots endowed with\ncomplex behavior for operation on information systems. As an outlook, we\npropose alternate architectural assumptions to consider in future bot design\nand bot development frameworks.",
                "How Do UX Practitioners Communicate AI as a Design Material? Artifacts,\n  Conceptions, and Propositions\nUX practitioners (UXPs) face novel challenges when working with and\ncommunicating artificial intelligence (AI) as a design material. We explore how\nUXPs communicate AI concepts when given hands-on experience training and\nexperimenting with AI models. To do so, we conducted a task-based design study\nwith 27 UXPs in which they prototyped and created a design presentation for a\nAI-enabled interface while having access to a simple AI model training tool.\nThrough analyzing UXPs' design presentations and post-activity interviews, we\nfound that although UXPs struggled to clearly communicate some AI concepts,\ntinkering with AI broadened common ground when communicating with technical\nstakeholders. UXPs also identified key risks and benefits of AI in their\ndesigns, and proposed concrete next steps for both UX and AI work. We conclude\nwith a sensitizing concept and recommendations for design and AI tools to\nenhance multi-stakeholder communication and collaboration when crafting\nhuman-centered AI experiences.",
                "Mindstorms in Natural Language-Based Societies of Mind\nBoth Minsky's \"society of mind\" and Schmidhuber's \"learning to think\" inspire\ndiverse societies of large multimodal neural networks (NNs) that solve problems\nby interviewing each other in a \"mindstorm.\" Recent implementations of NN-based\nsocieties of minds consist of large language models (LLMs) and other NN-based\nexperts communicating through a natural language interface. In doing so, they\novercome the limitations of single LLMs, improving multimodal zero-shot\nreasoning. In these natural language-based societies of mind (NLSOMs), new\nagents -- all communicating through the same universal symbolic language -- are\neasily added in a modular fashion. To demonstrate the power of NLSOMs, we\nassemble and experiment with several of them (having up to 129 members),\nleveraging mindstorms in them to solve some practical AI tasks: visual question\nanswering, image captioning, text-to-image synthesis, 3D generation, egocentric\nretrieval, embodied AI, and general language-based task solving. We view this\nas a starting point towards much larger NLSOMs with billions of agents-some of\nwhich may be humans. And with this emergence of great societies of\nheterogeneous minds, many new research questions have suddenly become paramount\nto the future of artificial intelligence. What should be the social structure\nof an NLSOM? What would be the (dis)advantages of having a monarchical rather\nthan a democratic structure? How can principles of NN economies be used to\nmaximize the total reward of a reinforcement learning NLSOM? In this work, we\nidentify, discuss, and try to answer some of these questions.",
                "MADiff: Offline Multi-agent Learning with Diffusion Models\nDiffusion model (DM) recently achieved huge success in various scenarios\nincluding offline reinforcement learning, where the diffusion planner learn to\ngenerate desired trajectories during online evaluations. However, despite the\neffectiveness in single-agent learning, it remains unclear how DMs can operate\nin multi-agent problems, where agents can hardly complete teamwork without good\ncoordination by independently modeling each agent's trajectories. In this\npaper, we propose MADiff, a novel generative multi-agent learning framework to\ntackle this problem. MADiff is realized with an attention-based diffusion model\nto model the complex coordination among behaviors of multiple agents. To the\nbest of our knowledge, MADiff is the first diffusion-based multi-agent learning\nframework, which behaves as both a decentralized policy and a centralized\ncontroller. During decentralized executions, MADiff simultaneously performs\nteammate modeling, and the centralized controller can also be applied in\nmulti-agent trajectory predictions. Our experiments show the superior\nperformance of MADiff compared to baseline algorithms in a wide range of\nmulti-agent learning tasks, which emphasizes the effectiveness of MADiff in\nmodeling complex multi-agent interactions. Our code is available at\nhttps://github.com/zbzhu99/madiff.",
                "Attention Schema in Neural Agents\nAttention has become a common ingredient in deep learning architectures. It\nadds a dynamical selection of information on top of the static selection of\ninformation supported by weights. In the same way, we can imagine a\nhigher-order informational filter built on top of attention: an Attention\nSchema (AS), namely, a descriptive and predictive model of attention. In\ncognitive neuroscience, Attention Schema Theory (AST) supports this idea of\ndistinguishing attention from AS. A strong prediction of this theory is that an\nagent can use its own AS to also infer the states of other agents' attention\nand consequently enhance coordination with other agents. As such, multi-agent\nreinforcement learning would be an ideal setting to experimentally test the\nvalidity of AST. We explore different ways in which attention and AS interact\nwith each other. Our preliminary results indicate that agents that implement\nthe AS as a recurrent internal control achieve the best performance. In\ngeneral, these exploratory experiments suggest that equipping artificial agents\nwith a model of attention can enhance their social intelligence.",
                "A Categorical Representation Language and Computational System for\n  Knowledge-Based Planning\nClassical planning representation languages based on first-order logic have\npreliminarily been used to model and solve robotic task planning problems.\nWider adoption of these representation languages, however, is hindered by the\nlimitations present when managing implicit world changes with concise action\nmodels. To address this problem, we propose an alternative approach to\nrepresenting and managing updates to world states during planning. Based on the\ncategory-theoretic concepts of $\\mathsf{C}$-sets and double-pushout rewriting\n(DPO), our proposed representation can effectively handle structured knowledge\nabout world states that support domain abstractions at all levels. It\nformalizes the semantics of predicates according to a user-provided ontology\nand preserves the semantics when transitioning between world states. This\nmethod provides a formal semantics for using knowledge graphs and relational\ndatabases to model world states and updates in planning. In this paper, we\nconceptually compare our category-theoretic representation with the classical\nplanning representation. We show that our proposed representation has\nadvantages over the classical representation in terms of handling implicit\npreconditions and effects, and provides a more structured framework in which to\nmodel and solve planning problems."
            ],
            "interesting paper": 3
        },
        {
            "papers": [
                "How Do UX Practitioners Communicate AI as a Design Material? Artifacts,\n  Conceptions, and Propositions\nUX practitioners (UXPs) face novel challenges when working with and\ncommunicating artificial intelligence (AI) as a design material. We explore how\nUXPs communicate AI concepts when given hands-on experience training and\nexperimenting with AI models. To do so, we conducted a task-based design study\nwith 27 UXPs in which they prototyped and created a design presentation for a\nAI-enabled interface while having access to a simple AI model training tool.\nThrough analyzing UXPs' design presentations and post-activity interviews, we\nfound that although UXPs struggled to clearly communicate some AI concepts,\ntinkering with AI broadened common ground when communicating with technical\nstakeholders. UXPs also identified key risks and benefits of AI in their\ndesigns, and proposed concrete next steps for both UX and AI work. We conclude\nwith a sensitizing concept and recommendations for design and AI tools to\nenhance multi-stakeholder communication and collaboration when crafting\nhuman-centered AI experiences.",
                "Towards Explainable Conversational Recommender Systems\nExplanations in conventional recommender systems have demonstrated benefits\nin helping the user understand the rationality of the recommendations and\nimproving the system's efficiency, transparency, and trustworthiness. In the\nconversational environment, multiple contextualized explanations need to be\ngenerated, which poses further challenges for explanations. To better measure\nexplainability in conversational recommender systems (CRS), we propose ten\nevaluation perspectives based on concepts from conventional recommender systems\ntogether with the characteristics of CRS. We assess five existing CRS benchmark\ndatasets using these metrics and observe the necessity of improving the\nexplanation quality of CRS. To achieve this, we conduct manual and automatic\napproaches to extend these dialogues and construct a new CRS dataset, namely\nExplainable Recommendation Dialogues (E-ReDial). It includes 756 dialogues with\nover 2,000 high-quality rewritten explanations. We compare two baseline\napproaches to perform explanation generation based on E-ReDial. Experimental\nresults suggest that models trained on E-ReDial can significantly improve\nexplainability while introducing knowledge into the models can further improve\nthe performance. GPT-3 in the in-context learning setting can generate more\nrealistic and diverse movie descriptions. In contrast, T5 training on E-ReDial\ncan better generate clear reasons for recommendations based on user\npreferences. E-ReDial is available at https://github.com/Superbooming/E-ReDial.",
                "Towards Cognitive Bots: Architectural Research Challenges\nSoftware bots operating in multiple virtual digital platforms must understand\nthe platforms' affordances and behave like human users. Platform affordances or\nfeatures differ from one application platform to another or through a life\ncycle, requiring such bots to be adaptable. Moreover, bots in such platforms\ncould cooperate with humans or other software agents for work or to learn\nspecific behavior patterns. However, present-day bots, particularly chatbots,\nother than language processing and prediction, are far from reaching a human\nuser's behavior level within complex business information systems. They lack\nthe cognitive capabilities to sense and act in such virtual environments,\nrendering their development a challenge to artificial general intelligence\nresearch. In this study, we problematize and investigate assumptions in\nconceptualizing software bot architecture by directing attention to significant\narchitectural research challenges in developing cognitive bots endowed with\ncomplex behavior for operation on information systems. As an outlook, we\npropose alternate architectural assumptions to consider in future bot design\nand bot development frameworks.",
                "BiomedGPT: A Generalist Vision-Language Foundation Model for Diverse\n  Biomedical Tasks\nTraditional biomedical artificial intelligence (AI) models, designed for\nspecific tasks or modalities, often exhibit limited flexibility in real-world\ndeployment and struggle to utilize holistic information. Generalist AI holds\nthe potential to address these limitations due to its versatility in\ninterpreting different data types and generating tailored outputs for diverse\nneeds. However, existing biomedical generalist AI solutions are typically\nheavyweight and closed source to researchers, practitioners, and patients.\nHere, we propose BiomedGPT, the first open-source and lightweight\nvision-language foundation model, designed as a generalist capable of\nperforming various biomedical tasks. BiomedGPT achieved state-of-the-art\nresults in 16 out of 25 experiments while maintaining a computing-friendly\nmodel scale. We also conducted human evaluations to assess the capabilities of\nBiomedGPT in radiology visual question answering, report generation, and\nsummarization. BiomedGPT exhibits robust prediction ability with a low error\nrate of 3.8% in question answering, satisfactory performance with an error rate\nof 8.3% in writing complex radiology reports, and competitive summarization\nability with a nearly equivalent preference score to human experts. Our method\ndemonstrates that effective training with diverse data can lead to more\npractical biomedical AI for improving diagnosis and workflow efficiency.",
                "Training Socially Aligned Language Models on Simulated Social\n  Interactions\nSocial alignment in AI systems aims to ensure that these models behave\naccording to established societal values. However, unlike humans, who derive\nconsensus on value judgments through social interaction, current language\nmodels (LMs) are trained to rigidly replicate their training corpus in\nisolation, leading to subpar generalization in unfamiliar scenarios and\nvulnerability to adversarial attacks. This work presents a novel training\nparadigm that permits LMs to learn from simulated social interactions. In\ncomparison to existing methodologies, our approach is considerably more\nscalable and efficient, demonstrating superior performance in alignment\nbenchmarks and human evaluations. This paradigm shift in the training of LMs\nbrings us a step closer to developing AI systems that can robustly and\naccurately reflect societal norms and values.",
                "Justification vs. Transparency: Why and How Visual Explanations in a\n  Scientific Literature Recommender System\nSignificant attention has been paid to enhancing recommender systems (RS)\nwith explanation facilities to help users make informed decisions and increase\ntrust in and satisfaction with the RS. Justification and transparency represent\ntwo crucial goals in explainable recommendation. Different from transparency,\nwhich faithfully exposes the reasoning behind the recommendation mechanism,\njustification conveys a conceptual model that may differ from that of the\nunderlying algorithm. An explanation is an answer to a question. In explainable\nrecommendation, a user would want to ask questions (referred to as\nintelligibility types) to understand results given by the RS. In this paper, we\nidentify relationships between Why and How explanation intelligibility types\nand the explanation goals of justification and transparency. We followed the\nHuman-Centered Design (HCD) approach and leveraged the What-Why-How\nvisualization framework to systematically design and implement Why and How\nvisual explanations in the transparent Recommendation and Interest Modeling\nApplication (RIMA). Furthermore, we conducted a qualitative user study (N=12)\nto investigate the potential effects of providing Why and How explanations\ntogether in an explainable RS on the users' perceptions regarding transparency,\ntrust, and satisfaction. Our study showed qualitative evidence confirming that\nthe choice of the explanation intelligibility types depends on the explanation\ngoal and user type."
            ],
            "interesting paper": 2
        },
        {
            "papers": [
                "AI Coach Assist: An Automated Approach for Call Recommendation in\n  Contact Centers for Agent Coaching\nIn recent years, the utilization of Artificial Intelligence (AI) in the\ncontact center industry is on the rise. One area where AI can have a\nsignificant impact is in the coaching of contact center agents. By analyzing\ncall transcripts using Natural Language Processing (NLP) techniques, it would\nbe possible to quickly determine which calls are most relevant for coaching\npurposes. In this paper, we present AI Coach Assist, which leverages the\npre-trained transformer-based language models to determine whether a given call\nis coachable or not based on the quality assurance (QA) questions asked by the\ncontact center managers or supervisors. The system was trained and evaluated on\na large dataset collected from real-world contact centers and provides an\neffective way to recommend calls to the contact center managers that are more\nlikely to contain coachable moments. Our experimental findings demonstrate the\npotential of AI Coach Assist to improve the coaching process, resulting in\nenhancing the performance of contact center agents.",
                "MLOps: A Step Forward to Enterprise Machine Learning\nMachine Learning Operations (MLOps) is becoming a highly crucial part of\nbusinesses looking to capitalize on the benefits of AI and ML models. This\nresearch presents a detailed review of MLOps, its benefits, difficulties,\nevolutions, and important underlying technologies such as MLOps frameworks,\nDocker, GitHub actions, and Kubernetes. The MLOps workflow, which includes\nmodel design, deployment, and operations, is explained in detail along with the\nvarious tools necessary for both model and data exploration and deployment.\nThis article also puts light on the end-to-end production of ML projects using\nvarious maturity levels of automated pipelines, with the least at no automation\nat all and the highest with complete CI/CD and CT capabilities. Furthermore, a\ndetailed example of an enterprise-level MLOps project for an object detection\nservice is used to explain the workflow of the technology in a real-world\nscenario. For this purpose, a web application hosting a pre-trained model from\nTensorFlow 2 Model Zoo is packaged and deployed to the internet making sure\nthat the system is scalable, reliable, and optimized for deployment at an\nenterprise level.",
                "Modeling Dynamic Environments with Scene Graph Memory\nEmbodied AI agents that search for objects in large environments such as\nhouseholds often need to make efficient decisions by predicting object\nlocations based on partial information. We pose this as a new type of link\nprediction problem: link prediction on partially observable dynamic graphs. Our\ngraph is a representation of a scene in which rooms and objects are nodes, and\ntheir relationships are encoded in the edges; only parts of the changing graph\nare known to the agent at each timestep. This partial observability poses a\nchallenge to existing link prediction approaches, which we address. We propose\na novel state representation -- Scene Graph Memory (SGM) -- with captures the\nagent's accumulated set of observations, as well as a neural net architecture\ncalled a Node Edge Predictor (NEP) that extracts information from the SGM to\nsearch efficiently. We evaluate our method in the Dynamic House Simulator, a\nnew benchmark that creates diverse dynamic graphs following the semantic\npatterns typically seen at homes, and show that NEP can be trained to predict\nthe locations of objects in a variety of environments with diverse object\nmovement dynamics, outperforming baselines both in terms of new scene\nadaptability and overall accuracy. The codebase and more can be found at\nhttps://www.scenegraphmemory.com.",
                "Integrating Action Knowledge and LLMs for Task Planning and Situation\n  Handling in Open Worlds\nTask planning systems have been developed to help robots use human knowledge\n(about actions) to complete long-horizon tasks. Most of them have been\ndeveloped for \"closed worlds\" while assuming the robot is provided with\ncomplete world knowledge. However, the real world is generally open, and the\nrobots frequently encounter unforeseen situations that can potentially break\nthe planner's completeness. Could we leverage the recent advances on\npre-trained Large Language Models (LLMs) to enable classical planning systems\nto deal with novel situations?\n  This paper introduces a novel framework, called COWP, for open-world task\nplanning and situation handling. COWP dynamically augments the robot's action\nknowledge, including the preconditions and effects of actions, with\ntask-oriented commonsense knowledge. COWP embraces the openness from LLMs, and\nis grounded to specific domains via action knowledge. For systematic\nevaluations, we collected a dataset that includes 1,085 execution-time\nsituations. Each situation corresponds to a state instance wherein a robot is\npotentially unable to complete a task using a solution that normally works.\nExperimental results show that our approach outperforms competitive baselines\nfrom the literature in the success rate of service tasks. Additionally, we have\ndemonstrated COWP using a mobile manipulator. Supplementary materials are\navailable at: https://cowplanning.github.io/",
                "On the Value of Myopic Behavior in Policy Reuse\nLeveraging learned strategies in unfamiliar scenarios is fundamental to human\nintelligence. In reinforcement learning, rationally reusing the policies\nacquired from other tasks or human experts is critical for tackling problems\nthat are difficult to learn from scratch. In this work, we present a framework\ncalled Selective Myopic bEhavior Control~(SMEC), which results from the insight\nthat the short-term behaviors of prior policies are sharable across tasks. By\nevaluating the behaviors of prior policies via a hybrid value function\narchitecture, SMEC adaptively aggregates the sharable short-term behaviors of\nprior policies and the long-term behaviors of the task policy, leading to\ncoordinated decisions. Empirical results on a collection of manipulation and\nlocomotion tasks demonstrate that SMEC outperforms existing methods, and\nvalidate the ability of SMEC to leverage related prior policies.",
                "Assumption Generation for the Verification of Learning-Enabled\n  Autonomous Systems\nProviding safety guarantees for autonomous systems is difficult as these\nsystems operate in complex environments that require the use of\nlearning-enabled components, such as deep neural networks (DNNs) for visual\nperception. DNNs are hard to analyze due to their size (they can have thousands\nor millions of parameters), lack of formal specifications (DNNs are typically\nlearnt from labeled data, in the absence of any formal requirements), and\nsensitivity to small changes in the environment. We present an assume-guarantee\nstyle compositional approach for the formal verification of system-level safety\nproperties of such autonomous systems. Our insight is that we can analyze the\nsystem in the absence of the DNN perception components by automatically\nsynthesizing assumptions on the DNN behaviour that guarantee the satisfaction\nof the required safety properties. The synthesized assumptions are the weakest\nin the sense that they characterize the output sequences of all the possible\nDNNs that, plugged into the autonomous system, guarantee the required safety\nproperties. The assumptions can be leveraged as run-time monitors over a\ndeployed DNN to guarantee the safety of the overall system; they can also be\nmined to extract local specifications for use during training and testing of\nDNNs. We illustrate our approach on a case study taken from the autonomous\nairplanes domain that uses a complex DNN for perception."
            ],
            "interesting paper": 3
        },
        {
            "papers": [
                "AI Coach Assist: An Automated Approach for Call Recommendation in\n  Contact Centers for Agent Coaching\nIn recent years, the utilization of Artificial Intelligence (AI) in the\ncontact center industry is on the rise. One area where AI can have a\nsignificant impact is in the coaching of contact center agents. By analyzing\ncall transcripts using Natural Language Processing (NLP) techniques, it would\nbe possible to quickly determine which calls are most relevant for coaching\npurposes. In this paper, we present AI Coach Assist, which leverages the\npre-trained transformer-based language models to determine whether a given call\nis coachable or not based on the quality assurance (QA) questions asked by the\ncontact center managers or supervisors. The system was trained and evaluated on\na large dataset collected from real-world contact centers and provides an\neffective way to recommend calls to the contact center managers that are more\nlikely to contain coachable moments. Our experimental findings demonstrate the\npotential of AI Coach Assist to improve the coaching process, resulting in\nenhancing the performance of contact center agents.",
                "Data Minimization at Inference Time\nIn domains with high stakes such as law, recruitment, and healthcare,\nlearning models frequently rely on sensitive user data for inference,\nnecessitating the complete set of features. This not only poses significant\nprivacy risks for individuals but also demands substantial human effort from\norganizations to verify information accuracy. This paper asks whether it is\nnecessary to use \\emph{all} input features for accurate predictions at\ninference time. The paper demonstrates that, in a personalized setting,\nindividuals may only need to disclose a small subset of their features without\ncompromising decision-making accuracy. The paper also provides an efficient\nsequential algorithm to determine the appropriate attributes for each\nindividual to provide. Evaluations across various learning tasks show that\nindividuals can potentially report as little as 10\\% of their information while\nmaintaining the same accuracy level as a model that employs the full set of\nuser information.",
                "Counterfactual Formulation of Patient-Specific Root Causes of Disease\nRoot causes of disease intuitively correspond to root vertices that increase\nthe likelihood of a diagnosis. This description of a root cause nevertheless\nlacks the rigorous mathematical formulation needed for the development of\ncomputer algorithms designed to automatically detect root causes from data.\nPrior work defined patient-specific root causes of disease using an\ninterventionalist account that only climbs to the second rung of Pearl's Ladder\nof Causation. In this theoretical piece, we climb to the third rung by\nproposing a counterfactual definition matching clinical intuition based on\nfixed factual data alone. We then show how to assign a root causal contribution\nscore to each variable using Shapley values from explainable artificial\nintelligence. The proposed counterfactual formulation of patient-specific root\ncauses of disease accounts for noisy labels, adapts to disease prevalence and\nadmits fast computation without the need for counterfactual simulation.",
                "Synthesizing a Progression of Subtasks for Block-Based Visual\n  Programming Tasks\nBlock-based visual programming environments play an increasingly important\nrole in introducing computing concepts to K-12 students. In recent years, they\nhave also gained popularity in neuro-symbolic AI, serving as a benchmark to\nevaluate general problem-solving and logical reasoning skills. The open-ended\nand conceptual nature of these visual programming tasks make them challenging,\nboth for state-of-the-art AI agents as well as for novice programmers. A\nnatural approach to providing assistance for problem-solving is breaking down a\ncomplex task into a progression of simpler subtasks; however, this is not\ntrivial given that the solution codes are typically nested and have non-linear\nexecution behavior. In this paper, we formalize the problem of synthesizing\nsuch a progression for a given reference block-based visual programming task.\nWe propose a novel synthesis algorithm that generates a progression of subtasks\nthat are high-quality, well-spaced in terms of their complexity, and solving\nthis progression leads to solving the reference task. We show the utility of\nour synthesis algorithm in improving the efficacy of AI agents (in this case,\nneural program synthesizers) for solving tasks in the Karel programming\nenvironment. Then, we conduct a user study to demonstrate that our synthesized\nprogression of subtasks can assist a novice programmer in solving tasks in the\nHour of Code: Maze Challenge by Code-dot-org.",
                "Modeling Dynamic Environments with Scene Graph Memory\nEmbodied AI agents that search for objects in large environments such as\nhouseholds often need to make efficient decisions by predicting object\nlocations based on partial information. We pose this as a new type of link\nprediction problem: link prediction on partially observable dynamic graphs. Our\ngraph is a representation of a scene in which rooms and objects are nodes, and\ntheir relationships are encoded in the edges; only parts of the changing graph\nare known to the agent at each timestep. This partial observability poses a\nchallenge to existing link prediction approaches, which we address. We propose\na novel state representation -- Scene Graph Memory (SGM) -- with captures the\nagent's accumulated set of observations, as well as a neural net architecture\ncalled a Node Edge Predictor (NEP) that extracts information from the SGM to\nsearch efficiently. We evaluate our method in the Dynamic House Simulator, a\nnew benchmark that creates diverse dynamic graphs following the semantic\npatterns typically seen at homes, and show that NEP can be trained to predict\nthe locations of objects in a variety of environments with diverse object\nmovement dynamics, outperforming baselines both in terms of new scene\nadaptability and overall accuracy. The codebase and more can be found at\nhttps://www.scenegraphmemory.com.",
                "The Curse of Recursion: Training on Generated Data Makes Models Forget\nStable Diffusion revolutionised image creation from descriptive text. GPT-2,\nGPT-3(.5) and GPT-4 demonstrated astonishing performance across a variety of\nlanguage tasks. ChatGPT introduced such language models to the general public.\nIt is now clear that large language models (LLMs) are here to stay, and will\nbring about drastic change in the whole ecosystem of online text and images. In\nthis paper we consider what the future might hold. What will happen to GPT-{n}\nonce LLMs contribute much of the language found online? We find that use of\nmodel-generated content in training causes irreversible defects in the\nresulting models, where tails of the original content distribution disappear.\nWe refer to this effect as Model Collapse and show that it can occur in\nVariational Autoencoders, Gaussian Mixture Models and LLMs. We build\ntheoretical intuition behind the phenomenon and portray its ubiquity amongst\nall learned generative models. We demonstrate that it has to be taken seriously\nif we are to sustain the benefits of training from large-scale data scraped\nfrom the web. Indeed, the value of data collected about genuine human\ninteractions with systems will be increasingly valuable in the presence of\ncontent generated by LLMs in data crawled from the Internet."
            ],
            "interesting paper": 1
        },
        {
            "papers": [
                "Re-imagining health and well-being in low resource African settings\n  using an augmented AI system and a 3D digital twin\nThis paper discusses and explores the potential and relevance of recent\ndevelopments in artificial intelligence (AI) and digital twins for health and\nwell-being in low-resource African countries. We use the case of public health\nemergency response to disease outbreaks and epidemic control. There is\npotential to take advantage of the increasing availability of data and\ndigitization to develop advanced AI methods for analysis and prediction. Using\nan AI systems perspective, we review emerging trends in AI systems and digital\ntwins and propose an initial augmented AI system architecture to illustrate how\nan AI system can work with a 3D digital twin to address public health goals. We\nhighlight scientific knowledge discovery, continual learning, pragmatic\ninteroperability, and interactive explanation and decision-making as essential\nresearch challenges for AI systems and digital twins.",
                "Towards Autonomous and Safe Last-mile Deliveries with AI-augmented\n  Self-driving Delivery Robots\nIn addition to its crucial impact on customer satisfaction, last-mile\ndelivery (LMD) is notorious for being the most time-consuming and costly stage\nof the shipping process. Pressing environmental concerns combined with the\nrecent surge of e-commerce sales have sparked renewed interest in automation\nand electrification of last-mile logistics. To address the hurdles faced by\nexisting robotic couriers, this paper introduces a customer-centric and\nsafety-conscious LMD system for small urban communities based on AI-assisted\nautonomous delivery robots. The presented framework enables end-to-end\nautomation and optimization of the logistic process while catering for\nreal-world imposed operational uncertainties, clients' preferred time\nschedules, and safety of pedestrians. To this end, the integrated optimization\ncomponent is modeled as a robust variant of the Cumulative Capacitated Vehicle\nRouting Problem with Time Windows, where routes are constructed under uncertain\ntravel times with an objective to minimize the total latency of deliveries\n(i.e., the overall waiting time of customers, which can negatively affect their\nsatisfaction). We demonstrate the proposed LMD system's utility through\nreal-world trials in a university campus with a single robotic courier.\nImplementation aspects as well as the findings and practical insights gained\nfrom the deployment are discussed in detail. Lastly, we round up the\ncontributions with numerical simulations to investigate the scalability of the\ndeveloped mathematical formulation with respect to the number of robotic\nvehicles and customers.",
                "The Digital Divide in Process Safety: Quantitative Risk Analysis of\n  Human-AI Collaboration\nDigital technologies have dramatically accelerated the digital transformation\nin process industries, boosted new industrial applications, upgraded the\nproduction system, and enhanced operational efficiency. In contrast, the\nchallenges and gaps between human and artificial intelligence (AI) have become\nmore and more prominent, whereas the digital divide in process safety is\naggregating. The study attempts to address the following questions: (i)What is\nAI in the process safety context? (ii)What is the difference between AI and\nhumans in process safety? (iii)How do AI and humans collaborate in process\nsafety? (iv)What are the challenges and gaps in human-AI collaboration? (v)How\nto quantify the risk of human-AI collaboration in process safety? Qualitative\nrisk analysis based on brainstorming and literature review, and quantitative\nrisk analysis based on layer of protection analysis (LOPA) and Bayesian network\n(BN), were applied to explore and model. The importance of human reliability\nshould be stressed in the digital age, not usually to increase the reliability\nof AI, and human-centered AI design in process safety needs to be propagated.",
                "Towards a Technology-Driven Adaptive Decision Support System for\n  Integrated Pavement and Maintenance strategies (TDADSS-IPM): focus on risk\n  assessment framework for climate change adaptation\nDecision Support Systems for pavement and maintenance strategies have\ntraditionally been designed as silos led to local optimum systems. Moreover,\nsince big data usage didn't exist as result of Industry 4.0 as of today, DSSs\nwere not initially designed adaptive to the sources of uncertainties led to\nrigid decisions. Motivated by the vulnerability of the road assets to the\nclimate phenomena, this paper takes a visionary step towards introducing a\nTechnology-Driven Adaptive Decision Support System for Integrated Pavement and\nMaintenance activities called TDADSS-IPM. As part of such DSS, a bottom-up risk\nassessment model is met via Bayesian Belief Networks (BBN) to realize the\nactual condition of the Danish roads due to weather condition. Such model fills\nthe gaps in the knowledge domain and develops a platform that can be trained\nover time, and applied in real-time to the actual event.",
                "AI Audit: A Card Game to Reflect on Everyday AI Systems\nAn essential element of K-12 AI literacy is educating learners about the\nethical and societal implications of AI systems. Previous work in AI ethics\nliteracy have developed curriculum and classroom activities that engage\nlearners in reflecting on the ethical implications of AI systems and developing\nresponsible AI. There is little work in using game-based learning methods in AI\nliteracy. Games are known to be compelling media to teach children about\ncomplex STEM concepts. In this work, we developed a competitive card game for\nmiddle and high school students called \"AI Audit\" where they play as AI\nstart-up founders building novel AI-powered technology. Players can challenge\nother players with potential harms of their technology or defend their own\nbusinesses by features that mitigate these harms. The game mechanics reward\nsystems that are ethically developed or that take steps to mitigate potential\nharms. In this paper, we present the game design, teacher resources for\nclassroom deployment and early playtesting results. We discuss our reflections\nabout using games as teaching tools for AI literacy in K-12 classrooms.",
                "IoT based Personal Voice Assistant\nToday, technological advancement is increasing day by day. Earlier, there was\nonly a computer system in which we could only perform a few tasks. But now,\nmachine learning, artificial intelligence, deep learning, and a few more\ntechnologies have made computer systems so advanced that we can perform any\ntype of task. In this era of advancement, if people are still struggling to\ninteract using various input devices, then it's not worth it. For this reason,\nwe developed a voice assistant using Python that allows the user to run any\ntype of command in Linux without interaction with the keyboard. The main task\nof the voice assistant is to minimize the use of input devices like the\nkeyboard and mouse. It will also reduce hardware space and cost."
            ],
            "interesting paper": 2
        },
        {
            "papers": [
                "Ask an Expert: Leveraging Language Models to Improve Strategic Reasoning\n  in Goal-Oriented Dialogue Models\nExisting dialogue models may encounter scenarios which are not\nwell-represented in the training data, and as a result generate responses that\nare unnatural, inappropriate, or unhelpful. We propose the \"Ask an Expert\"\nframework in which the model is trained with access to an \"expert\" which it can\nconsult at each turn. Advice is solicited via a structured dialogue with the\nexpert, and the model is optimized to selectively utilize (or ignore) it given\nthe context and dialogue history. In this work the expert takes the form of an\nLLM. We evaluate this framework in a mental health support domain, where the\nstructure of the expert conversation is outlined by pre-specified prompts which\nreflect a reasoning strategy taught to practitioners in the field. Blenderbot\nmodels utilizing \"Ask an Expert\" show quality improvements across all expert\nsizes, including those with fewer parameters than the dialogue model itself.\nOur best model provides a $\\sim 10\\%$ improvement over baselines, approaching\nhuman-level scores on \"engingingness\" and \"helpfulness\" metrics.",
                "Taming AI Bots: Controllability of Neural States in Large Language\n  Models\nWe tackle the question of whether an agent can, by suitable choice of\nprompts, control an AI bot to any state. To that end, we first introduce a\nformal definition of ``meaning'' that is amenable to analysis. Then, we\ncharacterize ``meaningful data'' on which large language models (LLMs) are\nostensibly trained, and ``well-trained LLMs'' through conditions that are\nlargely met by today's LLMs. While a well-trained LLM constructs an embedding\nspace of meanings that is Euclidean, meanings themselves do not form a vector\n(linear) subspace, but rather a quotient space within. We then characterize the\nsubset of meanings that can be reached by the state of the LLMs for some input\nprompt, and show that a well-trained bot can reach any meaning albeit with\nsmall probability. We then introduce a stronger notion of controllability as\n{\\em almost certain reachability}, and show that, when restricted to the space\nof meanings, an AI bot is controllable. We do so after introducing a functional\ncharacterization of attentive AI bots, and finally derive necessary and\nsufficient conditions for controllability. The fact that AI bots are\ncontrollable means that an adversary could steer them towards any state.\nHowever, the sampling process can be designed to counteract adverse actions and\navoid reaching undesirable regions of state space before their boundary is\ncrossed.",
                "Re-imagining health and well-being in low resource African settings\n  using an augmented AI system and a 3D digital twin\nThis paper discusses and explores the potential and relevance of recent\ndevelopments in artificial intelligence (AI) and digital twins for health and\nwell-being in low-resource African countries. We use the case of public health\nemergency response to disease outbreaks and epidemic control. There is\npotential to take advantage of the increasing availability of data and\ndigitization to develop advanced AI methods for analysis and prediction. Using\nan AI systems perspective, we review emerging trends in AI systems and digital\ntwins and propose an initial augmented AI system architecture to illustrate how\nan AI system can work with a 3D digital twin to address public health goals. We\nhighlight scientific knowledge discovery, continual learning, pragmatic\ninteroperability, and interactive explanation and decision-making as essential\nresearch challenges for AI systems and digital twins.",
                "Sequential Condition Evolved Interaction Knowledge Graph for Traditional\n  Chinese Medicine Recommendation\nTraditional Chinese Medicine (TCM) has a rich history of utilizing natural\nherbs to treat a diversity of illnesses. In practice, TCM diagnosis and\ntreatment are highly personalized and organically holistic, requiring\ncomprehensive consideration of the patient's state and symptoms over time.\nHowever, existing TCM recommendation approaches overlook the changes in patient\nstatus and only explore potential patterns between symptoms and prescriptions.\nIn this paper, we propose a novel Sequential Condition Evolved Interaction\nKnowledge Graph (SCEIKG), a framework that treats the model as a sequential\nprescription-making problem by considering the dynamics of the patient's\ncondition across multiple visits. In addition, we incorporate an interaction\nknowledge graph to enhance the accuracy of recommendations by considering the\ninteractions between different herbs and the patient's condition. Experimental\nresults on a real-world dataset demonstrate that our approach outperforms\nexisting TCM recommendation methods, achieving state-of-the-art performance.",
                "ContrastNER: Contrastive-based Prompt Tuning for Few-shot NER\nPrompt-based language models have produced encouraging results in numerous\napplications, including Named Entity Recognition (NER) tasks. NER aims to\nidentify entities in a sentence and provide their types. However, the strong\nperformance of most available NER approaches is heavily dependent on the design\nof discrete prompts and a verbalizer to map the model-predicted outputs to\nentity categories, which are complicated undertakings. To address these\nchallenges, we present ContrastNER, a prompt-based NER framework that employs\nboth discrete and continuous tokens in prompts and uses a contrastive learning\napproach to learn the continuous prompts and forecast entity types. The\nexperimental results demonstrate that ContrastNER obtains competitive\nperformance to the state-of-the-art NER methods in high-resource settings and\noutperforms the state-of-the-art models in low-resource circumstances without\nrequiring extensive manual prompt engineering and verbalizer design.",
                "Key-Value Transformer\nTransformers have emerged as the prevailing standard solution for various AI\ntasks, including computer vision and natural language processing. The widely\nadopted Query, Key, and Value formulation (QKV) has played a significant role\nin this. Nevertheless, no research has examined the essentiality of these three\ncomponents for transformer performance. Therefore, we conducted an evaluation\nof the key-value formulation (KV), which generates symmetric attention maps,\nalong with an asymmetric version that incorporates a 2D positional encoding\ninto the attention matrix. Remarkably, this transformer requires fewer\nparameters and computation than the original one. Through experiments\nencompassing three task types -- synthetics (such as reversing or sorting a\nlist), vision (mnist or cifar classification), and NLP (character generation\nand translation) -- we discovered that the KV transformer occasionally\noutperforms the QKV transformer. However, it also exhibits instances of\nunderperformance compared to QKV, making it challenging to draw a definitive\nconclusion. Nonetheless, we consider the reported results to be encouraging and\nanticipate that they may pave the way for more efficient transformers in the\nfuture."
            ],
            "interesting paper": 1
        },
        {
            "papers": [
                "RE-centric Recommendations for the Development of Trustworthy(er)\n  Autonomous Systems\nComplying with the EU AI Act (AIA) guidelines while developing and\nimplementing AI systems will soon be mandatory within the EU. However,\npractitioners lack actionable instructions to operationalise ethics during AI\nsystems development. A literature review of different ethical guidelines\nrevealed inconsistencies in the principles addressed and the terminology used\nto describe them. Furthermore, requirements engineering (RE), which is\nidentified to foster trustworthiness in the AI development process from the\nearly stages was observed to be absent in a lot of frameworks that support the\ndevelopment of ethical and trustworthy AI. This incongruous phrasing combined\nwith a lack of concrete development practices makes trustworthy AI development\nharder. To address this concern, we formulated a comparison table for the\nterminology used and the coverage of the ethical AI principles in major ethical\nAI guidelines. We then examined the applicability of ethical AI development\nframeworks for performing effective RE during the development of trustworthy AI\nsystems. A tertiary review and meta-analysis of literature discussing ethical\nAI frameworks revealed their limitations when developing trustworthy AI. Based\non our findings, we propose recommendations to address such limitations during\nthe development of trustworthy AI.",
                "An Emergency Disposal Decision-making Method with Human--Machine\n  Collaboration\nRapid developments in artificial intelligence technology have led to unmanned\nsystems replacing human beings in many fields requiring high-precision\npredictions and decisions. In modern operational environments, all job plans\nare affected by emergency events such as equipment failures and resource\nshortages, making a quick resolution critical. The use of unmanned systems to\nassist decision-making can improve resolution efficiency, but their\ndecision-making is not interpretable and may make the wrong decisions. Current\nunmanned systems require human supervision and control. Based on this, we\npropose a collaborative human--machine method for resolving unplanned events\nusing two phases: task filtering and task scheduling. In the task filtering\nphase, we propose a human--machine collaborative decision-making algorithm for\ndynamic tasks. The GACRNN model is used to predict the state of the job nodes,\nlocate the key nodes, and generate a machine-predicted resolution task list. A\nhuman decision-maker supervises the list in real time and modifies and confirms\nthe machine-predicted list through the human--machine interface. In the task\nscheduling phase, we propose a scheduling algorithm that integrates human\nexperience constraints. The steps to resolve an event are inserted into the\nnormal job sequence to schedule the resolution. We propose several\nhuman--machine collaboration methods in each phase to generate steps to resolve\nan unplanned event while minimizing the impact on the original job plan.",
                "Towards a Unifying Model of Rationality in Multiagent Systems\nMultiagent systems deployed in the real world need to cooperate with other\nagents (including humans) nearly as effectively as these agents cooperate with\none another. To design such AI, and provide guarantees of its effectiveness, we\nneed to clearly specify what types of agents our AI must be able to cooperate\nwith. In this work we propose a generic model of socially intelligent agents,\nwhich are individually rational learners that are also able to cooperate with\none another (in the sense that their joint behavior is Pareto efficient). We\ndefine rationality in terms of the regret incurred by each agent over its\nlifetime, and show how we can construct socially intelligent agents for\ndifferent forms of regret. We then discuss the implications of this model for\nthe development of \"robust\" MAS that can cooperate with a wide variety of\nsocially intelligent agents.",
                "Development of a ROS-based Architecture for Intelligent Autonomous on\n  Demand Last Mile Delivery\nThis paper presents the development of the JKU-ITS Last Mile Delivery Robot.\nThe proposed approach utilizes a combination of one 3D LIDAR, RGB-D camera, IMU\nand GPS sensor on top of a mobile robot slope mower. An embedded computer,\nrunning ROS1, is utilized to process the sensor data streams to enable 2D and\n3D Simultaneous Localization and Mapping, 2D localization and object detection\nusing a convolutional neural network.",
                "Graph Neural Network for spatiotemporal data: methods and applications\nIn the era of big data, there has been a surge in the availability of data\ncontaining rich spatial and temporal information, offering valuable insights\ninto dynamic systems and processes for applications such as weather\nforecasting, natural disaster management, intelligent transport systems, and\nprecision agriculture. Graph neural networks (GNNs) have emerged as a powerful\ntool for modeling and understanding data with dependencies to each other such\nas spatial and temporal dependencies. There is a large amount of existing work\nthat focuses on addressing the complex spatial and temporal dependencies in\nspatiotemporal data using GNNs. However, the strong interdisciplinary nature of\nspatiotemporal data has created numerous GNNs variants specifically designed\nfor distinct application domains. Although the techniques are generally\napplicable across various domains, cross-referencing these methods remains\nessential yet challenging due to the absence of a comprehensive literature\nreview on GNNs for spatiotemporal data. This article aims to provide a\nsystematic and comprehensive overview of the technologies and applications of\nGNNs in the spatiotemporal domain. First, the ways of constructing graphs from\nspatiotemporal data are summarized to help domain experts understand how to\ngenerate graphs from various types of spatiotemporal data. Then, a systematic\ncategorization and summary of existing spatiotemporal GNNs are presented to\nenable domain experts to identify suitable techniques and to support model\ndevelopers in advancing their research. Moreover, a comprehensive overview of\nsignificant applications in the spatiotemporal domain is offered to introduce a\nbroader range of applications to model developers and domain experts, assisting\nthem in exploring potential research topics and enhancing the impact of their\nwork. Finally, open challenges and future directions are discussed.",
                "Generalized Disparate Impact for Configurable Fairness Solutions in ML\nWe make two contributions in the field of AI fairness over continuous\nprotected attributes. First, we show that the Hirschfeld-Gebelein-Renyi (HGR)\nindicator (the only one currently available for such a case) is valuable but\nsubject to a few crucial limitations regarding semantics, interpretability, and\nrobustness. Second, we introduce a family of indicators that are: 1)\ncomplementary to HGR in terms of semantics; 2) fully interpretable and\ntransparent; 3) robust over finite samples; 4) configurable to suit specific\napplications. Our approach also allows us to define fine-grained constraints to\npermit certain types of dependence and forbid others selectively. By expanding\nthe available options for continuous protected attributes, our approach\nrepresents a significant contribution to the area of fair artificial\nintelligence."
            ],
            "interesting paper": 5
        },
        {
            "papers": [
                "Partially Personalized Federated Learning: Breaking the Curse of Data\n  Heterogeneity\nWe present a partially personalized formulation of Federated Learning (FL)\nthat strikes a balance between the flexibility of personalization and\ncooperativeness of global training. In our framework, we split the variables\ninto global parameters, which are shared across all clients, and individual\nlocal parameters, which are kept private. We prove that under the right split\nof parameters, it is possible to find global parameters that allow each client\nto fit their data perfectly, and refer to the obtained problem as\noverpersonalized. For instance, the shared global parameters can be used to\nlearn good data representations, whereas the personalized layers are fine-tuned\nfor a specific client. Moreover, we present a simple algorithm for the\npartially personalized formulation that offers significant benefits to all\nclients. In particular, it breaks the curse of data heterogeneity in several\nsettings, such as training with local steps, asynchronous training, and\nByzantine-robust training.",
                "An Annotated Dataset for Explainable Interpersonal Risk Factors of\n  Mental Disturbance in Social Media Posts\nWith a surge in identifying suicidal risk and its severity in social media\nposts, we argue that a more consequential and explainable research is required\nfor optimal impact on clinical psychology practice and personalized mental\nhealthcare. The success of computational intelligence techniques for inferring\nmental illness from social media resources, points to natural language\nprocessing as a lens for determining Interpersonal Risk Factors (IRF) in human\nwritings. Motivated with limited availability of datasets for social NLP\nresearch community, we construct and release a new annotated dataset with\nhuman-labelled explanations and classification of IRF affecting mental\ndisturbance on social media: (i) Thwarted Belongingness (TBe), and (ii)\nPerceived Burdensomeness (PBu). We establish baseline models on our dataset\nfacilitating future research directions to develop real-time personalized AI\nmodels by detecting patterns of TBe and PBu in emotional spectrum of user's\nhistorical social media profile.",
                "Contextual Bandits with Budgeted Information Reveal\nContextual bandit algorithms are commonly used in digital health to recommend\npersonalized treatments. However, to ensure the effectiveness of the\ntreatments, patients are often requested to take actions that have no immediate\nbenefit to them, which we refer to as pro-treatment actions. In practice,\nclinicians have a limited budget to encourage patients to take these actions\nand collect additional information. We introduce a novel optimization and\nlearning algorithm to address this problem. This algorithm effectively combines\nthe strengths of two algorithmic approaches in a seamless manner, including 1)\nan online primal-dual algorithm for deciding the optimal timing to reach out to\npatients, and 2) a contextual bandit learning algorithm to deliver personalized\ntreatment to the patient. We prove that this algorithm admits a sub-linear\nregret bound. We illustrate the usefulness of this algorithm on both synthetic\nand real-world data.",
                "Perceived Trustworthiness of Natural Language Generators\nNatural Language Generation tools, such as chatbots that can generate\nhuman-like conversational text, are becoming more common both for personal and\nprofessional use. However, there are concerns about their trustworthiness and\nethical implications. The paper addresses the problem of understanding how\ndifferent users (e.g., linguists, engineers) perceive and adopt these tools and\ntheir perception of machine-generated text quality. It also discusses the\nperceived advantages and limitations of Natural Language Generation tools, as\nwell as users' beliefs on governance strategies. The main findings of this\nstudy include the impact of users' field and level of expertise on the\nperceived trust and adoption of Natural Language Generation tools, the users'\nassessment of the accuracy, fluency, and potential biases of machine-generated\ntext in comparison to human-written text, and an analysis of the advantages and\nethical risks associated with these tools as identified by the participants.\nMoreover, this paper discusses the potential implications of these findings for\nenhancing the AI development process. The paper sheds light on how different\nuser characteristics shape their beliefs on the quality and overall\ntrustworthiness of machine-generated text. Furthermore, it examines the\nbenefits and risks of these tools from the perspectives of different users.",
                "Code Prompting: a Neural Symbolic Method for Complex Reasoning in Large\n  Language Models\nLarge language models (LLMs) have scaled up to unlock a wide range of complex\nreasoning tasks with the aid of various prompting methods. However, current\nprompting methods generate natural language intermediate steps to help\nreasoning, which can cause imperfect task reduction and confusion. To mitigate\nsuch limitations, we explore code prompting, a neural symbolic prompting method\nwith both zero-shot and few-shot versions which triggers code as intermediate\nsteps. We conduct experiments on 7 widely-used benchmarks involving symbolic\nreasoning and arithmetic reasoning. Code prompting generally outperforms\nchain-of-thought (CoT) prompting. To further understand the performance and\nlimitations of code prompting, we perform extensive ablation studies and error\nanalyses, and identify several exclusive advantages of using symbolic\npromptings compared to natural language. We also consider the ensemble of code\nprompting and CoT prompting to combine the strengths of both. Finally, we show\nthrough experiments how code annotations and their locations affect code\nprompting.",
                "Shapley Based Residual Decomposition for Instance Analysis\nIn this paper, we introduce the idea of decomposing the residuals of\nregression with respect to the data instances instead of features. This allows\nus to determine the effects of each individual instance on the model and each\nother, and in doing so makes for a model-agnostic method of identifying\ninstances of interest. In doing so, we can also determine the appropriateness\nof the model and data in the wider context of a given study. The paper focuses\non the possible applications that such a framework brings to the relatively\nunexplored field of instance analysis in the context of Explainable AI tasks."
            ],
            "interesting paper": 1
        },
        {
            "papers": [
                "Intent-aligned AI systems deplete human agency: the need for agency\n  foundations research in AI safety\nThe rapid advancement of artificial intelligence (AI) systems suggests that\nartificial general intelligence (AGI) systems may soon arrive. Many researchers\nare concerned that AIs and AGIs will harm humans via intentional misuse\n(AI-misuse) or through accidents (AI-accidents). In respect of AI-accidents,\nthere is an increasing effort focused on developing algorithms and paradigms\nthat ensure AI systems are aligned to what humans intend, e.g. AI systems that\nyield actions or recommendations that humans might judge as consistent with\ntheir intentions and goals. Here we argue that alignment to human intent is\ninsufficient for safe AI systems and that preservation of long-term agency of\nhumans may be a more robust standard, and one that needs to be separated\nexplicitly and a priori during optimization. We argue that AI systems can\nreshape human intention and discuss the lack of biological and psychological\nmechanisms that protect humans from loss of agency. We provide the first formal\ndefinition of agency-preserving AI-human interactions which focuses on\nforward-looking agency evaluations and argue that AI systems - not humans -\nmust be increasingly tasked with making these evaluations. We show how agency\nloss can occur in simple environments containing embedded agents that use\ntemporal-difference learning to make action recommendations. Finally, we\npropose a new area of research called \"agency foundations\" and pose four\ninitial topics designed to improve our understanding of agency in AI-human\ninteractions: benevolent game theory, algorithmic foundations of human rights,\nmechanistic interpretability of agency representation in neural-networks and\nreinforcement learning from internal states.",
                "Disentangling and Operationalizing AI Fairness at LinkedIn\nOperationalizing AI fairness at LinkedIn's scale is challenging not only\nbecause there are multiple mutually incompatible definitions of fairness but\nalso because determining what is fair depends on the specifics and context of\nthe product where AI is deployed. Moreover, AI practitioners need clarity on\nwhat fairness expectations need to be addressed at the AI level. In this paper,\nwe present the evolving AI fairness framework used at LinkedIn to address these\nthree challenges. The framework disentangles AI fairness by separating out\nequal treatment and equitable product expectations. Rather than imposing a\ntrade-off between these two commonly opposing interpretations of fairness, the\nframework provides clear guidelines for operationalizing equal AI treatment\ncomplemented with a product equity strategy. This paper focuses on the equal AI\ntreatment component of LinkedIn's AI fairness framework, shares the principles\nthat support it, and illustrates their application through a case study. We\nhope this paper will encourage other big tech companies to join us in sharing\ntheir approach to operationalizing AI fairness at scale, so that together we\ncan keep advancing this constantly evolving field.",
                "Evaluating GPT's Programming Capability through CodeWars' Katas\nIn the burgeoning field of artificial intelligence (AI), understanding the\ncapabilities and limitations of programming-oriented models is crucial. This\npaper presents a novel evaluation of the programming proficiency of Generative\nPretrained Transformer (GPT) models, specifically GPT-3.5 and GPT-4, against\ncoding problems of varying difficulty levels drawn from Codewars. The\nexperiments reveal a distinct boundary at the 3kyu level, beyond which these\nGPT models struggle to provide solutions. These findings led to the proposal of\na measure for coding problem complexity that incorporates both problem\ndifficulty and the time required for solution. The research emphasizes the need\nfor validation and creative thinking capabilities in AI models to better\nemulate human problem-solving techniques. Future work aims to refine this\nproposed complexity measure, enhance AI models with these suggested\ncapabilities, and develop an objective measure for programming problem\ndifficulty. The results of this research offer invaluable insights for\nimproving AI programming capabilities and advancing the frontier of AI\nproblem-solving abilities.",
                "Traffic Prediction using Artificial Intelligence: Review of Recent\n  Advances and Emerging Opportunities\nTraffic prediction plays a crucial role in alleviating traffic congestion\nwhich represents a critical problem globally, resulting in negative\nconsequences such as lost hours of additional travel time and increased fuel\nconsumption. Integrating emerging technologies into transportation systems\nprovides opportunities for improving traffic prediction significantly and\nbrings about new research problems. In order to lay the foundation for\nunderstanding the open research challenges in traffic prediction, this survey\naims to provide a comprehensive overview of traffic prediction methodologies.\nSpecifically, we focus on the recent advances and emerging research\nopportunities in Artificial Intelligence (AI)-based traffic prediction methods,\ndue to their recent success and potential in traffic prediction, with an\nemphasis on multivariate traffic time series modeling. We first provide a list\nand explanation of the various data types and resources used in the literature.\nNext, the essential data preprocessing methods within the traffic prediction\ncontext are categorized, and the prediction methods and applications are\nsubsequently summarized. Lastly, we present primary research challenges in\ntraffic prediction and discuss some directions for future research.",
                "Data and Knowledge for Overtaking Scenarios in Autonomous Driving\nAutonomous driving has become one of the most popular research topics within\nArtificial Intelligence. An autonomous vehicle is understood as a system that\ncombines perception, decision-making, planning, and control. All of those tasks\nrequire that the vehicle collects surrounding data in order to make a good\ndecision and action. In particular, the overtaking maneuver is one of the most\ncritical actions of driving. The process involves lane changes, acceleration\nand deceleration actions, and estimation of the speed and distance of the\nvehicle in front or in the lane in which it is moving. Despite the amount of\nwork available in the literature, just a few handle overtaking maneuvers and,\nbecause overtaking can be risky, no real-world dataset is available. This work\ncontributes in this area by presenting a new synthetic dataset whose focus is\nthe overtaking maneuver. We start by performing a thorough review of the state\nof the art in autonomous driving and then explore the main datasets found in\nthe literature (public and private, synthetic and real), highlighting their\nlimitations, and suggesting a new set of features whose focus is the overtaking\nmaneuver.",
                "Bigger, Better, Faster: Human-level Atari with human-level efficiency\nWe introduce a value-based RL agent, which we call BBF, that achieves\nsuper-human performance in the Atari 100K benchmark. BBF relies on scaling the\nneural networks used for value estimation, as well as a number of other design\nchoices that enable this scaling in a sample-efficient manner. We conduct\nextensive analyses of these design choices and provide insights for future\nwork. We end with a discussion about updating the goalposts for\nsample-efficient RL research on the ALE. We make our code and data publicly\navailable at\nhttps://github.com/google-research/google-research/tree/master/bigger_better_faster."
            ],
            "interesting paper": 1
        },
        {
            "papers": [
                "Less Likely Brainstorming: Using Language Models to Generate Alternative\n  Hypotheses\nA human decision-maker benefits the most from an AI assistant that corrects\nfor their biases. For problems such as generating interpretation of a radiology\nreport given findings, a system predicting only highly likely outcomes may be\nless useful, where such outcomes are already obvious to the user. To alleviate\nbiases in human decision-making, it is worth considering a broad differential\ndiagnosis, going beyond the most likely options. We introduce a new task, \"less\nlikely brainstorming,\" that asks a model to generate outputs that humans think\nare relevant but less likely to happen. We explore the task in two settings: a\nbrain MRI interpretation generation setting and an everyday commonsense\nreasoning setting. We found that a baseline approach of training with less\nlikely hypotheses as targets generates outputs that humans evaluate as either\nlikely or irrelevant nearly half of the time; standard MLE training is not\neffective. To tackle this problem, we propose a controlled text generation\nmethod that uses a novel contrastive learning strategy to encourage models to\ndifferentiate between generating likely and less likely outputs according to\nhumans. We compare our method with several state-of-the-art controlled text\ngeneration models via automatic and human evaluations and show that our models'\ncapability of generating less likely outputs is improved.",
                "Conceptual Design Generation Using Large Language Models\nConcept generation is a creative step in the conceptual design phase, where\ndesigners often turn to brainstorming, mindmapping, or crowdsourcing design\nideas to complement their own knowledge of the domain. Recent advances in\nnatural language processing (NLP) and machine learning (ML) have led to the\nrise of Large Language Models (LLMs) capable of generating seemingly creative\noutputs from textual prompts. The success of these models has led to their\nintegration and application across a variety of domains, including art,\nentertainment, and other creative work. In this paper, we leverage LLMs to\ngenerate solutions for a set of 12 design problems and compare them to a\nbaseline of crowdsourced solutions. We evaluate the differences between\ngenerated and crowdsourced design solutions through multiple perspectives,\nincluding human expert evaluations and computational metrics. Expert\nevaluations indicate that the LLM-generated solutions have higher average\nfeasibility and usefulness while the crowdsourced solutions have more novelty.\nWe experiment with prompt engineering and find that leveraging few-shot\nlearning can lead to the generation of solutions that are more similar to the\ncrowdsourced solutions. These findings provide insight into the quality of\ndesign solutions generated with LLMs and begins to evaluate prompt engineering\ntechniques that could be leveraged by practitioners to generate higher-quality\ndesign solutions synergistically with LLMs.",
                "Probabilistic computation and uncertainty quantification with emerging\n  covariance\nBuilding robust, interpretable, and secure AI system requires quantifying and\nrepresenting uncertainty under a probabilistic perspective to mimic human\ncognitive abilities. However, probabilistic computation presents significant\nchallenges for most conventional artificial neural network, as they are\nessentially implemented in a deterministic manner. In this paper, we develop an\nefficient probabilistic computation framework by truncating the probabilistic\nrepresentation of neural activation up to its mean and covariance and construct\na moment neural network that encapsulates the nonlinear coupling between the\nmean and covariance of the underlying stochastic network. We reveal that when\nonly the mean but not the covariance is supervised during gradient-based\nlearning, the unsupervised covariance spontaneously emerges from its nonlinear\ncoupling with the mean and faithfully captures the uncertainty associated with\nmodel predictions. Our findings highlight the inherent simplicity of\nprobabilistic computation by seamlessly incorporating uncertainty into model\nprediction, paving the way for integrating it into large-scale AI systems.",
                "Intent-aligned AI systems deplete human agency: the need for agency\n  foundations research in AI safety\nThe rapid advancement of artificial intelligence (AI) systems suggests that\nartificial general intelligence (AGI) systems may soon arrive. Many researchers\nare concerned that AIs and AGIs will harm humans via intentional misuse\n(AI-misuse) or through accidents (AI-accidents). In respect of AI-accidents,\nthere is an increasing effort focused on developing algorithms and paradigms\nthat ensure AI systems are aligned to what humans intend, e.g. AI systems that\nyield actions or recommendations that humans might judge as consistent with\ntheir intentions and goals. Here we argue that alignment to human intent is\ninsufficient for safe AI systems and that preservation of long-term agency of\nhumans may be a more robust standard, and one that needs to be separated\nexplicitly and a priori during optimization. We argue that AI systems can\nreshape human intention and discuss the lack of biological and psychological\nmechanisms that protect humans from loss of agency. We provide the first formal\ndefinition of agency-preserving AI-human interactions which focuses on\nforward-looking agency evaluations and argue that AI systems - not humans -\nmust be increasingly tasked with making these evaluations. We show how agency\nloss can occur in simple environments containing embedded agents that use\ntemporal-difference learning to make action recommendations. Finally, we\npropose a new area of research called \"agency foundations\" and pose four\ninitial topics designed to improve our understanding of agency in AI-human\ninteractions: benevolent game theory, algorithmic foundations of human rights,\nmechanistic interpretability of agency representation in neural-networks and\nreinforcement learning from internal states.",
                "Unsupervised Melody-to-Lyric Generation\nAutomatic melody-to-lyric generation is a task in which song lyrics are\ngenerated to go with a given melody. It is of significant practical interest\nand more challenging than unconstrained lyric generation as the music imposes\nadditional constraints onto the lyrics. The training data is limited as most\nsongs are copyrighted, resulting in models that underfit the complicated\ncross-modal relationship between melody and lyrics. In this work, we propose a\nmethod for generating high-quality lyrics without training on any aligned\nmelody-lyric data. Specifically, we design a hierarchical lyric generation\nframework that first generates a song outline and second the complete lyrics.\nThe framework enables disentanglement of training (based purely on text) from\ninference (melody-guided text generation) to circumvent the shortage of\nparallel data.\n  We leverage the segmentation and rhythm alignment between melody and lyrics\nto compile the given melody into decoding constraints as guidance during\ninference. The two-step hierarchical design also enables content control via\nthe lyric outline, a much-desired feature for democratizing collaborative song\ncreation. Experimental results show that our model can generate high-quality\nlyrics that are more on-topic, singable, intelligible, and coherent than strong\nbaselines, for example SongMASS, a SOTA model trained on a parallel dataset,\nwith a 24% relative overall quality improvement based on human ratings.",
                "Towards Explainable and Language-Agnostic LLMs: Symbolic Reverse\n  Engineering of Language at Scale\nLarge language models (LLMs) have achieved a milestone that undenia-bly\nchanged many held beliefs in artificial intelligence (AI). However, there\nremains many limitations of these LLMs when it comes to true language\nunderstanding, limitations that are a byproduct of the under-lying architecture\nof deep neural networks. Moreover, and due to their subsymbolic nature,\nwhatever knowledge these models acquire about how language works will always be\nburied in billions of microfeatures (weights), none of which is meaningful on\nits own, making such models hopelessly unexplainable. To address these\nlimitations, we suggest com-bining the strength of symbolic representations\nwith what we believe to be the key to the success of LLMs, namely a successful\nbottom-up re-verse engineering of language at scale. As such we argue for a\nbottom-up reverse engineering of language in a symbolic setting. Hints on what\nthis project amounts to have been suggested by several authors, and we discuss\nin some detail here how this project could be accomplished."
            ],
            "interesting paper": 2
        },
        {
            "papers": [
                "Citizen Perspectives on Necessary Safeguards to the Use of AI by Law\n  Enforcement Agencies\nIn the light of modern technological advances, Artificial Intelligence (AI)\nis relied upon to enhance performance, increase efficiency, and maximize gains.\nFor Law Enforcement Agencies (LEAs), it can prove valuable in optimizing\nevidence analysis and establishing proactive prevention measures. Nevertheless,\ncitizens raise legitimate concerns around privacy invasions, biases,\ninequalities, and inaccurate decisions. This study explores the views of 111\ncitizens towards AI use by police through interviews, and integrates societal\nconcerns along with propositions of safeguards from negative effects of AI use\nby LEAs in the context of cybercrime and terrorism.",
                "Traffic Road Congestion System using by the internet of vehicles (IoV)\nTraffic problems have increased in modern life due to a huge number of\nvehicles, big cities, and ignoring the traffic rules. Vehicular ad hoc network\n(VANET) has improved the traffic system in previous some and plays a vital role\nin the best traffic control system in big cities. But due to some limitations,\nit is not enough to control some problems in specific conditions. Now a day\ninvention of new technologies of the Internet of Things (IoT) is used for\ncollaboratively and efficiently performing tasks. This technology was also\nintroduced in the transportation system which makes it an intelligent\ntransportation system (ITS), this is called the Internet of vehicles (IOV). We\nwill elaborate on traffic problems in the traditional system and elaborate on\nthe benefits, enhancements, and reasons to better IOV by Systematic Literature\nReview (SLR). This technique will be implemented by targeting needed papers\nthrough many search phrases. A systematic literature review is used for 121\narticles between 2014 and 2023. The IoV technologies and tools are required to\ncreate the IoV and resolve some traffic rules through SUMO (simulation of urban\nmobility) which is used for the design and simulation the road traffic. We have\ntried to contribute to the best model of the traffic control system. This paper\nwill analysis two vehicular congestion control models in term of select the\noptimized and efficient model and elaborate on the reasons for efficiency by\nsearching the solution SLR based questions. Due to some efficient features, we\nhave suggested the IOV based on vehicular clouds. These efficient features make\nthis model the best and most effective than the traditional model which is a\ngreat reason to enhance the network system.",
                "Responsible Design Patterns for Machine Learning Pipelines\nIntegrating ethical practices into the AI development process for artificial\nintelligence (AI) is essential to ensure safe, fair, and responsible operation.\nAI ethics involves applying ethical principles to the entire life cycle of AI\nsystems. This is essential to mitigate potential risks and harms associated\nwith AI, such as algorithm biases. To achieve this goal, responsible design\npatterns (RDPs) are critical for Machine Learning (ML) pipelines to guarantee\nethical and fair outcomes. In this paper, we propose a comprehensive framework\nincorporating RDPs into ML pipelines to mitigate risks and ensure the ethical\ndevelopment of AI systems. Our framework comprises new responsible AI design\npatterns for ML pipelines identified through a survey of AI ethics and data\nmanagement experts and validated through real-world scenarios with expert\nfeedback. The framework guides AI developers, data scientists, and\npolicy-makers to implement ethical practices in AI development and deploy\nresponsible AI systems in production.",
                "Human or Not? A Gamified Approach to the Turing Test\nWe present \"Human or Not?\", an online game inspired by the Turing test, that\nmeasures the capability of AI chatbots to mimic humans in dialog, and of humans\nto tell bots from other humans. Over the course of a month, the game was played\nby over 1.5 million users who engaged in anonymous two-minute chat sessions\nwith either another human or an AI language model which was prompted to behave\nlike humans. The task of the players was to correctly guess whether they spoke\nto a person or to an AI. This largest scale Turing-style test conducted to date\nrevealed some interesting facts. For example, overall users guessed the\nidentity of their partners correctly in only 68% of the games. In the subset of\nthe games in which users faced an AI bot, users had even lower correct guess\nrates of 60% (that is, not much higher than chance). This white paper details\nthe development, deployment, and results of this unique experiment. While this\nexperiment calls for many extensions and refinements, these findings already\nbegin to shed light on the inevitable near future which will commingle humans\nand AI.",
                "Human Control: Definitions and Algorithms\nHow can humans stay in control of advanced artificial intelligence systems?\nOne proposal is corrigibility, which requires the agent to follow the\ninstructions of a human overseer, without inappropriately influencing them. In\nthis paper, we formally define a variant of corrigibility called shutdown\ninstructability, and show that it implies appropriate shutdown behavior,\nretention of human autonomy, and avoidance of user harm. We also analyse the\nrelated concepts of non-obstruction and shutdown alignment, three previously\nproposed algorithms for human control, and one new algorithm.",
                "From Human-Centered to Social-Centered Artificial Intelligence:\n  Assessing ChatGPT's Impact through Disruptive Events\nLarge language models (LLMs) and dialogue agents have existed for years, but\nthe release of recent GPT models has been a watershed moment for artificial\nintelligence (AI) research and society at large. Immediately recognized for its\ngenerative capabilities and versatility, ChatGPT's impressive proficiency\nacross technical and creative domains led to its widespread adoption. While\nsociety grapples with the emerging cultural impacts of ChatGPT, critiques of\nChatGPT's impact within the machine learning community have coalesced around\nits performance or other conventional Responsible AI evaluations relating to\nbias, toxicity, and 'hallucination.' We argue that these latter critiques draw\nheavily on a particular conceptualization of the 'human-centered' framework,\nwhich tends to cast atomized individuals as the key recipients of both the\nbenefits and detriments of technology. In this article, we direct attention to\nanother dimension of LLMs and dialogue agents' impact: their effect on social\ngroups, institutions, and accompanying norms and practices. By illustrating\nChatGPT's social impact through three disruptive events, we challenge\nindividualistic approaches in AI development and contribute to ongoing debates\naround the ethical and responsible implementation of AI systems. We hope this\neffort will call attention to more comprehensive and longitudinal evaluation\ntools and compel technologists to go beyond human-centered thinking and ground\ntheir efforts through social-centered AI."
            ],
            "interesting paper": 5
        },
        {
            "papers": [
                "Decision-Oriented Dialogue for Human-AI Collaboration\nWe describe a class of tasks called decision-oriented dialogues, in which AI\nassistants such as large language models (LMs) must collaborate with one or\nmore humans via natural language to help them make complex decisions. We\nformalize three domains in which users face everyday decisions: (1) choosing an\nassignment of reviewers to conference papers, (2) planning a multi-step\nitinerary in a city, and (3) negotiating travel plans for a group of friends.\nIn each of these settings, AI assistants and users have disparate abilities\nthat they must combine to arrive at the best decision: assistants can access\nand process large amounts of information, while users have preferences and\nconstraints external to the system. For each task, we build a dialogue\nenvironment where agents receive a reward based on the quality of the final\ndecision they reach. We evaluate LMs in self-play and in collaboration with\nhumans and find that they fall short compared to human assistants, achieving\nmuch lower rewards despite engaging in longer dialogues. We highlight a number\nof challenges models face in decision-oriented dialogues, ranging from\ngoal-directed behavior to reasoning and optimization, and release our\nenvironments as a testbed for future work.",
                "Speaking the Language of Your Listener: Audience-Aware Adaptation via\n  Plug-and-Play Theory of Mind\nDialogue participants may have varying levels of knowledge about the topic\nunder discussion. In such cases, it is essential for speakers to adapt their\nutterances by taking their audience into account. Yet, it is an open question\nhow such adaptation can be modelled in computational agents. In this paper, we\nmodel a visually grounded referential game between a knowledgeable speaker and\na listener with more limited visual and linguistic experience. Inspired by\npsycholinguistic theories, we endow our speaker with the ability to adapt its\nreferring expressions via a simulation module that monitors the effectiveness\nof planned utterances from the listener's perspective. We propose an adaptation\nmechanism building on plug-and-play approaches to controlled language\ngeneration, where utterance generation is steered on the fly by the simulator\nwithout finetuning the speaker's underlying language model. Our results and\nanalyses show that our approach is effective: the speaker's utterances become\ncloser to the listener's domain of expertise, which leads to higher\ncommunicative success.",
                "TransAct: Transformer-based Realtime User Action Model for\n  Recommendation at Pinterest\nSequential models that encode user activity for next action prediction have\nbecome a popular design choice for building web-scale personalized\nrecommendation systems. Traditional methods of sequential recommendation either\nutilize end-to-end learning on realtime user actions, or learn user\nrepresentations separately in an offline batch-generated manner. This paper (1)\npresents Pinterest's ranking architecture for Homefeed, our personalized\nrecommendation product and the largest engagement surface; (2) proposes\nTransAct, a sequential model that extracts users' short-term preferences from\ntheir realtime activities; (3) describes our hybrid approach to ranking, which\ncombines end-to-end sequential modeling via TransAct with batch-generated user\nembeddings. The hybrid approach allows us to combine the advantages of\nresponsiveness from learning directly on realtime user activity with the\ncost-effectiveness of batch user representations learned over a longer time\nperiod. We describe the results of ablation studies, the challenges we faced\nduring productionization, and the outcome of an online A/B experiment, which\nvalidates the effectiveness of our hybrid ranking model. We further demonstrate\nthe effectiveness of TransAct on other surfaces such as contextual\nrecommendations and search. Our model has been deployed to production in\nHomefeed, Related Pins, Notifications, and Search at Pinterest.",
                "AI Imagery and the Overton Window\nAI-based text-to-image generation has undergone a significant leap in the\nproduction of visually comprehensive and aesthetic imagery over the past year,\nto the point where differentiating between a man-made piece of art and an\nAI-generated image is becoming more difficult. Generative Models such as Stable\nDiffusion, Midjourney and others are expected to affect several major\nindustries in technological and ethical aspects. Striking the balance between\nraising human standard of life and work vs exploiting one group of people to\nenrich another is a complex and crucial part of the discussion. Due to the\nrapid growth of this technology, the way in which its models operate, and gray\narea legalities, visual and artistic domains - including the video game\nindustry, are at risk of being taken over from creators by AI infrastructure\nowners. This paper is a literature review examining the concerns facing both AI\ndevelopers and users today, including identity theft, data laundering and more.\nIt discusses legalization challenges and ethical concerns, and concludes with\nhow AI generative models can be tremendously useful in streamlining the process\nof visual creativity in both static and interactive media given proper\nregulation.\n  Keywords: AI text-to-image generation, Midjourney, Stable Diffusion, AI\nEthics, Game Design, Digital Art, Data Laundering",
                "Human Control: Definitions and Algorithms\nHow can humans stay in control of advanced artificial intelligence systems?\nOne proposal is corrigibility, which requires the agent to follow the\ninstructions of a human overseer, without inappropriately influencing them. In\nthis paper, we formally define a variant of corrigibility called shutdown\ninstructability, and show that it implies appropriate shutdown behavior,\nretention of human autonomy, and avoidance of user harm. We also analyse the\nrelated concepts of non-obstruction and shutdown alignment, three previously\nproposed algorithms for human control, and one new algorithm.",
                "Enrichment of the NLST and NSCLC-Radiomics computed tomography\n  collections with AI-derived annotations\nPublic imaging datasets are critical for the development and evaluation of\nautomated tools in cancer imaging. Unfortunately, many do not include\nannotations or image-derived features, complicating their downstream analysis.\nArtificial intelligence-based annotation tools have been shown to achieve\nacceptable performance and thus can be used to automatically annotate large\ndatasets. As part of the effort to enrich public data available within NCI\nImaging Data Commons (IDC), here we introduce AI-generated annotations for two\ncollections of computed tomography images of the chest, NSCLC-Radiomics, and\nthe National Lung Screening Trial. Using publicly available AI algorithms we\nderived volumetric annotations of thoracic organs at risk, their corresponding\nradiomics features, and slice-level annotations of anatomical landmarks and\nregions. The resulting annotations are publicly available within IDC, where the\nDICOM format is used to harmonize the data and achieve FAIR principles. The\nannotations are accompanied by cloud-enabled notebooks demonstrating their use.\nThis study reinforces the need for large, publicly accessible curated datasets\nand demonstrates how AI can be used to aid in cancer imaging."
            ],
            "interesting paper": 3
        },
        {
            "papers": [
                "AI Liability Insurance With an Example in AI-Powered E-diagnosis System\nArtificial Intelligence (AI) has received an increasing amount of attention\nin multiple areas. The uncertainties and risks in AI-powered systems have\ncreated reluctance in their wild adoption. As an economic solution to\ncompensate for potential damages, AI liability insurance is a promising market\nto enhance the integration of AI into daily life. In this work, we use an\nAI-powered E-diagnosis system as an example to study AI liability insurance. We\nprovide a quantitative risk assessment model with evidence-based numerical\nanalysis. We discuss the insurability criteria for AI technologies and suggest\nnecessary adjustments to accommodate the features of AI products. We show that\nAI liability insurance can act as a regulatory mechanism to incentivize\ncompliant behaviors and serve as a certificate of high-quality AI systems.\nFurthermore, we suggest premium adjustment to reflect the dynamic evolution of\nthe inherent uncertainty in AI. Moral hazard problems are discussed and\nsuggestions for AI liability insurance are provided.",
                "AI and the creative realm: A short review of current and future\n  applications\nThis study explores the concept of creativity and artificial intelligence\n(AI) and their recent integration. While AI has traditionally been perceived as\nincapable of generating new ideas or creating art, the development of more\nsophisticated AI models and the proliferation of human-computer interaction\ntools have opened up new possibilities for AI in artistic creation. This study\ninvestigates the various applications of AI in a creative context,\ndifferentiating between the type of art, language, and algorithms used. It also\nconsiders the philosophical implications of AI and creativity, questioning\nwhether consciousness can be researched in machines and AI's potential\ninterests and decision-making capabilities. Overall, we aim to stimulate a\nreflection on AI's use and ethical implications in creative contexts.",
                "Recent Advances in Graph-based Machine Learning for Applications in\n  Smart Urban Transportation Systems\nThe Intelligent Transportation System (ITS) is an important part of modern\ntransportation infrastructure, employing a combination of communication\ntechnology, information processing and control systems to manage transportation\nnetworks. This integration of various components such as roads, vehicles, and\ncommunication systems, is expected to improve efficiency and safety by\nproviding better information, services, and coordination of transportation\nmodes. In recent years, graph-based machine learning has become an increasingly\nimportant research focus in the field of ITS aiming at the development of\ncomplex, data-driven solutions to address various ITS-related challenges. This\nchapter presents background information on the key technical challenges for ITS\ndesign, along with a review of research methods ranging from classic\nstatistical approaches to modern machine learning and deep learning-based\napproaches. Specifically, we provide an in-depth review of graph-based machine\nlearning methods, including basic concepts of graphs, graph data\nrepresentation, graph neural network architectures and their relation to ITS\napplications. Additionally, two case studies of graph-based ITS applications\nproposed in our recent work are presented in detail to demonstrate the\npotential of graph-based machine learning in the ITS domain.",
                "An Overview on Generative AI at Scale with Edge-Cloud Computing\nAs a specific category of artificial intelligence (AI), generative artificial\nintelligence (GenAI) generates new content that resembles what is created by\nhumans. The rapid development of GenAI systems has created a huge amount of new\ndata on the Internet, posing new challenges to current computing and\ncommunication frameworks. Currently, GenAI services rely on the traditional\ncloud computing framework due to the need for large computation resources.\nHowever, such services will encounter high latency because of data transmission\nand a high volume of requests. On the other hand, edge-cloud computing can\nprovide adequate computation power and low latency at the same time through the\ncollaboration between edges and the cloud. Thus, it is attractive to build\nGenAI systems at scale by leveraging the edge-cloud computing paradigm. In this\noverview paper, we review recent developments in GenAI and edge-cloud\ncomputing, respectively. Then, we use two exemplary GenAI applications to\ndiscuss technical challenges in scaling up their solutions using edge-cloud\ncollaborative systems. Finally, we list design considerations for training and\ndeploying GenAI systems at scale and point out future research directions.",
                "The ethical ambiguity of AI data enrichment: Measuring gaps in research\n  ethics norms and practices\nThe technical progression of artificial intelligence (AI) research has been\nbuilt on breakthroughs in fields such as computer science, statistics, and\nmathematics. However, in the past decade AI researchers have increasingly\nlooked to the social sciences, turning to human interactions to solve the\nchallenges of model development. Paying crowdsourcing workers to generate or\ncurate data, or data enrichment, has become indispensable for many areas of AI\nresearch, from natural language processing to reinforcement learning from human\nfeedback (RLHF). Other fields that routinely interact with crowdsourcing\nworkers, such as Psychology, have developed common governance requirements and\nnorms to ensure research is undertaken ethically. This study explores how, and\nto what extent, comparable research ethics requirements and norms have\ndeveloped for AI research and data enrichment. We focus on the approach taken\nby two leading conferences: ICLR and NeurIPS, and journal publisher Springer.\nIn a longitudinal study of accepted papers, and via a comparison with\nPsychology and CHI papers, this work finds that leading AI venues have begun to\nestablish protocols for human data collection, but these are are inconsistently\nfollowed by authors. Whilst Psychology papers engaging with crowdsourcing\nworkers frequently disclose ethics reviews, payment data, demographic data and\nother information, similar disclosures are far less common in leading AI venues\ndespite similar guidance. The work concludes with hypotheses to explain these\ngaps in research ethics practices and considerations for its implications.",
                "Integrated Sensing-Communication-Computation for Edge Artificial\n  Intelligence\nEdge artificial intelligence (AI) has been a promising solution towards 6G to\nempower a series of advanced techniques such as digital twins, holographic\nprojection, semantic communications, and auto-driving, for achieving\nintelligence of everything. The performance of edge AI tasks, including edge\nlearning and edge AI inference, depends on the quality of three highly coupled\nprocesses, i.e., sensing for data acquisition, computation for information\nextraction, and communication for information transmission. However, these\nthree modules need to compete for network resources for enhancing their own\nquality-of-services. To this end, integrated sensing-communication-computation\n(ISCC) is of paramount significance for improving resource utilization as well\nas achieving the customized goals of edge AI tasks. By investigating the\ninterplay among the three modules, this article presents various kinds of ISCC\nschemes for federated edge learning tasks and edge AI inference tasks in both\napplication and physical layers."
            ],
            "interesting paper": 3
        },
        {
            "papers": [
                "AI Liability Insurance With an Example in AI-Powered E-diagnosis System\nArtificial Intelligence (AI) has received an increasing amount of attention\nin multiple areas. The uncertainties and risks in AI-powered systems have\ncreated reluctance in their wild adoption. As an economic solution to\ncompensate for potential damages, AI liability insurance is a promising market\nto enhance the integration of AI into daily life. In this work, we use an\nAI-powered E-diagnosis system as an example to study AI liability insurance. We\nprovide a quantitative risk assessment model with evidence-based numerical\nanalysis. We discuss the insurability criteria for AI technologies and suggest\nnecessary adjustments to accommodate the features of AI products. We show that\nAI liability insurance can act as a regulatory mechanism to incentivize\ncompliant behaviors and serve as a certificate of high-quality AI systems.\nFurthermore, we suggest premium adjustment to reflect the dynamic evolution of\nthe inherent uncertainty in AI. Moral hazard problems are discussed and\nsuggestions for AI liability insurance are provided.",
                "Self Contrastive Learning for Session-based Recommendation\nSession-based recommendation, which aims to predict the next item of users'\ninterest as per an existing sequence interaction of items, has attracted\ngrowing applications of Contrastive Learning (CL) with improved user and item\nrepresentations. However, these contrastive objectives: (1) serve a similar\nrole as the cross-entropy loss while ignoring the item representation space\noptimisation; and (2) commonly require complicated modelling, including complex\npositive/negative sample constructions and extra data augmentation. In this\nwork, we introduce Self-Contrastive Learning (SCL), which simplifies the\napplication of CL and enhances the performance of state-of-the-art CL-based\nrecommendation techniques. Specifically, SCL is formulated as an objective\nfunction that directly promotes a uniform distribution among item\nrepresentations and efficiently replaces all the existing contrastive objective\ncomponents of state-of-the-art models. Unlike previous works, SCL eliminates\nthe need for any positive/negative sample construction or data augmentation,\nleading to enhanced interpretability of the item representation space and\nfacilitating its extensibility to existing recommender systems. Through\nexperiments on three benchmark datasets, we demonstrate that SCL consistently\nimproves the performance of state-of-the-art models with statistical\nsignificance. Notably, our experiments show that SCL improves the performance\nof two best-performing models by 8.2% and 9.5% in P@10 (Precision) and 9.9% and\n11.2% in MRR@10 (Mean Reciprocal Rank) on average across different benchmarks.\nAdditionally, our analysis elucidates the improvement in terms of alignment and\nuniformity of representations, as well as the effectiveness of SCL with a low\ncomputational cost.",
                "Cook-Gen: Robust Generative Modeling of Cooking Actions from Recipes\nAs people become more aware of their food choices, food computation models\nhave become increasingly popular in assisting people in maintaining healthy\neating habits. For example, food recommendation systems analyze recipe\ninstructions to assess nutritional contents and provide recipe recommendations.\nThe recent and remarkable successes of generative AI methods, such as\nauto-regressive large language models, can lead to robust methods for a more\ncomprehensive understanding of recipes for healthy food recommendations beyond\nsurface-level nutrition content assessments. In this study, we explore the use\nof generative AI methods to extend current food computation models, primarily\ninvolving the analysis of nutrition and ingredients, to also incorporate\ncooking actions (e.g., add salt, fry the meat, boil the vegetables, etc.).\nCooking actions are notoriously hard to model using statistical learning\nmethods due to irregular data patterns - significantly varying natural language\ndescriptions for the same action (e.g., marinate the meat vs. marinate the meat\nand leave overnight) and infrequently occurring patterns (e.g., add salt occurs\nfar more frequently than marinating the meat). The prototypical approach to\nhandling irregular data patterns is to increase the volume of data that the\nmodel ingests by orders of magnitude. Unfortunately, in the cooking domain,\nthese problems are further compounded with larger data volumes presenting a\nunique challenge that is not easily handled by simply scaling up. In this work,\nwe propose novel aggregation-based generative AI methods, Cook-Gen, that\nreliably generate cooking actions from recipes, despite difficulties with\nirregular data patterns, while also outperforming Large Language Models and\nother strong baselines.",
                "ViCo: Plug-and-play Visual Condition for Personalized Text-to-image\n  Generation\nPersonalized text-to-image generation using diffusion models has recently\nemerged and garnered significant interest. This task learns a novel concept\n(e.g., a unique toy), illustrated in a handful of images, into a generative\nmodel that captures fine visual details and generates photorealistic images\nbased on textual embeddings. In this paper, we present ViCo, a novel\nlightweight plug-and-play method that seamlessly integrates visual condition\ninto personalized text-to-image generation. ViCo stands out for its unique\nfeature of not requiring any fine-tuning of the original diffusion model\nparameters, thereby facilitating more flexible and scalable model deployment.\nThis key advantage distinguishes ViCo from most existing models that\nnecessitate partial or full diffusion fine-tuning. ViCo incorporates an image\nattention module that conditions the diffusion process on patch-wise visual\nsemantics, and an attention-based object mask that comes at no extra cost from\nthe attention module. Despite only requiring light parameter training (~6%\ncompared to the diffusion U-Net), ViCo delivers performance that is on par\nwith, or even surpasses, all state-of-the-art models, both qualitatively and\nquantitatively. This underscores the efficacy of ViCo, making it a highly\npromising solution for personalized text-to-image generation without the need\nfor diffusion model fine-tuning. Code: https://github.com/haoosz/ViCo",
                "KL-Divergence Guided Temperature Sampling\nTemperature sampling is a conventional approach to diversify large language\nmodel predictions. As temperature increases, the prediction becomes diverse but\nalso vulnerable to hallucinations -- generating tokens that are sensible but\nnot factual. One common approach to mitigate hallucinations is to provide\nsource/grounding documents and the model is trained to produce predictions that\nbind to and are attributable to the provided source. It appears that there is a\ntrade-off between diversity and attribution. To mitigate any such trade-off, we\npropose to relax the constraint of having a fixed temperature over decoding\nsteps, and a mechanism to guide the dynamic temperature according to its\nrelevance to the source through KL-divergence. Our experiments justifies the\ntrade-off, and shows that our sampling algorithm outperforms the conventional\ntop-k and top-p algorithms in conversational question-answering and\nsummarization tasks.",
                "Modeling and Analyzing Scorer Preferences in Short-Answer Math Questions\nAutomated scoring of student responses to open-ended questions, including\nshort-answer questions, has great potential to scale to a large number of\nresponses. Recent approaches for automated scoring rely on supervised learning,\ni.e., training classifiers or fine-tuning language models on a small number of\nresponses with human-provided score labels. However, since scoring is a\nsubjective process, these human scores are noisy and can be highly variable,\ndepending on the scorer. In this paper, we investigate a collection of models\nthat account for the individual preferences and tendencies of each human scorer\nin the automated scoring task. We apply these models to a short-answer math\nresponse dataset where each response is scored (often differently) by multiple\ndifferent human scorers. We conduct quantitative experiments to show that our\nscorer models lead to improved automated scoring accuracy. We also conduct\nquantitative experiments and case studies to analyze the individual preferences\nand tendencies of scorers. We found that scorers can be grouped into several\nobvious clusters, with each cluster having distinct features, and analyzed them\nin detail."
            ],
            "interesting paper": 2
        },
        {
            "papers": [
                "Enough With \"Human-AI Collaboration\"\nDescribing our interaction with Artificial Intelligence (AI) systems as\n'collaboration' is well-intentioned, but flawed. Not only is it misleading, but\nit also takes away the credit of AI 'labour' from the humans behind it, and\nerases and obscures an often exploitative arrangement between AI producers and\nconsumers. The AI 'collaboration' metaphor is merely the latest episode in a\nlong history of labour appropriation and credit reassignment that\ndisenfranchises labourers in the Global South. I propose that viewing AI as a\ntool or an instrument, rather than a collaborator, is more accurate, and\nultimately fairer.",
                "Milestones in Autonomous Driving and Intelligent Vehicles Part II:\n  Perception and Planning\nGrowing interest in autonomous driving (AD) and intelligent vehicles (IVs) is\nfueled by their promise for enhanced safety, efficiency, and economic benefits.\nWhile previous surveys have captured progress in this field, a comprehensive\nand forward-looking summary is needed. Our work fills this gap through three\ndistinct articles. The first part, a \"Survey of Surveys\" (SoS), outlines the\nhistory, surveys, ethics, and future directions of AD and IV technologies. The\nsecond part, \"Milestones in Autonomous Driving and Intelligent Vehicles Part I:\nControl, Computing System Design, Communication, HD Map, Testing, and Human\nBehaviors\" delves into the development of control, computing system,\ncommunication, HD map, testing, and human behaviors in IVs. This part, the\nthird part, reviews perception and planning in the context of IVs. Aiming to\nprovide a comprehensive overview of the latest advancements in AD and IVs, this\nwork caters to both newcomers and seasoned researchers. By integrating the SoS\nand Part I, we offer unique insights and strive to serve as a bridge between\npast achievements and future possibilities in this dynamic field.",
                "AI Transparency in the Age of LLMs: A Human-Centered Research Roadmap\nThe rise of powerful large language models (LLMs) brings about tremendous\nopportunities for innovation but also looming risks for individuals and society\nat large. We have reached a pivotal moment for ensuring that LLMs and\nLLM-infused applications are developed and deployed responsibly. However, a\ncentral pillar of responsible AI -- transparency -- is largely missing from the\ncurrent discourse around LLMs. It is paramount to pursue new approaches to\nprovide transparency for LLMs, and years of research at the intersection of AI\nand human-computer interaction (HCI) highlight that we must do so with a\nhuman-centered perspective: Transparency is fundamentally about supporting\nappropriate human understanding, and this understanding is sought by different\nstakeholders with different goals in different contexts. In this new era of\nLLMs, we must develop and design approaches to transparency by considering the\nneeds of stakeholders in the emerging LLM ecosystem, the novel types of\nLLM-infused applications being built, and the new usage patterns and challenges\naround LLMs, all while building on lessons learned about how people process,\ninteract with, and make use of information. We reflect on the unique challenges\nthat arise in providing transparency for LLMs, along with lessons learned from\nHCI and responsible AI research that has taken a human-centered perspective on\nAI transparency. We then lay out four common approaches that the community has\ntaken to achieve transparency -- model reporting, publishing evaluation\nresults, providing explanations, and communicating uncertainty -- and call out\nopen questions around how these approaches may or may not be applied to LLMs.\nWe hope this provides a starting point for discussion and a useful roadmap for\nfuture research.",
                "DataAI-6G: A System Parameters Configurable Channel Dataset for AI-6G\n  Research\nWith the acceleration of the commercialization of fifth generation (5G)\nmobile communication technology and the research for 6G communication systems,\nthe communication system has the characteristics of high frequency, multi-band,\nhigh speed movement of users and large antenna array. These bring many\ndifficulties to obtain accurate channel state information (CSI), which makes\nthe performance of traditional communication methods be greatly restricted.\nTherefore, there has been a lot of interest in using artificial intelligence\n(AI) instead of traditional methods to improve performance. A common and\naccurate dataset is essential for the research of AI communication. However,\nthe common datasets nowadays still lack some important features, such as mobile\nfeatures, spatial non-stationary features etc. To address these issues, we give\na dataset for future 6G communication. In this dataset, we address these issues\nwith specific simulation methods and accompanying code processing.",
                "XAI Renaissance: Redefining Interpretability in Medical Diagnostic\n  Models\nAs machine learning models become increasingly prevalent in medical\ndiagnostics, the need for interpretability and transparency becomes paramount.\nThe XAI Renaissance signifies a significant shift in the field, aiming to\nredefine the interpretability of medical diagnostic models. This paper explores\nthe innovative approaches and methodologies within the realm of Explainable AI\n(XAI) that are revolutionizing the interpretability of medical diagnostic\nmodels. By shedding light on the underlying decision-making process, XAI\ntechniques empower healthcare professionals to understand, trust, and\neffectively utilize these models for accurate and reliable medical diagnoses.\nThis review highlights the key advancements in XAI for medical diagnostics and\ntheir potential to transform the healthcare landscape, ultimately improving\npatient outcomes and fostering trust in AI-driven diagnostic systems.",
                "A Hybrid Approach for Smart Alert Generation\nAnomaly detection is an important task in network management. However,\ndeploying intelligent alert systems in real-world large-scale networking\nsystems is challenging when we take into account (i) scalability, (ii) data\nheterogeneity, and (iii) generalizability and maintainability. In this paper,\nwe propose a hybrid model for an alert system that combines statistical models\nwith a whitelist mechanism to tackle these challenges and reduce false positive\nalerts. The statistical models take advantage of a large database to detect\nanomalies in time-series data, while the whitelist filters out persistently\nalerted nodes to further reduce false positives. Our model is validated using\nqualitative data from customer support cases. Future work includes more feature\nengineering and input data, as well as including human feedback in the model\ndevelopment process."
            ],
            "interesting paper": 2
        }
    ],
    "Morgan Hernandez": [
        {
            "papers": [
                "Asking Before Acting: Gather Information in Embodied Decision Making\n  with Language Models\nWith strong capabilities of reasoning and a broad understanding of the world,\nLarge Language Models (LLMs) have demonstrated immense potential in building\nversatile embodied decision-making agents capable of executing a wide array of\ntasks. Nevertheless, when deployed in unfamiliar environments, we show that LLM\nagents encounter challenges in efficiently gathering essential information,\nleading to suboptimal performance. Conversely, human individuals often seek\nadditional information from their peers prior to taking action, harnessing\nexternal knowledge to avoid unnecessary trial and error. Drawing inspiration\nfrom this behavior, we propose \\textit{Asking Before Acting} (ABA), a method\nthat empowers the agent to proactively inquire with external sources for\npertinent information using natural language during their interactions within\nthe environment. In this way, the agent is able to enhance its efficiency and\nperformance by circumventing potentially laborious steps and combating the\ndifficulties associated with exploration in unfamiliar environments and\nvagueness of the instructions. We conduct extensive experiments involving a\nspectrum of environments including text-based household everyday tasks, robot\narm manipulation tasks, and real world open domain image based embodied tasks.\nThe experiments involve various models from Vicuna to GPT-4. The results\ndemonstrate that, even with modest prompts modifications, ABA exhibits\nsubstantial advantages on both performance and efficiency over baseline LLM\nagents. Further finetuning ABA with reformulated metadata (ABA-FT) faciliates\nlearning the rationale for asking and allows for additional enhancements\nespecially in tasks that baselines struggle to solve.",
                "From Interactive to Co-Constructive Task Learning\nHumans have developed the capability to teach relevant aspects of new or\nadapted tasks to a social peer with very few task demonstrations by making use\nof scaffolding strategies that leverage prior knowledge and importantly prior\njoint experience to yield a joint understanding and a joint execution of the\nrequired steps to solve the task. This process has been discovered and analyzed\nin parent-infant interaction and constitutes a ``co-construction'' as it allows\nboth, the teacher and the learner, to jointly contribute to the task. We\npropose to focus research in robot interactive learning on this co-construction\nprocess to enable robots to learn from non-expert users in everyday situations.\nIn the following, we will review current proposals for interactive task\nlearning and discuss their main contributions with respect to the entailing\ninteraction. We then discuss our notion of co-construction and summarize\nresearch insights from adult-child and human-robot interactions to elucidate\nits nature in more detail. From this overview we finally derive research\ndesiderata that entail the dimensions architecture, representation, interaction\nand explainability.",
                "Role-Play with Large Language Models\nAs dialogue agents become increasingly human-like in their performance, it is\nimperative that we develop effective ways to describe their behaviour in\nhigh-level terms without falling into the trap of anthropomorphism. In this\npaper, we foreground the concept of role-play. Casting dialogue agent behaviour\nin terms of role-play allows us to draw on familiar folk psychological terms,\nwithout ascribing human characteristics to language models they in fact lack.\nTwo important cases of dialogue agent behaviour are addressed this way, namely\n(apparent) deception and (apparent) self-awareness.",
                "An Experimental Investigation into the Evaluation of Explainability\n  Methods\nEXplainable Artificial Intelligence (XAI) aims to help users to grasp the\nreasoning behind the predictions of an Artificial Intelligence (AI) system.\nMany XAI approaches have emerged in recent years. Consequently, a subfield\nrelated to the evaluation of XAI methods has gained considerable attention,\nwith the aim to determine which methods provide the best explanation using\nvarious approaches and criteria. However, the literature lacks a comparison of\nthe evaluation metrics themselves, that one can use to evaluate XAI methods.\nThis work aims to fill this gap by comparing 14 different metrics when applied\nto nine state-of-the-art XAI methods and three dummy methods (e.g., random\nsaliency maps) used as references. Experimental results show which of these\nmetrics produces highly correlated results, indicating potential redundancy. We\nalso demonstrate the significant impact of varying the baseline hyperparameter\non the evaluation metric values. Finally, we use dummy methods to assess the\nreliability of metrics in terms of ranking, pointing out their limitations.",
                "HuatuoGPT, towards Taming Language Model to Be a Doctor\nIn this paper, we present HuatuoGPT, a large language model (LLM) for medical\nconsultation. The core recipe of HuatuoGPT is to leverage both\n\\textit{distilled data from ChatGPT} and \\textit{real-world data from doctors}\nin the supervised fine-tuned stage. The responses of ChatGPT are usually\ndetailed, well-presented and informative while it cannot perform like a doctor\nin many aspects, e.g. for integrative diagnosis. We argue that real-world data\nfrom doctors would be complementary to distilled data in the sense the former\ncould tame a distilled language model to perform like doctors. To better\nleverage the strengths of both data, we train a reward model to align the\nlanguage model with the merits that both data bring, following an RLAIF\n(reinforced learning from AI feedback) fashion. To evaluate and benchmark the\nmodels, we propose a comprehensive evaluation scheme (including automatic and\nmanual metrics). Experimental results demonstrate that HuatuoGPT achieves\nstate-of-the-art results in performing medical consultation among open-source\nLLMs in GPT-4 evaluation, human evaluation, and medical benchmark datasets. It\nis worth noting that by using additional real-world data and RLAIF, the\ndistilled language model (i.e., HuatuoGPT) outperforms its teacher model\nChatGPT in most cases. Our code, data, and models are publicly available at\n\\url{https://github.com/FreedomIntelligence/HuatuoGPT}. The online demo is\navailable at \\url{https://www.HuatuoGPT.cn/}.",
                "MPE4G: Multimodal Pretrained Encoder for Co-Speech Gesture Generation\nWhen virtual agents interact with humans, gestures are crucial to delivering\ntheir intentions with speech. Previous multimodal co-speech gesture generation\nmodels required encoded features of all modalities to generate gestures. If\nsome input modalities are removed or contain noise, the model may not generate\nthe gestures properly. To acquire robust and generalized encodings, we propose\na novel framework with a multimodal pre-trained encoder for co-speech gesture\ngeneration. In the proposed method, the multi-head-attention-based encoder is\ntrained with self-supervised learning to contain the information on each\nmodality. Moreover, we collect full-body gestures that consist of 3D joint\nrotations to improve visualization and apply gestures to the extensible body\nmodel. Through the series of experiments and human evaluation, the proposed\nmethod renders realistic co-speech gestures not only when all input modalities\nare given but also when the input modalities are missing or noisy."
            ],
            "interesting paper": 1
        },
        {
            "papers": [
                "Alert of the Second Decision-maker: An Introduction to Human-AI Conflict\nThe collaboration between humans and artificial intelligence (AI) is a\nsignificant feature in this digital age. However, humans and AI may have\nobservation, interpretation, and action conflicts when working synchronously.\nThis phenomenon is often masked by faults and, unfortunately, overlooked. This\npaper systematically introduces the human-AI conflict concept, causes,\nmeasurement methods, and risk assessment. The results highlight that there is a\npotential second decision-maker besides the human, which is the AI; the\nhuman-AI conflict is a unique and emerging risk in digitalized process systems;\nand this is an interdisciplinary field that needs to be distinguished from\ntraditional fault and failure analysis; the conflict risk is significant and\ncannot be ignored.",
                "Enhancing Human Capabilities through Symbiotic Artificial Intelligence\n  with Shared Sensory Experiences\nThe merging of human intelligence and artificial intelligence has long been a\nsubject of interest in both science fiction and academia. In this paper, we\nintroduce a novel concept in Human-AI interaction called Symbiotic Artificial\nIntelligence with Shared Sensory Experiences (SAISSE), which aims to establish\na mutually beneficial relationship between AI systems and human users through\nshared sensory experiences. By integrating multiple sensory input channels and\nprocessing human experiences, SAISSE fosters a strong human-AI bond, enabling\nAI systems to learn from and adapt to individual users, providing personalized\nsupport, assistance, and enhancement. Furthermore, we discuss the incorporation\nof memory storage units for long-term growth and development of both the AI\nsystem and its human user. As we address user privacy and ethical guidelines\nfor responsible AI-human symbiosis, we also explore potential biases and\ninequalities in AI-human symbiosis and propose strategies to mitigate these\nchallenges. Our research aims to provide a comprehensive understanding of the\nSAISSE concept and its potential to effectively support and enhance individual\nhuman users through symbiotic AI systems. This position article aims at\ndiscussing poteintial AI-human interaction related topics within the scientific\ncommunity, rather than providing experimental or theoretical results.",
                "A Hierarchical Approach to Population Training for Human-AI\n  Collaboration\nA major challenge for deep reinforcement learning (DRL) agents is to\ncollaborate with novel partners that were not encountered by them during the\ntraining phase. This is specifically worsened by an increased variance in\naction responses when the DRL agents collaborate with human partners due to the\nlack of consistency in human behaviors. Recent work have shown that training a\nsingle agent as the best response to a diverse population of training partners\nsignificantly increases an agent's robustness to novel partners. We further\nenhance the population-based training approach by introducing a Hierarchical\nReinforcement Learning (HRL) based method for Human-AI Collaboration. Our agent\nis able to learn multiple best-response policies as its low-level policy while\nat the same time, it learns a high-level policy that acts as a manager which\nallows the agent to dynamically switch between the low-level best-response\npolicies based on its current partner. We demonstrate that our method is able\nto dynamically adapt to novel partners of different play styles and skill\nlevels in the 2-player collaborative Overcooked game environment. We also\nconducted a human study in the same environment to test the effectiveness of\nour method when partnering with real human subjects.",
                "Passive learning of active causal strategies in agents and language\n  models\nWhat can be learned about causality and experimentation from passive data?\nThis question is salient given recent successes of passively-trained language\nmodels in interactive domains such as tool use. Passive learning is inherently\nlimited. However, we show that purely passive learning can in fact allow an\nagent to learn generalizable strategies for determining and using causal\nstructures, as long as the agent can intervene at test time. We formally\nillustrate that learning a strategy of first experimenting, then seeking goals,\ncan allow generalization from passive learning in principle. We then show\nempirically that agents trained via imitation on expert data can indeed\ngeneralize at test time to infer and use causal links which are never present\nin the training data; these agents can also generalize experimentation\nstrategies to novel variable sets never observed in training. We then show that\nstrategies for causal intervention and exploitation can be generalized from\npassive data even in a more complex environment with high-dimensional\nobservations, with the support of natural language explanations. Explanations\ncan even allow passive learners to generalize out-of-distribution from\nperfectly-confounded training data. Finally, we show that language models,\ntrained only on passive next-word prediction, can generalize causal\nintervention strategies from a few-shot prompt containing examples of\nexperimentation, together with explanations and reasoning. These results\nhighlight the surprising power of passive learning of active causal strategies,\nand may help to understand the behaviors and capabilities of language models.",
                "Ghost in the Minecraft: Generally Capable Agents for Open-World\n  Environments via Large Language Models with Text-based Knowledge and Memory\nThe captivating realm of Minecraft has attracted substantial research\ninterest in recent years, serving as a rich platform for developing intelligent\nagents capable of functioning in open-world environments. However, the current\nresearch landscape predominantly focuses on specific objectives, such as the\npopular \"ObtainDiamond\" task, and has not yet shown effective generalization to\na broader spectrum of tasks. Furthermore, the current leading success rate for\nthe \"ObtainDiamond\" task stands at around 20%, highlighting the limitations of\nReinforcement Learning (RL) based controllers used in existing methods. To\ntackle these challenges, we introduce Ghost in the Minecraft (GITM), a novel\nframework integrates Large Language Models (LLMs) with text-based knowledge and\nmemory, aiming to create Generally Capable Agents (GCAs) in Minecraft. These\nagents, equipped with the logic and common sense capabilities of LLMs, can\nskillfully navigate complex, sparse-reward environments with text-based\ninteractions. We develop a set of structured actions and leverage LLMs to\ngenerate action plans for the agents to execute. The resulting LLM-based agent\nmarkedly surpasses previous methods, achieving a remarkable improvement of\n+47.5% in success rate on the \"ObtainDiamond\" task, demonstrating superior\nrobustness compared to traditional RL-based controllers. Notably, our agent is\nthe first to procure all items in the Minecraft Overworld technology tree,\ndemonstrating its extensive capabilities. GITM does not need any GPU for\ntraining, but a single CPU node with 32 CPU cores is enough. This research\nshows the potential of LLMs in developing capable agents for handling\nlong-horizon, complex tasks and adapting to uncertainties in open-world\nenvironments. See the project website at https://github.com/OpenGVLab/GITM.",
                "Combining Gamification and Intelligent Tutoring Systems in a Serious\n  Game for Engineering Education\nWe provide ongoing results from the development of a personalized learning\nsystem integrated into a serious game. Given limited instructor resources, the\nuse of computerized systems to help tutor students offers a way to provide\nhigher quality education and to improve educational efficacy. Personalized\nlearning systems like the one proposed in this paper offer an accessible\nsolution. Furthermore, by combining such a system with a serious game, students\nare further engaged in interacting with the system. The proposed learning\nsystem combines expert-driven structure and lesson planning with computational\nintelligence methods and gamification to provide students with a fun and\neducational experience. As the project is ongoing from past years, numerous\ndesign iterations have been made on the system based on feedback from students\nand classroom observations. Using computational intelligence, the system\nadaptively provides support to students based on data collected from both their\nin-game actions and by estimating their emotional state from webcam images. For\nour evaluation, we focus on student data gathered from in-classroom testing in\nrelevant courses, with both educational efficacy, results and student\nobservations. To demonstrate the effect of our proposed system, students in an\nearly electrical engineering course were instructed to interact with the system\nin place of a standard lab assignment. The system would then measure and help\nthem improve their background knowledge before allowing them to complete the\nlab assignment. As they played through the game, we observed their interactions\nwith the system to gather insights for future work. Additionally, we\ndemonstrate the system's educational efficacy through pre-post-test results\nfrom students who played the game with and without the personalized learning\nsystem."
            ],
            "interesting paper": 2
        },
        {
            "papers": [
                "Mindstorms in Natural Language-Based Societies of Mind\nBoth Minsky's \"society of mind\" and Schmidhuber's \"learning to think\" inspire\ndiverse societies of large multimodal neural networks (NNs) that solve problems\nby interviewing each other in a \"mindstorm.\" Recent implementations of NN-based\nsocieties of minds consist of large language models (LLMs) and other NN-based\nexperts communicating through a natural language interface. In doing so, they\novercome the limitations of single LLMs, improving multimodal zero-shot\nreasoning. In these natural language-based societies of mind (NLSOMs), new\nagents -- all communicating through the same universal symbolic language -- are\neasily added in a modular fashion. To demonstrate the power of NLSOMs, we\nassemble and experiment with several of them (having up to 129 members),\nleveraging mindstorms in them to solve some practical AI tasks: visual question\nanswering, image captioning, text-to-image synthesis, 3D generation, egocentric\nretrieval, embodied AI, and general language-based task solving. We view this\nas a starting point towards much larger NLSOMs with billions of agents-some of\nwhich may be humans. And with this emergence of great societies of\nheterogeneous minds, many new research questions have suddenly become paramount\nto the future of artificial intelligence. What should be the social structure\nof an NLSOM? What would be the (dis)advantages of having a monarchical rather\nthan a democratic structure? How can principles of NN economies be used to\nmaximize the total reward of a reinforcement learning NLSOM? In this work, we\nidentify, discuss, and try to answer some of these questions.",
                "How Do UX Practitioners Communicate AI as a Design Material? Artifacts,\n  Conceptions, and Propositions\nUX practitioners (UXPs) face novel challenges when working with and\ncommunicating artificial intelligence (AI) as a design material. We explore how\nUXPs communicate AI concepts when given hands-on experience training and\nexperimenting with AI models. To do so, we conducted a task-based design study\nwith 27 UXPs in which they prototyped and created a design presentation for a\nAI-enabled interface while having access to a simple AI model training tool.\nThrough analyzing UXPs' design presentations and post-activity interviews, we\nfound that although UXPs struggled to clearly communicate some AI concepts,\ntinkering with AI broadened common ground when communicating with technical\nstakeholders. UXPs also identified key risks and benefits of AI in their\ndesigns, and proposed concrete next steps for both UX and AI work. We conclude\nwith a sensitizing concept and recommendations for design and AI tools to\nenhance multi-stakeholder communication and collaboration when crafting\nhuman-centered AI experiences.",
                "SwiftSage: A Generative Agent with Fast and Slow Thinking for Complex\n  Interactive Tasks\nWe introduce SwiftSage, a novel agent framework inspired by the dual-process\ntheory of human cognition, designed to excel in action planning for complex\ninteractive reasoning tasks. SwiftSage integrates the strengths of behavior\ncloning and prompting large language models (LLMs) to enhance task completion\nperformance. The framework comprises two primary modules: the Swift module,\nrepresenting fast and intuitive thinking, and the Sage module, emulating\ndeliberate thought processes. The Swift module is a small encoder-decoder LM\nfine-tuned on the oracle agent's action trajectories, while the Sage module\nemploys LLMs such as GPT-4 for subgoal planning and grounding. We develop a\nheuristic method to harmoniously integrate the two modules, resulting in a more\nefficient and robust problem-solving process. In 30 tasks from the ScienceWorld\nbenchmark, SwiftSage significantly outperforms other methods such as SayCan,\nReAct, and Reflexion, demonstrating its effectiveness in solving complex\ninteractive tasks.",
                "Acting as Inverse Inverse Planning\nGreat storytellers know how to take us on a journey. They direct characters\nto act -- not necessarily in the most rational way -- but rather in a way that\nleads to interesting situations, and ultimately creates an impactful experience\nfor audience members looking on.\n  If audience experience is what matters most, then can we help artists and\nanimators *directly* craft such experiences, independent of the concrete\ncharacter actions needed to evoke those experiences? In this paper, we offer a\nnovel computational framework for such tools. Our key idea is to optimize\nanimations with respect to *simulated* audience members' experiences. To\nsimulate the audience, we borrow an established principle from cognitive\nscience: that human social intuition can be modeled as \"inverse planning,\" the\ntask of inferring an agent's (hidden) goals from its (observed) actions.\nBuilding on this model, we treat storytelling as \"*inverse* inverse planning,\"\nthe task of choosing actions to manipulate an inverse planner's inferences. Our\nframework is grounded in literary theory, naturally capturing many storytelling\nelements from first principles. We give a series of examples to demonstrate\nthis, with supporting evidence from human subject studies.",
                "Attention Schema in Neural Agents\nAttention has become a common ingredient in deep learning architectures. It\nadds a dynamical selection of information on top of the static selection of\ninformation supported by weights. In the same way, we can imagine a\nhigher-order informational filter built on top of attention: an Attention\nSchema (AS), namely, a descriptive and predictive model of attention. In\ncognitive neuroscience, Attention Schema Theory (AST) supports this idea of\ndistinguishing attention from AS. A strong prediction of this theory is that an\nagent can use its own AS to also infer the states of other agents' attention\nand consequently enhance coordination with other agents. As such, multi-agent\nreinforcement learning would be an ideal setting to experimentally test the\nvalidity of AST. We explore different ways in which attention and AS interact\nwith each other. Our preliminary results indicate that agents that implement\nthe AS as a recurrent internal control achieve the best performance. In\ngeneral, these exploratory experiments suggest that equipping artificial agents\nwith a model of attention can enhance their social intelligence.",
                "Towards Cognitive Bots: Architectural Research Challenges\nSoftware bots operating in multiple virtual digital platforms must understand\nthe platforms' affordances and behave like human users. Platform affordances or\nfeatures differ from one application platform to another or through a life\ncycle, requiring such bots to be adaptable. Moreover, bots in such platforms\ncould cooperate with humans or other software agents for work or to learn\nspecific behavior patterns. However, present-day bots, particularly chatbots,\nother than language processing and prediction, are far from reaching a human\nuser's behavior level within complex business information systems. They lack\nthe cognitive capabilities to sense and act in such virtual environments,\nrendering their development a challenge to artificial general intelligence\nresearch. In this study, we problematize and investigate assumptions in\nconceptualizing software bot architecture by directing attention to significant\narchitectural research challenges in developing cognitive bots endowed with\ncomplex behavior for operation on information systems. As an outlook, we\npropose alternate architectural assumptions to consider in future bot design\nand bot development frameworks."
            ],
            "interesting paper": 1
        },
        {
            "papers": [
                "Integrating Action Knowledge and LLMs for Task Planning and Situation\n  Handling in Open Worlds\nTask planning systems have been developed to help robots use human knowledge\n(about actions) to complete long-horizon tasks. Most of them have been\ndeveloped for \"closed worlds\" while assuming the robot is provided with\ncomplete world knowledge. However, the real world is generally open, and the\nrobots frequently encounter unforeseen situations that can potentially break\nthe planner's completeness. Could we leverage the recent advances on\npre-trained Large Language Models (LLMs) to enable classical planning systems\nto deal with novel situations?\n  This paper introduces a novel framework, called COWP, for open-world task\nplanning and situation handling. COWP dynamically augments the robot's action\nknowledge, including the preconditions and effects of actions, with\ntask-oriented commonsense knowledge. COWP embraces the openness from LLMs, and\nis grounded to specific domains via action knowledge. For systematic\nevaluations, we collected a dataset that includes 1,085 execution-time\nsituations. Each situation corresponds to a state instance wherein a robot is\npotentially unable to complete a task using a solution that normally works.\nExperimental results show that our approach outperforms competitive baselines\nfrom the literature in the success rate of service tasks. Additionally, we have\ndemonstrated COWP using a mobile manipulator. Supplementary materials are\navailable at: https://cowplanning.github.io/",
                "AI Coach Assist: An Automated Approach for Call Recommendation in\n  Contact Centers for Agent Coaching\nIn recent years, the utilization of Artificial Intelligence (AI) in the\ncontact center industry is on the rise. One area where AI can have a\nsignificant impact is in the coaching of contact center agents. By analyzing\ncall transcripts using Natural Language Processing (NLP) techniques, it would\nbe possible to quickly determine which calls are most relevant for coaching\npurposes. In this paper, we present AI Coach Assist, which leverages the\npre-trained transformer-based language models to determine whether a given call\nis coachable or not based on the quality assurance (QA) questions asked by the\ncontact center managers or supervisors. The system was trained and evaluated on\na large dataset collected from real-world contact centers and provides an\neffective way to recommend calls to the contact center managers that are more\nlikely to contain coachable moments. Our experimental findings demonstrate the\npotential of AI Coach Assist to improve the coaching process, resulting in\nenhancing the performance of contact center agents.",
                "On the Value of Myopic Behavior in Policy Reuse\nLeveraging learned strategies in unfamiliar scenarios is fundamental to human\nintelligence. In reinforcement learning, rationally reusing the policies\nacquired from other tasks or human experts is critical for tackling problems\nthat are difficult to learn from scratch. In this work, we present a framework\ncalled Selective Myopic bEhavior Control~(SMEC), which results from the insight\nthat the short-term behaviors of prior policies are sharable across tasks. By\nevaluating the behaviors of prior policies via a hybrid value function\narchitecture, SMEC adaptively aggregates the sharable short-term behaviors of\nprior policies and the long-term behaviors of the task policy, leading to\ncoordinated decisions. Empirical results on a collection of manipulation and\nlocomotion tasks demonstrate that SMEC outperforms existing methods, and\nvalidate the ability of SMEC to leverage related prior policies.",
                "In-Context Analogical Reasoning with Pre-Trained Language Models\nAnalogical reasoning is a fundamental capacity of human cognition that allows\nus to reason abstractly about novel situations by relating them to past\nexperiences. While it is thought to be essential for robust reasoning in AI\nsystems, conventional approaches require significant training and/or\nhard-coding of domain knowledge to be applied to benchmark tasks. Inspired by\ncognitive science research that has found connections between human language\nand analogy-making, we explore the use of intuitive language-based abstractions\nto support analogy in AI systems. Specifically, we apply large pre-trained\nlanguage models (PLMs) to visual Raven's Progressive Matrices (RPM), a common\nrelational reasoning test. By simply encoding the perceptual features of the\nproblem into language form, we find that PLMs exhibit a striking capacity for\nzero-shot relational reasoning, exceeding human performance and nearing\nsupervised vision-based methods. We explore different encodings that vary the\nlevel of abstraction over task features, finding that higher-level abstractions\nfurther strengthen PLMs' analogical reasoning. Our detailed analysis reveals\ninsights on the role of model complexity, in-context learning, and prior\nknowledge in solving RPM tasks.",
                "Modeling Dynamic Environments with Scene Graph Memory\nEmbodied AI agents that search for objects in large environments such as\nhouseholds often need to make efficient decisions by predicting object\nlocations based on partial information. We pose this as a new type of link\nprediction problem: link prediction on partially observable dynamic graphs. Our\ngraph is a representation of a scene in which rooms and objects are nodes, and\ntheir relationships are encoded in the edges; only parts of the changing graph\nare known to the agent at each timestep. This partial observability poses a\nchallenge to existing link prediction approaches, which we address. We propose\na novel state representation -- Scene Graph Memory (SGM) -- with captures the\nagent's accumulated set of observations, as well as a neural net architecture\ncalled a Node Edge Predictor (NEP) that extracts information from the SGM to\nsearch efficiently. We evaluate our method in the Dynamic House Simulator, a\nnew benchmark that creates diverse dynamic graphs following the semantic\npatterns typically seen at homes, and show that NEP can be trained to predict\nthe locations of objects in a variety of environments with diverse object\nmovement dynamics, outperforming baselines both in terms of new scene\nadaptability and overall accuracy. The codebase and more can be found at\nhttps://www.scenegraphmemory.com.",
                "Probing reaction channels via reinforcement learning\nWe propose a reinforcement learning based method to identify important\nconfigurations that connect reactant and product states along chemical reaction\npaths. By shooting multiple trajectories from these configurations, we can\ngenerate an ensemble of configurations that concentrate on the transition path\nensemble. This configuration ensemble can be effectively employed in a neural\nnetwork-based partial differential equation solver to obtain an approximation\nsolution of a restricted Backward Kolmogorov equation, even when the dimension\nof the problem is very high. The resulting solution, known as the committor\nfunction, encodes mechanistic information for the reaction and can in turn be\nused to evaluate reaction rates."
            ],
            "interesting paper": 4
        },
        {
            "papers": [
                "The Digital Divide in Process Safety: Quantitative Risk Analysis of\n  Human-AI Collaboration\nDigital technologies have dramatically accelerated the digital transformation\nin process industries, boosted new industrial applications, upgraded the\nproduction system, and enhanced operational efficiency. In contrast, the\nchallenges and gaps between human and artificial intelligence (AI) have become\nmore and more prominent, whereas the digital divide in process safety is\naggregating. The study attempts to address the following questions: (i)What is\nAI in the process safety context? (ii)What is the difference between AI and\nhumans in process safety? (iii)How do AI and humans collaborate in process\nsafety? (iv)What are the challenges and gaps in human-AI collaboration? (v)How\nto quantify the risk of human-AI collaboration in process safety? Qualitative\nrisk analysis based on brainstorming and literature review, and quantitative\nrisk analysis based on layer of protection analysis (LOPA) and Bayesian network\n(BN), were applied to explore and model. The importance of human reliability\nshould be stressed in the digital age, not usually to increase the reliability\nof AI, and human-centered AI design in process safety needs to be propagated.",
                "IoT based Personal Voice Assistant\nToday, technological advancement is increasing day by day. Earlier, there was\nonly a computer system in which we could only perform a few tasks. But now,\nmachine learning, artificial intelligence, deep learning, and a few more\ntechnologies have made computer systems so advanced that we can perform any\ntype of task. In this era of advancement, if people are still struggling to\ninteract using various input devices, then it's not worth it. For this reason,\nwe developed a voice assistant using Python that allows the user to run any\ntype of command in Linux without interaction with the keyboard. The main task\nof the voice assistant is to minimize the use of input devices like the\nkeyboard and mouse. It will also reduce hardware space and cost.",
                "Visual Affordance Prediction for Guiding Robot Exploration\nMotivated by the intuitive understanding humans have about the space of\npossible interactions, and the ease with which they can generalize this\nunderstanding to previously unseen scenes, we develop an approach for learning\nvisual affordances for guiding robot exploration. Given an input image of a\nscene, we infer a distribution over plausible future states that can be\nachieved via interactions with it. We use a Transformer-based model to learn a\nconditional distribution in the latent embedding space of a VQ-VAE and show\nthat these models can be trained using large-scale and diverse passive data,\nand that the learned models exhibit compositional generalization to diverse\nobjects beyond the training distribution. We show how the trained affordance\nmodel can be used for guiding exploration by acting as a goal-sampling\ndistribution, during visual goal-conditioned policy learning in robotic\nmanipulation.",
                "Taming AI Bots: Controllability of Neural States in Large Language\n  Models\nWe tackle the question of whether an agent can, by suitable choice of\nprompts, control an AI bot to any state. To that end, we first introduce a\nformal definition of ``meaning'' that is amenable to analysis. Then, we\ncharacterize ``meaningful data'' on which large language models (LLMs) are\nostensibly trained, and ``well-trained LLMs'' through conditions that are\nlargely met by today's LLMs. While a well-trained LLM constructs an embedding\nspace of meanings that is Euclidean, meanings themselves do not form a vector\n(linear) subspace, but rather a quotient space within. We then characterize the\nsubset of meanings that can be reached by the state of the LLMs for some input\nprompt, and show that a well-trained bot can reach any meaning albeit with\nsmall probability. We then introduce a stronger notion of controllability as\n{\\em almost certain reachability}, and show that, when restricted to the space\nof meanings, an AI bot is controllable. We do so after introducing a functional\ncharacterization of attentive AI bots, and finally derive necessary and\nsufficient conditions for controllability. The fact that AI bots are\ncontrollable means that an adversary could steer them towards any state.\nHowever, the sampling process can be designed to counteract adverse actions and\navoid reaching undesirable regions of state space before their boundary is\ncrossed.",
                "Re-imagining health and well-being in low resource African settings\n  using an augmented AI system and a 3D digital twin\nThis paper discusses and explores the potential and relevance of recent\ndevelopments in artificial intelligence (AI) and digital twins for health and\nwell-being in low-resource African countries. We use the case of public health\nemergency response to disease outbreaks and epidemic control. There is\npotential to take advantage of the increasing availability of data and\ndigitization to develop advanced AI methods for analysis and prediction. Using\nan AI systems perspective, we review emerging trends in AI systems and digital\ntwins and propose an initial augmented AI system architecture to illustrate how\nan AI system can work with a 3D digital twin to address public health goals. We\nhighlight scientific knowledge discovery, continual learning, pragmatic\ninteroperability, and interactive explanation and decision-making as essential\nresearch challenges for AI systems and digital twins.",
                "Reinforcement Learning with Human Feedback: Learning Dynamic Choices via\n  Pessimism\nIn this paper, we study offline Reinforcement Learning with Human Feedback\n(RLHF) where we aim to learn the human's underlying reward and the MDP's\noptimal policy from a set of trajectories induced by human choices. RLHF is\nchallenging for multiple reasons: large state space but limited human feedback,\nthe bounded rationality of human decisions, and the off-policy distribution\nshift. In this paper, we focus on the Dynamic Discrete Choice (DDC) model for\nmodeling and understanding human choices. DCC, rooted in econometrics and\ndecision theory, is widely used to model a human decision-making process with\nforward-looking and bounded rationality. We propose a\n\\underline{D}ynamic-\\underline{C}hoice-\\underline{P}essimistic-\\underline{P}olicy-\\underline{O}ptimization\n(DCPPO) method. \\ The method involves a three-stage process: The first step is\nto estimate the human behavior policy and the state-action value function via\nmaximum likelihood estimation (MLE); the second step recovers the human reward\nfunction via minimizing Bellman mean squared error using the learned value\nfunctions; the third step is to plug in the learned reward and invoke\npessimistic value iteration for finding a near-optimal policy. With only\nsingle-policy coverage (i.e., optimal policy) of the dataset, we prove that the\nsuboptimality of DCPPO almost matches the classical pessimistic offline RL\nalgorithm in terms of suboptimality's dependency on distribution shift and\ndimension. To the best of our knowledge, this paper presents the first\ntheoretical guarantees for off-policy offline RLHF with dynamic discrete choice\nmodel."
            ],
            "interesting paper": 1
        },
        {
            "papers": [
                "AlphaBlock: Embodied Finetuning for Vision-Language Reasoning in Robot\n  Manipulation\nWe propose a novel framework for learning high-level cognitive capabilities\nin robot manipulation tasks, such as making a smiley face using building\nblocks. These tasks often involve complex multi-step reasoning, presenting\nsignificant challenges due to the limited paired data connecting human\ninstructions (e.g., making a smiley face) and robot actions (e.g., end-effector\nmovement). Existing approaches relieve this challenge by adopting an open-loop\nparadigm decomposing high-level instructions into simple sub-task plans, and\nexecuting them step-by-step using low-level control models. However, these\napproaches are short of instant observations in multi-step reasoning, leading\nto sub-optimal results. To address this issue, we propose to automatically\ncollect a cognitive robot dataset by Large Language Models (LLMs). The\nresulting dataset AlphaBlock consists of 35 comprehensive high-level tasks of\nmulti-step text plans and paired observation sequences. To enable efficient\ndata acquisition, we employ elaborated multi-round prompt designs that\neffectively reduce the burden of extensive human involvement. We further\npropose a closed-loop multi-modal embodied planning model that autoregressively\ngenerates plans by taking image observations as input. To facilitate effective\nlearning, we leverage MiniGPT-4 with a frozen visual encoder and LLM, and\nfinetune additional vision adapter and Q-former to enable fine-grained spatial\nperception for manipulation tasks. We conduct experiments to verify the\nsuperiority over existing open and closed-loop methods, and achieve a\nsignificant increase in success rate by 21.4% and 14.5% over ChatGPT and GPT-4\nbased robot tasks. Real-world demos are shown in\nhttps://www.youtube.com/watch?v=ayAzID1_qQk .",
                "An Emergency Disposal Decision-making Method with Human--Machine\n  Collaboration\nRapid developments in artificial intelligence technology have led to unmanned\nsystems replacing human beings in many fields requiring high-precision\npredictions and decisions. In modern operational environments, all job plans\nare affected by emergency events such as equipment failures and resource\nshortages, making a quick resolution critical. The use of unmanned systems to\nassist decision-making can improve resolution efficiency, but their\ndecision-making is not interpretable and may make the wrong decisions. Current\nunmanned systems require human supervision and control. Based on this, we\npropose a collaborative human--machine method for resolving unplanned events\nusing two phases: task filtering and task scheduling. In the task filtering\nphase, we propose a human--machine collaborative decision-making algorithm for\ndynamic tasks. The GACRNN model is used to predict the state of the job nodes,\nlocate the key nodes, and generate a machine-predicted resolution task list. A\nhuman decision-maker supervises the list in real time and modifies and confirms\nthe machine-predicted list through the human--machine interface. In the task\nscheduling phase, we propose a scheduling algorithm that integrates human\nexperience constraints. The steps to resolve an event are inserted into the\nnormal job sequence to schedule the resolution. We propose several\nhuman--machine collaboration methods in each phase to generate steps to resolve\nan unplanned event while minimizing the impact on the original job plan.",
                "Towards a Unifying Model of Rationality in Multiagent Systems\nMultiagent systems deployed in the real world need to cooperate with other\nagents (including humans) nearly as effectively as these agents cooperate with\none another. To design such AI, and provide guarantees of its effectiveness, we\nneed to clearly specify what types of agents our AI must be able to cooperate\nwith. In this work we propose a generic model of socially intelligent agents,\nwhich are individually rational learners that are also able to cooperate with\none another (in the sense that their joint behavior is Pareto efficient). We\ndefine rationality in terms of the regret incurred by each agent over its\nlifetime, and show how we can construct socially intelligent agents for\ndifferent forms of regret. We then discuss the implications of this model for\nthe development of \"robust\" MAS that can cooperate with a wide variety of\nsocially intelligent agents.",
                "Reason to explain: Interactive contrastive explanations (REASONX)\nMany high-performing machine learning models are not interpretable. As they\nare increasingly used in decision scenarios that can critically affect\nindividuals, it is necessary to develop tools to better understand their\noutputs. Popular explanation methods include contrastive explanations. However,\nthey suffer several shortcomings, among others an insufficient incorporation of\nbackground knowledge, and a lack of interactivity. While (dialogue-like)\ninteractivity is important to better communicate an explanation, background\nknowledge has the potential to significantly improve their quality, e.g., by\nadapting the explanation to the needs of the end-user. To close this gap, we\npresent REASONX, an explanation tool based on Constraint Logic Programming\n(CLP). REASONX provides interactive contrastive explanations that can be\naugmented by background knowledge, and allows to operate under a setting of\nunder-specified information, leading to increased flexibility in the provided\nexplanations. REASONX computes factual and constrative decision rules, as well\nas closest constrative examples. It provides explanations for decision trees,\nwhich can be the ML models under analysis, or global/local surrogate models of\nany ML model. While the core part of REASONX is built on CLP, we also provide a\nprogram layer that allows to compute the explanations via Python, making the\ntool accessible to a wider audience. We illustrate the capability of REASONX on\na synthetic data set, and on a a well-developed example in the credit domain.\nIn both cases, we can show how REASONX can be flexibly used and tailored to the\nneeds of the user.",
                "Contextual Object Detection with Multimodal Large Language Models\nRecent Multimodal Large Language Models (MLLMs) are remarkable in\nvision-language tasks, such as image captioning and question answering, but\nlack the essential perception ability, i.e., object detection. In this work, we\naddress this limitation by introducing a novel research problem of contextual\nobject detection -- understanding visible objects within different human-AI\ninteractive contexts. Three representative scenarios are investigated,\nincluding the language cloze test, visual captioning, and question answering.\nMoreover, we present ContextDET, a unified multimodal model that is capable of\nend-to-end differentiable modeling of visual-language contexts, so as to\nlocate, identify, and associate visual objects with language inputs for\nhuman-AI interaction. Our ContextDET involves three key submodels: (i) a visual\nencoder for extracting visual representations, (ii) a pre-trained LLM for\nmultimodal context decoding, and (iii) a visual decoder for predicting bounding\nboxes given contextual object words. The new generate-then-detect framework\nenables us to detect object words within human vocabulary. Extensive\nexperiments show the advantages of ContextDET on our proposed CODE benchmark,\nopen-vocabulary detection, and referring image segmentation. Github:\nhttps://github.com/yuhangzang/ContextDET.",
                "Code Prompting: a Neural Symbolic Method for Complex Reasoning in Large\n  Language Models\nLarge language models (LLMs) have scaled up to unlock a wide range of complex\nreasoning tasks with the aid of various prompting methods. However, current\nprompting methods generate natural language intermediate steps to help\nreasoning, which can cause imperfect task reduction and confusion. To mitigate\nsuch limitations, we explore code prompting, a neural symbolic prompting method\nwith both zero-shot and few-shot versions which triggers code as intermediate\nsteps. We conduct experiments on 7 widely-used benchmarks involving symbolic\nreasoning and arithmetic reasoning. Code prompting generally outperforms\nchain-of-thought (CoT) prompting. To further understand the performance and\nlimitations of code prompting, we perform extensive ablation studies and error\nanalyses, and identify several exclusive advantages of using symbolic\npromptings compared to natural language. We also consider the ensemble of code\nprompting and CoT prompting to combine the strengths of both. Finally, we show\nthrough experiments how code annotations and their locations affect code\nprompting."
            ],
            "interesting paper": 1
        },
        {
            "papers": [
                "Intent-aligned AI systems deplete human agency: the need for agency\n  foundations research in AI safety\nThe rapid advancement of artificial intelligence (AI) systems suggests that\nartificial general intelligence (AGI) systems may soon arrive. Many researchers\nare concerned that AIs and AGIs will harm humans via intentional misuse\n(AI-misuse) or through accidents (AI-accidents). In respect of AI-accidents,\nthere is an increasing effort focused on developing algorithms and paradigms\nthat ensure AI systems are aligned to what humans intend, e.g. AI systems that\nyield actions or recommendations that humans might judge as consistent with\ntheir intentions and goals. Here we argue that alignment to human intent is\ninsufficient for safe AI systems and that preservation of long-term agency of\nhumans may be a more robust standard, and one that needs to be separated\nexplicitly and a priori during optimization. We argue that AI systems can\nreshape human intention and discuss the lack of biological and psychological\nmechanisms that protect humans from loss of agency. We provide the first formal\ndefinition of agency-preserving AI-human interactions which focuses on\nforward-looking agency evaluations and argue that AI systems - not humans -\nmust be increasingly tasked with making these evaluations. We show how agency\nloss can occur in simple environments containing embedded agents that use\ntemporal-difference learning to make action recommendations. Finally, we\npropose a new area of research called \"agency foundations\" and pose four\ninitial topics designed to improve our understanding of agency in AI-human\ninteractions: benevolent game theory, algorithmic foundations of human rights,\nmechanistic interpretability of agency representation in neural-networks and\nreinforcement learning from internal states.",
                "A Computational Account Of Self-Supervised Visual Learning From\n  Egocentric Object Play\nResearch in child development has shown that embodied experience handling\nphysical objects contributes to many cognitive abilities, including visual\nlearning. One characteristic of such experience is that the learner sees the\nsame object from several different viewpoints. In this paper, we study how\nlearning signals that equate different viewpoints -- e.g., assigning similar\nrepresentations to different views of a single object -- can support robust\nvisual learning. We use the Toybox dataset, which contains egocentric videos of\nhumans manipulating different objects, and conduct experiments using a computer\nvision framework for self-supervised contrastive learning. We find that\nrepresentations learned by equating different physical viewpoints of an object\nbenefit downstream image classification accuracy. Further experiments show that\nthis performance improvement is robust to variations in the gaps between\nviewpoints, and that the benefits transfer to several different image\nclassification tasks.",
                "Strategic Reasoning with Language Models\nStrategic reasoning enables agents to cooperate, communicate, and compete\nwith other agents in diverse situations. Existing approaches to solving\nstrategic games rely on extensive training, yielding strategies that do not\ngeneralize to new scenarios or games without retraining. Large Language Models\n(LLMs), with their ability to comprehend and generate complex, context-rich\nlanguage, could prove powerful as tools for strategic gameplay. This paper\nintroduces an approach that uses pretrained LLMs with few-shot chain-of-thought\nexamples to enable strategic reasoning for AI agents. Our approach uses\nsystematically generated demonstrations of reasoning about states, values, and\nbeliefs to prompt the model. Using extensive variations of simple matrix games,\nwe show that strategies that are derived based on systematically generated\nprompts generalize almost perfectly to new game structures, alternate\nobjectives, and hidden information. Additionally, we demonstrate our approach\ncan lead to human-like negotiation strategies in realistic scenarios without\nany extra training or fine-tuning. Our results highlight the ability of LLMs,\nguided by systematic reasoning demonstrations, to adapt and excel in diverse\nstrategic scenarios.",
                "Less Likely Brainstorming: Using Language Models to Generate Alternative\n  Hypotheses\nA human decision-maker benefits the most from an AI assistant that corrects\nfor their biases. For problems such as generating interpretation of a radiology\nreport given findings, a system predicting only highly likely outcomes may be\nless useful, where such outcomes are already obvious to the user. To alleviate\nbiases in human decision-making, it is worth considering a broad differential\ndiagnosis, going beyond the most likely options. We introduce a new task, \"less\nlikely brainstorming,\" that asks a model to generate outputs that humans think\nare relevant but less likely to happen. We explore the task in two settings: a\nbrain MRI interpretation generation setting and an everyday commonsense\nreasoning setting. We found that a baseline approach of training with less\nlikely hypotheses as targets generates outputs that humans evaluate as either\nlikely or irrelevant nearly half of the time; standard MLE training is not\neffective. To tackle this problem, we propose a controlled text generation\nmethod that uses a novel contrastive learning strategy to encourage models to\ndifferentiate between generating likely and less likely outputs according to\nhumans. We compare our method with several state-of-the-art controlled text\ngeneration models via automatic and human evaluations and show that our models'\ncapability of generating less likely outputs is improved.",
                "Bigger, Better, Faster: Human-level Atari with human-level efficiency\nWe introduce a value-based RL agent, which we call BBF, that achieves\nsuper-human performance in the Atari 100K benchmark. BBF relies on scaling the\nneural networks used for value estimation, as well as a number of other design\nchoices that enable this scaling in a sample-efficient manner. We conduct\nextensive analyses of these design choices and provide insights for future\nwork. We end with a discussion about updating the goalposts for\nsample-efficient RL research on the ALE. We make our code and data publicly\navailable at\nhttps://github.com/google-research/google-research/tree/master/bigger_better_faster.",
                "Language-Conditioned Imitation Learning with Base Skill Priors under\n  Unstructured Data\nThe growing interest in language-conditioned robot manipulation aims to\ndevelop robots capable of understanding and executing complex tasks, with the\nobjective of enabling robots to interpret language commands and manipulate\nobjects accordingly. While language-conditioned approaches demonstrate\nimpressive capabilities for addressing tasks in familiar environments, they\nencounter limitations in adapting to unfamiliar environment settings. In this\nstudy, we propose a general-purpose, language-conditioned approach that\ncombines base skill priors and imitation learning under unstructured data to\nenhance the algorithm's generalization in adapting to unfamiliar environments.\nWe assess our model's performance in both simulated and real-world environments\nusing a zero-shot setting. In the simulated environment, the proposed approach\nsurpasses previously reported scores for CALVIN benchmark, especially in the\nchallenging Zero-Shot Multi-Environment setting. The average completed task\nlength, indicating the average number of tasks the agent can continuously\ncomplete, improves more than 2.5 times compared to the state-of-the-art method\nHULC. In addition, we conduct a zero-shot evaluation of our policy in a\nreal-world setting, following training exclusively in simulated environments\nwithout additional specific adaptations. In this evaluation, we set up ten\ntasks and achieved an average 30% improvement in our approach compared to the\ncurrent state-of-the-art approach, demonstrating a high generalization\ncapability in both simulated environments and the real world. For further\ndetails, including access to our code and videos, please refer to\nhttps://hk-zh.github.io/spil/"
            ],
            "interesting paper": 1
        },
        {
            "papers": [
                "Human Control: Definitions and Algorithms\nHow can humans stay in control of advanced artificial intelligence systems?\nOne proposal is corrigibility, which requires the agent to follow the\ninstructions of a human overseer, without inappropriately influencing them. In\nthis paper, we formally define a variant of corrigibility called shutdown\ninstructability, and show that it implies appropriate shutdown behavior,\nretention of human autonomy, and avoidance of user harm. We also analyse the\nrelated concepts of non-obstruction and shutdown alignment, three previously\nproposed algorithms for human control, and one new algorithm.",
                "Human or Not? A Gamified Approach to the Turing Test\nWe present \"Human or Not?\", an online game inspired by the Turing test, that\nmeasures the capability of AI chatbots to mimic humans in dialog, and of humans\nto tell bots from other humans. Over the course of a month, the game was played\nby over 1.5 million users who engaged in anonymous two-minute chat sessions\nwith either another human or an AI language model which was prompted to behave\nlike humans. The task of the players was to correctly guess whether they spoke\nto a person or to an AI. This largest scale Turing-style test conducted to date\nrevealed some interesting facts. For example, overall users guessed the\nidentity of their partners correctly in only 68% of the games. In the subset of\nthe games in which users faced an AI bot, users had even lower correct guess\nrates of 60% (that is, not much higher than chance). This white paper details\nthe development, deployment, and results of this unique experiment. While this\nexperiment calls for many extensions and refinements, these findings already\nbegin to shed light on the inevitable near future which will commingle humans\nand AI.",
                "Decision-Oriented Dialogue for Human-AI Collaboration\nWe describe a class of tasks called decision-oriented dialogues, in which AI\nassistants such as large language models (LMs) must collaborate with one or\nmore humans via natural language to help them make complex decisions. We\nformalize three domains in which users face everyday decisions: (1) choosing an\nassignment of reviewers to conference papers, (2) planning a multi-step\nitinerary in a city, and (3) negotiating travel plans for a group of friends.\nIn each of these settings, AI assistants and users have disparate abilities\nthat they must combine to arrive at the best decision: assistants can access\nand process large amounts of information, while users have preferences and\nconstraints external to the system. For each task, we build a dialogue\nenvironment where agents receive a reward based on the quality of the final\ndecision they reach. We evaluate LMs in self-play and in collaboration with\nhumans and find that they fall short compared to human assistants, achieving\nmuch lower rewards despite engaging in longer dialogues. We highlight a number\nof challenges models face in decision-oriented dialogues, ranging from\ngoal-directed behavior to reasoning and optimization, and release our\nenvironments as a testbed for future work.",
                "From Human-Centered to Social-Centered Artificial Intelligence:\n  Assessing ChatGPT's Impact through Disruptive Events\nLarge language models (LLMs) and dialogue agents have existed for years, but\nthe release of recent GPT models has been a watershed moment for artificial\nintelligence (AI) research and society at large. Immediately recognized for its\ngenerative capabilities and versatility, ChatGPT's impressive proficiency\nacross technical and creative domains led to its widespread adoption. While\nsociety grapples with the emerging cultural impacts of ChatGPT, critiques of\nChatGPT's impact within the machine learning community have coalesced around\nits performance or other conventional Responsible AI evaluations relating to\nbias, toxicity, and 'hallucination.' We argue that these latter critiques draw\nheavily on a particular conceptualization of the 'human-centered' framework,\nwhich tends to cast atomized individuals as the key recipients of both the\nbenefits and detriments of technology. In this article, we direct attention to\nanother dimension of LLMs and dialogue agents' impact: their effect on social\ngroups, institutions, and accompanying norms and practices. By illustrating\nChatGPT's social impact through three disruptive events, we challenge\nindividualistic approaches in AI development and contribute to ongoing debates\naround the ethical and responsible implementation of AI systems. We hope this\neffort will call attention to more comprehensive and longitudinal evaluation\ntools and compel technologists to go beyond human-centered thinking and ground\ntheir efforts through social-centered AI.",
                "EMOTE: An Explainable architecture for Modelling the Other Through\n  Empathy\nWe can usually assume others have goals analogous to our own. This assumption\ncan also, at times, be applied to multi-agent games - e.g. Agent 1's attraction\nto green pellets is analogous to Agent 2's attraction to red pellets. This\n\"analogy\" assumption is tied closely to the cognitive process known as empathy.\nInspired by empathy, we design a simple and explainable architecture to model\nanother agent's action-value function. This involves learning an \"Imagination\nNetwork\" to transform the other agent's observed state in order to produce a\nhuman-interpretable \"empathetic state\" which, when presented to the learning\nagent, produces behaviours that mimic the other agent. Our approach is\napplicable to multi-agent scenarios consisting of a single learning agent and\nother (independent) agents acting according to fixed policies. This\narchitecture is particularly beneficial for (but not limited to) algorithms\nusing a composite value or reward function. We show our method produces better\nperformance in multi-agent games, where it robustly estimates the other's model\nin different environment configurations. Additionally, we show that the\nempathetic states are human interpretable, and thus verifiable.",
                "Speaking the Language of Your Listener: Audience-Aware Adaptation via\n  Plug-and-Play Theory of Mind\nDialogue participants may have varying levels of knowledge about the topic\nunder discussion. In such cases, it is essential for speakers to adapt their\nutterances by taking their audience into account. Yet, it is an open question\nhow such adaptation can be modelled in computational agents. In this paper, we\nmodel a visually grounded referential game between a knowledgeable speaker and\na listener with more limited visual and linguistic experience. Inspired by\npsycholinguistic theories, we endow our speaker with the ability to adapt its\nreferring expressions via a simulation module that monitors the effectiveness\nof planned utterances from the listener's perspective. We propose an adaptation\nmechanism building on plug-and-play approaches to controlled language\ngeneration, where utterance generation is steered on the fly by the simulator\nwithout finetuning the speaker's underlying language model. Our results and\nanalyses show that our approach is effective: the speaker's utterances become\ncloser to the listener's domain of expertise, which leads to higher\ncommunicative success."
            ],
            "interesting paper": 2
        },
        {
            "papers": [
                "Egocentric Planning for Scalable Embodied Task Achievement\nEmbodied agents face significant challenges when tasked with performing\nactions in diverse environments, particularly in generalizing across object\ntypes and executing suitable actions to accomplish tasks. Furthermore, agents\nshould exhibit robustness, minimizing the execution of illegal actions. In this\nwork, we present Egocentric Planning, an innovative approach that combines\nsymbolic planning and Object-oriented POMDPs to solve tasks in complex\nenvironments, harnessing existing models for visual perception and natural\nlanguage processing. We evaluated our approach in ALFRED, a simulated\nenvironment designed for domestic tasks, and demonstrated its high scalability,\nachieving an impressive 36.07% unseen success rate in the ALFRED benchmark and\nwinning the ALFRED challenge at CVPR Embodied AI workshop. Our method requires\nreliable perception and the specification or learning of a symbolic description\nof the preconditions and effects of the agent's actions, as well as what object\ntypes reveal information about others. It is capable of naturally scaling to\nsolve new tasks beyond ALFRED, as long as they can be solved using the\navailable skills. This work offers a solid baseline for studying end-to-end and\nhybrid methods that aim to generalize to new tasks, including recent approaches\nrelying on LLMs, but often struggle to scale to long sequences of actions or\nproduce robust plans for novel tasks.",
                "AI and the creative realm: A short review of current and future\n  applications\nThis study explores the concept of creativity and artificial intelligence\n(AI) and their recent integration. While AI has traditionally been perceived as\nincapable of generating new ideas or creating art, the development of more\nsophisticated AI models and the proliferation of human-computer interaction\ntools have opened up new possibilities for AI in artistic creation. This study\ninvestigates the various applications of AI in a creative context,\ndifferentiating between the type of art, language, and algorithms used. It also\nconsiders the philosophical implications of AI and creativity, questioning\nwhether consciousness can be researched in machines and AI's potential\ninterests and decision-making capabilities. Overall, we aim to stimulate a\nreflection on AI's use and ethical implications in creative contexts.",
                "AI Liability Insurance With an Example in AI-Powered E-diagnosis System\nArtificial Intelligence (AI) has received an increasing amount of attention\nin multiple areas. The uncertainties and risks in AI-powered systems have\ncreated reluctance in their wild adoption. As an economic solution to\ncompensate for potential damages, AI liability insurance is a promising market\nto enhance the integration of AI into daily life. In this work, we use an\nAI-powered E-diagnosis system as an example to study AI liability insurance. We\nprovide a quantitative risk assessment model with evidence-based numerical\nanalysis. We discuss the insurability criteria for AI technologies and suggest\nnecessary adjustments to accommodate the features of AI products. We show that\nAI liability insurance can act as a regulatory mechanism to incentivize\ncompliant behaviors and serve as a certificate of high-quality AI systems.\nFurthermore, we suggest premium adjustment to reflect the dynamic evolution of\nthe inherent uncertainty in AI. Moral hazard problems are discussed and\nsuggestions for AI liability insurance are provided.",
                "STEVE-1: A Generative Model for Text-to-Behavior in Minecraft\nConstructing AI models that respond to text instructions is challenging,\nespecially for sequential decision-making tasks. This work introduces a\nmethodology, inspired by unCLIP, for instruction-tuning generative models of\nbehavior without relying on a large dataset of instruction-labeled\ntrajectories. Using this methodology, we create an instruction-tuned Video\nPretraining (VPT) model called STEVE-1, which can follow short-horizon\nopen-ended text and visual instructions in Minecraft. STEVE-1 is trained in two\nsteps: adapting the pretrained VPT model to follow commands in MineCLIP's\nlatent space, then training a prior to predict latent codes from text. This\nallows us to finetune VPT through self-supervised behavioral cloning and\nhindsight relabeling, reducing the need for costly human text annotations, and\nall for only $60 of compute. By leveraging pretrained models like VPT and\nMineCLIP and employing best practices from text-conditioned image generation,\nSTEVE-1 sets a new bar for open-ended instruction-following in Minecraft with\nlow-level controls (mouse and keyboard) and raw pixel inputs, far outperforming\nprevious baselines and robustly completing 12 of 13 tasks in our early-game\nevaluation suite. We provide experimental evidence highlighting key factors for\ndownstream performance, including pretraining, classifier-free guidance, and\ndata scaling. All resources, including our model weights, training scripts, and\nevaluation tools are made available for further research.",
                "The ethical ambiguity of AI data enrichment: Measuring gaps in research\n  ethics norms and practices\nThe technical progression of artificial intelligence (AI) research has been\nbuilt on breakthroughs in fields such as computer science, statistics, and\nmathematics. However, in the past decade AI researchers have increasingly\nlooked to the social sciences, turning to human interactions to solve the\nchallenges of model development. Paying crowdsourcing workers to generate or\ncurate data, or data enrichment, has become indispensable for many areas of AI\nresearch, from natural language processing to reinforcement learning from human\nfeedback (RLHF). Other fields that routinely interact with crowdsourcing\nworkers, such as Psychology, have developed common governance requirements and\nnorms to ensure research is undertaken ethically. This study explores how, and\nto what extent, comparable research ethics requirements and norms have\ndeveloped for AI research and data enrichment. We focus on the approach taken\nby two leading conferences: ICLR and NeurIPS, and journal publisher Springer.\nIn a longitudinal study of accepted papers, and via a comparison with\nPsychology and CHI papers, this work finds that leading AI venues have begun to\nestablish protocols for human data collection, but these are are inconsistently\nfollowed by authors. Whilst Psychology papers engaging with crowdsourcing\nworkers frequently disclose ethics reviews, payment data, demographic data and\nother information, similar disclosures are far less common in leading AI venues\ndespite similar guidance. The work concludes with hypotheses to explain these\ngaps in research ethics practices and considerations for its implications.",
                "LIV: Language-Image Representations and Rewards for Robotic Control\nWe present Language-Image Value learning (LIV), a unified objective for\nvision-language representation and reward learning from action-free videos with\ntext annotations. Exploiting a novel connection between dual reinforcement\nlearning and mutual information contrastive learning, the LIV objective trains\na multi-modal representation that implicitly encodes a universal value function\nfor tasks specified as language or image goals. We use LIV to pre-train the\nfirst control-centric vision-language representation from large human video\ndatasets such as EpicKitchen. Given only a language or image goal, the\npre-trained LIV model can assign dense rewards to each frame in videos of\nunseen robots or humans attempting that task in unseen environments. Further,\nwhen some target domain-specific data is available, the same objective can be\nused to fine-tune and improve LIV and even other pre-trained representations\nfor robotic control and reward specification in that domain. In our experiments\non several simulated and real-world robot environments, LIV models consistently\noutperform the best prior input state representations for imitation learning,\nas well as reward specification methods for policy synthesis. Our results\nvalidate the advantages of joint vision-language representation and reward\nlearning within the unified, compact LIV framework."
            ],
            "interesting paper": 4
        },
        {
            "papers": [
                "Enough With \"Human-AI Collaboration\"\nDescribing our interaction with Artificial Intelligence (AI) systems as\n'collaboration' is well-intentioned, but flawed. Not only is it misleading, but\nit also takes away the credit of AI 'labour' from the humans behind it, and\nerases and obscures an often exploitative arrangement between AI producers and\nconsumers. The AI 'collaboration' metaphor is merely the latest episode in a\nlong history of labour appropriation and credit reassignment that\ndisenfranchises labourers in the Global South. I propose that viewing AI as a\ntool or an instrument, rather than a collaborator, is more accurate, and\nultimately fairer.",
                "AI Transparency in the Age of LLMs: A Human-Centered Research Roadmap\nThe rise of powerful large language models (LLMs) brings about tremendous\nopportunities for innovation but also looming risks for individuals and society\nat large. We have reached a pivotal moment for ensuring that LLMs and\nLLM-infused applications are developed and deployed responsibly. However, a\ncentral pillar of responsible AI -- transparency -- is largely missing from the\ncurrent discourse around LLMs. It is paramount to pursue new approaches to\nprovide transparency for LLMs, and years of research at the intersection of AI\nand human-computer interaction (HCI) highlight that we must do so with a\nhuman-centered perspective: Transparency is fundamentally about supporting\nappropriate human understanding, and this understanding is sought by different\nstakeholders with different goals in different contexts. In this new era of\nLLMs, we must develop and design approaches to transparency by considering the\nneeds of stakeholders in the emerging LLM ecosystem, the novel types of\nLLM-infused applications being built, and the new usage patterns and challenges\naround LLMs, all while building on lessons learned about how people process,\ninteract with, and make use of information. We reflect on the unique challenges\nthat arise in providing transparency for LLMs, along with lessons learned from\nHCI and responsible AI research that has taken a human-centered perspective on\nAI transparency. We then lay out four common approaches that the community has\ntaken to achieve transparency -- model reporting, publishing evaluation\nresults, providing explanations, and communicating uncertainty -- and call out\nopen questions around how these approaches may or may not be applied to LLMs.\nWe hope this provides a starting point for discussion and a useful roadmap for\nfuture research.",
                "Accelerating science with human-aware artificial intelligence\nArtificial intelligence (AI) models trained on published scientific findings\nhave been used to invent valuable materials and targeted therapies, but they\ntypically ignore the human scientists who continually alter the landscape of\ndiscovery. Here we show that incorporating the distribution of human expertise\nby training unsupervised models on simulated inferences cognitively accessible\nto experts dramatically improves (up to 400%) AI prediction of future\ndiscoveries beyond those focused on research content alone, especially when\nrelevant literature is sparse. These models succeed by predicting human\npredictions and the scientists who will make them. By tuning human-aware AI to\navoid the crowd, we can generate scientifically promising \"alien\" hypotheses\nunlikely to be imagined or pursued without intervention until the distant\nfuture, which hold promise to punctuate scientific advance beyond questions\ncurrently pursued. Accelerating human discovery or probing its blind spots,\nhuman-aware AI enables us to move toward and beyond the contemporary scientific\nfrontier.",
                "Guided scenarios with simulated expert personae: a remarkable strategy\n  to perform cognitive work\nLarge language models (LLMs) trained on a substantial corpus of human\nknowledge and literature productively work with a large array of facts from\nthat corpus. Surprisingly, they are also able to re-create the behaviors of\npersonae that are captured within the corpus. By forming teams of simulated\npersonae, supplying contexts that set the stage, and providing gentle prompts,\none can move through scenarios that elicit expert behavior to perform\nmeaningful cognitive work. The power of this strategy is demonstrated with two\nexamples, one attacking factuality of LLM responses and the other reproducing a\nvery recently published result in quantum optics.",
                "Milestones in Autonomous Driving and Intelligent Vehicles Part II:\n  Perception and Planning\nGrowing interest in autonomous driving (AD) and intelligent vehicles (IVs) is\nfueled by their promise for enhanced safety, efficiency, and economic benefits.\nWhile previous surveys have captured progress in this field, a comprehensive\nand forward-looking summary is needed. Our work fills this gap through three\ndistinct articles. The first part, a \"Survey of Surveys\" (SoS), outlines the\nhistory, surveys, ethics, and future directions of AD and IV technologies. The\nsecond part, \"Milestones in Autonomous Driving and Intelligent Vehicles Part I:\nControl, Computing System Design, Communication, HD Map, Testing, and Human\nBehaviors\" delves into the development of control, computing system,\ncommunication, HD map, testing, and human behaviors in IVs. This part, the\nthird part, reviews perception and planning in the context of IVs. Aiming to\nprovide a comprehensive overview of the latest advancements in AD and IVs, this\nwork caters to both newcomers and seasoned researchers. By integrating the SoS\nand Part I, we offer unique insights and strive to serve as a bridge between\npast achievements and future possibilities in this dynamic field.",
                "OMNI: Open-endedness via Models of human Notions of Interestingness\nOpen-ended algorithms aim to learn new, interesting behaviors forever. That\nrequires a vast environment search space, but there are thus infinitely many\npossible tasks. Even after filtering for tasks the current agent can learn\n(i.e., learning progress), countless learnable yet uninteresting tasks remain\n(e.g., minor variations of previously learned tasks). An Achilles Heel of\nopen-endedness research is the inability to quantify (and thus prioritize)\ntasks that are not just learnable, but also $\\textit{interesting}$ (e.g.,\nworthwhile and novel). We propose solving this problem by\n$\\textit{Open-endedness via Models of human Notions of Interestingness}$\n(OMNI). The insight is that we can utilize foundation models (FMs) as a model\nof interestingness (MoI), because they $\\textit{already}$ internalize human\nconcepts of interestingness from training on vast amounts of human-generated\ndata, where humans naturally write about what they find interesting or boring.\nWe show that FM-based MoIs improve open-ended learning by focusing on tasks\nthat are both learnable $\\textit{and interesting}$, outperforming baselines\nbased on uniform task sampling or learning progress alone. This approach has\nthe potential to dramatically advance the ability to intelligently select which\ntasks to focus on next (i.e., auto-curricula), and could be seen as AI\nselecting its own next task to learn, facilitating self-improving AI and\nAI-Generating Algorithms. Project website at https://www.jennyzhangzt.com/omni/"
            ],
            "interesting paper": 2
        }
    ],
    "Jordan Johnson": [
        {
            "papers": [
                "Prompt Evolution for Generative AI: A Classifier-Guided Approach\nSynthesis of digital artifacts conditioned on user prompts has become an\nimportant paradigm facilitating an explosion of use cases with generative AI.\nHowever, such models often fail to connect the generated outputs and desired\ntarget concepts/preferences implied by the prompts. Current research addressing\nthis limitation has largely focused on enhancing the prompts before output\ngeneration or improving the model's performance up front. In contrast, this\npaper conceptualizes prompt evolution, imparting evolutionary selection\npressure and variation during the generative process to produce multiple\noutputs that satisfy the target concepts/preferences better. We propose a\nmulti-objective instantiation of this broader idea that uses a multi-label\nimage classifier-guided approach. The predicted labels from the classifiers\nserve as multiple objectives to optimize, with the aim of producing diversified\nimages that meet user preferences. A novelty of our evolutionary algorithm is\nthat the pre-trained generative model gives us implicit mutation operations,\nleveraging the model's stochastic generative capability to automate the\ncreation of Pareto-optimized images more faithful to user preferences.",
                "Large Language Models for User Interest Journeys\nLarge language models (LLMs) have shown impressive capabilities in natural\nlanguage understanding and generation. Their potential for deeper user\nunderstanding and improved personalized user experience on recommendation\nplatforms is, however, largely untapped. This paper aims to address this gap.\nRecommender systems today capture users' interests through encoding their\nhistorical activities on the platforms. The generated user representations are\nhard to examine or interpret. On the other hand, if we were to ask people about\ninterests they pursue in their life, they might talk about their hobbies, like\nI just started learning the ukulele, or their relaxation routines, e.g., I like\nto watch Saturday Night Live, or I want to plant a vertical garden. We argue,\nand demonstrate through extensive experiments, that LLMs as foundation models\ncan reason through user activities, and describe their interests in nuanced and\ninteresting ways, similar to how a human would.\n  We define interest journeys as the persistent and overarching user interests,\nin other words, the non-transient ones. These are the interests that we believe\nwill benefit most from the nuanced and personalized descriptions. We introduce\na framework in which we first perform personalized extraction of interest\njourneys, and then summarize the extracted journeys via LLMs, using techniques\nlike few-shot prompting, prompt-tuning and fine-tuning. Together, our results\nin prompting LLMs to name extracted user journeys in a large-scale industrial\nplatform demonstrate great potential of these models in providing deeper, more\ninterpretable, and controllable user understanding. We believe LLM powered user\nunderstanding can be a stepping stone to entirely new user experiences on\nrecommendation platforms that are journey-aware, assistive, and enabling\nfrictionless conversation down the line.",
                "HuatuoGPT, towards Taming Language Model to Be a Doctor\nIn this paper, we present HuatuoGPT, a large language model (LLM) for medical\nconsultation. The core recipe of HuatuoGPT is to leverage both\n\\textit{distilled data from ChatGPT} and \\textit{real-world data from doctors}\nin the supervised fine-tuned stage. The responses of ChatGPT are usually\ndetailed, well-presented and informative while it cannot perform like a doctor\nin many aspects, e.g. for integrative diagnosis. We argue that real-world data\nfrom doctors would be complementary to distilled data in the sense the former\ncould tame a distilled language model to perform like doctors. To better\nleverage the strengths of both data, we train a reward model to align the\nlanguage model with the merits that both data bring, following an RLAIF\n(reinforced learning from AI feedback) fashion. To evaluate and benchmark the\nmodels, we propose a comprehensive evaluation scheme (including automatic and\nmanual metrics). Experimental results demonstrate that HuatuoGPT achieves\nstate-of-the-art results in performing medical consultation among open-source\nLLMs in GPT-4 evaluation, human evaluation, and medical benchmark datasets. It\nis worth noting that by using additional real-world data and RLAIF, the\ndistilled language model (i.e., HuatuoGPT) outperforms its teacher model\nChatGPT in most cases. Our code, data, and models are publicly available at\n\\url{https://github.com/FreedomIntelligence/HuatuoGPT}. The online demo is\navailable at \\url{https://www.HuatuoGPT.cn/}.",
                "An Experimental Investigation into the Evaluation of Explainability\n  Methods\nEXplainable Artificial Intelligence (XAI) aims to help users to grasp the\nreasoning behind the predictions of an Artificial Intelligence (AI) system.\nMany XAI approaches have emerged in recent years. Consequently, a subfield\nrelated to the evaluation of XAI methods has gained considerable attention,\nwith the aim to determine which methods provide the best explanation using\nvarious approaches and criteria. However, the literature lacks a comparison of\nthe evaluation metrics themselves, that one can use to evaluate XAI methods.\nThis work aims to fill this gap by comparing 14 different metrics when applied\nto nine state-of-the-art XAI methods and three dummy methods (e.g., random\nsaliency maps) used as references. Experimental results show which of these\nmetrics produces highly correlated results, indicating potential redundancy. We\nalso demonstrate the significant impact of varying the baseline hyperparameter\non the evaluation metric values. Finally, we use dummy methods to assess the\nreliability of metrics in terms of ranking, pointing out their limitations.",
                "Topic-Guided Self-Introduction Generation for Social Media Users\nMillions of users are active on social media. To allow users to better\nshowcase themselves and network with others, we explore the auto-generation of\nsocial media self-introduction, a short sentence outlining a user's personal\ninterests. While most prior work profiles users with tags (e.g., ages), we\ninvestigate sentence-level self-introductions to provide a more natural and\nengaging way for users to know each other. Here we exploit a user's tweeting\nhistory to generate their self-introduction. The task is non-trivial because\nthe history content may be lengthy, noisy, and exhibit various personal\ninterests. To address this challenge, we propose a novel unified topic-guided\nencoder-decoder (UTGED) framework; it models latent topics to reflect salient\nuser interest, whose topic mixture then guides encoding a user's history and\ntopic words control decoding their self-introduction. For experiments, we\ncollect a large-scale Twitter dataset, and extensive results show the\nsuperiority of our UTGED to the advanced encoder-decoder models without topic\nmodeling.",
                "Visually-Situated Natural Language Understanding with Contrastive\n  Reading Model and Frozen Large Language Models\nRecent advances in Large Language Models (LLMs) have stimulated a surge of\nresearch aimed at extending their applications to the visual domain. While\nthese models exhibit promise in generating abstract image captions and\nfacilitating natural conversations, their performance on text-rich images still\nrequires improvement. In this paper, we introduce Contrastive Reading Model\n(Cream), a novel neural architecture designed to enhance the language-image\nunderstanding capability of LLMs by capturing intricate details that are often\noverlooked in existing methods. Cream combines vision and auxiliary encoders,\nfortified by a contrastive feature alignment technique, to achieve a more\neffective comprehension of language information in visually situated contexts\nwithin the images. Our approach bridges the gap between vision and language\nunderstanding, paving the way for the development of more sophisticated\nDocument Intelligence Assistants. Through rigorous evaluations across diverse\nvisually-situated language understanding tasks that demand reasoning\ncapabilities, we demonstrate the compelling performance of Cream, positioning\nit as a prominent model in the field of visual document understanding. We\nprovide our codebase and newly-generated datasets at\nhttps://github.com/naver-ai/cream ."
            ],
            "interesting paper": 2
        },
        {
            "papers": [
                "Multimodal Recommendation Dialog with Subjective Preference: A New\n  Challenge and Benchmark\nExisting multimodal task-oriented dialog data fails to demonstrate the\ndiverse expressions of user subjective preferences and recommendation acts in\nthe real-life shopping scenario. This paper introduces a new dataset SURE\n(Multimodal Recommendation Dialog with SUbjective PREference), which contains\n12K shopping dialogs in complex store scenes. The data is built in two phases\nwith human annotations to ensure quality and diversity. SURE is well-annotated\nwith subjective preferences and recommendation acts proposed by sales experts.\nA comprehensive analysis is given to reveal the distinguishing features of\nSURE. Three benchmark tasks are then proposed on the data to evaluate the\ncapability of multimodal recommendation agents. Based on the SURE, we propose a\nbaseline model, powered by a state-of-the-art multimodal model, for these\ntasks.",
                "Enhancing Human Capabilities through Symbiotic Artificial Intelligence\n  with Shared Sensory Experiences\nThe merging of human intelligence and artificial intelligence has long been a\nsubject of interest in both science fiction and academia. In this paper, we\nintroduce a novel concept in Human-AI interaction called Symbiotic Artificial\nIntelligence with Shared Sensory Experiences (SAISSE), which aims to establish\na mutually beneficial relationship between AI systems and human users through\nshared sensory experiences. By integrating multiple sensory input channels and\nprocessing human experiences, SAISSE fosters a strong human-AI bond, enabling\nAI systems to learn from and adapt to individual users, providing personalized\nsupport, assistance, and enhancement. Furthermore, we discuss the incorporation\nof memory storage units for long-term growth and development of both the AI\nsystem and its human user. As we address user privacy and ethical guidelines\nfor responsible AI-human symbiosis, we also explore potential biases and\ninequalities in AI-human symbiosis and propose strategies to mitigate these\nchallenges. Our research aims to provide a comprehensive understanding of the\nSAISSE concept and its potential to effectively support and enhance individual\nhuman users through symbiotic AI systems. This position article aims at\ndiscussing poteintial AI-human interaction related topics within the scientific\ncommunity, rather than providing experimental or theoretical results.",
                "Explainability Techniques for Chemical Language Models\nExplainability techniques are crucial in gaining insights into the reasons\nbehind the predictions of deep learning models, which have not yet been applied\nto chemical language models. We propose an explainable AI technique that\nattributes the importance of individual atoms towards the predictions made by\nthese models. Our method backpropagates the relevance information towards the\nchemical input string and visualizes the importance of individual atoms. We\nfocus on self-attention Transformers operating on molecular string\nrepresentations and leverage a pretrained encoder for finetuning. We showcase\nthe method by predicting and visualizing solubility in water and organic\nsolvents. We achieve competitive model performance while obtaining\ninterpretable predictions, which we use to inspect the pretrained model.",
                "Mitigating Exploitation Bias in Learning to Rank with an\n  Uncertainty-aware Empirical Bayes Approach\nRanking is at the core of many artificial intelligence (AI) applications,\nincluding search engines, recommender systems, etc. Modern ranking systems are\noften constructed with learning-to-rank (LTR) models built from user behavior\nsignals. While previous studies have demonstrated the effectiveness of using\nuser behavior signals (e.g., clicks) as both features and labels of LTR\nalgorithms, we argue that existing LTR algorithms that indiscriminately treat\nbehavior and non-behavior signals in input features could lead to suboptimal\nperformance in practice. Particularly because user behavior signals often have\nstrong correlations with the ranking objective and can only be collected on\nitems that have already been shown to users, directly using behavior signals in\nLTR could create an exploitation bias that hurts the system performance in the\nlong run.\n  To address the exploitation bias, we propose EBRank, an empirical Bayes-based\nuncertainty-aware ranking algorithm. Specifically, to overcome exploitation\nbias brought by behavior features in ranking models, EBRank uses a sole\nnon-behavior feature based prior model to get a prior estimation of relevance.\nIn the dynamic training and serving of ranking systems, EBRank uses the\nobserved user behaviors to update posterior relevance estimation instead of\nconcatenating behaviors as features in ranking models. Besides, EBRank\nadditionally applies an uncertainty-aware exploration strategy to explore\nactively, collect user behaviors for empirical Bayesian modeling and improve\nranking performance. Experiments on three public datasets show that EBRank is\neffective, practical and significantly outperforms state-of-the-art ranking\nalgorithms.",
                "Multiview Identifiers Enhanced Generative Retrieval\nInstead of simply matching a query to pre-existing passages, generative\nretrieval generates identifier strings of passages as the retrieval target. At\na cost, the identifier must be distinctive enough to represent a passage.\nCurrent approaches use either a numeric ID or a text piece (such as a title or\nsubstrings) as the identifier. However, these identifiers cannot cover a\npassage's content well. As such, we are motivated to propose a new type of\nidentifier, synthetic identifiers, that are generated based on the content of a\npassage and could integrate contextualized information that text pieces lack.\nFurthermore, we simultaneously consider multiview identifiers, including\nsynthetic identifiers, titles, and substrings. These views of identifiers\ncomplement each other and facilitate the holistic ranking of passages from\nmultiple perspectives. We conduct a series of experiments on three public\ndatasets, and the results indicate that our proposed approach performs the best\nin generative retrieval, demonstrating its effectiveness and robustness.",
                "GenQ: Automated Question Generation to Support Caregivers While Reading\n  Stories with Children\nWhen caregivers ask open--ended questions to motivate dialogue with children,\nit facilitates the child's reading comprehension skills.Although there is scope\nfor use of technological tools, referred here as \"intelligent tutoring\nsystems\", to scaffold this process, it is currently unclear whether existing\nintelligent systems that generate human--language like questions is beneficial.\nAdditionally, training data used in the development of these automated question\ngeneration systems is typically sourced without attention to demographics, but\npeople with different cultural backgrounds may ask different questions. As a\npart of a broader project to design an intelligent reading support app for\nLatinx children, we crowdsourced questions from Latinx caregivers and\nnoncaregivers as well as caregivers and noncaregivers from other demographics.\nWe examine variations in question--asking within this dataset mediated by\nindividual, cultural, and contextual factors. We then design a system that\nautomatically extracts templates from this data to generate open--ended\nquestions that are representative of those asked by Latinx caregivers."
            ],
            "interesting paper": 1
        },
        {
            "papers": [
                "How Do UX Practitioners Communicate AI as a Design Material? Artifacts,\n  Conceptions, and Propositions\nUX practitioners (UXPs) face novel challenges when working with and\ncommunicating artificial intelligence (AI) as a design material. We explore how\nUXPs communicate AI concepts when given hands-on experience training and\nexperimenting with AI models. To do so, we conducted a task-based design study\nwith 27 UXPs in which they prototyped and created a design presentation for a\nAI-enabled interface while having access to a simple AI model training tool.\nThrough analyzing UXPs' design presentations and post-activity interviews, we\nfound that although UXPs struggled to clearly communicate some AI concepts,\ntinkering with AI broadened common ground when communicating with technical\nstakeholders. UXPs also identified key risks and benefits of AI in their\ndesigns, and proposed concrete next steps for both UX and AI work. We conclude\nwith a sensitizing concept and recommendations for design and AI tools to\nenhance multi-stakeholder communication and collaboration when crafting\nhuman-centered AI experiences.",
                "Towards Explainable Conversational Recommender Systems\nExplanations in conventional recommender systems have demonstrated benefits\nin helping the user understand the rationality of the recommendations and\nimproving the system's efficiency, transparency, and trustworthiness. In the\nconversational environment, multiple contextualized explanations need to be\ngenerated, which poses further challenges for explanations. To better measure\nexplainability in conversational recommender systems (CRS), we propose ten\nevaluation perspectives based on concepts from conventional recommender systems\ntogether with the characteristics of CRS. We assess five existing CRS benchmark\ndatasets using these metrics and observe the necessity of improving the\nexplanation quality of CRS. To achieve this, we conduct manual and automatic\napproaches to extend these dialogues and construct a new CRS dataset, namely\nExplainable Recommendation Dialogues (E-ReDial). It includes 756 dialogues with\nover 2,000 high-quality rewritten explanations. We compare two baseline\napproaches to perform explanation generation based on E-ReDial. Experimental\nresults suggest that models trained on E-ReDial can significantly improve\nexplainability while introducing knowledge into the models can further improve\nthe performance. GPT-3 in the in-context learning setting can generate more\nrealistic and diverse movie descriptions. In contrast, T5 training on E-ReDial\ncan better generate clear reasons for recommendations based on user\npreferences. E-ReDial is available at https://github.com/Superbooming/E-ReDial.",
                "Towards Cognitive Bots: Architectural Research Challenges\nSoftware bots operating in multiple virtual digital platforms must understand\nthe platforms' affordances and behave like human users. Platform affordances or\nfeatures differ from one application platform to another or through a life\ncycle, requiring such bots to be adaptable. Moreover, bots in such platforms\ncould cooperate with humans or other software agents for work or to learn\nspecific behavior patterns. However, present-day bots, particularly chatbots,\nother than language processing and prediction, are far from reaching a human\nuser's behavior level within complex business information systems. They lack\nthe cognitive capabilities to sense and act in such virtual environments,\nrendering their development a challenge to artificial general intelligence\nresearch. In this study, we problematize and investigate assumptions in\nconceptualizing software bot architecture by directing attention to significant\narchitectural research challenges in developing cognitive bots endowed with\ncomplex behavior for operation on information systems. As an outlook, we\npropose alternate architectural assumptions to consider in future bot design\nand bot development frameworks.",
                "BiomedGPT: A Generalist Vision-Language Foundation Model for Diverse\n  Biomedical Tasks\nTraditional biomedical artificial intelligence (AI) models, designed for\nspecific tasks or modalities, often exhibit limited flexibility in real-world\ndeployment and struggle to utilize holistic information. Generalist AI holds\nthe potential to address these limitations due to its versatility in\ninterpreting different data types and generating tailored outputs for diverse\nneeds. However, existing biomedical generalist AI solutions are typically\nheavyweight and closed source to researchers, practitioners, and patients.\nHere, we propose BiomedGPT, the first open-source and lightweight\nvision-language foundation model, designed as a generalist capable of\nperforming various biomedical tasks. BiomedGPT achieved state-of-the-art\nresults in 16 out of 25 experiments while maintaining a computing-friendly\nmodel scale. We also conducted human evaluations to assess the capabilities of\nBiomedGPT in radiology visual question answering, report generation, and\nsummarization. BiomedGPT exhibits robust prediction ability with a low error\nrate of 3.8% in question answering, satisfactory performance with an error rate\nof 8.3% in writing complex radiology reports, and competitive summarization\nability with a nearly equivalent preference score to human experts. Our method\ndemonstrates that effective training with diverse data can lead to more\npractical biomedical AI for improving diagnosis and workflow efficiency.",
                "Training Socially Aligned Language Models on Simulated Social\n  Interactions\nSocial alignment in AI systems aims to ensure that these models behave\naccording to established societal values. However, unlike humans, who derive\nconsensus on value judgments through social interaction, current language\nmodels (LMs) are trained to rigidly replicate their training corpus in\nisolation, leading to subpar generalization in unfamiliar scenarios and\nvulnerability to adversarial attacks. This work presents a novel training\nparadigm that permits LMs to learn from simulated social interactions. In\ncomparison to existing methodologies, our approach is considerably more\nscalable and efficient, demonstrating superior performance in alignment\nbenchmarks and human evaluations. This paradigm shift in the training of LMs\nbrings us a step closer to developing AI systems that can robustly and\naccurately reflect societal norms and values.",
                "Justification vs. Transparency: Why and How Visual Explanations in a\n  Scientific Literature Recommender System\nSignificant attention has been paid to enhancing recommender systems (RS)\nwith explanation facilities to help users make informed decisions and increase\ntrust in and satisfaction with the RS. Justification and transparency represent\ntwo crucial goals in explainable recommendation. Different from transparency,\nwhich faithfully exposes the reasoning behind the recommendation mechanism,\njustification conveys a conceptual model that may differ from that of the\nunderlying algorithm. An explanation is an answer to a question. In explainable\nrecommendation, a user would want to ask questions (referred to as\nintelligibility types) to understand results given by the RS. In this paper, we\nidentify relationships between Why and How explanation intelligibility types\nand the explanation goals of justification and transparency. We followed the\nHuman-Centered Design (HCD) approach and leveraged the What-Why-How\nvisualization framework to systematically design and implement Why and How\nvisual explanations in the transparent Recommendation and Interest Modeling\nApplication (RIMA). Furthermore, we conducted a qualitative user study (N=12)\nto investigate the potential effects of providing Why and How explanations\ntogether in an explainable RS on the users' perceptions regarding transparency,\ntrust, and satisfaction. Our study showed qualitative evidence confirming that\nthe choice of the explanation intelligibility types depends on the explanation\ngoal and user type."
            ],
            "interesting paper": 2
        },
        {
            "papers": [
                "AI Coach Assist: An Automated Approach for Call Recommendation in\n  Contact Centers for Agent Coaching\nIn recent years, the utilization of Artificial Intelligence (AI) in the\ncontact center industry is on the rise. One area where AI can have a\nsignificant impact is in the coaching of contact center agents. By analyzing\ncall transcripts using Natural Language Processing (NLP) techniques, it would\nbe possible to quickly determine which calls are most relevant for coaching\npurposes. In this paper, we present AI Coach Assist, which leverages the\npre-trained transformer-based language models to determine whether a given call\nis coachable or not based on the quality assurance (QA) questions asked by the\ncontact center managers or supervisors. The system was trained and evaluated on\na large dataset collected from real-world contact centers and provides an\neffective way to recommend calls to the contact center managers that are more\nlikely to contain coachable moments. Our experimental findings demonstrate the\npotential of AI Coach Assist to improve the coaching process, resulting in\nenhancing the performance of contact center agents.",
                "Data Minimization at Inference Time\nIn domains with high stakes such as law, recruitment, and healthcare,\nlearning models frequently rely on sensitive user data for inference,\nnecessitating the complete set of features. This not only poses significant\nprivacy risks for individuals but also demands substantial human effort from\norganizations to verify information accuracy. This paper asks whether it is\nnecessary to use \\emph{all} input features for accurate predictions at\ninference time. The paper demonstrates that, in a personalized setting,\nindividuals may only need to disclose a small subset of their features without\ncompromising decision-making accuracy. The paper also provides an efficient\nsequential algorithm to determine the appropriate attributes for each\nindividual to provide. Evaluations across various learning tasks show that\nindividuals can potentially report as little as 10\\% of their information while\nmaintaining the same accuracy level as a model that employs the full set of\nuser information.",
                "Counterfactual Formulation of Patient-Specific Root Causes of Disease\nRoot causes of disease intuitively correspond to root vertices that increase\nthe likelihood of a diagnosis. This description of a root cause nevertheless\nlacks the rigorous mathematical formulation needed for the development of\ncomputer algorithms designed to automatically detect root causes from data.\nPrior work defined patient-specific root causes of disease using an\ninterventionalist account that only climbs to the second rung of Pearl's Ladder\nof Causation. In this theoretical piece, we climb to the third rung by\nproposing a counterfactual definition matching clinical intuition based on\nfixed factual data alone. We then show how to assign a root causal contribution\nscore to each variable using Shapley values from explainable artificial\nintelligence. The proposed counterfactual formulation of patient-specific root\ncauses of disease accounts for noisy labels, adapts to disease prevalence and\nadmits fast computation without the need for counterfactual simulation.",
                "Synthesizing a Progression of Subtasks for Block-Based Visual\n  Programming Tasks\nBlock-based visual programming environments play an increasingly important\nrole in introducing computing concepts to K-12 students. In recent years, they\nhave also gained popularity in neuro-symbolic AI, serving as a benchmark to\nevaluate general problem-solving and logical reasoning skills. The open-ended\nand conceptual nature of these visual programming tasks make them challenging,\nboth for state-of-the-art AI agents as well as for novice programmers. A\nnatural approach to providing assistance for problem-solving is breaking down a\ncomplex task into a progression of simpler subtasks; however, this is not\ntrivial given that the solution codes are typically nested and have non-linear\nexecution behavior. In this paper, we formalize the problem of synthesizing\nsuch a progression for a given reference block-based visual programming task.\nWe propose a novel synthesis algorithm that generates a progression of subtasks\nthat are high-quality, well-spaced in terms of their complexity, and solving\nthis progression leads to solving the reference task. We show the utility of\nour synthesis algorithm in improving the efficacy of AI agents (in this case,\nneural program synthesizers) for solving tasks in the Karel programming\nenvironment. Then, we conduct a user study to demonstrate that our synthesized\nprogression of subtasks can assist a novice programmer in solving tasks in the\nHour of Code: Maze Challenge by Code-dot-org.",
                "Modeling Dynamic Environments with Scene Graph Memory\nEmbodied AI agents that search for objects in large environments such as\nhouseholds often need to make efficient decisions by predicting object\nlocations based on partial information. We pose this as a new type of link\nprediction problem: link prediction on partially observable dynamic graphs. Our\ngraph is a representation of a scene in which rooms and objects are nodes, and\ntheir relationships are encoded in the edges; only parts of the changing graph\nare known to the agent at each timestep. This partial observability poses a\nchallenge to existing link prediction approaches, which we address. We propose\na novel state representation -- Scene Graph Memory (SGM) -- with captures the\nagent's accumulated set of observations, as well as a neural net architecture\ncalled a Node Edge Predictor (NEP) that extracts information from the SGM to\nsearch efficiently. We evaluate our method in the Dynamic House Simulator, a\nnew benchmark that creates diverse dynamic graphs following the semantic\npatterns typically seen at homes, and show that NEP can be trained to predict\nthe locations of objects in a variety of environments with diverse object\nmovement dynamics, outperforming baselines both in terms of new scene\nadaptability and overall accuracy. The codebase and more can be found at\nhttps://www.scenegraphmemory.com.",
                "The Curse of Recursion: Training on Generated Data Makes Models Forget\nStable Diffusion revolutionised image creation from descriptive text. GPT-2,\nGPT-3(.5) and GPT-4 demonstrated astonishing performance across a variety of\nlanguage tasks. ChatGPT introduced such language models to the general public.\nIt is now clear that large language models (LLMs) are here to stay, and will\nbring about drastic change in the whole ecosystem of online text and images. In\nthis paper we consider what the future might hold. What will happen to GPT-{n}\nonce LLMs contribute much of the language found online? We find that use of\nmodel-generated content in training causes irreversible defects in the\nresulting models, where tails of the original content distribution disappear.\nWe refer to this effect as Model Collapse and show that it can occur in\nVariational Autoencoders, Gaussian Mixture Models and LLMs. We build\ntheoretical intuition behind the phenomenon and portray its ubiquity amongst\nall learned generative models. We demonstrate that it has to be taken seriously\nif we are to sustain the benefits of training from large-scale data scraped\nfrom the web. Indeed, the value of data collected about genuine human\ninteractions with systems will be increasingly valuable in the presence of\ncontent generated by LLMs in data crawled from the Internet."
            ],
            "interesting paper": 2
        },
        {
            "papers": [
                "Ask an Expert: Leveraging Language Models to Improve Strategic Reasoning\n  in Goal-Oriented Dialogue Models\nExisting dialogue models may encounter scenarios which are not\nwell-represented in the training data, and as a result generate responses that\nare unnatural, inappropriate, or unhelpful. We propose the \"Ask an Expert\"\nframework in which the model is trained with access to an \"expert\" which it can\nconsult at each turn. Advice is solicited via a structured dialogue with the\nexpert, and the model is optimized to selectively utilize (or ignore) it given\nthe context and dialogue history. In this work the expert takes the form of an\nLLM. We evaluate this framework in a mental health support domain, where the\nstructure of the expert conversation is outlined by pre-specified prompts which\nreflect a reasoning strategy taught to practitioners in the field. Blenderbot\nmodels utilizing \"Ask an Expert\" show quality improvements across all expert\nsizes, including those with fewer parameters than the dialogue model itself.\nOur best model provides a $\\sim 10\\%$ improvement over baselines, approaching\nhuman-level scores on \"engingingness\" and \"helpfulness\" metrics.",
                "Taming AI Bots: Controllability of Neural States in Large Language\n  Models\nWe tackle the question of whether an agent can, by suitable choice of\nprompts, control an AI bot to any state. To that end, we first introduce a\nformal definition of ``meaning'' that is amenable to analysis. Then, we\ncharacterize ``meaningful data'' on which large language models (LLMs) are\nostensibly trained, and ``well-trained LLMs'' through conditions that are\nlargely met by today's LLMs. While a well-trained LLM constructs an embedding\nspace of meanings that is Euclidean, meanings themselves do not form a vector\n(linear) subspace, but rather a quotient space within. We then characterize the\nsubset of meanings that can be reached by the state of the LLMs for some input\nprompt, and show that a well-trained bot can reach any meaning albeit with\nsmall probability. We then introduce a stronger notion of controllability as\n{\\em almost certain reachability}, and show that, when restricted to the space\nof meanings, an AI bot is controllable. We do so after introducing a functional\ncharacterization of attentive AI bots, and finally derive necessary and\nsufficient conditions for controllability. The fact that AI bots are\ncontrollable means that an adversary could steer them towards any state.\nHowever, the sampling process can be designed to counteract adverse actions and\navoid reaching undesirable regions of state space before their boundary is\ncrossed.",
                "Re-imagining health and well-being in low resource African settings\n  using an augmented AI system and a 3D digital twin\nThis paper discusses and explores the potential and relevance of recent\ndevelopments in artificial intelligence (AI) and digital twins for health and\nwell-being in low-resource African countries. We use the case of public health\nemergency response to disease outbreaks and epidemic control. There is\npotential to take advantage of the increasing availability of data and\ndigitization to develop advanced AI methods for analysis and prediction. Using\nan AI systems perspective, we review emerging trends in AI systems and digital\ntwins and propose an initial augmented AI system architecture to illustrate how\nan AI system can work with a 3D digital twin to address public health goals. We\nhighlight scientific knowledge discovery, continual learning, pragmatic\ninteroperability, and interactive explanation and decision-making as essential\nresearch challenges for AI systems and digital twins.",
                "Sequential Condition Evolved Interaction Knowledge Graph for Traditional\n  Chinese Medicine Recommendation\nTraditional Chinese Medicine (TCM) has a rich history of utilizing natural\nherbs to treat a diversity of illnesses. In practice, TCM diagnosis and\ntreatment are highly personalized and organically holistic, requiring\ncomprehensive consideration of the patient's state and symptoms over time.\nHowever, existing TCM recommendation approaches overlook the changes in patient\nstatus and only explore potential patterns between symptoms and prescriptions.\nIn this paper, we propose a novel Sequential Condition Evolved Interaction\nKnowledge Graph (SCEIKG), a framework that treats the model as a sequential\nprescription-making problem by considering the dynamics of the patient's\ncondition across multiple visits. In addition, we incorporate an interaction\nknowledge graph to enhance the accuracy of recommendations by considering the\ninteractions between different herbs and the patient's condition. Experimental\nresults on a real-world dataset demonstrate that our approach outperforms\nexisting TCM recommendation methods, achieving state-of-the-art performance.",
                "ContrastNER: Contrastive-based Prompt Tuning for Few-shot NER\nPrompt-based language models have produced encouraging results in numerous\napplications, including Named Entity Recognition (NER) tasks. NER aims to\nidentify entities in a sentence and provide their types. However, the strong\nperformance of most available NER approaches is heavily dependent on the design\nof discrete prompts and a verbalizer to map the model-predicted outputs to\nentity categories, which are complicated undertakings. To address these\nchallenges, we present ContrastNER, a prompt-based NER framework that employs\nboth discrete and continuous tokens in prompts and uses a contrastive learning\napproach to learn the continuous prompts and forecast entity types. The\nexperimental results demonstrate that ContrastNER obtains competitive\nperformance to the state-of-the-art NER methods in high-resource settings and\noutperforms the state-of-the-art models in low-resource circumstances without\nrequiring extensive manual prompt engineering and verbalizer design.",
                "Key-Value Transformer\nTransformers have emerged as the prevailing standard solution for various AI\ntasks, including computer vision and natural language processing. The widely\nadopted Query, Key, and Value formulation (QKV) has played a significant role\nin this. Nevertheless, no research has examined the essentiality of these three\ncomponents for transformer performance. Therefore, we conducted an evaluation\nof the key-value formulation (KV), which generates symmetric attention maps,\nalong with an asymmetric version that incorporates a 2D positional encoding\ninto the attention matrix. Remarkably, this transformer requires fewer\nparameters and computation than the original one. Through experiments\nencompassing three task types -- synthetics (such as reversing or sorting a\nlist), vision (mnist or cifar classification), and NLP (character generation\nand translation) -- we discovered that the KV transformer occasionally\noutperforms the QKV transformer. However, it also exhibits instances of\nunderperformance compared to QKV, making it challenging to draw a definitive\nconclusion. Nonetheless, we consider the reported results to be encouraging and\nanticipate that they may pave the way for more efficient transformers in the\nfuture."
            ],
            "interesting paper": 1
        },
        {
            "papers": [
                "Partially Personalized Federated Learning: Breaking the Curse of Data\n  Heterogeneity\nWe present a partially personalized formulation of Federated Learning (FL)\nthat strikes a balance between the flexibility of personalization and\ncooperativeness of global training. In our framework, we split the variables\ninto global parameters, which are shared across all clients, and individual\nlocal parameters, which are kept private. We prove that under the right split\nof parameters, it is possible to find global parameters that allow each client\nto fit their data perfectly, and refer to the obtained problem as\noverpersonalized. For instance, the shared global parameters can be used to\nlearn good data representations, whereas the personalized layers are fine-tuned\nfor a specific client. Moreover, we present a simple algorithm for the\npartially personalized formulation that offers significant benefits to all\nclients. In particular, it breaks the curse of data heterogeneity in several\nsettings, such as training with local steps, asynchronous training, and\nByzantine-robust training.",
                "An Annotated Dataset for Explainable Interpersonal Risk Factors of\n  Mental Disturbance in Social Media Posts\nWith a surge in identifying suicidal risk and its severity in social media\nposts, we argue that a more consequential and explainable research is required\nfor optimal impact on clinical psychology practice and personalized mental\nhealthcare. The success of computational intelligence techniques for inferring\nmental illness from social media resources, points to natural language\nprocessing as a lens for determining Interpersonal Risk Factors (IRF) in human\nwritings. Motivated with limited availability of datasets for social NLP\nresearch community, we construct and release a new annotated dataset with\nhuman-labelled explanations and classification of IRF affecting mental\ndisturbance on social media: (i) Thwarted Belongingness (TBe), and (ii)\nPerceived Burdensomeness (PBu). We establish baseline models on our dataset\nfacilitating future research directions to develop real-time personalized AI\nmodels by detecting patterns of TBe and PBu in emotional spectrum of user's\nhistorical social media profile.",
                "Contextual Bandits with Budgeted Information Reveal\nContextual bandit algorithms are commonly used in digital health to recommend\npersonalized treatments. However, to ensure the effectiveness of the\ntreatments, patients are often requested to take actions that have no immediate\nbenefit to them, which we refer to as pro-treatment actions. In practice,\nclinicians have a limited budget to encourage patients to take these actions\nand collect additional information. We introduce a novel optimization and\nlearning algorithm to address this problem. This algorithm effectively combines\nthe strengths of two algorithmic approaches in a seamless manner, including 1)\nan online primal-dual algorithm for deciding the optimal timing to reach out to\npatients, and 2) a contextual bandit learning algorithm to deliver personalized\ntreatment to the patient. We prove that this algorithm admits a sub-linear\nregret bound. We illustrate the usefulness of this algorithm on both synthetic\nand real-world data.",
                "Perceived Trustworthiness of Natural Language Generators\nNatural Language Generation tools, such as chatbots that can generate\nhuman-like conversational text, are becoming more common both for personal and\nprofessional use. However, there are concerns about their trustworthiness and\nethical implications. The paper addresses the problem of understanding how\ndifferent users (e.g., linguists, engineers) perceive and adopt these tools and\ntheir perception of machine-generated text quality. It also discusses the\nperceived advantages and limitations of Natural Language Generation tools, as\nwell as users' beliefs on governance strategies. The main findings of this\nstudy include the impact of users' field and level of expertise on the\nperceived trust and adoption of Natural Language Generation tools, the users'\nassessment of the accuracy, fluency, and potential biases of machine-generated\ntext in comparison to human-written text, and an analysis of the advantages and\nethical risks associated with these tools as identified by the participants.\nMoreover, this paper discusses the potential implications of these findings for\nenhancing the AI development process. The paper sheds light on how different\nuser characteristics shape their beliefs on the quality and overall\ntrustworthiness of machine-generated text. Furthermore, it examines the\nbenefits and risks of these tools from the perspectives of different users.",
                "Code Prompting: a Neural Symbolic Method for Complex Reasoning in Large\n  Language Models\nLarge language models (LLMs) have scaled up to unlock a wide range of complex\nreasoning tasks with the aid of various prompting methods. However, current\nprompting methods generate natural language intermediate steps to help\nreasoning, which can cause imperfect task reduction and confusion. To mitigate\nsuch limitations, we explore code prompting, a neural symbolic prompting method\nwith both zero-shot and few-shot versions which triggers code as intermediate\nsteps. We conduct experiments on 7 widely-used benchmarks involving symbolic\nreasoning and arithmetic reasoning. Code prompting generally outperforms\nchain-of-thought (CoT) prompting. To further understand the performance and\nlimitations of code prompting, we perform extensive ablation studies and error\nanalyses, and identify several exclusive advantages of using symbolic\npromptings compared to natural language. We also consider the ensemble of code\nprompting and CoT prompting to combine the strengths of both. Finally, we show\nthrough experiments how code annotations and their locations affect code\nprompting.",
                "Shapley Based Residual Decomposition for Instance Analysis\nIn this paper, we introduce the idea of decomposing the residuals of\nregression with respect to the data instances instead of features. This allows\nus to determine the effects of each individual instance on the model and each\nother, and in doing so makes for a model-agnostic method of identifying\ninstances of interest. In doing so, we can also determine the appropriateness\nof the model and data in the wider context of a given study. The paper focuses\non the possible applications that such a framework brings to the relatively\nunexplored field of instance analysis in the context of Explainable AI tasks."
            ],
            "interesting paper": 1
        },
        {
            "papers": [
                "Less Likely Brainstorming: Using Language Models to Generate Alternative\n  Hypotheses\nA human decision-maker benefits the most from an AI assistant that corrects\nfor their biases. For problems such as generating interpretation of a radiology\nreport given findings, a system predicting only highly likely outcomes may be\nless useful, where such outcomes are already obvious to the user. To alleviate\nbiases in human decision-making, it is worth considering a broad differential\ndiagnosis, going beyond the most likely options. We introduce a new task, \"less\nlikely brainstorming,\" that asks a model to generate outputs that humans think\nare relevant but less likely to happen. We explore the task in two settings: a\nbrain MRI interpretation generation setting and an everyday commonsense\nreasoning setting. We found that a baseline approach of training with less\nlikely hypotheses as targets generates outputs that humans evaluate as either\nlikely or irrelevant nearly half of the time; standard MLE training is not\neffective. To tackle this problem, we propose a controlled text generation\nmethod that uses a novel contrastive learning strategy to encourage models to\ndifferentiate between generating likely and less likely outputs according to\nhumans. We compare our method with several state-of-the-art controlled text\ngeneration models via automatic and human evaluations and show that our models'\ncapability of generating less likely outputs is improved.",
                "Conceptual Design Generation Using Large Language Models\nConcept generation is a creative step in the conceptual design phase, where\ndesigners often turn to brainstorming, mindmapping, or crowdsourcing design\nideas to complement their own knowledge of the domain. Recent advances in\nnatural language processing (NLP) and machine learning (ML) have led to the\nrise of Large Language Models (LLMs) capable of generating seemingly creative\noutputs from textual prompts. The success of these models has led to their\nintegration and application across a variety of domains, including art,\nentertainment, and other creative work. In this paper, we leverage LLMs to\ngenerate solutions for a set of 12 design problems and compare them to a\nbaseline of crowdsourced solutions. We evaluate the differences between\ngenerated and crowdsourced design solutions through multiple perspectives,\nincluding human expert evaluations and computational metrics. Expert\nevaluations indicate that the LLM-generated solutions have higher average\nfeasibility and usefulness while the crowdsourced solutions have more novelty.\nWe experiment with prompt engineering and find that leveraging few-shot\nlearning can lead to the generation of solutions that are more similar to the\ncrowdsourced solutions. These findings provide insight into the quality of\ndesign solutions generated with LLMs and begins to evaluate prompt engineering\ntechniques that could be leveraged by practitioners to generate higher-quality\ndesign solutions synergistically with LLMs.",
                "Probabilistic computation and uncertainty quantification with emerging\n  covariance\nBuilding robust, interpretable, and secure AI system requires quantifying and\nrepresenting uncertainty under a probabilistic perspective to mimic human\ncognitive abilities. However, probabilistic computation presents significant\nchallenges for most conventional artificial neural network, as they are\nessentially implemented in a deterministic manner. In this paper, we develop an\nefficient probabilistic computation framework by truncating the probabilistic\nrepresentation of neural activation up to its mean and covariance and construct\na moment neural network that encapsulates the nonlinear coupling between the\nmean and covariance of the underlying stochastic network. We reveal that when\nonly the mean but not the covariance is supervised during gradient-based\nlearning, the unsupervised covariance spontaneously emerges from its nonlinear\ncoupling with the mean and faithfully captures the uncertainty associated with\nmodel predictions. Our findings highlight the inherent simplicity of\nprobabilistic computation by seamlessly incorporating uncertainty into model\nprediction, paving the way for integrating it into large-scale AI systems.",
                "Intent-aligned AI systems deplete human agency: the need for agency\n  foundations research in AI safety\nThe rapid advancement of artificial intelligence (AI) systems suggests that\nartificial general intelligence (AGI) systems may soon arrive. Many researchers\nare concerned that AIs and AGIs will harm humans via intentional misuse\n(AI-misuse) or through accidents (AI-accidents). In respect of AI-accidents,\nthere is an increasing effort focused on developing algorithms and paradigms\nthat ensure AI systems are aligned to what humans intend, e.g. AI systems that\nyield actions or recommendations that humans might judge as consistent with\ntheir intentions and goals. Here we argue that alignment to human intent is\ninsufficient for safe AI systems and that preservation of long-term agency of\nhumans may be a more robust standard, and one that needs to be separated\nexplicitly and a priori during optimization. We argue that AI systems can\nreshape human intention and discuss the lack of biological and psychological\nmechanisms that protect humans from loss of agency. We provide the first formal\ndefinition of agency-preserving AI-human interactions which focuses on\nforward-looking agency evaluations and argue that AI systems - not humans -\nmust be increasingly tasked with making these evaluations. We show how agency\nloss can occur in simple environments containing embedded agents that use\ntemporal-difference learning to make action recommendations. Finally, we\npropose a new area of research called \"agency foundations\" and pose four\ninitial topics designed to improve our understanding of agency in AI-human\ninteractions: benevolent game theory, algorithmic foundations of human rights,\nmechanistic interpretability of agency representation in neural-networks and\nreinforcement learning from internal states.",
                "Unsupervised Melody-to-Lyric Generation\nAutomatic melody-to-lyric generation is a task in which song lyrics are\ngenerated to go with a given melody. It is of significant practical interest\nand more challenging than unconstrained lyric generation as the music imposes\nadditional constraints onto the lyrics. The training data is limited as most\nsongs are copyrighted, resulting in models that underfit the complicated\ncross-modal relationship between melody and lyrics. In this work, we propose a\nmethod for generating high-quality lyrics without training on any aligned\nmelody-lyric data. Specifically, we design a hierarchical lyric generation\nframework that first generates a song outline and second the complete lyrics.\nThe framework enables disentanglement of training (based purely on text) from\ninference (melody-guided text generation) to circumvent the shortage of\nparallel data.\n  We leverage the segmentation and rhythm alignment between melody and lyrics\nto compile the given melody into decoding constraints as guidance during\ninference. The two-step hierarchical design also enables content control via\nthe lyric outline, a much-desired feature for democratizing collaborative song\ncreation. Experimental results show that our model can generate high-quality\nlyrics that are more on-topic, singable, intelligible, and coherent than strong\nbaselines, for example SongMASS, a SOTA model trained on a parallel dataset,\nwith a 24% relative overall quality improvement based on human ratings.",
                "Towards Explainable and Language-Agnostic LLMs: Symbolic Reverse\n  Engineering of Language at Scale\nLarge language models (LLMs) have achieved a milestone that undenia-bly\nchanged many held beliefs in artificial intelligence (AI). However, there\nremains many limitations of these LLMs when it comes to true language\nunderstanding, limitations that are a byproduct of the under-lying architecture\nof deep neural networks. Moreover, and due to their subsymbolic nature,\nwhatever knowledge these models acquire about how language works will always be\nburied in billions of microfeatures (weights), none of which is meaningful on\nits own, making such models hopelessly unexplainable. To address these\nlimitations, we suggest com-bining the strength of symbolic representations\nwith what we believe to be the key to the success of LLMs, namely a successful\nbottom-up re-verse engineering of language at scale. As such we argue for a\nbottom-up reverse engineering of language in a symbolic setting. Hints on what\nthis project amounts to have been suggested by several authors, and we discuss\nin some detail here how this project could be accomplished."
            ],
            "interesting paper": 2
        },
        {
            "papers": [
                "Decision-Oriented Dialogue for Human-AI Collaboration\nWe describe a class of tasks called decision-oriented dialogues, in which AI\nassistants such as large language models (LMs) must collaborate with one or\nmore humans via natural language to help them make complex decisions. We\nformalize three domains in which users face everyday decisions: (1) choosing an\nassignment of reviewers to conference papers, (2) planning a multi-step\nitinerary in a city, and (3) negotiating travel plans for a group of friends.\nIn each of these settings, AI assistants and users have disparate abilities\nthat they must combine to arrive at the best decision: assistants can access\nand process large amounts of information, while users have preferences and\nconstraints external to the system. For each task, we build a dialogue\nenvironment where agents receive a reward based on the quality of the final\ndecision they reach. We evaluate LMs in self-play and in collaboration with\nhumans and find that they fall short compared to human assistants, achieving\nmuch lower rewards despite engaging in longer dialogues. We highlight a number\nof challenges models face in decision-oriented dialogues, ranging from\ngoal-directed behavior to reasoning and optimization, and release our\nenvironments as a testbed for future work.",
                "Speaking the Language of Your Listener: Audience-Aware Adaptation via\n  Plug-and-Play Theory of Mind\nDialogue participants may have varying levels of knowledge about the topic\nunder discussion. In such cases, it is essential for speakers to adapt their\nutterances by taking their audience into account. Yet, it is an open question\nhow such adaptation can be modelled in computational agents. In this paper, we\nmodel a visually grounded referential game between a knowledgeable speaker and\na listener with more limited visual and linguistic experience. Inspired by\npsycholinguistic theories, we endow our speaker with the ability to adapt its\nreferring expressions via a simulation module that monitors the effectiveness\nof planned utterances from the listener's perspective. We propose an adaptation\nmechanism building on plug-and-play approaches to controlled language\ngeneration, where utterance generation is steered on the fly by the simulator\nwithout finetuning the speaker's underlying language model. Our results and\nanalyses show that our approach is effective: the speaker's utterances become\ncloser to the listener's domain of expertise, which leads to higher\ncommunicative success.",
                "TransAct: Transformer-based Realtime User Action Model for\n  Recommendation at Pinterest\nSequential models that encode user activity for next action prediction have\nbecome a popular design choice for building web-scale personalized\nrecommendation systems. Traditional methods of sequential recommendation either\nutilize end-to-end learning on realtime user actions, or learn user\nrepresentations separately in an offline batch-generated manner. This paper (1)\npresents Pinterest's ranking architecture for Homefeed, our personalized\nrecommendation product and the largest engagement surface; (2) proposes\nTransAct, a sequential model that extracts users' short-term preferences from\ntheir realtime activities; (3) describes our hybrid approach to ranking, which\ncombines end-to-end sequential modeling via TransAct with batch-generated user\nembeddings. The hybrid approach allows us to combine the advantages of\nresponsiveness from learning directly on realtime user activity with the\ncost-effectiveness of batch user representations learned over a longer time\nperiod. We describe the results of ablation studies, the challenges we faced\nduring productionization, and the outcome of an online A/B experiment, which\nvalidates the effectiveness of our hybrid ranking model. We further demonstrate\nthe effectiveness of TransAct on other surfaces such as contextual\nrecommendations and search. Our model has been deployed to production in\nHomefeed, Related Pins, Notifications, and Search at Pinterest.",
                "AI Imagery and the Overton Window\nAI-based text-to-image generation has undergone a significant leap in the\nproduction of visually comprehensive and aesthetic imagery over the past year,\nto the point where differentiating between a man-made piece of art and an\nAI-generated image is becoming more difficult. Generative Models such as Stable\nDiffusion, Midjourney and others are expected to affect several major\nindustries in technological and ethical aspects. Striking the balance between\nraising human standard of life and work vs exploiting one group of people to\nenrich another is a complex and crucial part of the discussion. Due to the\nrapid growth of this technology, the way in which its models operate, and gray\narea legalities, visual and artistic domains - including the video game\nindustry, are at risk of being taken over from creators by AI infrastructure\nowners. This paper is a literature review examining the concerns facing both AI\ndevelopers and users today, including identity theft, data laundering and more.\nIt discusses legalization challenges and ethical concerns, and concludes with\nhow AI generative models can be tremendously useful in streamlining the process\nof visual creativity in both static and interactive media given proper\nregulation.\n  Keywords: AI text-to-image generation, Midjourney, Stable Diffusion, AI\nEthics, Game Design, Digital Art, Data Laundering",
                "Human Control: Definitions and Algorithms\nHow can humans stay in control of advanced artificial intelligence systems?\nOne proposal is corrigibility, which requires the agent to follow the\ninstructions of a human overseer, without inappropriately influencing them. In\nthis paper, we formally define a variant of corrigibility called shutdown\ninstructability, and show that it implies appropriate shutdown behavior,\nretention of human autonomy, and avoidance of user harm. We also analyse the\nrelated concepts of non-obstruction and shutdown alignment, three previously\nproposed algorithms for human control, and one new algorithm.",
                "Enrichment of the NLST and NSCLC-Radiomics computed tomography\n  collections with AI-derived annotations\nPublic imaging datasets are critical for the development and evaluation of\nautomated tools in cancer imaging. Unfortunately, many do not include\nannotations or image-derived features, complicating their downstream analysis.\nArtificial intelligence-based annotation tools have been shown to achieve\nacceptable performance and thus can be used to automatically annotate large\ndatasets. As part of the effort to enrich public data available within NCI\nImaging Data Commons (IDC), here we introduce AI-generated annotations for two\ncollections of computed tomography images of the chest, NSCLC-Radiomics, and\nthe National Lung Screening Trial. Using publicly available AI algorithms we\nderived volumetric annotations of thoracic organs at risk, their corresponding\nradiomics features, and slice-level annotations of anatomical landmarks and\nregions. The resulting annotations are publicly available within IDC, where the\nDICOM format is used to harmonize the data and achieve FAIR principles. The\nannotations are accompanied by cloud-enabled notebooks demonstrating their use.\nThis study reinforces the need for large, publicly accessible curated datasets\nand demonstrates how AI can be used to aid in cancer imaging."
            ],
            "interesting paper": 3
        },
        {
            "papers": [
                "AI Liability Insurance With an Example in AI-Powered E-diagnosis System\nArtificial Intelligence (AI) has received an increasing amount of attention\nin multiple areas. The uncertainties and risks in AI-powered systems have\ncreated reluctance in their wild adoption. As an economic solution to\ncompensate for potential damages, AI liability insurance is a promising market\nto enhance the integration of AI into daily life. In this work, we use an\nAI-powered E-diagnosis system as an example to study AI liability insurance. We\nprovide a quantitative risk assessment model with evidence-based numerical\nanalysis. We discuss the insurability criteria for AI technologies and suggest\nnecessary adjustments to accommodate the features of AI products. We show that\nAI liability insurance can act as a regulatory mechanism to incentivize\ncompliant behaviors and serve as a certificate of high-quality AI systems.\nFurthermore, we suggest premium adjustment to reflect the dynamic evolution of\nthe inherent uncertainty in AI. Moral hazard problems are discussed and\nsuggestions for AI liability insurance are provided.",
                "Self Contrastive Learning for Session-based Recommendation\nSession-based recommendation, which aims to predict the next item of users'\ninterest as per an existing sequence interaction of items, has attracted\ngrowing applications of Contrastive Learning (CL) with improved user and item\nrepresentations. However, these contrastive objectives: (1) serve a similar\nrole as the cross-entropy loss while ignoring the item representation space\noptimisation; and (2) commonly require complicated modelling, including complex\npositive/negative sample constructions and extra data augmentation. In this\nwork, we introduce Self-Contrastive Learning (SCL), which simplifies the\napplication of CL and enhances the performance of state-of-the-art CL-based\nrecommendation techniques. Specifically, SCL is formulated as an objective\nfunction that directly promotes a uniform distribution among item\nrepresentations and efficiently replaces all the existing contrastive objective\ncomponents of state-of-the-art models. Unlike previous works, SCL eliminates\nthe need for any positive/negative sample construction or data augmentation,\nleading to enhanced interpretability of the item representation space and\nfacilitating its extensibility to existing recommender systems. Through\nexperiments on three benchmark datasets, we demonstrate that SCL consistently\nimproves the performance of state-of-the-art models with statistical\nsignificance. Notably, our experiments show that SCL improves the performance\nof two best-performing models by 8.2% and 9.5% in P@10 (Precision) and 9.9% and\n11.2% in MRR@10 (Mean Reciprocal Rank) on average across different benchmarks.\nAdditionally, our analysis elucidates the improvement in terms of alignment and\nuniformity of representations, as well as the effectiveness of SCL with a low\ncomputational cost.",
                "Cook-Gen: Robust Generative Modeling of Cooking Actions from Recipes\nAs people become more aware of their food choices, food computation models\nhave become increasingly popular in assisting people in maintaining healthy\neating habits. For example, food recommendation systems analyze recipe\ninstructions to assess nutritional contents and provide recipe recommendations.\nThe recent and remarkable successes of generative AI methods, such as\nauto-regressive large language models, can lead to robust methods for a more\ncomprehensive understanding of recipes for healthy food recommendations beyond\nsurface-level nutrition content assessments. In this study, we explore the use\nof generative AI methods to extend current food computation models, primarily\ninvolving the analysis of nutrition and ingredients, to also incorporate\ncooking actions (e.g., add salt, fry the meat, boil the vegetables, etc.).\nCooking actions are notoriously hard to model using statistical learning\nmethods due to irregular data patterns - significantly varying natural language\ndescriptions for the same action (e.g., marinate the meat vs. marinate the meat\nand leave overnight) and infrequently occurring patterns (e.g., add salt occurs\nfar more frequently than marinating the meat). The prototypical approach to\nhandling irregular data patterns is to increase the volume of data that the\nmodel ingests by orders of magnitude. Unfortunately, in the cooking domain,\nthese problems are further compounded with larger data volumes presenting a\nunique challenge that is not easily handled by simply scaling up. In this work,\nwe propose novel aggregation-based generative AI methods, Cook-Gen, that\nreliably generate cooking actions from recipes, despite difficulties with\nirregular data patterns, while also outperforming Large Language Models and\nother strong baselines.",
                "ViCo: Plug-and-play Visual Condition for Personalized Text-to-image\n  Generation\nPersonalized text-to-image generation using diffusion models has recently\nemerged and garnered significant interest. This task learns a novel concept\n(e.g., a unique toy), illustrated in a handful of images, into a generative\nmodel that captures fine visual details and generates photorealistic images\nbased on textual embeddings. In this paper, we present ViCo, a novel\nlightweight plug-and-play method that seamlessly integrates visual condition\ninto personalized text-to-image generation. ViCo stands out for its unique\nfeature of not requiring any fine-tuning of the original diffusion model\nparameters, thereby facilitating more flexible and scalable model deployment.\nThis key advantage distinguishes ViCo from most existing models that\nnecessitate partial or full diffusion fine-tuning. ViCo incorporates an image\nattention module that conditions the diffusion process on patch-wise visual\nsemantics, and an attention-based object mask that comes at no extra cost from\nthe attention module. Despite only requiring light parameter training (~6%\ncompared to the diffusion U-Net), ViCo delivers performance that is on par\nwith, or even surpasses, all state-of-the-art models, both qualitatively and\nquantitatively. This underscores the efficacy of ViCo, making it a highly\npromising solution for personalized text-to-image generation without the need\nfor diffusion model fine-tuning. Code: https://github.com/haoosz/ViCo",
                "KL-Divergence Guided Temperature Sampling\nTemperature sampling is a conventional approach to diversify large language\nmodel predictions. As temperature increases, the prediction becomes diverse but\nalso vulnerable to hallucinations -- generating tokens that are sensible but\nnot factual. One common approach to mitigate hallucinations is to provide\nsource/grounding documents and the model is trained to produce predictions that\nbind to and are attributable to the provided source. It appears that there is a\ntrade-off between diversity and attribution. To mitigate any such trade-off, we\npropose to relax the constraint of having a fixed temperature over decoding\nsteps, and a mechanism to guide the dynamic temperature according to its\nrelevance to the source through KL-divergence. Our experiments justifies the\ntrade-off, and shows that our sampling algorithm outperforms the conventional\ntop-k and top-p algorithms in conversational question-answering and\nsummarization tasks.",
                "Modeling and Analyzing Scorer Preferences in Short-Answer Math Questions\nAutomated scoring of student responses to open-ended questions, including\nshort-answer questions, has great potential to scale to a large number of\nresponses. Recent approaches for automated scoring rely on supervised learning,\ni.e., training classifiers or fine-tuning language models on a small number of\nresponses with human-provided score labels. However, since scoring is a\nsubjective process, these human scores are noisy and can be highly variable,\ndepending on the scorer. In this paper, we investigate a collection of models\nthat account for the individual preferences and tendencies of each human scorer\nin the automated scoring task. We apply these models to a short-answer math\nresponse dataset where each response is scored (often differently) by multiple\ndifferent human scorers. We conduct quantitative experiments to show that our\nscorer models lead to improved automated scoring accuracy. We also conduct\nquantitative experiments and case studies to analyze the individual preferences\nand tendencies of scorers. We found that scorers can be grouped into several\nobvious clusters, with each cluster having distinct features, and analyzed them\nin detail."
            ],
            "interesting paper": 2
        },
        {
            "papers": [
                "XAI Renaissance: Redefining Interpretability in Medical Diagnostic\n  Models\nAs machine learning models become increasingly prevalent in medical\ndiagnostics, the need for interpretability and transparency becomes paramount.\nThe XAI Renaissance signifies a significant shift in the field, aiming to\nredefine the interpretability of medical diagnostic models. This paper explores\nthe innovative approaches and methodologies within the realm of Explainable AI\n(XAI) that are revolutionizing the interpretability of medical diagnostic\nmodels. By shedding light on the underlying decision-making process, XAI\ntechniques empower healthcare professionals to understand, trust, and\neffectively utilize these models for accurate and reliable medical diagnoses.\nThis review highlights the key advancements in XAI for medical diagnostics and\ntheir potential to transform the healthcare landscape, ultimately improving\npatient outcomes and fostering trust in AI-driven diagnostic systems.",
                "AI Transparency in the Age of LLMs: A Human-Centered Research Roadmap\nThe rise of powerful large language models (LLMs) brings about tremendous\nopportunities for innovation but also looming risks for individuals and society\nat large. We have reached a pivotal moment for ensuring that LLMs and\nLLM-infused applications are developed and deployed responsibly. However, a\ncentral pillar of responsible AI -- transparency -- is largely missing from the\ncurrent discourse around LLMs. It is paramount to pursue new approaches to\nprovide transparency for LLMs, and years of research at the intersection of AI\nand human-computer interaction (HCI) highlight that we must do so with a\nhuman-centered perspective: Transparency is fundamentally about supporting\nappropriate human understanding, and this understanding is sought by different\nstakeholders with different goals in different contexts. In this new era of\nLLMs, we must develop and design approaches to transparency by considering the\nneeds of stakeholders in the emerging LLM ecosystem, the novel types of\nLLM-infused applications being built, and the new usage patterns and challenges\naround LLMs, all while building on lessons learned about how people process,\ninteract with, and make use of information. We reflect on the unique challenges\nthat arise in providing transparency for LLMs, along with lessons learned from\nHCI and responsible AI research that has taken a human-centered perspective on\nAI transparency. We then lay out four common approaches that the community has\ntaken to achieve transparency -- model reporting, publishing evaluation\nresults, providing explanations, and communicating uncertainty -- and call out\nopen questions around how these approaches may or may not be applied to LLMs.\nWe hope this provides a starting point for discussion and a useful roadmap for\nfuture research.",
                "ACI-BENCH: a Novel Ambient Clinical Intelligence Dataset for\n  Benchmarking Automatic Visit Note Generation\nRecent immense breakthroughs in generative models such as in GPT4 have\nprecipitated re-imagined ubiquitous usage of these models in all applications.\nOne area that can benefit by improvements in artificial intelligence (AI) is\nhealthcare. The note generation task from doctor-patient encounters, and its\nassociated electronic medical record documentation, is one of the most arduous\ntime-consuming tasks for physicians. It is also a natural prime potential\nbeneficiary to advances in generative models. However with such advances,\nbenchmarking is more critical than ever. Whether studying model weaknesses or\ndeveloping new evaluation metrics, shared open datasets are an imperative part\nof understanding the current state-of-the-art. Unfortunately as clinic\nencounter conversations are not routinely recorded and are difficult to\nethically share due to patient confidentiality, there are no sufficiently large\nclinic dialogue-note datasets to benchmark this task. Here we present the\nAmbient Clinical Intelligence Benchmark (ACI-BENCH) corpus, the largest dataset\nto date tackling the problem of AI-assisted note generation from visit\ndialogue. We also present the benchmark performances of several common\nstate-of-the-art approaches.",
                "Accelerating science with human-aware artificial intelligence\nArtificial intelligence (AI) models trained on published scientific findings\nhave been used to invent valuable materials and targeted therapies, but they\ntypically ignore the human scientists who continually alter the landscape of\ndiscovery. Here we show that incorporating the distribution of human expertise\nby training unsupervised models on simulated inferences cognitively accessible\nto experts dramatically improves (up to 400%) AI prediction of future\ndiscoveries beyond those focused on research content alone, especially when\nrelevant literature is sparse. These models succeed by predicting human\npredictions and the scientists who will make them. By tuning human-aware AI to\navoid the crowd, we can generate scientifically promising \"alien\" hypotheses\nunlikely to be imagined or pursued without intervention until the distant\nfuture, which hold promise to punctuate scientific advance beyond questions\ncurrently pursued. Accelerating human discovery or probing its blind spots,\nhuman-aware AI enables us to move toward and beyond the contemporary scientific\nfrontier.",
                "AlerTiger: Deep Learning for AI Model Health Monitoring at LinkedIn\nData-driven companies use AI models extensively to develop products and\nintelligent business solutions, making the health of these models crucial for\nbusiness success. Model monitoring and alerting in industries pose unique\nchallenges, including a lack of clear model health metrics definition, label\nsparsity, and fast model iterations that result in short-lived models and\nfeatures. As a product, there are also requirements for scalability,\ngeneralizability, and explainability. To tackle these challenges, we propose\nAlerTiger, a deep-learning-based MLOps model monitoring system that helps AI\nteams across the company monitor their AI models' health by detecting anomalies\nin models' input features and output score over time. The system consists of\nfour major steps: model statistics generation, deep-learning-based anomaly\ndetection, anomaly post-processing, and user alerting. Our solution generates\nthree categories of statistics to indicate AI model health, offers a two-stage\ndeep anomaly detection solution to address label sparsity and attain the\ngeneralizability of monitoring new models, and provides holistic reports for\nactionable alerts. This approach has been deployed to most of LinkedIn's\nproduction AI models for over a year and has identified several model issues\nthat later led to significant business metric gains after fixing.",
                "STUDY: Socially Aware Temporally Causal Decoder Recommender Systems\nRecommender systems are widely used to help people find items that are\ntailored to their interests. These interests are often influenced by social\nnetworks, making it important to use social network information effectively in\nrecommender systems. This is especially true for demographic groups with\ninterests that differ from the majority. This paper introduces STUDY, a\nSocially-aware Temporally caUsal Decoder recommender sYstem. STUDY introduces a\nnew socially-aware recommender system architecture that is significantly more\nefficient to learn and train than existing methods. STUDY performs joint\ninference over socially connected groups in a single forward pass of a modified\ntransformer decoder network. We demonstrate the benefits of STUDY in the\nrecommendation of books for students who are dyslexic, or struggling readers.\nDyslexic students often have difficulty engaging with reading material, making\nit critical to recommend books that are tailored to their interests. We worked\nwith our non-profit partner Learning Ally to evaluate STUDY on a dataset of\nstruggling readers. STUDY was able to generate recommendations that more\naccurately predicted student engagement, when compared with existing methods."
            ],
            "interesting paper": 3
        }
    ],
    "Alex Clark": [
        {
            "papers": [
                "\"What if?\" in Probabilistic Logic Programming\nA ProbLog program is a logic program with facts that only hold with a\nspecified probability. In this contribution we extend this ProbLog language by\nthe ability to answer \"What if\" queries. Intuitively, a ProbLog program defines\na distribution by solving a system of equations in terms of mutually\nindependent predefined Boolean random variables. In the theory of causality,\nJudea Pearl proposes a counterfactual reasoning for such systems of equations.\nBased on Pearl's calculus, we provide a procedure for processing these\ncounterfactual queries on ProbLog programs, together with a proof of\ncorrectness and a full implementation. Using the latter, we provide insights\ninto the influence of different parameters on the scalability of inference.\nFinally, we also show that our approach is consistent with CP-logic, i.e. with\nthe causal semantics for logic programs with annotated with disjunctions.",
                "Decomposing the Enigma: Subgoal-based Demonstration Learning for Formal\n  Theorem Proving\nLarge language models~(LLMs) present an intriguing avenue of exploration in\nthe domain of formal theorem proving. Nonetheless, the full utilization of\nthese models, particularly in terms of demonstration formatting and\norganization, remains an underexplored area. In an endeavor to enhance the\nefficacy of LLMs, we introduce a subgoal-based demonstration learning\nframework, consisting of two primary elements: Firstly, drawing upon the\ninsights of subgoal learning from the domains of reinforcement learning and\nrobotics, we propose the construction of distinct subgoals for each\ndemonstration example and refine these subgoals in accordance with the\npertinent theories of subgoal learning. Secondly, we build upon recent advances\nin diffusion models to predict the optimal organization, simultaneously\naddressing two intricate issues that persist within the domain of demonstration\norganization: subset selection and order determination. Through the integration\nof subgoal-based learning methodologies, we have successfully increased the\nprevailing proof accuracy from 38.9\\% to 44.3\\% on the miniF2F benchmark.\nFurthermore, the adoption of diffusion models for demonstration organization\ncan lead to an additional enhancement in accuracy to 45.5\\%, or a $5\\times$\nimprovement in sampling efficiency compared with the long-standing\nstate-of-the-art method. Our code is available at\n\\url{https://github.com/HKUNLP/subgoal-theorem-prover}.",
                "Symbolic Regression via Control Variable Genetic Programming\nLearning symbolic expressions directly from experiment data is a vital step\nin AI-driven scientific discovery. Nevertheless, state-of-the-art approaches\nare limited to learning simple expressions. Regressing expressions involving\nmany independent variables still remain out of reach. Motivated by the control\nvariable experiments widely utilized in science, we propose Control Variable\nGenetic Programming (CVGP) for symbolic regression over many independent\nvariables. CVGP expedites symbolic expression discovery via customized\nexperiment design, rather than learning from a fixed dataset collected a\npriori. CVGP starts by fitting simple expressions involving a small set of\nindependent variables using genetic programming, under controlled experiments\nwhere other variables are held as constants. It then extends expressions\nlearned in previous generations by adding new independent variables, using new\ncontrol variable experiments in which these variables are allowed to vary.\nTheoretically, we show CVGP as an incremental building approach can yield an\nexponential reduction in the search space when learning a class of expressions.\nExperimentally, CVGP outperforms several baselines in learning symbolic\nexpressions involving multiple independent variables.",
                "SPRING: Studying the Paper and Reasoning to Play Games\nOpen-world survival games pose significant challenges for AI algorithms due\nto their multi-tasking, deep exploration, and goal prioritization requirements.\nDespite reinforcement learning (RL) being popular for solving games, its high\nsample complexity limits its effectiveness in complex open-world games like\nCrafter or Minecraft. We propose a novel approach, SPRING, to read the game's\noriginal academic paper and use the knowledge learned to reason and play the\ngame through a large language model (LLM). Prompted with the LaTeX source as\ngame context and a description of the agent's current observation, our SPRING\nframework employs a directed acyclic graph (DAG) with game-related questions as\nnodes and dependencies as edges. We identify the optimal action to take in the\nenvironment by traversing the DAG and calculating LLM responses for each node\nin topological order, with the LLM's answer to final node directly translating\nto environment actions. In our experiments, we study the quality of in-context\n\"reasoning\" induced by different forms of prompts under the setting of the\nCrafter open-world environment. Our experiments suggest that LLMs, when\nprompted with consistent chain-of-thought, have great potential in completing\nsophisticated high-level trajectories. Quantitatively, SPRING with GPT-4\noutperforms all state-of-the-art RL baselines, trained for 1M steps, without\nany training. Finally, we show the potential of games as a test bed for LLMs.",
                "How to Turn Your Knowledge Graph Embeddings into Generative Models\nSome of the most successful knowledge graph embedding (KGE) models for link\nprediction -- CP, RESCAL, TuckER, ComplEx -- can be interpreted as energy-based\nmodels. Under this perspective they are not amenable for exact\nmaximum-likelihood estimation (MLE), sampling and struggle to integrate logical\nconstraints. This work re-interprets the score functions of these KGEs as\ncircuits -- constrained computational graphs allowing efficient\nmarginalisation. Then, we design two recipes to obtain efficient generative\ncircuit models by either restricting their activations to be non-negative or\nsquaring their outputs. Our interpretation comes with little or no loss of\nperformance for link prediction, while the circuits framework unlocks exact\nlearning by MLE, efficient sampling of new triples, and guarantee that logical\nconstraints are satisfied by design. Furthermore, our models scale more\ngracefully than the original KGEs on graphs with millions of entities.",
                "An Experimental Investigation into the Evaluation of Explainability\n  Methods\nEXplainable Artificial Intelligence (XAI) aims to help users to grasp the\nreasoning behind the predictions of an Artificial Intelligence (AI) system.\nMany XAI approaches have emerged in recent years. Consequently, a subfield\nrelated to the evaluation of XAI methods has gained considerable attention,\nwith the aim to determine which methods provide the best explanation using\nvarious approaches and criteria. However, the literature lacks a comparison of\nthe evaluation metrics themselves, that one can use to evaluate XAI methods.\nThis work aims to fill this gap by comparing 14 different metrics when applied\nto nine state-of-the-art XAI methods and three dummy methods (e.g., random\nsaliency maps) used as references. Experimental results show which of these\nmetrics produces highly correlated results, indicating potential redundancy. We\nalso demonstrate the significant impact of varying the baseline hyperparameter\non the evaluation metric values. Finally, we use dummy methods to assess the\nreliability of metrics in terms of ranking, pointing out their limitations."
            ],
            "interesting paper": 1
        },
        {
            "papers": [
                "Passive learning of active causal strategies in agents and language\n  models\nWhat can be learned about causality and experimentation from passive data?\nThis question is salient given recent successes of passively-trained language\nmodels in interactive domains such as tool use. Passive learning is inherently\nlimited. However, we show that purely passive learning can in fact allow an\nagent to learn generalizable strategies for determining and using causal\nstructures, as long as the agent can intervene at test time. We formally\nillustrate that learning a strategy of first experimenting, then seeking goals,\ncan allow generalization from passive learning in principle. We then show\nempirically that agents trained via imitation on expert data can indeed\ngeneralize at test time to infer and use causal links which are never present\nin the training data; these agents can also generalize experimentation\nstrategies to novel variable sets never observed in training. We then show that\nstrategies for causal intervention and exploitation can be generalized from\npassive data even in a more complex environment with high-dimensional\nobservations, with the support of natural language explanations. Explanations\ncan even allow passive learners to generalize out-of-distribution from\nperfectly-confounded training data. Finally, we show that language models,\ntrained only on passive next-word prediction, can generalize causal\nintervention strategies from a few-shot prompt containing examples of\nexperimentation, together with explanations and reasoning. These results\nhighlight the surprising power of passive learning of active causal strategies,\nand may help to understand the behaviors and capabilities of language models.",
                "Quantifying the Intrinsic Usefulness of Attributional Explanations for\n  Graph Neural Networks with Artificial Simulatability Studies\nDespite the increasing relevance of explainable AI, assessing the quality of\nexplanations remains a challenging issue. Due to the high costs associated with\nhuman-subject experiments, various proxy metrics are often used to\napproximately quantify explanation quality. Generally, one possible\ninterpretation of the quality of an explanation is its inherent value for\nteaching a related concept to a student. In this work, we extend artificial\nsimulatability studies to the domain of graph neural networks. Instead of\ncostly human trials, we use explanation-supervisable graph neural networks to\nperform simulatability studies to quantify the inherent usefulness of\nattributional graph explanations. We perform an extensive ablation study to\ninvestigate the conditions under which the proposed analyses are most\nmeaningful. We additionally validate our methods applicability on real-world\ngraph classification and regression datasets. We find that relevant\nexplanations can significantly boost the sample efficiency of graph neural\nnetworks and analyze the robustness towards noise and bias in the explanations.\nWe believe that the notion of usefulness obtained from our proposed\nsimulatability analysis provides a dimension of explanation quality that is\nlargely orthogonal to the common practice of faithfulness and has great\npotential to expand the toolbox of explanation quality assessments,\nspecifically for graph explanations.",
                "Dynamic Inter-treatment Information Sharing for Individualized Treatment\n  Effects Estimation\nEstimation of individualized treatment effects (ITE) from observational\nstudies is a fundamental problem in causal inference and holds significant\nimportance across domains, including healthcare. However, limited observational\ndatasets pose challenges in reliable ITE estimation as data have to be split\namong treatment groups to train an ITE learner. While information sharing among\ntreatment groups can partially alleviate the problem, there is currently no\ngeneral framework for end-to-end information sharing in ITE estimation. To\ntackle this problem, we propose a deep learning framework based on\n`\\textit{soft weight sharing}' to train ITE learners, enabling \\textit{dynamic\nend-to-end} information sharing among treatment groups. The proposed framework\ncomplements existing ITE learners, and introduces a new class of ITE learners,\nreferred to as \\textit{HyperITE}. We extend state-of-the-art ITE learners with\n\\textit{HyperITE} versions and evaluate them on IHDP, ACIC-2016, and Twins\nbenchmarks. Our experimental results show that the proposed framework improves\nITE estimation error, with increasing effectiveness for smaller datasets.",
                "Monitoring Algorithmic Fairness\nMachine-learned systems are in widespread use for making decisions about\nhumans, and it is important that they are fair, i.e., not biased against\nindividuals based on sensitive attributes. We present runtime verification of\nalgorithmic fairness for systems whose models are unknown, but are assumed to\nhave a Markov chain structure. We introduce a specification language that can\nmodel many common algorithmic fairness properties, such as demographic parity,\nequal opportunity, and social burden. We build monitors that observe a long\nsequence of events as generated by a given system, and output, after each\nobservation, a quantitative estimate of how fair or biased the system was on\nthat run until that point in time. The estimate is proven to be correct modulo\na variable error bound and a given confidence level, where the error bound gets\ntighter as the observed sequence gets longer. Our monitors are of two types,\nand use, respectively, frequentist and Bayesian statistical inference\ntechniques. While the frequentist monitors compute estimates that are\nobjectively correct with respect to the ground truth, the Bayesian monitors\ncompute estimates that are correct subject to a given prior belief about the\nsystem's model. Using a prototype implementation, we show how we can monitor if\na bank is fair in giving loans to applicants from different social backgrounds,\nand if a college is fair in admitting students while maintaining a reasonable\nfinancial burden on the society. Although they exhibit different theoretical\ncomplexities in certain cases, in our experiments, both frequentist and\nBayesian monitors took less than a millisecond to update their verdicts after\neach observation.",
                "Alert of the Second Decision-maker: An Introduction to Human-AI Conflict\nThe collaboration between humans and artificial intelligence (AI) is a\nsignificant feature in this digital age. However, humans and AI may have\nobservation, interpretation, and action conflicts when working synchronously.\nThis phenomenon is often masked by faults and, unfortunately, overlooked. This\npaper systematically introduces the human-AI conflict concept, causes,\nmeasurement methods, and risk assessment. The results highlight that there is a\npotential second decision-maker besides the human, which is the AI; the\nhuman-AI conflict is a unique and emerging risk in digitalized process systems;\nand this is an interdisciplinary field that needs to be distinguished from\ntraditional fault and failure analysis; the conflict risk is significant and\ncannot be ignored.",
                "Applying Interdisciplinary Frameworks to Understand Algorithmic\n  Decision-Making\nWe argue that explanations for \"algorithmic decision-making\" (ADM) systems\ncan profit by adopting practices that are already used in the learning\nsciences. We shortly introduce the importance of explaining ADM systems, give a\nbrief overview of approaches drawing from other disciplines to improve\nexplanations, and present the results of our qualitative task-based study\nincorporating the \"six facets of understanding\" framework. We close with\nquestions guiding the discussion of how future studies can leverage an\ninterdisciplinary approach."
            ],
            "interesting paper": 1
        },
        {
            "papers": [
                "Causal Component Analysis\nIndependent Component Analysis (ICA) aims to recover independent latent\nvariables from observed mixtures thereof. Causal Representation Learning (CRL)\naims instead to infer causally related (thus often statistically dependent)\nlatent variables, together with the unknown graph encoding their causal\nrelationships. We introduce an intermediate problem termed Causal Component\nAnalysis (CauCA). CauCA can be viewed as a generalization of ICA, modelling the\ncausal dependence among the latent components, and as a special case of CRL. In\ncontrast to CRL, it presupposes knowledge of the causal graph, focusing solely\non learning the unmixing function and the causal mechanisms. Any impossibility\nresults regarding the recovery of the ground truth in CauCA also apply for CRL,\nwhile possibility results may serve as a stepping stone for extensions to CRL.\nWe characterize CauCA identifiability from multiple datasets generated through\ndifferent types of interventions on the latent causal variables. As a\ncorollary, this interventional perspective also leads to new identifiability\nresults for nonlinear ICA -- a special case of CauCA with an empty graph --\nrequiring strictly fewer datasets than previous results. We introduce a\nlikelihood-based approach using normalizing flows to estimate both the unmixing\nfunction and the causal mechanisms, and demonstrate its effectiveness through\nextensive synthetic experiments in the CauCA and ICA setting.",
                "Inferring the Future by Imagining the Past\nA single panel of a comic book can say a lot: it can depict not only where\nthe characters currently are, but also their motions, their motivations, their\nemotions, and what they might do next. More generally, humans routinely infer\ncomplex sequences of past and future events from a *static snapshot* of a\n*dynamic scene*, even in situations they have never seen before.\n  In this paper, we model how humans make such rapid and flexible inferences.\nBuilding on a long line of work in cognitive science, we offer a Monte Carlo\nalgorithm whose inferences correlate well with human intuitions in a wide\nvariety of domains, while only using a small, cognitively-plausible number of\nsamples. Our key technical insight is a surprising connection between our\ninference problem and Monte Carlo path tracing, which allows us to apply\ndecades of ideas from the computer graphics community to this\nseemingly-unrelated theory of mind task.",
                "Sharp Bounds for Generalized Causal Sensitivity Analysis\nCausal inference from observational data is crucial for many disciplines such\nas medicine and economics. However, sharp bounds for causal effects under\nrelaxations of the unconfoundedness assumption (causal sensitivity analysis)\nare subject to ongoing research. So far, works with sharp bounds are restricted\nto fairly simple settings (e.g., a single binary treatment). In this paper, we\npropose a unified framework for causal sensitivity analysis under unobserved\nconfounding in various settings. For this, we propose a flexible generalization\nof the marginal sensitivity model (MSM) and then derive sharp bounds for a\nlarge class of causal effects. This includes (conditional) average treatment\neffects, effects for mediation analysis and path analysis, and distributional\neffects. Furthermore, our sensitivity model is applicable to discrete,\ncontinuous, and time-varying treatments. It allows us to interpret the partial\nidentification problem under unobserved confounding as a distribution shift in\nthe latent confounders while evaluating the causal effect of interest. In the\nspecial case of a single binary treatment, our bounds for (conditional) average\ntreatment effects coincide with recent optimality results for causal\nsensitivity analysis. Finally, we propose a scalable algorithm to estimate our\nsharp bounds from observational data.",
                "RFiD: Towards Rational Fusion-in-Decoder for Open-Domain Question\n  Answering\nOpen-Domain Question Answering (ODQA) systems necessitate a reader model\ncapable of generating answers by simultaneously referring to multiple passages.\nAlthough representative models like Fusion-in-Decoder (FiD) have been proposed\nto address this challenge, these systems can inadvertently rely on spurious\nfeatures instead of genuine causal relationships between the question and the\npassages to generate answers. To counter this problem, we introduce the\nRational Fusion-in-Decoder (RFiD) model. Our model leverages the encoders of\nFiD to differentiate between causal relationships and spurious features,\nsubsequently guiding the decoder to generate answers informed by this\ndiscernment. Experimental results on two ODQA datasets, Natural Questions (NQ)\nand TriviaQA (TQ), demonstrate that our model surpasses previous methods,\nachieving improvements of up to 1.5 and 0.7 in Exact Match scores on NQ, and\nexhibits an enhanced ability to identify causal relationships.",
                "Attention Schema in Neural Agents\nAttention has become a common ingredient in deep learning architectures. It\nadds a dynamical selection of information on top of the static selection of\ninformation supported by weights. In the same way, we can imagine a\nhigher-order informational filter built on top of attention: an Attention\nSchema (AS), namely, a descriptive and predictive model of attention. In\ncognitive neuroscience, Attention Schema Theory (AST) supports this idea of\ndistinguishing attention from AS. A strong prediction of this theory is that an\nagent can use its own AS to also infer the states of other agents' attention\nand consequently enhance coordination with other agents. As such, multi-agent\nreinforcement learning would be an ideal setting to experimentally test the\nvalidity of AST. We explore different ways in which attention and AS interact\nwith each other. Our preliminary results indicate that agents that implement\nthe AS as a recurrent internal control achieve the best performance. In\ngeneral, these exploratory experiments suggest that equipping artificial agents\nwith a model of attention can enhance their social intelligence.",
                "Im-Promptu: In-Context Composition from Image Prompts\nLarge language models are few-shot learners that can solve diverse tasks from\na handful of demonstrations. This implicit understanding of tasks suggests that\nthe attention mechanisms over word tokens may play a role in analogical\nreasoning. In this work, we investigate whether analogical reasoning can enable\nin-context composition over composable elements of visual stimuli. First, we\nintroduce a suite of three benchmarks to test the generalization properties of\na visual in-context learner. We formalize the notion of an analogy-based\nin-context learner and use it to design a meta-learning framework called\nIm-Promptu. Whereas the requisite token granularity for language is well\nestablished, the appropriate compositional granularity for enabling in-context\ngeneralization in visual stimuli is usually unspecified. To this end, we use\nIm-Promptu to train multiple agents with different levels of compositionality,\nincluding vector representations, patch representations, and object slots. Our\nexperiments reveal tradeoffs between extrapolation abilities and the degree of\ncompositionality, with non-compositional representations extending learned\ncomposition rules to unseen domains but performing poorly on combinatorial\ntasks. Patch-based representations require patches to contain entire objects\nfor robust extrapolation. At the same time, object-centric tokenizers coupled\nwith a cross-attention module generate consistent and high-fidelity solutions,\nwith these inductive biases being particularly crucial for compositional\ngeneralization. Lastly, we demonstrate a use case of Im-Promptu as an intuitive\nprogramming interface for image generation."
            ],
            "interesting paper": 1
        },
        {
            "papers": [
                "Counterfactual Formulation of Patient-Specific Root Causes of Disease\nRoot causes of disease intuitively correspond to root vertices that increase\nthe likelihood of a diagnosis. This description of a root cause nevertheless\nlacks the rigorous mathematical formulation needed for the development of\ncomputer algorithms designed to automatically detect root causes from data.\nPrior work defined patient-specific root causes of disease using an\ninterventionalist account that only climbs to the second rung of Pearl's Ladder\nof Causation. In this theoretical piece, we climb to the third rung by\nproposing a counterfactual definition matching clinical intuition based on\nfixed factual data alone. We then show how to assign a root causal contribution\nscore to each variable using Shapley values from explainable artificial\nintelligence. The proposed counterfactual formulation of patient-specific root\ncauses of disease accounts for noisy labels, adapts to disease prevalence and\nadmits fast computation without the need for counterfactual simulation.",
                "In-Context Analogical Reasoning with Pre-Trained Language Models\nAnalogical reasoning is a fundamental capacity of human cognition that allows\nus to reason abstractly about novel situations by relating them to past\nexperiences. While it is thought to be essential for robust reasoning in AI\nsystems, conventional approaches require significant training and/or\nhard-coding of domain knowledge to be applied to benchmark tasks. Inspired by\ncognitive science research that has found connections between human language\nand analogy-making, we explore the use of intuitive language-based abstractions\nto support analogy in AI systems. Specifically, we apply large pre-trained\nlanguage models (PLMs) to visual Raven's Progressive Matrices (RPM), a common\nrelational reasoning test. By simply encoding the perceptual features of the\nproblem into language form, we find that PLMs exhibit a striking capacity for\nzero-shot relational reasoning, exceeding human performance and nearing\nsupervised vision-based methods. We explore different encodings that vary the\nlevel of abstraction over task features, finding that higher-level abstractions\nfurther strengthen PLMs' analogical reasoning. Our detailed analysis reveals\ninsights on the role of model complexity, in-context learning, and prior\nknowledge in solving RPM tasks.",
                "Choose your Data Wisely: A Framework for Semantic Counterfactuals\nCounterfactual explanations have been argued to be one of the most intuitive\nforms of explanation. They are typically defined as a minimal set of edits on a\ngiven data sample that, when applied, changes the output of a model on that\nsample. However, a minimal set of edits is not always clear and understandable\nto an end-user, as it could, for instance, constitute an adversarial example\n(which is indistinguishable from the original data sample to an end-user).\nInstead, there are recent ideas that the notion of minimality in the context of\ncounterfactuals should refer to the semantics of the data sample, and not to\nthe feature space. In this work, we build on these ideas, and propose a\nframework that provides counterfactual explanations in terms of knowledge\ngraphs. We provide an algorithm for computing such explanations (given some\nassumptions about the underlying knowledge), and quantitatively evaluate the\nframework with a user study.",
                "Backdoor Attacks Against Incremental Learners: An Empirical Evaluation\n  Study\nLarge amounts of incremental learning algorithms have been proposed to\nalleviate the catastrophic forgetting issue arises while dealing with\nsequential data on a time series. However, the adversarial robustness of\nincremental learners has not been widely verified, leaving potential security\nrisks. Specifically, for poisoning-based backdoor attacks, we argue that the\nnature of streaming data in IL provides great convenience to the adversary by\ncreating the possibility of distributed and cross-task attacks -- an adversary\ncan affect \\textbf{any unknown} previous or subsequent task by data poisoning\n\\textbf{at any time or time series} with extremely small amount of backdoor\nsamples injected (e.g., $0.1\\%$ based on our observations). To attract the\nattention of the research community, in this paper, we empirically reveal the\nhigh vulnerability of 11 typical incremental learners against poisoning-based\nbackdoor attack on 3 learning scenarios, especially the cross-task\ngeneralization effect of backdoor knowledge, while the poison ratios range from\n$5\\%$ to as low as $0.1\\%$. Finally, the defense mechanism based on activation\nclustering is found to be effective in detecting our trigger pattern to\nmitigate potential security risks.",
                "On the Value of Myopic Behavior in Policy Reuse\nLeveraging learned strategies in unfamiliar scenarios is fundamental to human\nintelligence. In reinforcement learning, rationally reusing the policies\nacquired from other tasks or human experts is critical for tackling problems\nthat are difficult to learn from scratch. In this work, we present a framework\ncalled Selective Myopic bEhavior Control~(SMEC), which results from the insight\nthat the short-term behaviors of prior policies are sharable across tasks. By\nevaluating the behaviors of prior policies via a hybrid value function\narchitecture, SMEC adaptively aggregates the sharable short-term behaviors of\nprior policies and the long-term behaviors of the task policy, leading to\ncoordinated decisions. Empirical results on a collection of manipulation and\nlocomotion tasks demonstrate that SMEC outperforms existing methods, and\nvalidate the ability of SMEC to leverage related prior policies.",
                "Synthesizing a Progression of Subtasks for Block-Based Visual\n  Programming Tasks\nBlock-based visual programming environments play an increasingly important\nrole in introducing computing concepts to K-12 students. In recent years, they\nhave also gained popularity in neuro-symbolic AI, serving as a benchmark to\nevaluate general problem-solving and logical reasoning skills. The open-ended\nand conceptual nature of these visual programming tasks make them challenging,\nboth for state-of-the-art AI agents as well as for novice programmers. A\nnatural approach to providing assistance for problem-solving is breaking down a\ncomplex task into a progression of simpler subtasks; however, this is not\ntrivial given that the solution codes are typically nested and have non-linear\nexecution behavior. In this paper, we formalize the problem of synthesizing\nsuch a progression for a given reference block-based visual programming task.\nWe propose a novel synthesis algorithm that generates a progression of subtasks\nthat are high-quality, well-spaced in terms of their complexity, and solving\nthis progression leads to solving the reference task. We show the utility of\nour synthesis algorithm in improving the efficacy of AI agents (in this case,\nneural program synthesizers) for solving tasks in the Karel programming\nenvironment. Then, we conduct a user study to demonstrate that our synthesized\nprogression of subtasks can assist a novice programmer in solving tasks in the\nHour of Code: Maze Challenge by Code-dot-org."
            ],
            "interesting paper": 1
        },
        {
            "papers": [
                "Shift-Robust Molecular Relational Learning with Causal Substructure\nRecently, molecular relational learning, whose goal is to predict the\ninteraction behavior between molecular pairs, got a surge of interest in\nmolecular sciences due to its wide range of applications. In this work, we\npropose CMRL that is robust to the distributional shift in molecular relational\nlearning by detecting the core substructure that is causally related to\nchemical reactions. To do so, we first assume a causal relationship based on\nthe domain knowledge of molecular sciences and construct a structural causal\nmodel (SCM) that reveals the relationship between variables. Based on the SCM,\nwe introduce a novel conditional intervention framework whose intervention is\nconditioned on the paired molecule. With the conditional intervention\nframework, our model successfully learns from the causal substructure and\nalleviates the confounding effect of shortcut substructures that are spuriously\ncorrelated to chemical reactions. Extensive experiments on various tasks with\nreal-world and synthetic datasets demonstrate the superiority of CMRL over\nstate-of-the-art baseline models. Our code is available at\nhttps://github.com/Namkyeong/CMRL.",
                "Taming AI Bots: Controllability of Neural States in Large Language\n  Models\nWe tackle the question of whether an agent can, by suitable choice of\nprompts, control an AI bot to any state. To that end, we first introduce a\nformal definition of ``meaning'' that is amenable to analysis. Then, we\ncharacterize ``meaningful data'' on which large language models (LLMs) are\nostensibly trained, and ``well-trained LLMs'' through conditions that are\nlargely met by today's LLMs. While a well-trained LLM constructs an embedding\nspace of meanings that is Euclidean, meanings themselves do not form a vector\n(linear) subspace, but rather a quotient space within. We then characterize the\nsubset of meanings that can be reached by the state of the LLMs for some input\nprompt, and show that a well-trained bot can reach any meaning albeit with\nsmall probability. We then introduce a stronger notion of controllability as\n{\\em almost certain reachability}, and show that, when restricted to the space\nof meanings, an AI bot is controllable. We do so after introducing a functional\ncharacterization of attentive AI bots, and finally derive necessary and\nsufficient conditions for controllability. The fact that AI bots are\ncontrollable means that an adversary could steer them towards any state.\nHowever, the sampling process can be designed to counteract adverse actions and\navoid reaching undesirable regions of state space before their boundary is\ncrossed.",
                "Faithfulness Tests for Natural Language Explanations\nExplanations of neural models aim to reveal a model's decision-making process\nfor its predictions. However, recent work shows that current methods giving\nexplanations such as saliency maps or counterfactuals can be misleading, as\nthey are prone to present reasons that are unfaithful to the model's inner\nworkings. This work explores the challenging question of evaluating the\nfaithfulness of natural language explanations (NLEs). To this end, we present\ntwo tests. First, we propose a counterfactual input editor for inserting\nreasons that lead to counterfactual predictions but are not reflected by the\nNLEs. Second, we reconstruct inputs from the reasons stated in the generated\nNLEs and check how often they lead to the same predictions. Our tests can\nevaluate emerging NLE models, proving a fundamental tool in the development of\nfaithful NLEs.",
                "The Computational Complexity of Single-Player Imperfect-Recall Games\nWe study single-player extensive-form games with imperfect recall, such as\nthe Sleeping Beauty problem or the Absentminded Driver game. For such games,\ntwo natural equilibrium concepts have been proposed as alternative solution\nconcepts to ex-ante optimality. One equilibrium concept uses generalized double\nhalving (GDH) as a belief system and evidential decision theory (EDT), and\nanother one uses generalized thirding (GT) as a belief system and causal\ndecision theory (CDT). Our findings relate those three solution concepts of a\ngame to solution concepts of a polynomial maximization problem: global optima,\noptimal points with respect to subsets of variables and Karush-Kuhn-Tucker\n(KKT) points. Based on these correspondences, we are able to settle various\ncomplexity-theoretic questions on the computation of such strategies. For\nex-ante optimality and (EDT,GDH)-equilibria, we obtain NP-hardness and\ninapproximability, and for (CDT,GT)-equilibria we obtain CLS-completeness\nresults.",
                "Short-term Temporal Dependency Detection under Heterogeneous Event\n  Dynamic with Hawkes Processes\nMany event sequence data exhibit mutually exciting or inhibiting patterns.\nReliable detection of such temporal dependency is crucial for scientific\ninvestigation. The de facto model is the Multivariate Hawkes Process (MHP),\nwhose impact function naturally encodes a causal structure in Granger\ncausality. However, the vast majority of existing methods use direct or\nnonlinear transform of standard MHP intensity with constant baseline,\ninconsistent with real-world data. Under irregular and unknown heterogeneous\nintensity, capturing temporal dependency is hard as one struggles to\ndistinguish the effect of mutual interaction from that of intensity\nfluctuation. In this paper, we address the short-term temporal dependency\ndetection issue. We show the maximum likelihood estimation (MLE) for\ncross-impact from MHP has an error that can not be eliminated but may be\nreduced by order of magnitude, using heterogeneous intensity not of the target\nHP but of the interacting HP. Then we proposed a robust and\ncomputationally-efficient method modified from MLE that does not rely on the\nprior estimation of the heterogeneous intensity and is thus applicable in a\ndata-limited regime (e.g., few-shot, no repeated observations). Extensive\nexperiments on various datasets show that our method outperforms existing ones\nby notable margins, with highlighted novel applications in neuroscience.",
                "The Digital Divide in Process Safety: Quantitative Risk Analysis of\n  Human-AI Collaboration\nDigital technologies have dramatically accelerated the digital transformation\nin process industries, boosted new industrial applications, upgraded the\nproduction system, and enhanced operational efficiency. In contrast, the\nchallenges and gaps between human and artificial intelligence (AI) have become\nmore and more prominent, whereas the digital divide in process safety is\naggregating. The study attempts to address the following questions: (i)What is\nAI in the process safety context? (ii)What is the difference between AI and\nhumans in process safety? (iii)How do AI and humans collaborate in process\nsafety? (iv)What are the challenges and gaps in human-AI collaboration? (v)How\nto quantify the risk of human-AI collaboration in process safety? Qualitative\nrisk analysis based on brainstorming and literature review, and quantitative\nrisk analysis based on layer of protection analysis (LOPA) and Bayesian network\n(BN), were applied to explore and model. The importance of human reliability\nshould be stressed in the digital age, not usually to increase the reliability\nof AI, and human-centered AI design in process safety needs to be propagated."
            ],
            "interesting paper": 1
        },
        {
            "papers": [
                "Shapley Based Residual Decomposition for Instance Analysis\nIn this paper, we introduce the idea of decomposing the residuals of\nregression with respect to the data instances instead of features. This allows\nus to determine the effects of each individual instance on the model and each\nother, and in doing so makes for a model-agnostic method of identifying\ninstances of interest. In doing so, we can also determine the appropriateness\nof the model and data in the wider context of a given study. The paper focuses\non the possible applications that such a framework brings to the relatively\nunexplored field of instance analysis in the context of Explainable AI tasks.",
                "Generalized Disparate Impact for Configurable Fairness Solutions in ML\nWe make two contributions in the field of AI fairness over continuous\nprotected attributes. First, we show that the Hirschfeld-Gebelein-Renyi (HGR)\nindicator (the only one currently available for such a case) is valuable but\nsubject to a few crucial limitations regarding semantics, interpretability, and\nrobustness. Second, we introduce a family of indicators that are: 1)\ncomplementary to HGR in terms of semantics; 2) fully interpretable and\ntransparent; 3) robust over finite samples; 4) configurable to suit specific\napplications. Our approach also allows us to define fine-grained constraints to\npermit certain types of dependence and forbid others selectively. By expanding\nthe available options for continuous protected attributes, our approach\nrepresents a significant contribution to the area of fair artificial\nintelligence.",
                "Dissecting Chain-of-Thought: Compositionality through In-Context\n  Filtering and Learning\nChain-of-thought (CoT) is a method that enables language models to handle\ncomplex reasoning tasks by decomposing them into simpler steps. Despite its\nsuccess, the underlying mechanics of CoT are not yet fully understood. In an\nattempt to shed light on this, our study investigates the impact of CoT on the\nability of transformers to in-context learn a simple to study, yet general\nfamily of compositional functions: multi-layer perceptrons (MLPs). In this\nsetting, we find that the success of CoT can be attributed to breaking down\nin-context learning of a compositional function into two distinct phases:\nfocusing on and filtering data related to each step of the composition and\nin-context learning the single-step composition function. Through both\nexperimental and theoretical evidence, we demonstrate how CoT significantly\nreduces the sample complexity of in-context learning (ICL) and facilitates the\nlearning of complex functions that non-CoT methods struggle with. Furthermore,\nwe illustrate how transformers can transition from vanilla in-context learning\nto mastering a compositional function with CoT by simply incorporating\nadditional layers that perform the necessary data-filtering for CoT via the\nattention mechanism. In addition to these test-time benefits, we show CoT helps\naccelerate pretraining by learning shortcuts to represent complex functions and\nfiltering plays an important role in this process. These findings collectively\nprovide insights into the mechanics of CoT, inviting further investigation of\nits role in complex reasoning tasks.",
                "What is Essential for Unseen Goal Generalization of Offline\n  Goal-conditioned RL?\nOffline goal-conditioned RL (GCRL) offers a way to train general-purpose\nagents from fully offline datasets. In addition to being conservative within\nthe dataset, the generalization ability to achieve unseen goals is another\nfundamental challenge for offline GCRL. However, to the best of our knowledge,\nthis problem has not been well studied yet. In this paper, we study\nout-of-distribution (OOD) generalization of offline GCRL both theoretically and\nempirically to identify factors that are important. In a number of experiments,\nwe observe that weighted imitation learning enjoys better generalization than\npessimism-based offline RL method. Based on this insight, we derive a theory\nfor OOD generalization, which characterizes several important design choices.\nWe then propose a new offline GCRL method, Generalizable Offline\ngoAl-condiTioned RL (GOAT), by combining the findings from our theoretical and\nempirical studies. On a new benchmark containing 9 independent identically\ndistributed (IID) tasks and 17 OOD tasks, GOAT outperforms current\nstate-of-the-art methods by a large margin.",
                "Code Prompting: a Neural Symbolic Method for Complex Reasoning in Large\n  Language Models\nLarge language models (LLMs) have scaled up to unlock a wide range of complex\nreasoning tasks with the aid of various prompting methods. However, current\nprompting methods generate natural language intermediate steps to help\nreasoning, which can cause imperfect task reduction and confusion. To mitigate\nsuch limitations, we explore code prompting, a neural symbolic prompting method\nwith both zero-shot and few-shot versions which triggers code as intermediate\nsteps. We conduct experiments on 7 widely-used benchmarks involving symbolic\nreasoning and arithmetic reasoning. Code prompting generally outperforms\nchain-of-thought (CoT) prompting. To further understand the performance and\nlimitations of code prompting, we perform extensive ablation studies and error\nanalyses, and identify several exclusive advantages of using symbolic\npromptings compared to natural language. We also consider the ensemble of code\nprompting and CoT prompting to combine the strengths of both. Finally, we show\nthrough experiments how code annotations and their locations affect code\nprompting.",
                "Reason to explain: Interactive contrastive explanations (REASONX)\nMany high-performing machine learning models are not interpretable. As they\nare increasingly used in decision scenarios that can critically affect\nindividuals, it is necessary to develop tools to better understand their\noutputs. Popular explanation methods include contrastive explanations. However,\nthey suffer several shortcomings, among others an insufficient incorporation of\nbackground knowledge, and a lack of interactivity. While (dialogue-like)\ninteractivity is important to better communicate an explanation, background\nknowledge has the potential to significantly improve their quality, e.g., by\nadapting the explanation to the needs of the end-user. To close this gap, we\npresent REASONX, an explanation tool based on Constraint Logic Programming\n(CLP). REASONX provides interactive contrastive explanations that can be\naugmented by background knowledge, and allows to operate under a setting of\nunder-specified information, leading to increased flexibility in the provided\nexplanations. REASONX computes factual and constrative decision rules, as well\nas closest constrative examples. It provides explanations for decision trees,\nwhich can be the ML models under analysis, or global/local surrogate models of\nany ML model. While the core part of REASONX is built on CLP, we also provide a\nprogram layer that allows to compute the explanations via Python, making the\ntool accessible to a wider audience. We illustrate the capability of REASONX on\na synthetic data set, and on a a well-developed example in the credit domain.\nIn both cases, we can show how REASONX can be flexibly used and tailored to the\nneeds of the user."
            ],
            "interesting paper": 2
        },
        {
            "papers": [
                "Active causal structure learning with advice\nWe introduce the problem of active causal structure learning with advice. In\nthe typical well-studied setting, the learning algorithm is given the essential\ngraph for the observational distribution and is asked to recover the underlying\ncausal directed acyclic graph (DAG) $G^*$ while minimizing the number of\ninterventions made. In our setting, we are additionally given side information\nabout $G^*$ as advice, e.g. a DAG $G$ purported to be $G^*$. We ask whether the\nlearning algorithm can benefit from the advice when it is close to being\ncorrect, while still having worst-case guarantees even when the advice is\narbitrarily bad. Our work is in the same space as the growing body of research\non algorithms with predictions. When the advice is a DAG $G$, we design an\nadaptive search algorithm to recover $G^*$ whose intervention cost is at most\n$O(\\max\\{1, \\log \\psi\\})$ times the cost for verifying $G^*$; here, $\\psi$ is a\ndistance measure between $G$ and $G^*$ that is upper bounded by the number of\nvariables $n$, and is exactly 0 when $G=G^*$. Our approximation factor matches\nthe state-of-the-art for the advice-less setting.",
                "Causal Discovery with Latent Confounders Based on Higher-Order Cumulants\nCausal discovery with latent confounders is an important but challenging task\nin many scientific areas. Despite the success of some overcomplete independent\ncomponent analysis (OICA) based methods in certain domains, they are\ncomputationally expensive and can easily get stuck into local optima. We notice\nthat interestingly, by making use of higher-order cumulants, there exists a\nclosed-form solution to OICA in specific cases, e.g., when the mixing procedure\nfollows the One-Latent-Component structure. In light of the power of the\nclosed-form solution to OICA corresponding to the One-Latent-Component\nstructure, we formulate a way to estimate the mixing matrix using the\nhigher-order cumulants, and further propose the testable One-Latent-Component\ncondition to identify the latent variables and determine causal orders. By\niteratively removing the share identified latent components, we successfully\nextend the results on the One-Latent-Component structure to the\nMulti-Latent-Component structure and finally provide a practical and\nasymptotically correct algorithm to learn the causal structure with latent\nvariables. Experimental results illustrate the asymptotic correctness and\neffectiveness of the proposed method.",
                "Intent-aligned AI systems deplete human agency: the need for agency\n  foundations research in AI safety\nThe rapid advancement of artificial intelligence (AI) systems suggests that\nartificial general intelligence (AGI) systems may soon arrive. Many researchers\nare concerned that AIs and AGIs will harm humans via intentional misuse\n(AI-misuse) or through accidents (AI-accidents). In respect of AI-accidents,\nthere is an increasing effort focused on developing algorithms and paradigms\nthat ensure AI systems are aligned to what humans intend, e.g. AI systems that\nyield actions or recommendations that humans might judge as consistent with\ntheir intentions and goals. Here we argue that alignment to human intent is\ninsufficient for safe AI systems and that preservation of long-term agency of\nhumans may be a more robust standard, and one that needs to be separated\nexplicitly and a priori during optimization. We argue that AI systems can\nreshape human intention and discuss the lack of biological and psychological\nmechanisms that protect humans from loss of agency. We provide the first formal\ndefinition of agency-preserving AI-human interactions which focuses on\nforward-looking agency evaluations and argue that AI systems - not humans -\nmust be increasingly tasked with making these evaluations. We show how agency\nloss can occur in simple environments containing embedded agents that use\ntemporal-difference learning to make action recommendations. Finally, we\npropose a new area of research called \"agency foundations\" and pose four\ninitial topics designed to improve our understanding of agency in AI-human\ninteractions: benevolent game theory, algorithmic foundations of human rights,\nmechanistic interpretability of agency representation in neural-networks and\nreinforcement learning from internal states.",
                "What does the Failure to Reason with \"Respectively\" in Zero/Few-Shot\n  Settings Tell Us about Language Models?\nHumans can effortlessly understand the coordinate structure of sentences such\nas \"Niels Bohr and Kurt Cobain were born in Copenhagen and Seattle,\nrespectively\". In the context of natural language inference (NLI), we examine\nhow language models (LMs) reason with respective readings (Gawron and Kehler,\n2004) from two perspectives: syntactic-semantic and commonsense-world\nknowledge. We propose a controlled synthetic dataset WikiResNLI and a naturally\noccurring dataset NatResNLI to encompass various explicit and implicit\nrealizations of \"respectively\". We show that fine-tuned NLI models struggle\nwith understanding such readings without explicit supervision. While few-shot\nlearning is easy in the presence of explicit cues, longer training is required\nwhen the reading is evoked implicitly, leaving models to rely on common sense\ninferences. Furthermore, our fine-grained analysis indicates models fail to\ngeneralize across different constructions. To conclude, we demonstrate that LMs\nstill lag behind humans in generalizing to the long tail of linguistic\nconstructions.",
                "Less Likely Brainstorming: Using Language Models to Generate Alternative\n  Hypotheses\nA human decision-maker benefits the most from an AI assistant that corrects\nfor their biases. For problems such as generating interpretation of a radiology\nreport given findings, a system predicting only highly likely outcomes may be\nless useful, where such outcomes are already obvious to the user. To alleviate\nbiases in human decision-making, it is worth considering a broad differential\ndiagnosis, going beyond the most likely options. We introduce a new task, \"less\nlikely brainstorming,\" that asks a model to generate outputs that humans think\nare relevant but less likely to happen. We explore the task in two settings: a\nbrain MRI interpretation generation setting and an everyday commonsense\nreasoning setting. We found that a baseline approach of training with less\nlikely hypotheses as targets generates outputs that humans evaluate as either\nlikely or irrelevant nearly half of the time; standard MLE training is not\neffective. To tackle this problem, we propose a controlled text generation\nmethod that uses a novel contrastive learning strategy to encourage models to\ndifferentiate between generating likely and less likely outputs according to\nhumans. We compare our method with several state-of-the-art controlled text\ngeneration models via automatic and human evaluations and show that our models'\ncapability of generating less likely outputs is improved.",
                "Probabilistic computation and uncertainty quantification with emerging\n  covariance\nBuilding robust, interpretable, and secure AI system requires quantifying and\nrepresenting uncertainty under a probabilistic perspective to mimic human\ncognitive abilities. However, probabilistic computation presents significant\nchallenges for most conventional artificial neural network, as they are\nessentially implemented in a deterministic manner. In this paper, we develop an\nefficient probabilistic computation framework by truncating the probabilistic\nrepresentation of neural activation up to its mean and covariance and construct\na moment neural network that encapsulates the nonlinear coupling between the\nmean and covariance of the underlying stochastic network. We reveal that when\nonly the mean but not the covariance is supervised during gradient-based\nlearning, the unsupervised covariance spontaneously emerges from its nonlinear\ncoupling with the mean and faithfully captures the uncertainty associated with\nmodel predictions. Our findings highlight the inherent simplicity of\nprobabilistic computation by seamlessly incorporating uncertainty into model\nprediction, paving the way for integrating it into large-scale AI systems."
            ],
            "interesting paper": 1
        },
        {
            "papers": [
                "Causal Imitability Under Context-Specific Independence Relations\nDrawbacks of ignoring the causal mechanisms when performing imitation\nlearning have recently been acknowledged. Several approaches both to assess the\nfeasibility of imitation and to circumvent causal confounding and causal\nmisspecifications have been proposed in the literature. However, the potential\nbenefits of the incorporation of additional information about the underlying\ncausal structure are left unexplored. An example of such overlooked information\nis context-specific independence (CSI), i.e., independence that holds only in\ncertain contexts. We consider the problem of causal imitation learning when CSI\nrelations are known. We prove that the decision problem pertaining to the\nfeasibility of imitation in this setting is NP-hard. Further, we provide a\nnecessary graphical criterion for imitation learning under CSI and show that\nunder a structural assumption, this criterion is also sufficient. Finally, we\npropose a sound algorithmic approach for causal imitation learning which takes\nboth CSI relations and data into account.",
                "Constrained Causal Bayesian Optimization\nWe propose constrained causal Bayesian optimization (cCBO), an approach for\nfinding interventions in a known causal graph that optimize a target variable\nunder some constraints. cCBO first reduces the search space by exploiting the\ngraph structure and, if available, an observational dataset; and then solves\nthe restricted optimization problem by modelling target and constraint\nquantities using Gaussian processes and by sequentially selecting interventions\nvia a constrained expected improvement acquisition function. We propose\ndifferent surrogate models that enable to integrate observational and\ninterventional data while capturing correlation among effects with increasing\nlevels of sophistication. We evaluate cCBO on artificial and real-world causal\ngraphs showing successful trade off between fast convergence and percentage of\nfeasible interventions.",
                "Nonparametric Identifiability of Causal Representations from Unknown\n  Interventions\nWe study causal representation learning, the task of inferring latent causal\nvariables and their causal relations from high-dimensional mixtures of the\nvariables. Prior work relies on weak supervision, in the form of counterfactual\npre- and post-intervention views or temporal structure; places restrictive\nassumptions, such as linearity, on the mixing function or latent causal model;\nor requires partial knowledge of the generative process, such as the causal\ngraph or intervention targets. We instead consider the general setting in which\nboth the causal model and the mixing function are nonparametric. The learning\nsignal takes the form of multiple datasets, or environments, arising from\nunknown interventions in the underlying causal model. Our goal is to identify\nboth the ground truth latents and their causal graph up to a set of ambiguities\nwhich we show to be irresolvable from interventional data. We study the\nfundamental setting of two causal variables and prove that the observational\ndistribution and one perfect intervention per node suffice for identifiability,\nsubject to a genericity condition. This condition rules out spurious solutions\nthat involve fine-tuning of the intervened and observational distributions,\nmirroring similar conditions for nonlinear cause-effect inference. For an\narbitrary number of variables, we show that at least one pair of distinct\nperfect interventional domains per node guarantees identifiability. Further, we\ndemonstrate that the strengths of causal influences among the latent variables\nare preserved by all equivalent solutions, rendering the inferred\nrepresentation appropriate for drawing causal conclusions from new data. Our\nstudy provides the first identifiability results for the general nonparametric\nsetting with unknown interventions, and elucidates what is possible and\nimpossible for causal representation learning without more direct supervision.",
                "A Comparison of Decision Algorithms on Newcomblike Problems\nWhen formulated using Bayesian networks, two standard decision algorithms\n(Evidential Decision Theory and Causal Decision Theory) can be shown to fail\nsystematically when faced with aspects of the prisoner's dilemma and so-called\n\"Newcomblike\" problems. We describe a new form of decision algorithm, called\nTimeless Decision Theory, which consistently wins on these problems.",
                "Examining the Emergence of Deductive Reasoning in Generative Language\n  Models\nWe conduct a preliminary inquiry into the ability of generative transformer\nmodels to deductively reason from premises provided. We observe notable\ndifferences in the performance of models coming from different training setups\nand find that the deductive reasoning ability increases with scale. Further, we\ndiscover that the performance generally does not decrease with the length of\nthe deductive chain needed to reach the conclusion, with the exception of\nOpenAI GPT-3 and GPT-3.5 models. Our study considers a wide variety of\ntransformer-decoder models, ranging from 117 million to 175 billion parameters\nin size.",
                "CFL: Causally Fair Language Models Through Token-level Attribute\n  Controlled Generation\nWe propose a method to control the attributes of Language Models (LMs) for\nthe text generation task using Causal Average Treatment Effect (ATE) scores and\ncounterfactual augmentation. We explore this method, in the context of LM\ndetoxification, and propose the Causally Fair Language (CFL) architecture for\ndetoxifying pre-trained LMs in a plug-and-play manner. Our architecture is\nbased on a Structural Causal Model (SCM) that is mathematically transparent and\ncomputationally efficient as compared with many existing detoxification\ntechniques. We also propose several new metrics that aim to better understand\nthe behaviour of LMs in the context of toxic text generation. Further, we\nachieve state of the art performance for toxic degeneration, which are computed\nusing \\RTP (RTP) benchmark. Our experiments show that CFL achieves such a\ndetoxification without much impact on the model perplexity. We also show that\nCFL mitigates the unintended bias problem through experiments on the BOLD\ndataset."
            ],
            "interesting paper": 3
        },
        {
            "papers": [
                "From Temporal to Contemporaneous Iterative Causal Discovery in the\n  Presence of Latent Confounders\nWe present a constraint-based algorithm for learning causal structures from\nobservational time-series data, in the presence of latent confounders. We\nassume a discrete-time, stationary structural vector autoregressive process,\nwith both temporal and contemporaneous causal relations. One may ask if\ntemporal and contemporaneous relations should be treated differently. The\npresented algorithm gradually refines a causal graph by learning long-term\ntemporal relations before short-term ones, where contemporaneous relations are\nlearned last. This ordering of causal relations to be learnt leads to a\nreduction in the required number of statistical tests. We validate this\nreduction empirically and demonstrate that it leads to higher accuracy for\nsynthetic data and more plausible causal graphs for real-world data compared to\nstate-of-the-art algorithms.",
                "Learning Causally Disentangled Representations via the Principle of\n  Independent Causal Mechanisms\nLearning disentangled causal representations is a challenging problem that\nhas gained significant attention recently due to its implications for\nextracting meaningful information for downstream tasks. In this work, we define\na new notion of causal disentanglement from the perspective of independent\ncausal mechanisms. We propose ICM-VAE, a framework for learning causally\ndisentangled representations supervised by causally related observed labels. We\nmodel causal mechanisms using nonlinear learnable flow-based diffeomorphic\nfunctions to map noise variables to latent causal variables. Further, to\npromote the disentanglement of causal factors, we propose a causal\ndisentanglement prior learned from auxiliary labels and the latent causal\nstructure. We theoretically show the identifiability of causal factors and\nmechanisms up to permutation and elementwise reparameterization. We empirically\ndemonstrate that our framework induces highly disentangled causal factors,\nimproves interventional robustness, and is compatible with counterfactual\ngeneration.",
                "AI Liability Insurance With an Example in AI-Powered E-diagnosis System\nArtificial Intelligence (AI) has received an increasing amount of attention\nin multiple areas. The uncertainties and risks in AI-powered systems have\ncreated reluctance in their wild adoption. As an economic solution to\ncompensate for potential damages, AI liability insurance is a promising market\nto enhance the integration of AI into daily life. In this work, we use an\nAI-powered E-diagnosis system as an example to study AI liability insurance. We\nprovide a quantitative risk assessment model with evidence-based numerical\nanalysis. We discuss the insurability criteria for AI technologies and suggest\nnecessary adjustments to accommodate the features of AI products. We show that\nAI liability insurance can act as a regulatory mechanism to incentivize\ncompliant behaviors and serve as a certificate of high-quality AI systems.\nFurthermore, we suggest premium adjustment to reflect the dynamic evolution of\nthe inherent uncertainty in AI. Moral hazard problems are discussed and\nsuggestions for AI liability insurance are provided.",
                "Joint Learning of Label and Environment Causal Independence for Graph\n  Out-of-Distribution Generalization\nWe tackle the problem of graph out-of-distribution (OOD) generalization.\nExisting graph OOD algorithms either rely on restricted assumptions or fail to\nexploit environment information in training data. In this work, we propose to\nsimultaneously incorporate label and environment causal independence (LECI) to\nfully make use of label and environment information, thereby addressing the\nchallenges faced by prior methods on identifying causal and invariant\nsubgraphs. We further develop an adversarial training strategy to jointly\noptimize these two properties for causal subgraph discovery with theoretical\nguarantees. Extensive experiments and analysis show that LECI significantly\noutperforms prior methods on both synthetic and real-world datasets,\nestablishing LECI as a practical and effective solution for graph OOD\ngeneralization.\n  Our code is available at https://github.com/divelab/LECI.",
                "Delphic Offline Reinforcement Learning under Nonidentifiable Hidden\n  Confounding\nA prominent challenge of offline reinforcement learning (RL) is the issue of\nhidden confounding: unobserved variables may influence both the actions taken\nby the agent and the observed outcomes. Hidden confounding can compromise the\nvalidity of any causal conclusion drawn from data and presents a major obstacle\nto effective offline RL. In the present paper, we tackle the problem of hidden\nconfounding in the nonidentifiable setting. We propose a definition of\nuncertainty due to hidden confounding bias, termed delphic uncertainty, which\nuses variation over world models compatible with the observations, and\ndifferentiate it from the well-known epistemic and aleatoric uncertainties. We\nderive a practical method for estimating the three types of uncertainties, and\nconstruct a pessimistic offline RL algorithm to account for them. Our method\ndoes not assume identifiability of the unobserved confounders, and attempts to\nreduce the amount of confounding bias. We demonstrate through extensive\nexperiments and ablations the efficacy of our approach on a sepsis management\nbenchmark, as well as on electronic health records. Our results suggest that\nnonidentifiable hidden confounding bias can be mitigated to improve offline RL\nsolutions in practice.",
                "Theoretical Behavior of XAI Methods in the Presence of Suppressor\n  Variables\nIn recent years, the community of 'explainable artificial intelligence' (XAI)\nhas created a vast body of methods to bridge a perceived gap between model\n'complexity' and 'interpretability'. However, a concrete problem to be solved\nby XAI methods has not yet been formally stated. As a result, XAI methods are\nlacking theoretical and empirical evidence for the 'correctness' of their\nexplanations, limiting their potential use for quality-control and transparency\npurposes. At the same time, Haufe et al. (2014) showed, using simple toy\nexamples, that even standard interpretations of linear models can be highly\nmisleading. Specifically, high importance may be attributed to so-called\nsuppressor variables lacking any statistical relation to the prediction target.\nThis behavior has been confirmed empirically for a large array of XAI methods\nin Wilming et al. (2022). Here, we go one step further by deriving analytical\nexpressions for the behavior of a variety of popular XAI methods on a simple\ntwo-dimensional binary classification problem involving Gaussian\nclass-conditional distributions. We show that the majority of the studied\napproaches will attribute non-zero importance to a non-class-related suppressor\nfeature in the presence of correlated noise. This poses important limitations\non the interpretations and conclusions that the outputs of these XAI methods\ncan afford."
            ],
            "interesting paper": 2
        },
        {
            "papers": [
                "Do we become wiser with time? On causal equivalence with tiered\n  background knowledge\nEquivalence classes of DAGs (represented by CPDAGs) may be too large to\nprovide useful causal information. Here, we address incorporating tiered\nbackground knowledge yielding restricted equivalence classes represented by\n'tiered MPDAGs'. Tiered knowledge leads to considerable gains in\ninformativeness and computational efficiency: We show that construction of\ntiered MPDAGs only requires application of Meek's 1st rule, and that tiered\nMPDAGs (unlike general MPDAGs) are chain graphs with chordal components. This\nentails simplifications e.g. of determining valid adjustment sets for causal\neffect estimation. Further, we characterise when one tiered ordering is more\ninformative than another, providing insights into useful aspects of background\nknowledge.",
                "Accelerating science with human-aware artificial intelligence\nArtificial intelligence (AI) models trained on published scientific findings\nhave been used to invent valuable materials and targeted therapies, but they\ntypically ignore the human scientists who continually alter the landscape of\ndiscovery. Here we show that incorporating the distribution of human expertise\nby training unsupervised models on simulated inferences cognitively accessible\nto experts dramatically improves (up to 400%) AI prediction of future\ndiscoveries beyond those focused on research content alone, especially when\nrelevant literature is sparse. These models succeed by predicting human\npredictions and the scientists who will make them. By tuning human-aware AI to\navoid the crowd, we can generate scientifically promising \"alien\" hypotheses\nunlikely to be imagined or pursued without intervention until the distant\nfuture, which hold promise to punctuate scientific advance beyond questions\ncurrently pursued. Accelerating human discovery or probing its blind spots,\nhuman-aware AI enables us to move toward and beyond the contemporary scientific\nfrontier.",
                "XAI Renaissance: Redefining Interpretability in Medical Diagnostic\n  Models\nAs machine learning models become increasingly prevalent in medical\ndiagnostics, the need for interpretability and transparency becomes paramount.\nThe XAI Renaissance signifies a significant shift in the field, aiming to\nredefine the interpretability of medical diagnostic models. This paper explores\nthe innovative approaches and methodologies within the realm of Explainable AI\n(XAI) that are revolutionizing the interpretability of medical diagnostic\nmodels. By shedding light on the underlying decision-making process, XAI\ntechniques empower healthcare professionals to understand, trust, and\neffectively utilize these models for accurate and reliable medical diagnoses.\nThis review highlights the key advancements in XAI for medical diagnostics and\ntheir potential to transform the healthcare landscape, ultimately improving\npatient outcomes and fostering trust in AI-driven diagnostic systems.",
                "Enough With \"Human-AI Collaboration\"\nDescribing our interaction with Artificial Intelligence (AI) systems as\n'collaboration' is well-intentioned, but flawed. Not only is it misleading, but\nit also takes away the credit of AI 'labour' from the humans behind it, and\nerases and obscures an often exploitative arrangement between AI producers and\nconsumers. The AI 'collaboration' metaphor is merely the latest episode in a\nlong history of labour appropriation and credit reassignment that\ndisenfranchises labourers in the Global South. I propose that viewing AI as a\ntool or an instrument, rather than a collaborator, is more accurate, and\nultimately fairer.",
                "Guided scenarios with simulated expert personae: a remarkable strategy\n  to perform cognitive work\nLarge language models (LLMs) trained on a substantial corpus of human\nknowledge and literature productively work with a large array of facts from\nthat corpus. Surprisingly, they are also able to re-create the behaviors of\npersonae that are captured within the corpus. By forming teams of simulated\npersonae, supplying contexts that set the stage, and providing gentle prompts,\none can move through scenarios that elicit expert behavior to perform\nmeaningful cognitive work. The power of this strategy is demonstrated with two\nexamples, one attacking factuality of LLM responses and the other reproducing a\nvery recently published result in quantum optics.",
                "Probabilistic Concept Bottleneck Models\nInterpretable models are designed to make decisions in a human-interpretable\nmanner. Representatively, Concept Bottleneck Models (CBM) follow a two-step\nprocess of concept prediction and class prediction based on the predicted\nconcepts. CBM provides explanations with high-level concepts derived from\nconcept predictions; thus, reliable concept predictions are important for\ntrustworthiness. In this study, we address the ambiguity issue that can harm\nreliability. While the existence of a concept can often be ambiguous in the\ndata, CBM predicts concepts deterministically without considering this\nambiguity. To provide a reliable interpretation against this ambiguity, we\npropose Probabilistic Concept Bottleneck Models (ProbCBM). By leveraging\nprobabilistic concept embeddings, ProbCBM models uncertainty in concept\nprediction and provides explanations based on the concept and its corresponding\nuncertainty. This uncertainty enhances the reliability of the explanations.\nFurthermore, as class uncertainty is derived from concept uncertainty in\nProbCBM, we can explain class uncertainty by means of concept uncertainty. Code\nis publicly available at https://github.com/ejkim47/prob-cbm."
            ],
            "interesting paper": 2
        }
    ],
    "Morgan Davis": [
        {
            "papers": [
                "Towards a Capability Assessment Model for the Comprehension and Adoption\n  of AI in Organisations\nThe comprehension and adoption of Artificial Intelligence (AI) are beset with\npractical and ethical problems. This article presents a 5-level AI Capability\nAssessment Model (AI-CAM) and a related AI Capabilities Matrix (AI-CM) to\nassist practitioners in AI comprehension and adoption. These practical tools\nwere developed with business executives, technologists, and other\norganisational stakeholders in mind. They are founded on a comprehensive\nconception of AI compared to those in other AI adoption models and are also\nopen-source artefacts. Thus, the AI-CAM and AI-CM present an accessible\nresource to help inform organisational decision-makers on the capability\nrequirements for (1) AI-based data analytics use cases based on machine\nlearning technologies; (2) Knowledge representation to engineer and represent\ndata, information and knowledge using semantic technologies; and (3) AI-based\nsolutions that seek to emulate human reasoning and decision-making. The AI-CAM\ncovers the core capability dimensions (business, data, technology,\norganisation, AI skills, risks, and ethical considerations) required at the\nfive capability maturity levels to achieve optimal use of AI in organisations.",
                "Learning-Based Automatic Synthesis of Software Code and Configuration\nIncreasing demands in software industry and scarcity of software engineers\nmotivates researchers and practitioners to automate the process of software\ngeneration and configuration. Large scale automatic software generation and\nconfiguration is a very complex and challenging task. In this proposal, we set\nout to investigate this problem by breaking down automatic software generation\nand configuration into two different tasks. In first task, we propose to\nsynthesize software automatically with input output specifications. This task\nis further broken down into two sub-tasks. The first sub-task is about\nsynthesizing programs with a genetic algorithm which is driven by a neural\nnetwork based fitness function trained with program traces and specifications.\nFor the second sub-task, we formulate program synthesis as a continuous\noptimization problem and synthesize programs with covariance matrix adaption\nevolutionary strategy (a state-of-the-art continuous optimization method).\nFinally, for the second task, we propose to synthesize configurations of large\nscale software from different input files (e.g. software manuals,\nconfigurations files, online blogs, etc.) using a sequence-to-sequence deep\nlearning mechanism.",
                "A Mini Review on the utilization of Reinforcement Learning with OPC UA\nReinforcement Learning (RL) is a powerful machine learning paradigm that has\nbeen applied in various fields such as robotics, natural language processing\nand game playing achieving state-of-the-art results. Targeted to solve\nsequential decision making problems, it is by design able to learn from\nexperience and therefore adapt to changing dynamic environments. These\ncapabilities make it a prime candidate for controlling and optimizing complex\nprocesses in industry. The key to fully exploiting this potential is the\nseamless integration of RL into existing industrial systems. The industrial\ncommunication standard Open Platform Communications UnifiedArchitecture (OPC\nUA) could bridge this gap. However, since RL and OPC UA are from different\nfields,there is a need for researchers to bridge the gap between the two\ntechnologies. This work serves to bridge this gap by providing a brief\ntechnical overview of both technologies and carrying out a semi-exhaustive\nliterature review to gain insights on how RL and OPC UA are applied in\ncombination. With this survey, three main research topics have been identified,\nfollowing the intersection of RL with OPC UA. The results of the literature\nreview show that RL is a promising technology for the control and optimization\nof industrial processes, but does not yet have the necessary standardized\ninterfaces to be deployed in real-world scenarios with reasonably low effort.",
                "Trends and Challenges Towards an Effective Data-Driven Decision Making\n  in UK SMEs: Case Studies and Lessons Learnt from the Analysis of 85 SMEs\nThe adoption of data science brings vast benefits to Small and Medium-sized\nEnterprises (SMEs) including business productivity, economic growth, innovation\nand jobs creation. Data Science can support SMEs to optimise production\nprocesses, anticipate customers' needs, predict machinery failures and deliver\nefficient smart services. Businesses can also harness the power of Artificial\nIntelligence (AI) and Big Data and the smart use of digital technologies to\nenhance productivity and performance, paving the way for innovation. However,\nintegrating data science decisions into an SME requires both skills and IT\ninvestments. In most cases, such expenses are beyond the means of SMEs due to\nlimited resources and restricted access to financing. This paper presents\ntrends and challenges towards an effective data-driven decision making for\norganisations based on a case study of 85 SMEs, mostly from the West Midlands\nregion of England. The work is supported as part of a 3 years ERDF (European\nRegional Development Funded project) in the areas of big data management,\nanalytics and business intelligence. We present two case studies that\ndemonstrates the potential of Digitisation, AI and Machine Learning and use\nthese as examples to unveil challenges and showcase the wealth of current\navailable opportunities for SMEs.",
                "Symbolic Regression via Control Variable Genetic Programming\nLearning symbolic expressions directly from experiment data is a vital step\nin AI-driven scientific discovery. Nevertheless, state-of-the-art approaches\nare limited to learning simple expressions. Regressing expressions involving\nmany independent variables still remain out of reach. Motivated by the control\nvariable experiments widely utilized in science, we propose Control Variable\nGenetic Programming (CVGP) for symbolic regression over many independent\nvariables. CVGP expedites symbolic expression discovery via customized\nexperiment design, rather than learning from a fixed dataset collected a\npriori. CVGP starts by fitting simple expressions involving a small set of\nindependent variables using genetic programming, under controlled experiments\nwhere other variables are held as constants. It then extends expressions\nlearned in previous generations by adding new independent variables, using new\ncontrol variable experiments in which these variables are allowed to vary.\nTheoretically, we show CVGP as an incremental building approach can yield an\nexponential reduction in the search space when learning a class of expressions.\nExperimentally, CVGP outperforms several baselines in learning symbolic\nexpressions involving multiple independent variables.",
                "Learning to Act through Evolution of Neural Diversity in Random Neural\n  Networks\nBiological nervous systems consist of networks of diverse, sophisticated\ninformation processors in the form of neurons of different classes. In most\nartificial neural networks (ANNs), neural computation is abstracted to an\nactivation function that is usually shared between all neurons within a layer\nor even the whole network; training of ANNs focuses on synaptic optimization.\nIn this paper, we propose the optimization of neuro-centric parameters to\nattain a set of diverse neurons that can perform complex computations.\nDemonstrating the promise of the approach, we show that evolving neural\nparameters alone allows agents to solve various reinforcement learning tasks\nwithout optimizing any synaptic weights. While not aiming to be an accurate\nbiological model, parameterizing neurons to a larger degree than the current\ncommon practice, allows us to ask questions about the computational abilities\nafforded by neural diversity in random neural networks. The presented results\nopen up interesting future research directions, such as combining evolved\nneural diversity with activity-dependent plasticity."
            ],
            "interesting paper": 3
        },
        {
            "papers": [
                "Attention Paper: How Generative AI Reshapes Digital Shadow Industry?\nThe rapid development of digital economy has led to the emergence of various\nblack and shadow internet industries, which pose potential risks that can be\nidentified and managed through digital risk management (DRM) that uses\ndifferent techniques such as machine learning and deep learning. The evolution\nof DRM architecture has been driven by changes in data forms. However, the\ndevelopment of AI-generated content (AIGC) technology, such as ChatGPT and\nStable Diffusion, has given black and shadow industries powerful tools to\npersonalize data and generate realistic images and conversations for fraudulent\nactivities. This poses a challenge for DRM systems to control risks from the\nsource of data generation and to respond quickly to the fast-changing risk\nenvironment. This paper aims to provide a technical analysis of the challenges\nand opportunities of AIGC from upstream, midstream, and downstream paths of\nblack/shadow industries and suggest future directions for improving existing\nrisk control systems. The paper will explore the new black and shadow\ntechniques triggered by generative AI technology and provide insights for\nbuilding the next-generation DRM system.",
                "On Computing Universal Plans for Partially Observable Multi-Agent Path\n  Finding\nMulti-agent routing problems have drawn significant attention nowadays due to\ntheir broad industrial applications in, e.g., warehouse robots, logistics\nautomation, and traffic control. Conventionally, they are modelled as classical\nplanning problems. In this paper, we argue that it is beneficial to formulate\nthem as universal planning problems. We therefore propose universal plans, also\nknown as policies, as the solution concepts, and implement a system called\nASP-MAUPF (Answer Set Programming for Multi-Agent Universal Plan Finding) for\ncomputing them. Given an arbitrary two-dimensional map and a profile of goals\nfor the agents, the system finds a feasible universal plan for each agent that\nensures no collision with others. We use the system to conduct some\nexperiments, and make some observations on the types of goal profiles and\nenvironments that will have feasible policies, and how they may depend on\nagents' sensors. We also demonstrate how users can customize action preferences\nto compute more efficient policies, even (near-)optimal ones.",
                "AI Techniques in the Microservices Life-Cycle: A Survey\nMicroservices is a popular architectural style for the development of\ndistributed software, with an emphasis on modularity, scalability, and\nflexibility. Indeed, in microservice systems, functionalities are provided by\nloosely coupled, small services, each focusing on a specific business\ncapability. Building a system according to the microservices architectural\nstyle brings a number of challenges, mainly related to how the different\nmicroservices are deployed and coordinated and how they interact. In this\npaper, we provide a survey about how techniques in the area of Artificial\nIntelligence have been used to tackle these challenges.",
                "Generating Synergistic Formulaic Alpha Collections via Reinforcement\n  Learning\nIn the field of quantitative trading, it is common practice to transform raw\nhistorical stock data into indicative signals for the market trend. Such\nsignals are called alpha factors. Alphas in formula forms are more\ninterpretable and thus favored by practitioners concerned with risk. In\npractice, a set of formulaic alphas is often used together for better modeling\nprecision, so we need to find synergistic formulaic alpha sets that work well\ntogether. However, most traditional alpha generators mine alphas one by one\nseparately, overlooking the fact that the alphas would be combined later. In\nthis paper, we propose a new alpha-mining framework that prioritizes mining a\nsynergistic set of alphas, i.e., it directly uses the performance of the\ndownstream combination model to optimize the alpha generator. Our framework\nalso leverages the strong exploratory capabilities of reinforcement\nlearning~(RL) to better explore the vast search space of formulaic alphas. The\ncontribution to the combination models' performance is assigned to be the\nreturn used in the RL process, driving the alpha generator to find better\nalphas that improve upon the current set. Experimental evaluations on\nreal-world stock market data demonstrate both the effectiveness and the\nefficiency of our framework for stock trend forecasting. The investment\nsimulation results show that our framework is able to achieve higher returns\ncompared to previous approaches.",
                "Alert of the Second Decision-maker: An Introduction to Human-AI Conflict\nThe collaboration between humans and artificial intelligence (AI) is a\nsignificant feature in this digital age. However, humans and AI may have\nobservation, interpretation, and action conflicts when working synchronously.\nThis phenomenon is often masked by faults and, unfortunately, overlooked. This\npaper systematically introduces the human-AI conflict concept, causes,\nmeasurement methods, and risk assessment. The results highlight that there is a\npotential second decision-maker besides the human, which is the AI; the\nhuman-AI conflict is a unique and emerging risk in digitalized process systems;\nand this is an interdisciplinary field that needs to be distinguished from\ntraditional fault and failure analysis; the conflict risk is significant and\ncannot be ignored.",
                "C-MCTS: Safe Planning with Monte Carlo Tree Search\nThe Constrained Markov Decision Process (CMDP) formulation allows to solve\nsafety-critical decision making tasks that are subject to constraints. While\nCMDPs have been extensively studied in the Reinforcement Learning literature,\nlittle attention has been given to sampling-based planning algorithms such as\nMCTS for solving them. Previous approaches perform conservatively with respect\nto costs as they avoid constraint violations by using Monte Carlo cost\nestimates that suffer from high variance. We propose Constrained MCTS (C-MCTS),\nwhich estimates cost using a safety critic that is trained with Temporal\nDifference learning in an offline phase prior to agent deployment. The critic\nlimits exploration by pruning unsafe trajectories within MCTS during\ndeployment. C-MCTS satisfies cost constraints but operates closer to the\nconstraint boundary, achieving higher rewards than previous work. As a nice\nbyproduct, the planner is more efficient w.r.t. planning steps. Most\nimportantly, under model mismatch between the planner and the real world,\nC-MCTS is less susceptible to cost violations than previous work."
            ],
            "interesting paper": 2
        },
        {
            "papers": [
                "Optimization for truss design using Bayesian optimization\nIn this work, geometry optimization of mechanical truss using computer-aided\nfinite element analysis is presented. The shape of the truss is a dominant\nfactor in determining the capacity of load it can bear. At a given parameter\nspace, our goal is to find the parameters of a hull that maximize the\nload-bearing capacity and also don't yield to the induced stress. We rely on\nfinite element analysis, which is a computationally costly design analysis tool\nfor design evaluation. For such expensive to-evaluate functions, we chose\nBayesian optimization as our optimization framework which has empirically\nproven sample efficient than other simulation-based optimization methods.\n  By utilizing Bayesian optimization algorithms, the truss design involves\niteratively evaluating a set of candidate truss designs and updating a\nprobabilistic model of the design space based on the results. The model is used\nto predict the performance of each candidate design, and the next candidate\ndesign is selected based on the prediction and an acquisition function that\nbalances exploration and exploitation of the design space. Our result can be\nused as a baseline for future study on AI-based optimization in expensive\nengineering domains especially in finite element Analysis.",
                "Frontier AI developers need an internal audit function\nThis article argues that frontier artificial intelligence (AI) developers\nneed an internal audit function. First, it describes the role of internal audit\nin corporate governance: internal audit evaluates the adequacy and\neffectiveness of a company's risk management, control, and governance\nprocesses. It is organizationally independent from senior management and\nreports directly to the board of directors, typically its audit committee. In\nthe IIA's Three Lines Model, internal audit serves as the third line and is\nresponsible for providing assurance to the board, while the Combined Assurance\nFramework highlights the need to coordinate the activities of internal and\nexternal assurance providers. Next, the article provides an overview of key\ngovernance challenges in frontier AI development: dangerous capabilities can\narise unpredictably and undetected; it is difficult to prevent a deployed model\nfrom causing harm; frontier models can proliferate rapidly; it is inherently\ndifficult to assess frontier AI risks; and frontier AI developers do not seem\nto follow best practices in risk governance. Finally, the article discusses how\nan internal audit function could address some of these challenges: internal\naudit could identify ineffective risk management practices; it could ensure\nthat the board of directors has a more accurate understanding of the current\nlevel of risk and the adequacy of the developer's risk management practices;\nand it could serve as a contact point for whistleblowers. In light of rapid\nprogress in AI research and development, frontier AI developers need to\nstrengthen their risk governance. Instead of reinventing the wheel, they should\nfollow existing best practices. While this might not be sufficient, they should\nnot skip this obvious first step.",
                "Improved Sales Forecasting using Trend and Seasonality Decomposition\n  with LightGBM\nRetail sales forecasting presents a significant challenge for large retailers\nsuch as Walmart and Amazon, due to the vast assortment of products,\ngeographical location heterogeneity, seasonality, and external factors\nincluding weather, local economic conditions, and geopolitical events. Various\nmethods have been employed to tackle this challenge, including traditional time\nseries models, machine learning models, and neural network mechanisms, but the\ndifficulty persists. Categorizing data into relevant groups has been shown to\nimprove sales forecast accuracy as time series from different categories may\nexhibit distinct patterns. In this paper, we propose a new measure to indicate\nthe unique impacts of the trend and seasonality components on a time series and\nsuggest grouping time series based on this measure. We apply this approach to\nWalmart sales data from 01/29/2011 to 05/22/2016 and generate sales forecasts\nfrom 05/23/2016 to 06/19/2016. Our experiments show that the proposed strategy\ncan achieve improved accuracy. Furthermore, we present a robust pipeline for\nconducting retail sales forecasting.",
                "How Do UX Practitioners Communicate AI as a Design Material? Artifacts,\n  Conceptions, and Propositions\nUX practitioners (UXPs) face novel challenges when working with and\ncommunicating artificial intelligence (AI) as a design material. We explore how\nUXPs communicate AI concepts when given hands-on experience training and\nexperimenting with AI models. To do so, we conducted a task-based design study\nwith 27 UXPs in which they prototyped and created a design presentation for a\nAI-enabled interface while having access to a simple AI model training tool.\nThrough analyzing UXPs' design presentations and post-activity interviews, we\nfound that although UXPs struggled to clearly communicate some AI concepts,\ntinkering with AI broadened common ground when communicating with technical\nstakeholders. UXPs also identified key risks and benefits of AI in their\ndesigns, and proposed concrete next steps for both UX and AI work. We conclude\nwith a sensitizing concept and recommendations for design and AI tools to\nenhance multi-stakeholder communication and collaboration when crafting\nhuman-centered AI experiences.",
                "Let the Flows Tell: Solving Graph Combinatorial Optimization Problems\n  with GFlowNets\nCombinatorial optimization (CO) problems are often NP-hard and thus out of\nreach for exact algorithms, making them a tempting domain to apply machine\nlearning methods. The highly structured constraints in these problems can\nhinder either optimization or sampling directly in the solution space. On the\nother hand, GFlowNets have recently emerged as a powerful machinery to\nefficiently sample from composite unnormalized densities sequentially and have\nthe potential to amortize such solution-searching processes in CO, as well as\ngenerate diverse solution candidates. In this paper, we design Markov decision\nprocesses (MDPs) for different combinatorial problems and propose to train\nconditional GFlowNets to sample from the solution space. Efficient training\ntechniques are also developed to benefit long-range credit assignment. Through\nextensive experiments on a variety of different CO tasks with synthetic and\nrealistic data, we demonstrate that GFlowNet policies can efficiently find\nhigh-quality solutions. Our implementation is open-sourced at\nhttps://github.com/zdhNarsil/GFlowNet-CombOpt.",
                "Learning Interpretable Models of Aircraft Handling Behaviour by\n  Reinforcement Learning from Human Feedback\nWe propose a method to capture the handling abilities of fast jet pilots in a\nsoftware model via reinforcement learning (RL) from human preference feedback.\nWe use pairwise preferences over simulated flight trajectories to learn an\ninterpretable rule-based model called a reward tree, which enables the\nautomated scoring of trajectories alongside an explanatory rationale. We train\nan RL agent to execute high-quality handling behaviour by using the reward tree\nas the objective, and thereby generate data for iterative preference collection\nand further refinement of both tree and agent. Experiments with synthetic\npreferences show reward trees to be competitive with uninterpretable neural\nnetwork reward models on quantitative and qualitative evaluations."
            ],
            "interesting paper": 5
        },
        {
            "papers": [
                "Optimization's Neglected Normative Commitments\nOptimization is offered as an objective approach to resolving complex,\nreal-world decisions involving uncertainty and conflicting interests. It drives\nbusiness strategies as well as public policies and, increasingly, lies at the\nheart of sophisticated machine learning systems. A paradigm used to approach\npotentially high-stakes decisions, optimization relies on abstracting the real\nworld to a set of decision(s), objective(s) and constraint(s). Drawing from the\nmodeling process and a range of actual cases, this paper describes the\nnormative choices and assumptions that are necessarily part of using\noptimization. It then identifies six emergent problems that may be neglected:\n1) Misspecified values can yield optimizations that omit certain imperatives\naltogether or incorporate them incorrectly as a constraint or as part of the\nobjective, 2) Problematic decision boundaries can lead to faulty modularity\nassumptions and feedback loops, 3) Failing to account for multiple agents'\ndivergent goals and decisions can lead to policies that serve only certain\nnarrow interests, 4) Mislabeling and mismeasurement can introduce bias and\nimprecision, 5) Faulty use of relaxation and approximation methods,\nunaccompanied by formal characterizations and guarantees, can severely impede\napplicability, and 6) Treating optimization as a justification for action,\nwithout specifying the necessary contextual information, can lead to ethically\ndubious or faulty decisions. Suggestions are given to further understand and\ncurb the harms that can arise when optimization is used wrongfully.",
                "Probing reaction channels via reinforcement learning\nWe propose a reinforcement learning based method to identify important\nconfigurations that connect reactant and product states along chemical reaction\npaths. By shooting multiple trajectories from these configurations, we can\ngenerate an ensemble of configurations that concentrate on the transition path\nensemble. This configuration ensemble can be effectively employed in a neural\nnetwork-based partial differential equation solver to obtain an approximation\nsolution of a restricted Backward Kolmogorov equation, even when the dimension\nof the problem is very high. The resulting solution, known as the committor\nfunction, encodes mechanistic information for the reaction and can in turn be\nused to evaluate reaction rates.",
                "MLOps: A Step Forward to Enterprise Machine Learning\nMachine Learning Operations (MLOps) is becoming a highly crucial part of\nbusinesses looking to capitalize on the benefits of AI and ML models. This\nresearch presents a detailed review of MLOps, its benefits, difficulties,\nevolutions, and important underlying technologies such as MLOps frameworks,\nDocker, GitHub actions, and Kubernetes. The MLOps workflow, which includes\nmodel design, deployment, and operations, is explained in detail along with the\nvarious tools necessary for both model and data exploration and deployment.\nThis article also puts light on the end-to-end production of ML projects using\nvarious maturity levels of automated pipelines, with the least at no automation\nat all and the highest with complete CI/CD and CT capabilities. Furthermore, a\ndetailed example of an enterprise-level MLOps project for an object detection\nservice is used to explain the workflow of the technology in a real-world\nscenario. For this purpose, a web application hosting a pre-trained model from\nTensorFlow 2 Model Zoo is packaged and deployed to the internet making sure\nthat the system is scalable, reliable, and optimized for deployment at an\nenterprise level.",
                "AI Coach Assist: An Automated Approach for Call Recommendation in\n  Contact Centers for Agent Coaching\nIn recent years, the utilization of Artificial Intelligence (AI) in the\ncontact center industry is on the rise. One area where AI can have a\nsignificant impact is in the coaching of contact center agents. By analyzing\ncall transcripts using Natural Language Processing (NLP) techniques, it would\nbe possible to quickly determine which calls are most relevant for coaching\npurposes. In this paper, we present AI Coach Assist, which leverages the\npre-trained transformer-based language models to determine whether a given call\nis coachable or not based on the quality assurance (QA) questions asked by the\ncontact center managers or supervisors. The system was trained and evaluated on\na large dataset collected from real-world contact centers and provides an\neffective way to recommend calls to the contact center managers that are more\nlikely to contain coachable moments. Our experimental findings demonstrate the\npotential of AI Coach Assist to improve the coaching process, resulting in\nenhancing the performance of contact center agents.",
                "Synthesizing a Progression of Subtasks for Block-Based Visual\n  Programming Tasks\nBlock-based visual programming environments play an increasingly important\nrole in introducing computing concepts to K-12 students. In recent years, they\nhave also gained popularity in neuro-symbolic AI, serving as a benchmark to\nevaluate general problem-solving and logical reasoning skills. The open-ended\nand conceptual nature of these visual programming tasks make them challenging,\nboth for state-of-the-art AI agents as well as for novice programmers. A\nnatural approach to providing assistance for problem-solving is breaking down a\ncomplex task into a progression of simpler subtasks; however, this is not\ntrivial given that the solution codes are typically nested and have non-linear\nexecution behavior. In this paper, we formalize the problem of synthesizing\nsuch a progression for a given reference block-based visual programming task.\nWe propose a novel synthesis algorithm that generates a progression of subtasks\nthat are high-quality, well-spaced in terms of their complexity, and solving\nthis progression leads to solving the reference task. We show the utility of\nour synthesis algorithm in improving the efficacy of AI agents (in this case,\nneural program synthesizers) for solving tasks in the Karel programming\nenvironment. Then, we conduct a user study to demonstrate that our synthesized\nprogression of subtasks can assist a novice programmer in solving tasks in the\nHour of Code: Maze Challenge by Code-dot-org.",
                "PFNs4BO: In-Context Learning for Bayesian Optimization\nIn this paper, we use Prior-data Fitted Networks (PFNs) as a flexible\nsurrogate for Bayesian Optimization (BO). PFNs are neural processes that are\ntrained to approximate the posterior predictive distribution (PPD) through\nin-context learning on any prior distribution that can be efficiently sampled\nfrom. We describe how this flexibility can be exploited for surrogate modeling\nin BO. We use PFNs to mimic a naive Gaussian process (GP), an advanced GP, and\na Bayesian Neural Network (BNN). In addition, we show how to incorporate\nfurther information into the prior, such as allowing hints about the position\nof optima (user priors), ignoring irrelevant dimensions, and performing\nnon-myopic BO by learning the acquisition function. The flexibility underlying\nthese extensions opens up vast possibilities for using PFNs for BO. We\ndemonstrate the usefulness of PFNs for BO in a large-scale evaluation on\nartificial GP samples and three different hyperparameter optimization testbeds:\nHPO-B, Bayesmark, and PD1. We publish code alongside trained models at\ngithub.com/automl/PFNs4BO."
            ],
            "interesting paper": 6
        },
        {
            "papers": [
                "Improving Confidence in Evolutionary Mine Scheduling via Uncertainty\n  Discounting\nMine planning is a complex task that involves many uncertainties. During\nearly stage feasibility, available mineral resources can only be estimated\nbased on limited sampling of ore grades from sparse drilling, leading to large\nuncertainty in under-sampled parts of the deposit. Planning the extraction\nschedule of ore over the life of a mine is crucial for its economic viability.\nWe introduce a new approach for determining an \"optimal schedule under\nuncertainty\" that provides probabilistic bounds on the profits obtained in each\nperiod. This treatment of uncertainty within an economic framework reduces\npreviously difficult-to-use models of variability into actionable insights. The\nnew method discounts profits based on uncertainty within an evolutionary\nalgorithm, sacrificing economic optimality of a single geological model for\nimproving the downside risk over an ensemble of equally likely models. We\nprovide experimental studies using Maptek's mine planning software Evolution.\nOur results show that our new approach is successful for effectively making use\nof uncertainty information in the mine planning process.",
                "Towards Autonomous and Safe Last-mile Deliveries with AI-augmented\n  Self-driving Delivery Robots\nIn addition to its crucial impact on customer satisfaction, last-mile\ndelivery (LMD) is notorious for being the most time-consuming and costly stage\nof the shipping process. Pressing environmental concerns combined with the\nrecent surge of e-commerce sales have sparked renewed interest in automation\nand electrification of last-mile logistics. To address the hurdles faced by\nexisting robotic couriers, this paper introduces a customer-centric and\nsafety-conscious LMD system for small urban communities based on AI-assisted\nautonomous delivery robots. The presented framework enables end-to-end\nautomation and optimization of the logistic process while catering for\nreal-world imposed operational uncertainties, clients' preferred time\nschedules, and safety of pedestrians. To this end, the integrated optimization\ncomponent is modeled as a robust variant of the Cumulative Capacitated Vehicle\nRouting Problem with Time Windows, where routes are constructed under uncertain\ntravel times with an objective to minimize the total latency of deliveries\n(i.e., the overall waiting time of customers, which can negatively affect their\nsatisfaction). We demonstrate the proposed LMD system's utility through\nreal-world trials in a university campus with a single robotic courier.\nImplementation aspects as well as the findings and practical insights gained\nfrom the deployment are discussed in detail. Lastly, we round up the\ncontributions with numerical simulations to investigate the scalability of the\ndeveloped mathematical formulation with respect to the number of robotic\nvehicles and customers.",
                "Employing Explainable Artificial Intelligence (XAI) Methodologies to\n  Analyze the Correlation between Input Variables and Tensile Strength in\n  Additively Manufactured Samples\nThis research paper explores the impact of various input parameters,\nincluding Infill percentage, Layer Height, Extrusion Temperature, and Print\nSpeed, on the resulting Tensile Strength in objects produced through additive\nmanufacturing. The main objective of this study is to enhance our understanding\nof the correlation between the input parameters and Tensile Strength, as well\nas to identify the key factors influencing the performance of the additive\nmanufacturing process. To achieve this objective, we introduced the utilization\nof Explainable Artificial Intelligence (XAI) techniques for the first time,\nwhich allowed us to analyze the data and gain valuable insights into the\nsystem's behavior. Specifically, we employed SHAP (SHapley Additive\nexPlanations), a widely adopted framework for interpreting machine learning\nmodel predictions, to provide explanations for the behavior of a machine\nlearning model trained on the data. Our findings reveal that the Infill\npercentage and Extrusion Temperature have the most significant influence on\nTensile Strength, while the impact of Layer Height and Print Speed is\nrelatively minor. Furthermore, we discovered that the relationship between the\ninput parameters and Tensile Strength is highly intricate and nonlinear, making\nit difficult to accurately describe using simple linear models.",
                "The Digital Divide in Process Safety: Quantitative Risk Analysis of\n  Human-AI Collaboration\nDigital technologies have dramatically accelerated the digital transformation\nin process industries, boosted new industrial applications, upgraded the\nproduction system, and enhanced operational efficiency. In contrast, the\nchallenges and gaps between human and artificial intelligence (AI) have become\nmore and more prominent, whereas the digital divide in process safety is\naggregating. The study attempts to address the following questions: (i)What is\nAI in the process safety context? (ii)What is the difference between AI and\nhumans in process safety? (iii)How do AI and humans collaborate in process\nsafety? (iv)What are the challenges and gaps in human-AI collaboration? (v)How\nto quantify the risk of human-AI collaboration in process safety? Qualitative\nrisk analysis based on brainstorming and literature review, and quantitative\nrisk analysis based on layer of protection analysis (LOPA) and Bayesian network\n(BN), were applied to explore and model. The importance of human reliability\nshould be stressed in the digital age, not usually to increase the reliability\nof AI, and human-centered AI design in process safety needs to be propagated.",
                "ProcessGPT: Transforming Business Process Management with Generative\n  Artificial Intelligence\nGenerative Pre-trained Transformer (GPT) is a state-of-the-art machine\nlearning model capable of generating human-like text through natural language\nprocessing (NLP). GPT is trained on massive amounts of text data and uses deep\nlearning techniques to learn patterns and relationships within the data,\nenabling it to generate coherent and contextually appropriate text. This\nposition paper proposes using GPT technology to generate new process models\nwhen/if needed. We introduce ProcessGPT as a new technology that has the\npotential to enhance decision-making in data-centric and knowledge-intensive\nprocesses. ProcessGPT can be designed by training a generative pre-trained\ntransformer model on a large dataset of business process data. This model can\nthen be fine-tuned on specific process domains and trained to generate process\nflows and make decisions based on context and user input. The model can be\nintegrated with NLP and machine learning techniques to provide insights and\nrecommendations for process improvement. Furthermore, the model can automate\nrepetitive tasks and improve process efficiency while enabling knowledge\nworkers to communicate analysis findings, supporting evidence, and make\ndecisions. ProcessGPT can revolutionize business process management (BPM) by\noffering a powerful tool for process augmentation, automation and improvement.\nFinally, we demonstrate how ProcessGPT can be a powerful tool for augmenting\ndata engineers in maintaining data ecosystem processes within large bank\norganizations. Our scenario highlights the potential of this approach to\nimprove efficiency, reduce costs, and enhance the quality of business\noperations through the automation of data-centric and knowledge-intensive\nprocesses. These results underscore the promise of ProcessGPT as a\ntransformative technology for organizations looking to improve their process\nworkflows.",
                "Optimizing Airbnb Search Journey with Multi-task Learning\nAt Airbnb, an online marketplace for stays and experiences, guests often\nspend weeks exploring and comparing multiple items before making a final\nreservation request. Each reservation request may then potentially be rejected\nor cancelled by the host prior to check-in. The long and exploratory nature of\nthe search journey, as well as the need to balance both guest and host\npreferences, present unique challenges for Airbnb search ranking. In this\npaper, we present Journey Ranker, a new multi-task deep learning model\narchitecture that addresses these challenges. Journey Ranker leverages\nintermediate guest actions as milestones, both positive and negative, to better\nprogress the guest towards a successful booking. It also uses contextual\ninformation such as guest state and search query to balance guest and host\npreferences. Its modular and extensible design, consisting of four modules with\nclear separation of concerns, allows for easy application to use cases beyond\nthe Airbnb search ranking context. We conducted offline and online testing of\nthe Journey Ranker and successfully deployed it in production to four different\nAirbnb products with significant business metrics improvements."
            ],
            "interesting paper": 2
        },
        {
            "papers": [
                "Design of the Reverse Logistics System for Medical Waste Recycling Part\n  II: Route Optimization with Case Study under COVID-19 Pandemic\nMedical waste recycling and treatment has gradually drawn concerns from the\nwhole society, as the amount of medical waste generated is increasing\ndramatically, especially during the pandemic of COVID-19. To tackle the\nemerging challenges, this study designs a reverse logistics system architecture\nwith three modules, i.e., medical waste classification & monitoring module,\ntemporary storage & disposal site (disposal site for short) selection module,\nas well as route optimization module. This overall solution design won the\nGrand Prize of the \"YUNFENG CUP\" China National Contest on Green Supply and\nReverse Logistics Design ranking 1st. This paper focuses on the design of the\nroute optimization module. In this module, a route optimization problem is\ndesigned considering transportation costs and multiple risk costs (e.g.,\nenvironment risk, population risk, property risk, and other accident-related\nrisks). The Analytic Hierarchy Process is employed to determine the weights for\neach risk element, and a customized genetic algorithm is developed to solve the\nroute optimization problem. A case study under the COVID-19 pandemic is further\nprovided to verify the proposed model. Limited by length, detailed descriptions\nof the whole system and the other modules can be found at\nhttps://shorturl.at/cdY59.",
                "Towards a Unifying Model of Rationality in Multiagent Systems\nMultiagent systems deployed in the real world need to cooperate with other\nagents (including humans) nearly as effectively as these agents cooperate with\none another. To design such AI, and provide guarantees of its effectiveness, we\nneed to clearly specify what types of agents our AI must be able to cooperate\nwith. In this work we propose a generic model of socially intelligent agents,\nwhich are individually rational learners that are also able to cooperate with\none another (in the sense that their joint behavior is Pareto efficient). We\ndefine rationality in terms of the regret incurred by each agent over its\nlifetime, and show how we can construct socially intelligent agents for\ndifferent forms of regret. We then discuss the implications of this model for\nthe development of \"robust\" MAS that can cooperate with a wide variety of\nsocially intelligent agents.",
                "Generalized Disparate Impact for Configurable Fairness Solutions in ML\nWe make two contributions in the field of AI fairness over continuous\nprotected attributes. First, we show that the Hirschfeld-Gebelein-Renyi (HGR)\nindicator (the only one currently available for such a case) is valuable but\nsubject to a few crucial limitations regarding semantics, interpretability, and\nrobustness. Second, we introduce a family of indicators that are: 1)\ncomplementary to HGR in terms of semantics; 2) fully interpretable and\ntransparent; 3) robust over finite samples; 4) configurable to suit specific\napplications. Our approach also allows us to define fine-grained constraints to\npermit certain types of dependence and forbid others selectively. By expanding\nthe available options for continuous protected attributes, our approach\nrepresents a significant contribution to the area of fair artificial\nintelligence.",
                "RE-centric Recommendations for the Development of Trustworthy(er)\n  Autonomous Systems\nComplying with the EU AI Act (AIA) guidelines while developing and\nimplementing AI systems will soon be mandatory within the EU. However,\npractitioners lack actionable instructions to operationalise ethics during AI\nsystems development. A literature review of different ethical guidelines\nrevealed inconsistencies in the principles addressed and the terminology used\nto describe them. Furthermore, requirements engineering (RE), which is\nidentified to foster trustworthiness in the AI development process from the\nearly stages was observed to be absent in a lot of frameworks that support the\ndevelopment of ethical and trustworthy AI. This incongruous phrasing combined\nwith a lack of concrete development practices makes trustworthy AI development\nharder. To address this concern, we formulated a comparison table for the\nterminology used and the coverage of the ethical AI principles in major ethical\nAI guidelines. We then examined the applicability of ethical AI development\nframeworks for performing effective RE during the development of trustworthy AI\nsystems. A tertiary review and meta-analysis of literature discussing ethical\nAI frameworks revealed their limitations when developing trustworthy AI. Based\non our findings, we propose recommendations to address such limitations during\nthe development of trustworthy AI.",
                "What is Essential for Unseen Goal Generalization of Offline\n  Goal-conditioned RL?\nOffline goal-conditioned RL (GCRL) offers a way to train general-purpose\nagents from fully offline datasets. In addition to being conservative within\nthe dataset, the generalization ability to achieve unseen goals is another\nfundamental challenge for offline GCRL. However, to the best of our knowledge,\nthis problem has not been well studied yet. In this paper, we study\nout-of-distribution (OOD) generalization of offline GCRL both theoretically and\nempirically to identify factors that are important. In a number of experiments,\nwe observe that weighted imitation learning enjoys better generalization than\npessimism-based offline RL method. Based on this insight, we derive a theory\nfor OOD generalization, which characterizes several important design choices.\nWe then propose a new offline GCRL method, Generalizable Offline\ngoAl-condiTioned RL (GOAT), by combining the findings from our theoretical and\nempirical studies. On a new benchmark containing 9 independent identically\ndistributed (IID) tasks and 17 OOD tasks, GOAT outperforms current\nstate-of-the-art methods by a large margin.",
                "Maximize to Explore: One Objective Function Fusing Estimation, Planning,\n  and Exploration\nIn online reinforcement learning (online RL), balancing exploration and\nexploitation is crucial for finding an optimal policy in a sample-efficient\nway. To achieve this, existing sample-efficient online RL algorithms typically\nconsist of three components: estimation, planning, and exploration. However, in\norder to cope with general function approximators, most of them involve\nimpractical algorithmic components to incentivize exploration, such as\noptimization within data-dependent level-sets or complicated sampling\nprocedures. To address this challenge, we propose an easy-to-implement RL\nframework called \\textit{Maximize to Explore} (\\texttt{MEX}), which only needs\nto optimize \\emph{unconstrainedly} a single objective that integrates the\nestimation and planning components while balancing exploration and exploitation\nautomatically. Theoretically, we prove that \\texttt{MEX} achieves a sublinear\nregret with general function approximations for Markov decision processes (MDP)\nand is further extendable to two-player zero-sum Markov games (MG). Meanwhile,\nwe adapt deep RL baselines to design practical versions of \\texttt{MEX}, in\nboth model-free and model-based manners, which can outperform baselines by a\nstable margin in various MuJoCo environments with sparse rewards. Compared with\nexisting sample-efficient online RL algorithms with general function\napproximations, \\texttt{MEX} achieves similar sample efficiency while enjoying\na lower computational cost and is more compatible with modern deep RL methods."
            ],
            "interesting paper": 5
        },
        {
            "papers": [
                "Towards Omni-generalizable Neural Methods for Vehicle Routing Problems\nLearning heuristics for vehicle routing problems (VRPs) has gained much\nattention due to the less reliance on hand-crafted rules. However, existing\nmethods are typically trained and tested on the same task with a fixed size and\ndistribution (of nodes), and hence suffer from limited generalization\nperformance. This paper studies a challenging yet realistic setting, which\nconsiders generalization across both size and distribution in VRPs. We propose\na generic meta-learning framework, which enables effective training of an\ninitialized model with the capability of fast adaptation to new tasks during\ninference. We further develop a simple yet efficient approximation method to\nreduce the training overhead. Extensive experiments on both synthetic and\nbenchmark instances of the traveling salesman problem (TSP) and capacitated\nvehicle routing problem (CVRP) demonstrate the effectiveness of our method. The\ncode is available at: https://github.com/RoyalSkye/Omni-VRP.",
                "Disentangling and Operationalizing AI Fairness at LinkedIn\nOperationalizing AI fairness at LinkedIn's scale is challenging not only\nbecause there are multiple mutually incompatible definitions of fairness but\nalso because determining what is fair depends on the specifics and context of\nthe product where AI is deployed. Moreover, AI practitioners need clarity on\nwhat fairness expectations need to be addressed at the AI level. In this paper,\nwe present the evolving AI fairness framework used at LinkedIn to address these\nthree challenges. The framework disentangles AI fairness by separating out\nequal treatment and equitable product expectations. Rather than imposing a\ntrade-off between these two commonly opposing interpretations of fairness, the\nframework provides clear guidelines for operationalizing equal AI treatment\ncomplemented with a product equity strategy. This paper focuses on the equal AI\ntreatment component of LinkedIn's AI fairness framework, shares the principles\nthat support it, and illustrates their application through a case study. We\nhope this paper will encourage other big tech companies to join us in sharing\ntheir approach to operationalizing AI fairness at scale, so that together we\ncan keep advancing this constantly evolving field.",
                "Machine Learning Applications in Cascading Failure Analysis in Power\n  Systems: A Review\nCascading failures pose a significant threat to power grids and have garnered\nconsiderable research interest in the power system domain. The inherent\nuncertainty and severe impact associated with cascading failures have raised\nconcerns, prompting the development of various techniques to study these\ncomplex phenomena. In recent years, advancements in monitoring technologies and\nthe availability of large volumes of data from power systems, coupled with the\nemergence of intelligent algorithms, have made machine learning (ML) techniques\nincreasingly attractive for addressing cascading failure problems. This survey\nprovides a comprehensive overview of ML-based techniques for analyzing\ncascading failures in power systems. The survey categorizes these techniques\nbased on the evolutionary phases of the cascade process in power systems, as\nwell as studies focusing on cascade resiliency before the occurrence of\ncascades and problems related to cascades after their termination. By\norganizing and presenting these works into relevant categories, this survey\naims to offer insights and a systematic understanding the role of ML in\nmitigating cascading failures in power systems.",
                "Evaluating GPT's Programming Capability through CodeWars' Katas\nIn the burgeoning field of artificial intelligence (AI), understanding the\ncapabilities and limitations of programming-oriented models is crucial. This\npaper presents a novel evaluation of the programming proficiency of Generative\nPretrained Transformer (GPT) models, specifically GPT-3.5 and GPT-4, against\ncoding problems of varying difficulty levels drawn from Codewars. The\nexperiments reveal a distinct boundary at the 3kyu level, beyond which these\nGPT models struggle to provide solutions. These findings led to the proposal of\na measure for coding problem complexity that incorporates both problem\ndifficulty and the time required for solution. The research emphasizes the need\nfor validation and creative thinking capabilities in AI models to better\nemulate human problem-solving techniques. Future work aims to refine this\nproposed complexity measure, enhance AI models with these suggested\ncapabilities, and develop an objective measure for programming problem\ndifficulty. The results of this research offer invaluable insights for\nimproving AI programming capabilities and advancing the frontier of AI\nproblem-solving abilities.",
                "Necessary and Sufficient Conditions for Optimal Decision Trees using\n  Dynamic Programming\nGlobal optimization of decision trees has shown to be promising in terms of\naccuracy, size, and consequently human comprehensibility. However, many of the\nmethods used rely on general-purpose solvers for which scalability remains an\nissue. Dynamic programming methods have been shown to scale much better because\nthey exploit the tree structure by solving subtrees as independent subproblems.\nHowever, this only works when an objective can be optimized separately for\nsubtrees. We explore this relationship in detail and show the necessary and\nsufficient conditions for such separability and generalize previous dynamic\nprogramming approaches into a framework that can optimize any combination of\nseparable objectives and constraints. Experiments on five application domains\nshow the general applicability of this framework, while outperforming the\nscalability of general-purpose solvers by a large margin.",
                "GPT Models in Construction Industry: Opportunities, Limitations, and a\n  Use Case Validation\nLarge Language Models(LLMs) trained on large data sets came into prominence\nin 2018 after Google introduced BERT. Subsequently, different LLMs such as GPT\nmodels from OpenAI have been released. These models perform well on diverse\ntasks and have been gaining widespread applications in fields such as business\nand education. However, little is known about the opportunities and challenges\nof using LLMs in the construction industry. Thus, this study aims to assess GPT\nmodels in the construction industry. A critical review, expert discussion and\ncase study validation are employed to achieve the study objectives. The\nfindings revealed opportunities for GPT models throughout the project\nlifecycle. The challenges of leveraging GPT models are highlighted and a use\ncase prototype is developed for materials selection and optimization. The\nfindings of the study would be of benefit to researchers, practitioners and\nstakeholders, as it presents research vistas for LLMs in the construction\nindustry."
            ],
            "interesting paper": 1
        },
        {
            "papers": [
                "Responsible Design Patterns for Machine Learning Pipelines\nIntegrating ethical practices into the AI development process for artificial\nintelligence (AI) is essential to ensure safe, fair, and responsible operation.\nAI ethics involves applying ethical principles to the entire life cycle of AI\nsystems. This is essential to mitigate potential risks and harms associated\nwith AI, such as algorithm biases. To achieve this goal, responsible design\npatterns (RDPs) are critical for Machine Learning (ML) pipelines to guarantee\nethical and fair outcomes. In this paper, we propose a comprehensive framework\nincorporating RDPs into ML pipelines to mitigate risks and ensure the ethical\ndevelopment of AI systems. Our framework comprises new responsible AI design\npatterns for ML pipelines identified through a survey of AI ethics and data\nmanagement experts and validated through real-world scenarios with expert\nfeedback. The framework guides AI developers, data scientists, and\npolicy-makers to implement ethical practices in AI development and deploy\nresponsible AI systems in production.",
                "A Virtual-Force Based Swarm Algorithm for Balanced Circular Bin Packing\n  Problems\nBalanced circular bin packing problems consist in positioning a given number\nof weighted circles in order to minimize the radius of a circular container\nwhile satisfying equilibrium constraints. These problems are NP-hard, highly\nconstrained and dimensional. This paper describes a swarm algorithm based on a\nvirtual-force system in order to solve balanced circular bin packing problems.\nIn the proposed approach, a system of forces is applied to each component\nallowing to take into account the constraints and minimizing the objective\nfunction using the fundamental principle of dynamics. The proposed algorithm is\nexperimented and validated on benchmarks of various balanced circular bin\npacking problems with up to 300 circles. The reported results allow to assess\nthe effectiveness of the proposed approach compared to existing results from\nthe literature.",
                "Credit Card Fraud Detection Using Asexual Reproduction Optimization\nAs the number of credit card users has increased, detecting fraud in this\ndomain has become a vital issue. Previous literature has applied various\nsupervised and unsupervised machine learning methods to find an effective fraud\ndetection system. However, some of these methods require an enormous amount of\ntime to achieve reasonable accuracy. In this paper, an Asexual Reproduction\nOptimization (ARO) approach was employed, which is a supervised method to\ndetect credit card fraud. ARO refers to a kind of production in which one\nparent produces some offspring. By applying this method and sampling just from\nthe majority class, the effectiveness of the classification is increased. A\ncomparison to Artificial Immune Systems (AIS), which is one of the best methods\nimplemented on current datasets, has shown that the proposed method is able to\nremarkably reduce the required training time and at the same time increase the\nrecall that is important in fraud detection problems. The obtained results show\nthat ARO achieves the best cost in a short time, and consequently, it can be\nconsidered a real-time fraud detection system.",
                "Constrained Causal Bayesian Optimization\nWe propose constrained causal Bayesian optimization (cCBO), an approach for\nfinding interventions in a known causal graph that optimize a target variable\nunder some constraints. cCBO first reduces the search space by exploiting the\ngraph structure and, if available, an observational dataset; and then solves\nthe restricted optimization problem by modelling target and constraint\nquantities using Gaussian processes and by sequentially selecting interventions\nvia a constrained expected improvement acquisition function. We propose\ndifferent surrogate models that enable to integrate observational and\ninterventional data while capturing correlation among effects with increasing\nlevels of sophistication. We evaluate cCBO on artificial and real-world causal\ngraphs showing successful trade off between fast convergence and percentage of\nfeasible interventions.",
                "BetaZero: Belief-State Planning for Long-Horizon POMDPs using Learned\n  Approximations\nReal-world planning problems, including autonomous driving and sustainable\nenergy applications like carbon storage and resource exploration, have recently\nbeen modeled as partially observable Markov decision processes (POMDPs) and\nsolved using approximate methods. To solve high-dimensional POMDPs in practice,\nstate-of-the-art methods use online planning with problem-specific heuristics\nto reduce planning horizons and make the problems tractable. Algorithms that\nlearn approximations to replace heuristics have recently found success in\nlarge-scale fully observable domains. The key insight is the combination of\nonline Monte Carlo tree search with offline neural network approximations of\nthe optimal policy and value function. In this work, we bring this insight to\npartially observable domains and propose BetaZero, a belief-state planning\nalgorithm for high-dimensional POMDPs. BetaZero learns offline approximations\nthat replace heuristics to enable online decision making in long-horizon\nproblems. We address several challenges inherent in large-scale partially\nobservable domains; namely challenges of transitioning in stochastic\nenvironments, prioritizing action branching with a limited search budget, and\nrepresenting beliefs as input to the network. To formalize the use of all\nlimited search information, we train against a novel $Q$-weighted visit counts\npolicy. We test BetaZero on various well-established POMDP benchmarks found in\nthe literature and a real-world problem of critical mineral exploration.\nExperiments show that BetaZero outperforms state-of-the-art POMDP solvers on a\nvariety of tasks.",
                "Representation-Driven Reinforcement Learning\nWe present a representation-driven framework for reinforcement learning. By\nrepresenting policies as estimates of their expected values, we leverage\ntechniques from contextual bandits to guide exploration and exploitation.\nParticularly, embedding a policy network into a linear feature space allows us\nto reframe the exploration-exploitation problem as a\nrepresentation-exploitation problem, where good policy representations enable\noptimal exploration. We demonstrate the effectiveness of this framework through\nits application to evolutionary and policy gradient-based approaches, leading\nto significantly improved performance compared to traditional methods. Our\nframework provides a new perspective on reinforcement learning, highlighting\nthe importance of policy representation in determining optimal\nexploration-exploitation strategies."
            ],
            "interesting paper": 5
        },
        {
            "papers": [
                "A Modular Test Bed for Reinforcement Learning Incorporation into\n  Industrial Applications\nThis application paper explores the potential of using reinforcement learning\n(RL) to address the demands of Industry 4.0, including shorter time-to-market,\nmass customization, and batch size one production. Specifically, we present a\nuse case in which the task is to transport and assemble goods through a model\nfactory following predefined rules. Each simulation run involves placing a\nspecific number of goods of random color at the entry point. The objective is\nto transport the goods to the assembly station, where two rivets are installed\nin each product, connecting the upper part to the lower part. Following the\ninstallation of rivets, blue products must be transported to the exit, while\ngreen products are to be transported to storage. The study focuses on the\napplication of reinforcement learning techniques to address this problem and\nimprove the efficiency of the production process.",
                "AI Liability Insurance With an Example in AI-Powered E-diagnosis System\nArtificial Intelligence (AI) has received an increasing amount of attention\nin multiple areas. The uncertainties and risks in AI-powered systems have\ncreated reluctance in their wild adoption. As an economic solution to\ncompensate for potential damages, AI liability insurance is a promising market\nto enhance the integration of AI into daily life. In this work, we use an\nAI-powered E-diagnosis system as an example to study AI liability insurance. We\nprovide a quantitative risk assessment model with evidence-based numerical\nanalysis. We discuss the insurability criteria for AI technologies and suggest\nnecessary adjustments to accommodate the features of AI products. We show that\nAI liability insurance can act as a regulatory mechanism to incentivize\ncompliant behaviors and serve as a certificate of high-quality AI systems.\nFurthermore, we suggest premium adjustment to reflect the dynamic evolution of\nthe inherent uncertainty in AI. Moral hazard problems are discussed and\nsuggestions for AI liability insurance are provided.",
                "An Architecture for Deploying Reinforcement Learning in Industrial\n  Environments\nIndustry 4.0 is driven by demands like shorter time-to-market, mass\ncustomization of products, and batch size one production. Reinforcement\nLearning (RL), a machine learning paradigm shown to possess a great potential\nin improving and surpassing human level performance in numerous complex tasks,\nallows coping with the mentioned demands. In this paper, we present an OPC UA\nbased Operational Technology (OT)-aware RL architecture, which extends the\nstandard RL setting, combining it with the setting of digital twins. Moreover,\nwe define an OPC UA information model allowing for a generalized plug-and-play\nlike approach for exchanging the RL agent used. In conclusion, we demonstrate\nand evaluate the architecture, by creating a proof of concept. By means of\nsolving a toy example, we show that this architecture can be used to determine\nthe optimal policy using a real control system.",
                "Deep Q-Learning versus Proximal Policy Optimization: Performance\n  Comparison in a Material Sorting Task\nThis paper presents a comparison between two well-known deep Reinforcement\nLearning (RL) algorithms: Deep Q-Learning (DQN) and Proximal Policy\nOptimization (PPO) in a simulated production system. We utilize a Petri Net\n(PN)-based simulation environment, which was previously proposed in related\nwork. The performance of the two algorithms is compared based on several\nevaluation metrics, including average percentage of correctly assembled and\nsorted products, average episode length, and percentage of successful episodes.\nThe results show that PPO outperforms DQN in terms of all evaluation metrics.\nThe study highlights the advantages of policy-based algorithms in problems with\nhigh-dimensional state and action spaces. The study contributes to the field of\ndeep RL in context of production systems by providing insights into the\neffectiveness of different algorithms and their suitability for different\ntasks.",
                "Large-Batch, Iteration-Efficient Neural Bayesian Design Optimization\nBayesian optimization (BO) provides a powerful framework for optimizing\nblack-box, expensive-to-evaluate functions. It is therefore an attractive tool\nfor engineering design problems, typically involving multiple objectives.\nThanks to the rapid advances in fabrication and measurement methods as well as\nparallel computing infrastructure, querying many design problems can be heavily\nparallelized. This class of problems challenges BO with an unprecedented setup\nwhere it has to deal with very large batches, shifting its focus from sample\nefficiency to iteration efficiency. We present a novel Bayesian optimization\nframework specifically tailored to address these limitations. Our key\ncontribution is a highly scalable, sample-based acquisition function that\nperforms a non-dominated sorting of not only the objectives but also their\nassociated uncertainty. We show that our acquisition function in combination\nwith different Bayesian neural network surrogates is effective in\ndata-intensive environments with a minimal number of iterations. We demonstrate\nthe superiority of our method by comparing it with state-of-the-art\nmulti-objective optimizations. We perform our evaluation on two real-world\nproblems -- airfoil design and 3D printing -- showcasing the applicability and\nefficiency of our approach. Our code is available at:\nhttps://github.com/an-on-ym-ous/lbn_mobo",
                "Federated Learning Games for Reconfigurable Intelligent Surfaces via\n  Causal Representations\nIn this paper, we investigate the problem of robust Reconfigurable\nIntelligent Surface (RIS) phase-shifts configuration over heterogeneous\ncommunication environments. The problem is formulated as a distributed learning\nproblem over different environments in a Federated Learning (FL) setting.\nEquivalently, this corresponds to a game played between multiple RISs, as\nlearning agents, in heterogeneous environments. Using Invariant Risk\nMinimization (IRM) and its FL equivalent, dubbed FL Games, we solve the RIS\nconfiguration problem by learning invariant causal representations across\nmultiple environments and then predicting the phases. The solution corresponds\nto playing according to Best Response Dynamics (BRD) which yields the Nash\nEquilibrium of the FL game. The representation learner and the phase predictor\nare modeled by two neural networks, and their performance is validated via\nsimulations against other benchmarks from the literature. Our results show that\ncausality-based learning yields a predictor that is 15% more accurate in unseen\nOut-of-Distribution (OoD) environments."
            ],
            "interesting paper": 1
        },
        {
            "papers": [
                "Enough With \"Human-AI Collaboration\"\nDescribing our interaction with Artificial Intelligence (AI) systems as\n'collaboration' is well-intentioned, but flawed. Not only is it misleading, but\nit also takes away the credit of AI 'labour' from the humans behind it, and\nerases and obscures an often exploitative arrangement between AI producers and\nconsumers. The AI 'collaboration' metaphor is merely the latest episode in a\nlong history of labour appropriation and credit reassignment that\ndisenfranchises labourers in the Global South. I propose that viewing AI as a\ntool or an instrument, rather than a collaborator, is more accurate, and\nultimately fairer.",
                "AlerTiger: Deep Learning for AI Model Health Monitoring at LinkedIn\nData-driven companies use AI models extensively to develop products and\nintelligent business solutions, making the health of these models crucial for\nbusiness success. Model monitoring and alerting in industries pose unique\nchallenges, including a lack of clear model health metrics definition, label\nsparsity, and fast model iterations that result in short-lived models and\nfeatures. As a product, there are also requirements for scalability,\ngeneralizability, and explainability. To tackle these challenges, we propose\nAlerTiger, a deep-learning-based MLOps model monitoring system that helps AI\nteams across the company monitor their AI models' health by detecting anomalies\nin models' input features and output score over time. The system consists of\nfour major steps: model statistics generation, deep-learning-based anomaly\ndetection, anomaly post-processing, and user alerting. Our solution generates\nthree categories of statistics to indicate AI model health, offers a two-stage\ndeep anomaly detection solution to address label sparsity and attain the\ngeneralizability of monitoring new models, and provides holistic reports for\nactionable alerts. This approach has been deployed to most of LinkedIn's\nproduction AI models for over a year and has identified several model issues\nthat later led to significant business metric gains after fixing.",
                "XAI Renaissance: Redefining Interpretability in Medical Diagnostic\n  Models\nAs machine learning models become increasingly prevalent in medical\ndiagnostics, the need for interpretability and transparency becomes paramount.\nThe XAI Renaissance signifies a significant shift in the field, aiming to\nredefine the interpretability of medical diagnostic models. This paper explores\nthe innovative approaches and methodologies within the realm of Explainable AI\n(XAI) that are revolutionizing the interpretability of medical diagnostic\nmodels. By shedding light on the underlying decision-making process, XAI\ntechniques empower healthcare professionals to understand, trust, and\neffectively utilize these models for accurate and reliable medical diagnoses.\nThis review highlights the key advancements in XAI for medical diagnostics and\ntheir potential to transform the healthcare landscape, ultimately improving\npatient outcomes and fostering trust in AI-driven diagnostic systems.",
                "Painsight: An Extendable Opinion Mining Framework for Detecting Pain\n  Points Based on Online Customer Reviews\nAs the e-commerce market continues to expand and online transactions\nproliferate, customer reviews have emerged as a critical element in shaping the\npurchasing decisions of prospective buyers. Previous studies have endeavored to\nidentify key aspects of customer reviews through the development of sentiment\nanalysis models and topic models. However, extracting specific dissatisfaction\nfactors remains a challenging task. In this study, we delineate the pain point\ndetection problem and propose Painsight, an unsupervised framework for\nautomatically extracting distinct dissatisfaction factors from customer reviews\nwithout relying on ground truth labels. Painsight employs pre-trained language\nmodels to construct sentiment analysis and topic models, leveraging attribution\nscores derived from model gradients to extract dissatisfaction factors. Upon\napplication of the proposed methodology to customer review data spanning five\nproduct categories, we successfully identified and categorized dissatisfaction\nfactors within each group, as well as isolated factors for each type. Notably,\nPainsight outperformed benchmark methods, achieving substantial performance\nenhancements and exceptional results in human evaluations.",
                "Accelerating science with human-aware artificial intelligence\nArtificial intelligence (AI) models trained on published scientific findings\nhave been used to invent valuable materials and targeted therapies, but they\ntypically ignore the human scientists who continually alter the landscape of\ndiscovery. Here we show that incorporating the distribution of human expertise\nby training unsupervised models on simulated inferences cognitively accessible\nto experts dramatically improves (up to 400%) AI prediction of future\ndiscoveries beyond those focused on research content alone, especially when\nrelevant literature is sparse. These models succeed by predicting human\npredictions and the scientists who will make them. By tuning human-aware AI to\navoid the crowd, we can generate scientifically promising \"alien\" hypotheses\nunlikely to be imagined or pursued without intervention until the distant\nfuture, which hold promise to punctuate scientific advance beyond questions\ncurrently pursued. Accelerating human discovery or probing its blind spots,\nhuman-aware AI enables us to move toward and beyond the contemporary scientific\nfrontier.",
                "A Novel Correlation-optimized Deep Learning Method for Wind Speed\n  Forecast\nThe increasing installation rate of wind power poses great challenges to the\nglobal power system. In order to ensure the reliable operation of the power\nsystem, it is necessary to accurately forecast the wind speed and power of the\nwind turbines. At present, deep learning is progressively applied to the wind\nspeed prediction. Nevertheless, the recent deep learning methods still reflect\nthe embarrassment for practical applications due to model interpretability and\nhardware limitation. To this end, a novel deep knowledge-based learning method\nis proposed in this paper. The proposed method hybridizes pre-training method\nand auto-encoder structure to improve data representation and modeling of the\ndeep knowledge-based learning framework. In order to form knowledge and\ncorresponding absorbers, the original data is preprocessed by an optimization\nmodel based on correlation to construct multi-layer networks (knowledge) which\nare absorbed by sequence to sequence (Seq2Seq) models. Specifically, new\ncognition and memory units (CMU) are designed to reinforce traditional deep\nlearning framework. Finally, the effectiveness of the proposed method is\nverified by three wind prediction cases from a wind farm in Liaoning, China.\nExperimental results show that the proposed method increases the stability and\ntraining efficiency compared to the traditional LSTM method and LSTM/GRU-based\nSeq2Seq method for applications of wind speed forecasting."
            ],
            "interesting paper": 2
        }
    ],
    "Alex Johnson": [
        {
            "papers": [
                "ISLE: An Intelligent Streaming Framework for High-Throughput AI\n  Inference in Medical Imaging\nAs the adoption of Artificial Intelligence (AI) systems within the clinical\nenvironment grows, limitations in bandwidth and compute can create\ncommunication bottlenecks when streaming imaging data, leading to delays in\npatient care and increased cost. As such, healthcare providers and AI vendors\nwill require greater computational infrastructure, therefore dramatically\nincreasing costs. To that end, we developed ISLE, an intelligent streaming\nframework for high-throughput, compute- and bandwidth- optimized, and cost\neffective AI inference for clinical decision making at scale. In our\nexperiments, ISLE on average reduced data transmission by 98.02% and decoding\ntime by 98.09%, while increasing throughput by 2,730%. We show that ISLE\nresults in faster turnaround times, and reduced overall cost of data,\ntransmission, and compute, without negatively impacting clinical decision\nmaking using AI systems.",
                "Learning to Act through Evolution of Neural Diversity in Random Neural\n  Networks\nBiological nervous systems consist of networks of diverse, sophisticated\ninformation processors in the form of neurons of different classes. In most\nartificial neural networks (ANNs), neural computation is abstracted to an\nactivation function that is usually shared between all neurons within a layer\nor even the whole network; training of ANNs focuses on synaptic optimization.\nIn this paper, we propose the optimization of neuro-centric parameters to\nattain a set of diverse neurons that can perform complex computations.\nDemonstrating the promise of the approach, we show that evolving neural\nparameters alone allows agents to solve various reinforcement learning tasks\nwithout optimizing any synaptic weights. While not aiming to be an accurate\nbiological model, parameterizing neurons to a larger degree than the current\ncommon practice, allows us to ask questions about the computational abilities\nafforded by neural diversity in random neural networks. The presented results\nopen up interesting future research directions, such as combining evolved\nneural diversity with activity-dependent plasticity.",
                "Improved Algorithms for Allen's Interval Algebra by Dynamic Programming\n  with Sublinear Partitioning\nAllen's interval algebra is one of the most well-known calculi in qualitative\ntemporal reasoning with numerous applications in artificial intelligence.\nRecently, there has been a surge of improvements in the fine-grained complexity\nof NP-hard reasoning tasks, improving the running time from the naive\n$2^{O(n^2)}$ to $O^*((1.0615n)^{n})$, with even faster algorithms for unit\nintervals a bounded number of overlapping intervals (the $O^*(\\cdot)$ notation\nsuppresses polynomial factors). Despite these improvements the best known lower\nbound is still only $2^{o(n)}$ (under the exponential-time hypothesis) and\nmajor improvements in either direction seemingly require fundamental advances\nin computational complexity. In this paper we propose a novel framework for\nsolving NP-hard qualitative reasoning problems which we refer to as dynamic\nprogramming with sublinear partitioning. Using this technique we obtain a major\nimprovement of $O^*((\\frac{cn}{\\log{n}})^{n})$ for Allen's interval algebra. To\ndemonstrate that the technique is applicable to more domains we apply it to a\nproblem in qualitative spatial reasoning, the cardinal direction point algebra,\nand solve it in $O^*((\\frac{cn}{\\log{n}})^{2n/3})$ time. Hence, not only do we\nsignificantly advance the state-of-the-art for NP-hard qualitative reasoning\nproblems, but obtain a novel algorithmic technique that is likely applicable to\nmany problems where $2^{O(n)}$ time algorithms are unlikely.",
                "Deep Learning and Ethics\nThis article appears as chapter 21 of Prince (2023, Understanding Deep\nLearning); a complete draft of the textbook is available here:\nhttp://udlbook.com. This chapter considers potential harms arising from the\ndesign and use of AI systems. These include algorithmic bias, lack of\nexplainability, data privacy violations, militarization, fraud, and\nenvironmental concerns. The aim is not to provide advice on being more ethical.\nInstead, the goal is to express ideas and start conversations in key areas that\nhave received attention in philosophy, political science, and the broader\nsocial sciences.",
                "Applications of Intelligent Systems in Green Technology\nIntelligent Systems (ISs) are technologically advanced machines, which\nperceive and respond to the environment around them. They are usually of\nvarious forms ranging from software to hardware. ISs are generally the fusion\nof Artificial Intelligence (AI), robotics and Internet of things (IoT). In\norder to strengthen ISs, one of the key technologies is green technology (GT).\nIt refers to the continuously advancing methods and materials, which cover\ntechniques for producing energy to non-toxic cleaning products. It may also be\nbroadened to saving energy, and reducing toxic and waste materials in the\nenvironment. The motto of GT can be achieved by using the ISs. In this paper,\nwe present various applications of ISs in GT. Moreover, we discuss various\npossible solutions using ISs in order to overcome the on-going real-life\nproblems.",
                "Black-Box vs. Gray-Box: A Case Study on Learning Table Tennis Ball\n  Trajectory Prediction with Spin and Impacts\nIn this paper, we present a method for table tennis ball trajectory filtering\nand prediction. Our gray-box approach builds on a physical model. At the same\ntime, we use data to learn parameters of the dynamics model, of an extended\nKalman filter, and of a neural model that infers the ball's initial condition.\nWe demonstrate superior prediction performance of our approach over two\nblack-box approaches, which are not supplied with physical prior knowledge. We\ndemonstrate that initializing the spin from parameters of the ball launcher\nusing a neural network drastically improves long-time prediction performance\nover estimating the spin purely from measured ball positions. An accurate\nprediction of the ball trajectory is crucial for successful returns. We\ntherefore evaluate the return performance with a pneumatic artificial muscular\nrobot and achieve a return rate of 29/30 (97.7%)."
            ],
            "interesting paper": 2
        },
        {
            "papers": [
                "Artificial Intelligence-Based Methods for Precision Medicine: Diabetes\n  Risk Prediction\nThe rising prevalence of type 2 diabetes mellitus (T2DM) necessitates the\ndevelopment of predictive models for T2DM risk assessment. Artificial\nintelligence (AI) models are being extensively used for this purpose, but a\ncomprehensive review of their advancements and challenges is lacking. This\nscoping review analyzes existing literature on AI-based models for T2DM risk\nprediction. Forty studies were included, mainly published in the past four\nyears. Traditional machine learning models were more prevalent than deep\nlearning models. Electronic health records were the most commonly used data\nsource. Unimodal AI models relying on EHR data were prominent, while only a few\nutilized multimodal models. Both unimodal and multimodal models showed\npromising performance, with the latter outperforming the former. Internal\nvalidation was common, while external validation was limited. Interpretability\nmethods were reported in half of the studies. Few studies reported novel\nbiomarkers, and open-source code availability was limited. This review provides\ninsights into the current state and limitations of AI-based T2DM risk\nprediction models and highlights challenges for their development and clinical\nimplementation.",
                "Symbolic Regression via Control Variable Genetic Programming\nLearning symbolic expressions directly from experiment data is a vital step\nin AI-driven scientific discovery. Nevertheless, state-of-the-art approaches\nare limited to learning simple expressions. Regressing expressions involving\nmany independent variables still remain out of reach. Motivated by the control\nvariable experiments widely utilized in science, we propose Control Variable\nGenetic Programming (CVGP) for symbolic regression over many independent\nvariables. CVGP expedites symbolic expression discovery via customized\nexperiment design, rather than learning from a fixed dataset collected a\npriori. CVGP starts by fitting simple expressions involving a small set of\nindependent variables using genetic programming, under controlled experiments\nwhere other variables are held as constants. It then extends expressions\nlearned in previous generations by adding new independent variables, using new\ncontrol variable experiments in which these variables are allowed to vary.\nTheoretically, we show CVGP as an incremental building approach can yield an\nexponential reduction in the search space when learning a class of expressions.\nExperimentally, CVGP outperforms several baselines in learning symbolic\nexpressions involving multiple independent variables.",
                "HuatuoGPT, towards Taming Language Model to Be a Doctor\nIn this paper, we present HuatuoGPT, a large language model (LLM) for medical\nconsultation. The core recipe of HuatuoGPT is to leverage both\n\\textit{distilled data from ChatGPT} and \\textit{real-world data from doctors}\nin the supervised fine-tuned stage. The responses of ChatGPT are usually\ndetailed, well-presented and informative while it cannot perform like a doctor\nin many aspects, e.g. for integrative diagnosis. We argue that real-world data\nfrom doctors would be complementary to distilled data in the sense the former\ncould tame a distilled language model to perform like doctors. To better\nleverage the strengths of both data, we train a reward model to align the\nlanguage model with the merits that both data bring, following an RLAIF\n(reinforced learning from AI feedback) fashion. To evaluate and benchmark the\nmodels, we propose a comprehensive evaluation scheme (including automatic and\nmanual metrics). Experimental results demonstrate that HuatuoGPT achieves\nstate-of-the-art results in performing medical consultation among open-source\nLLMs in GPT-4 evaluation, human evaluation, and medical benchmark datasets. It\nis worth noting that by using additional real-world data and RLAIF, the\ndistilled language model (i.e., HuatuoGPT) outperforms its teacher model\nChatGPT in most cases. Our code, data, and models are publicly available at\n\\url{https://github.com/FreedomIntelligence/HuatuoGPT}. The online demo is\navailable at \\url{https://www.HuatuoGPT.cn/}.",
                "Learning-Based Automatic Synthesis of Software Code and Configuration\nIncreasing demands in software industry and scarcity of software engineers\nmotivates researchers and practitioners to automate the process of software\ngeneration and configuration. Large scale automatic software generation and\nconfiguration is a very complex and challenging task. In this proposal, we set\nout to investigate this problem by breaking down automatic software generation\nand configuration into two different tasks. In first task, we propose to\nsynthesize software automatically with input output specifications. This task\nis further broken down into two sub-tasks. The first sub-task is about\nsynthesizing programs with a genetic algorithm which is driven by a neural\nnetwork based fitness function trained with program traces and specifications.\nFor the second sub-task, we formulate program synthesis as a continuous\noptimization problem and synthesize programs with covariance matrix adaption\nevolutionary strategy (a state-of-the-art continuous optimization method).\nFinally, for the second task, we propose to synthesize configurations of large\nscale software from different input files (e.g. software manuals,\nconfigurations files, online blogs, etc.) using a sequence-to-sequence deep\nlearning mechanism.",
                "ISLE: An Intelligent Streaming Framework for High-Throughput AI\n  Inference in Medical Imaging\nAs the adoption of Artificial Intelligence (AI) systems within the clinical\nenvironment grows, limitations in bandwidth and compute can create\ncommunication bottlenecks when streaming imaging data, leading to delays in\npatient care and increased cost. As such, healthcare providers and AI vendors\nwill require greater computational infrastructure, therefore dramatically\nincreasing costs. To that end, we developed ISLE, an intelligent streaming\nframework for high-throughput, compute- and bandwidth- optimized, and cost\neffective AI inference for clinical decision making at scale. In our\nexperiments, ISLE on average reduced data transmission by 98.02% and decoding\ntime by 98.09%, while increasing throughput by 2,730%. We show that ISLE\nresults in faster turnaround times, and reduced overall cost of data,\ntransmission, and compute, without negatively impacting clinical decision\nmaking using AI systems.",
                "Automated discovery of interpretable hyperelastic material models for\n  human brain tissue with EUCLID\nWe propose an automated computational algorithm for simultaneous model\nselection and parameter identification for the hyperelastic mechanical\ncharacterization of human brain tissue. Following the motive of the recently\nproposed computational framework EUCLID (Efficient Unsupervised Constitutive\nLaw Identitication and Discovery) and in contrast to conventional parameter\ncalibration methods, we construct an extensive set of candidate hyperelastic\nmodels, i.e., a model library including popular models known from the\nliterature, and develop a computational strategy for automatically selecting a\nmodel from the library that conforms to the available experimental data while\nbeing represented as an interpretable symbolic mathematical expression. This\ncomputational strategy comprises sparse regression, i.e., a regression problem\nthat is regularized by a sparsity promoting penalty term that filters out\nirrelevant models from the model library, and a clustering method for grouping\ntogether highly correlated and thus redundant features in the model library.\nThe model selection procedure is driven by labelled data pairs stemming from\nmechanical tests under different deformation modes, i.e., uniaxial\ncompression/tension and simple torsion, and can thus be interpreted as a\nsupervised counterpart to the originally proposed EUCLID that is informed by\nfull-field displacement data and global reaction forces. The proposed method is\nverified on synthetical data with artificial noise and validated on\nexperimental data acquired through mechanical tests of human brain specimens,\nproving that the method is capable of discovering hyperelastic models that\nexhibit both high fitting accuracy to the data as well as concise and thus\ninterpretable mathematical representations."
            ],
            "interesting paper": 3
        },
        {
            "papers": [
                "Quantum 3.0: Quantum Learning, Quantum Heuristics and Beyond\nQuantum learning paradigms address the question of how best to harness\nconceptual elements of quantum mechanics and information processing to improve\noperability and functionality of a computing system for specific tasks through\nexperience. It is one of the fastest evolving framework, which lies at the\nintersection of physics, statistics and information processing, and is the next\nfrontier for data sciences, machine learning and artificial intelligence.\nProgress in quantum learning paradigms is driven by multiple factors: need for\nmore efficient data storage and computational speed, development of novel\nalgorithms as well as structural resonances between specific physical systems\nand learning architectures. Given the demand for better computation methods for\ndata-intensive processes in areas such as advanced scientific analysis and\ncommerce as well as for facilitating more data-driven decision-making in\neducation, energy, marketing, pharmaceuticals and health-care, finance and\nindustry.",
                "Exploring the Adaptive Behaviors of Particle Lenia: A\n  Perturbation-Response Analysis for Computational Agency\nA firm cognitive subject or ``individual'' is presupposed for the emergence\nof mind. However, with the development of recent information technology, the\n``individual'' has become more dispersed in society and the cognitive subject\nhas become increasingly unstable and adaptive, necessitating an update in our\nunderstanding of the ``individual''. Autopoiesis serves as a model of the\ncognitive subject, which is unstable and requires effort to maintain itself to\nadapt to the environment. In this study, we evaluated adaptivity for a highly\nextensible multi-particle system model Particle Lenia through the response\nperturbation. As a result, we found that Particle Lenia has a particle\nconfiguration that is both temporally unstable and has multiple stable states.\nThis result suggests that Particle Lenia can express adaptive characteristics\nand is expected to be used as a computational model toward building an\nautopoietic cognitive agent.",
                "DeepGate2: Functionality-Aware Circuit Representation Learning\nCircuit representation learning aims to obtain neural representations of\ncircuit elements and has emerged as a promising research direction that can be\napplied to various EDA and logic reasoning tasks. Existing solutions, such as\nDeepGate, have the potential to embed both circuit structural information and\nfunctional behavior. However, their capabilities are limited due to weak\nsupervision or flawed model design, resulting in unsatisfactory performance in\ndownstream tasks. In this paper, we introduce DeepGate2, a novel\nfunctionality-aware learning framework that significantly improves upon the\noriginal DeepGate solution in terms of both learning effectiveness and\nefficiency. Our approach involves using pairwise truth table differences\nbetween sampled logic gates as training supervision, along with a well-designed\nand scalable loss function that explicitly considers circuit functionality.\nAdditionally, we consider inherent circuit characteristics and design an\nefficient one-round graph neural network (GNN), resulting in an order of\nmagnitude faster learning speed than the original DeepGate solution.\nExperimental results demonstrate significant improvements in two practical\ndownstream tasks: logic synthesis and Boolean satisfiability solving. The code\nis available at https://github.com/cure-lab/DeepGate2",
                "Exploiting Noise as a Resource for Computation and Learning in Spiking\n  Neural Networks\n$\\textbf{Formal version available at}$\nhttps://cell.com/patterns/fulltext/S2666-3899(23)00200-3\n  Networks of spiking neurons underpin the extraordinary information-processing\ncapabilities of the brain and have become pillar models in neuromorphic\nartificial intelligence. Despite extensive research on spiking neural networks\n(SNNs), most studies are established on deterministic models, overlooking the\ninherent non-deterministic, noisy nature of neural computations. This study\nintroduces the noisy spiking neural network (NSNN) and the noise-driven\nlearning rule (NDL) by incorporating noisy neuronal dynamics to exploit the\ncomputational advantages of noisy neural processing. NSNN provides a\ntheoretical framework that yields scalable, flexible, and reliable computation.\nWe demonstrate that NSNN leads to spiking neural models with competitive\nperformance, improved robustness against challenging perturbations than\ndeterministic SNNs, and better reproducing probabilistic computations in neural\ncoding. This study offers a powerful and easy-to-use tool for machine learning,\nneuromorphic intelligence practitioners, and computational neuroscience\nresearchers.",
                "Applying Interdisciplinary Frameworks to Understand Algorithmic\n  Decision-Making\nWe argue that explanations for \"algorithmic decision-making\" (ADM) systems\ncan profit by adopting practices that are already used in the learning\nsciences. We shortly introduce the importance of explaining ADM systems, give a\nbrief overview of approaches drawing from other disciplines to improve\nexplanations, and present the results of our qualitative task-based study\nincorporating the \"six facets of understanding\" framework. We close with\nquestions guiding the discussion of how future studies can leverage an\ninterdisciplinary approach.",
                "Dendritic Integration Based Quadratic Neural Networks Outperform\n  Traditional Aritificial Ones\nIncorporating biological neuronal properties into Artificial Neural Networks\n(ANNs) to enhance computational capabilities poses a formidable challenge in\nthe field of machine learning. Inspired by recent findings indicating that\ndendrites adhere to quadratic integration rules for synaptic inputs, we propose\na novel ANN model, Dendritic Integration-Based Quadratic Neural Network\n(DIQNN). This model shows superior performance over traditional ANNs in a\nvariety of classification tasks. To reduce the computational cost of DIQNN, we\nintroduce the Low-Rank DIQNN, while we find it can retain the performance of\nthe original DIQNN. We further propose a margin to characterize the\ngeneralization error and theoretically prove this margin will increase\nmonotonically during training. And we show the consistency between\ngeneralization and our margin using numerical experiments. Finally, by\nintegrating this margin into the loss function, the change of test accuracy is\nindeed accelerated. Our work contributes a novel, brain-inspired ANN model that\nsurpasses traditional ANNs and provides a theoretical framework to analyze the\ngeneralization error in classification tasks."
            ],
            "interesting paper": 1
        },
        {
            "papers": [
                "Explainability Techniques for Chemical Language Models\nExplainability techniques are crucial in gaining insights into the reasons\nbehind the predictions of deep learning models, which have not yet been applied\nto chemical language models. We propose an explainable AI technique that\nattributes the importance of individual atoms towards the predictions made by\nthese models. Our method backpropagates the relevance information towards the\nchemical input string and visualizes the importance of individual atoms. We\nfocus on self-attention Transformers operating on molecular string\nrepresentations and leverage a pretrained encoder for finetuning. We showcase\nthe method by predicting and visualizing solubility in water and organic\nsolvents. We achieve competitive model performance while obtaining\ninterpretable predictions, which we use to inspect the pretrained model.",
                "Metrics for quantifying isotropy in high dimensional unsupervised\n  clustering tasks in a materials context\nClustering is a common task in machine learning, but clusters of unlabelled\ndata can be hard to quantify. The application of clustering algorithms in\nchemistry is often dependant on material representation. Ascertaining the\neffects of different representations, clustering algorithms, or data\ntransformations on the resulting clusters is difficult due to the\ndimensionality of these data. We present a thorough analysis of measures for\nisotropy of a cluster, including a novel implantation based on an existing\nderivation. Using fractional anisotropy, a common method used in medical\nimaging for comparison, we then expand these measures to examine the average\nisotropy of a set of clusters. A use case for such measures is demonstrated by\nquantifying the effects of kernel approximation functions on different\nrepresentations of the Inorganic Crystal Structure Database. Broader\napplicability of these methods is demonstrated in analysing learnt embedding of\nthe MNIST dataset. Random clusters are explored to examine the differences\nbetween isotropy measures presented, and to see how each method scales with the\ndimensionality. Python implementations of these measures are provided for use\nby the community.",
                "Attention Paper: How Generative AI Reshapes Digital Shadow Industry?\nThe rapid development of digital economy has led to the emergence of various\nblack and shadow internet industries, which pose potential risks that can be\nidentified and managed through digital risk management (DRM) that uses\ndifferent techniques such as machine learning and deep learning. The evolution\nof DRM architecture has been driven by changes in data forms. However, the\ndevelopment of AI-generated content (AIGC) technology, such as ChatGPT and\nStable Diffusion, has given black and shadow industries powerful tools to\npersonalize data and generate realistic images and conversations for fraudulent\nactivities. This poses a challenge for DRM systems to control risks from the\nsource of data generation and to respond quickly to the fast-changing risk\nenvironment. This paper aims to provide a technical analysis of the challenges\nand opportunities of AIGC from upstream, midstream, and downstream paths of\nblack/shadow industries and suggest future directions for improving existing\nrisk control systems. The paper will explore the new black and shadow\ntechniques triggered by generative AI technology and provide insights for\nbuilding the next-generation DRM system.",
                "Dendritic Integration Based Quadratic Neural Networks Outperform\n  Traditional Aritificial Ones\nIncorporating biological neuronal properties into Artificial Neural Networks\n(ANNs) to enhance computational capabilities poses a formidable challenge in\nthe field of machine learning. Inspired by recent findings indicating that\ndendrites adhere to quadratic integration rules for synaptic inputs, we propose\na novel ANN model, Dendritic Integration-Based Quadratic Neural Network\n(DIQNN). This model shows superior performance over traditional ANNs in a\nvariety of classification tasks. To reduce the computational cost of DIQNN, we\nintroduce the Low-Rank DIQNN, while we find it can retain the performance of\nthe original DIQNN. We further propose a margin to characterize the\ngeneralization error and theoretically prove this margin will increase\nmonotonically during training. And we show the consistency between\ngeneralization and our margin using numerical experiments. Finally, by\nintegrating this margin into the loss function, the change of test accuracy is\nindeed accelerated. Our work contributes a novel, brain-inspired ANN model that\nsurpasses traditional ANNs and provides a theoretical framework to analyze the\ngeneralization error in classification tasks.",
                "Enhancing Human Capabilities through Symbiotic Artificial Intelligence\n  with Shared Sensory Experiences\nThe merging of human intelligence and artificial intelligence has long been a\nsubject of interest in both science fiction and academia. In this paper, we\nintroduce a novel concept in Human-AI interaction called Symbiotic Artificial\nIntelligence with Shared Sensory Experiences (SAISSE), which aims to establish\na mutually beneficial relationship between AI systems and human users through\nshared sensory experiences. By integrating multiple sensory input channels and\nprocessing human experiences, SAISSE fosters a strong human-AI bond, enabling\nAI systems to learn from and adapt to individual users, providing personalized\nsupport, assistance, and enhancement. Furthermore, we discuss the incorporation\nof memory storage units for long-term growth and development of both the AI\nsystem and its human user. As we address user privacy and ethical guidelines\nfor responsible AI-human symbiosis, we also explore potential biases and\ninequalities in AI-human symbiosis and propose strategies to mitigate these\nchallenges. Our research aims to provide a comprehensive understanding of the\nSAISSE concept and its potential to effectively support and enhance individual\nhuman users through symbiotic AI systems. This position article aims at\ndiscussing poteintial AI-human interaction related topics within the scientific\ncommunity, rather than providing experimental or theoretical results.",
                "Dynamic Inter-treatment Information Sharing for Individualized Treatment\n  Effects Estimation\nEstimation of individualized treatment effects (ITE) from observational\nstudies is a fundamental problem in causal inference and holds significant\nimportance across domains, including healthcare. However, limited observational\ndatasets pose challenges in reliable ITE estimation as data have to be split\namong treatment groups to train an ITE learner. While information sharing among\ntreatment groups can partially alleviate the problem, there is currently no\ngeneral framework for end-to-end information sharing in ITE estimation. To\ntackle this problem, we propose a deep learning framework based on\n`\\textit{soft weight sharing}' to train ITE learners, enabling \\textit{dynamic\nend-to-end} information sharing among treatment groups. The proposed framework\ncomplements existing ITE learners, and introduces a new class of ITE learners,\nreferred to as \\textit{HyperITE}. We extend state-of-the-art ITE learners with\n\\textit{HyperITE} versions and evaluate them on IHDP, ACIC-2016, and Twins\nbenchmarks. Our experimental results show that the proposed framework improves\nITE estimation error, with increasing effectiveness for smaller datasets."
            ],
            "interesting paper": 1
        },
        {
            "papers": [
                "Reinforcement Learning With Reward Machines in Stochastic Games\nWe investigate multi-agent reinforcement learning for stochastic games with\ncomplex tasks, where the reward functions are non-Markovian. We utilize reward\nmachines to incorporate high-level knowledge of complex tasks. We develop an\nalgorithm called Q-learning with reward machines for stochastic games (QRM-SG),\nto learn the best-response strategy at Nash equilibrium for each agent. In\nQRM-SG, we define the Q-function at a Nash equilibrium in augmented state\nspace. The augmented state space integrates the state of the stochastic game\nand the state of reward machines. Each agent learns the Q-functions of all\nagents in the system. We prove that Q-functions learned in QRM-SG converge to\nthe Q-functions at a Nash equilibrium if the stage game at each time step\nduring learning has a global optimum point or a saddle point, and the agents\nupdate Q-functions based on the best-response strategy at this point. We use\nthe Lemke-Howson method to derive the best-response strategy given current\nQ-functions. The three case studies show that QRM-SG can learn the\nbest-response strategies effectively. QRM-SG learns the best-response\nstrategies after around 7500 episodes in Case Study I, 1000 episodes in Case\nStudy II, and 1500 episodes in Case Study III, while baseline methods such as\nNash Q-learning and MADDPG fail to converge to the Nash equilibrium in all\nthree case studies.",
                "Exploiting Large Neuroimaging Datasets to Create Connectome-Constrained\n  Approaches for more Robust, Efficient, and Adaptable Artificial Intelligence\nDespite the progress in deep learning networks, efficient learning at the\nedge (enabling adaptable, low-complexity machine learning solutions) remains a\ncritical need for defense and commercial applications. We envision a pipeline\nto utilize large neuroimaging datasets, including maps of the brain which\ncapture neuron and synapse connectivity, to improve machine learning\napproaches. We have pursued different approaches within this pipeline\nstructure. First, as a demonstration of data-driven discovery, the team has\ndeveloped a technique for discovery of repeated subcircuits, or motifs. These\nwere incorporated into a neural architecture search approach to evolve network\narchitectures. Second, we have conducted analysis of the heading direction\ncircuit in the fruit fly, which performs fusion of visual and angular velocity\nfeatures, to explore augmenting existing computational models with new insight.\nOur team discovered a novel pattern of connectivity, implemented a new model,\nand demonstrated sensor fusion on a robotic platform. Third, the team analyzed\ncircuitry for memory formation in the fruit fly connectome, enabling the design\nof a novel generative replay approach. Finally, the team has begun analysis of\nconnectivity in mammalian cortex to explore potential improvements to\ntransformer networks. These constraints increased network robustness on the\nmost challenging examples in the CIFAR-10-C computer vision robustness\nbenchmark task, while reducing learnable attention parameters by over an order\nof magnitude. Taken together, these results demonstrate multiple potential\napproaches to utilize insight from neural systems for developing robust and\nefficient machine learning techniques.",
                "Mindstorms in Natural Language-Based Societies of Mind\nBoth Minsky's \"society of mind\" and Schmidhuber's \"learning to think\" inspire\ndiverse societies of large multimodal neural networks (NNs) that solve problems\nby interviewing each other in a \"mindstorm.\" Recent implementations of NN-based\nsocieties of minds consist of large language models (LLMs) and other NN-based\nexperts communicating through a natural language interface. In doing so, they\novercome the limitations of single LLMs, improving multimodal zero-shot\nreasoning. In these natural language-based societies of mind (NLSOMs), new\nagents -- all communicating through the same universal symbolic language -- are\neasily added in a modular fashion. To demonstrate the power of NLSOMs, we\nassemble and experiment with several of them (having up to 129 members),\nleveraging mindstorms in them to solve some practical AI tasks: visual question\nanswering, image captioning, text-to-image synthesis, 3D generation, egocentric\nretrieval, embodied AI, and general language-based task solving. We view this\nas a starting point towards much larger NLSOMs with billions of agents-some of\nwhich may be humans. And with this emergence of great societies of\nheterogeneous minds, many new research questions have suddenly become paramount\nto the future of artificial intelligence. What should be the social structure\nof an NLSOM? What would be the (dis)advantages of having a monarchical rather\nthan a democratic structure? How can principles of NN economies be used to\nmaximize the total reward of a reinforcement learning NLSOM? In this work, we\nidentify, discuss, and try to answer some of these questions.",
                "AI-based analysis of super-resolution microscopy: Biological discovery\n  in the absence of ground truth\nSuper-resolution microscopy, or nanoscopy, enables the use of\nfluorescent-based molecular localization tools to study molecular structure at\nthe nanoscale level in the intact cell, bridging the mesoscale gap to classical\nstructural biology methodologies. Analysis of super-resolution data by\nartificial intelligence (AI), such as machine learning, offers tremendous\npotential for discovery of new biology, that, by definition, is not known and\nlacks ground truth. Herein, we describe the application of weakly supervised\nparadigms to super-resolution microscopy and its potential to enable the\naccelerated exploration of the nanoscale architecture of subcellular\nmacromolecules and organelles.",
                "The Galaxy Assembly and Interaction Neural Networks (GAINN) for\n  high-redshift JWST observations\nWe present the Galaxy Assembly and Interaction Neural Networks (GAINN), a\nseries of artificial neural networks for predicting the redshift, stellar mass,\nhalo mass, and mass-weighted age of simulated galaxies based on JWST\nphotometry. Our goal is to determine the best neural network for predicting\nthese variables at $11.5 < z < 15$. The parameters of the optimal neural\nnetwork can then be used to estimate these variables for real, observed\ngalaxies. The inputs of the neural networks are JWST filter magnitudes of a\nsubset of five broadband filters (F150W, F200W, F277W, F356W, and F444W) and\ntwo medium-band filters (F162M and F182M). We compare the performance of the\nneural networks using different combinations of these filters, as well as\ndifferent activation functions and numbers of layers. The best neural network\npredicted redshift with normalized root mean squared error NRMS =\n$0.009_{-0.002}^{+0.003}$, stellar mass with RMS = $0.073_{-0.008}^{+0.017}$,\nhalo mass with MSE = $ 0.022_{-0.004}^{+0.006}$, and mass-weighted age with RMS\n= $10.866_{-1.410}^{+3.189}$. We also test the performance of GAINN on real\ndata from MACS0647-JD, an object observed by JWST. Predictions from GAINN for\nthe first projection of the object (JD1) have mean absolute errors $\\langle\n\\Delta z \\rangle <0.00228$, which is significantly smaller than with\ntemplate-fitting methods. We find that the optimal filter combination is F277W,\nF356W, F162M, and F182M when considering both theoretical accuracy and\nobservational resources from JWST.",
                "Levin Tree Search with Context Models\nLevin Tree Search (LTS) is a search algorithm that makes use of a policy (a\nprobability distribution over actions) and comes with a theoretical guarantee\non the number of expansions before reaching a goal node, depending on the\nquality of the policy. This guarantee can be used as a loss function, which we\ncall the LTS loss, to optimize neural networks representing the policy\n(LTS+NN). In this work we show that the neural network can be substituted with\nparameterized context models originating from the online compression literature\n(LTS+CM). We show that the LTS loss is convex under this new model, which\nallows for using standard convex optimization tools, and obtain convergence\nguarantees to the optimal parameters in an online setting for a given set of\nsolution trajectories -- guarantees that cannot be provided for neural\nnetworks. The new LTS+CM algorithm compares favorably against LTS+NN on several\nbenchmarks: Sokoban (Boxoban), The Witness, and the 24-Sliding Tile puzzle\n(STP). The difference is particularly large on STP, where LTS+NN fails to solve\nmost of the test instances while LTS+CM solves each test instance in a fraction\nof a second. Furthermore, we show that LTS+CM is able to learn a policy that\nsolves the Rubik's cube in only a few hundred expansions, which considerably\nimproves upon previous machine learning techniques."
            ],
            "interesting paper": 3
        },
        {
            "papers": [
                "AI-based analysis of super-resolution microscopy: Biological discovery\n  in the absence of ground truth\nSuper-resolution microscopy, or nanoscopy, enables the use of\nfluorescent-based molecular localization tools to study molecular structure at\nthe nanoscale level in the intact cell, bridging the mesoscale gap to classical\nstructural biology methodologies. Analysis of super-resolution data by\nartificial intelligence (AI), such as machine learning, offers tremendous\npotential for discovery of new biology, that, by definition, is not known and\nlacks ground truth. Herein, we describe the application of weakly supervised\nparadigms to super-resolution microscopy and its potential to enable the\naccelerated exploration of the nanoscale architecture of subcellular\nmacromolecules and organelles.",
                "Exploiting Large Neuroimaging Datasets to Create Connectome-Constrained\n  Approaches for more Robust, Efficient, and Adaptable Artificial Intelligence\nDespite the progress in deep learning networks, efficient learning at the\nedge (enabling adaptable, low-complexity machine learning solutions) remains a\ncritical need for defense and commercial applications. We envision a pipeline\nto utilize large neuroimaging datasets, including maps of the brain which\ncapture neuron and synapse connectivity, to improve machine learning\napproaches. We have pursued different approaches within this pipeline\nstructure. First, as a demonstration of data-driven discovery, the team has\ndeveloped a technique for discovery of repeated subcircuits, or motifs. These\nwere incorporated into a neural architecture search approach to evolve network\narchitectures. Second, we have conducted analysis of the heading direction\ncircuit in the fruit fly, which performs fusion of visual and angular velocity\nfeatures, to explore augmenting existing computational models with new insight.\nOur team discovered a novel pattern of connectivity, implemented a new model,\nand demonstrated sensor fusion on a robotic platform. Third, the team analyzed\ncircuitry for memory formation in the fruit fly connectome, enabling the design\nof a novel generative replay approach. Finally, the team has begun analysis of\nconnectivity in mammalian cortex to explore potential improvements to\ntransformer networks. These constraints increased network robustness on the\nmost challenging examples in the CIFAR-10-C computer vision robustness\nbenchmark task, while reducing learnable attention parameters by over an order\nof magnitude. Taken together, these results demonstrate multiple potential\napproaches to utilize insight from neural systems for developing robust and\nefficient machine learning techniques.",
                "BiomedGPT: A Generalist Vision-Language Foundation Model for Diverse\n  Biomedical Tasks\nTraditional biomedical artificial intelligence (AI) models, designed for\nspecific tasks or modalities, often exhibit limited flexibility in real-world\ndeployment and struggle to utilize holistic information. Generalist AI holds\nthe potential to address these limitations due to its versatility in\ninterpreting different data types and generating tailored outputs for diverse\nneeds. However, existing biomedical generalist AI solutions are typically\nheavyweight and closed source to researchers, practitioners, and patients.\nHere, we propose BiomedGPT, the first open-source and lightweight\nvision-language foundation model, designed as a generalist capable of\nperforming various biomedical tasks. BiomedGPT achieved state-of-the-art\nresults in 16 out of 25 experiments while maintaining a computing-friendly\nmodel scale. We also conducted human evaluations to assess the capabilities of\nBiomedGPT in radiology visual question answering, report generation, and\nsummarization. BiomedGPT exhibits robust prediction ability with a low error\nrate of 3.8% in question answering, satisfactory performance with an error rate\nof 8.3% in writing complex radiology reports, and competitive summarization\nability with a nearly equivalent preference score to human experts. Our method\ndemonstrates that effective training with diverse data can lead to more\npractical biomedical AI for improving diagnosis and workflow efficiency.",
                "Statistically Significant Concept-based Explanation of Image Classifiers\n  via Model Knockoffs\nA concept-based classifier can explain the decision process of a deep\nlearning model by human-understandable concepts in image classification\nproblems. However, sometimes concept-based explanations may cause false\npositives, which misregards unrelated concepts as important for the prediction\ntask. Our goal is to find the statistically significant concept for\nclassification to prevent misinterpretation. In this study, we propose a method\nusing a deep learning model to learn the image concept and then using the\nKnockoff samples to select the important concepts for prediction by controlling\nthe False Discovery Rate (FDR) under a certain value. We evaluate the proposed\nmethod in our synthetic and real data experiments. Also, it shows that our\nmethod can control the FDR properly while selecting highly interpretable\nconcepts to improve the trustworthiness of the model.",
                "How Do UX Practitioners Communicate AI as a Design Material? Artifacts,\n  Conceptions, and Propositions\nUX practitioners (UXPs) face novel challenges when working with and\ncommunicating artificial intelligence (AI) as a design material. We explore how\nUXPs communicate AI concepts when given hands-on experience training and\nexperimenting with AI models. To do so, we conducted a task-based design study\nwith 27 UXPs in which they prototyped and created a design presentation for a\nAI-enabled interface while having access to a simple AI model training tool.\nThrough analyzing UXPs' design presentations and post-activity interviews, we\nfound that although UXPs struggled to clearly communicate some AI concepts,\ntinkering with AI broadened common ground when communicating with technical\nstakeholders. UXPs also identified key risks and benefits of AI in their\ndesigns, and proposed concrete next steps for both UX and AI work. We conclude\nwith a sensitizing concept and recommendations for design and AI tools to\nenhance multi-stakeholder communication and collaboration when crafting\nhuman-centered AI experiences.",
                "ProGroTrack: Deep Learning-Assisted Tracking of Intracellular Protein\n  Growth Dynamics\nAccurate tracking of cellular and subcellular structures, along with their\ndynamics, plays a pivotal role in understanding the underlying mechanisms of\nbiological systems. This paper presents a novel approach, ProGroTrack, that\ncombines the You Only Look Once (YOLO) and ByteTrack algorithms within the\ndetection-based tracking (DBT) framework to track intracellular protein\nnanostructures. Focusing on iPAK4 protein fibers as a representative case\nstudy, we conducted a comprehensive evaluation of YOLOv5 and YOLOv8 models,\nrevealing the superior performance of YOLOv5 on our dataset. Notably, YOLOv5x\nachieved an impressive mAP50 of 0.839 and F-score of 0.819. To further optimize\ndetection capabilities, we incorporated semi-supervised learning for model\nimprovement, resulting in enhanced performances in all metrics. Subsequently,\nwe successfully applied our approach to track the growth behavior of iPAK4\nprotein fibers, revealing their two distinct growth phases consistent with a\npreviously reported kinetic model. This research showcases the promising\npotential of our approach, extending beyond iPAK4 fibers. It also offers a\nsignificant advancement in precise tracking of dynamic processes in live cells,\nand fostering new avenues for biomedical research."
            ],
            "interesting paper": 3
        },
        {
            "papers": [
                "Evolving Connectivity for Recurrent Spiking Neural Networks\nRecurrent spiking neural networks (RSNNs) hold great potential for advancing\nartificial general intelligence, as they draw inspiration from the biological\nnervous system and show promise in modeling complex dynamics. However, the\nwidely-used surrogate gradient-based training methods for RSNNs are inherently\ninaccurate and unfriendly to neuromorphic hardware. To address these\nlimitations, we propose the evolving connectivity (EC) framework, an\ninference-only method for training RSNNs. The EC framework reformulates\nweight-tuning as a search into parameterized connection probability\ndistributions, and employs Natural Evolution Strategies (NES) for optimizing\nthese distributions. Our EC framework circumvents the need for gradients and\nfeatures hardware-friendly characteristics, including sparse boolean\nconnections and high scalability. We evaluate EC on a series of standard\nrobotic locomotion tasks, where it achieves comparable performance with deep\nneural networks and outperforms gradient-trained RSNNs, even solving the\ncomplex 17-DoF humanoid task. Additionally, the EC framework demonstrates a two\nto three fold speedup in efficiency compared to directly evolving parameters.\nBy providing a performant and hardware-friendly alternative, the EC framework\nlays the groundwork for further energy-efficient applications of RSNNs and\nadvances the development of neuromorphic devices.",
                "Modeling Dynamic Environments with Scene Graph Memory\nEmbodied AI agents that search for objects in large environments such as\nhouseholds often need to make efficient decisions by predicting object\nlocations based on partial information. We pose this as a new type of link\nprediction problem: link prediction on partially observable dynamic graphs. Our\ngraph is a representation of a scene in which rooms and objects are nodes, and\ntheir relationships are encoded in the edges; only parts of the changing graph\nare known to the agent at each timestep. This partial observability poses a\nchallenge to existing link prediction approaches, which we address. We propose\na novel state representation -- Scene Graph Memory (SGM) -- with captures the\nagent's accumulated set of observations, as well as a neural net architecture\ncalled a Node Edge Predictor (NEP) that extracts information from the SGM to\nsearch efficiently. We evaluate our method in the Dynamic House Simulator, a\nnew benchmark that creates diverse dynamic graphs following the semantic\npatterns typically seen at homes, and show that NEP can be trained to predict\nthe locations of objects in a variety of environments with diverse object\nmovement dynamics, outperforming baselines both in terms of new scene\nadaptability and overall accuracy. The codebase and more can be found at\nhttps://www.scenegraphmemory.com.",
                "Probing reaction channels via reinforcement learning\nWe propose a reinforcement learning based method to identify important\nconfigurations that connect reactant and product states along chemical reaction\npaths. By shooting multiple trajectories from these configurations, we can\ngenerate an ensemble of configurations that concentrate on the transition path\nensemble. This configuration ensemble can be effectively employed in a neural\nnetwork-based partial differential equation solver to obtain an approximation\nsolution of a restricted Backward Kolmogorov equation, even when the dimension\nof the problem is very high. The resulting solution, known as the committor\nfunction, encodes mechanistic information for the reaction and can in turn be\nused to evaluate reaction rates.",
                "Synthesizing a Progression of Subtasks for Block-Based Visual\n  Programming Tasks\nBlock-based visual programming environments play an increasingly important\nrole in introducing computing concepts to K-12 students. In recent years, they\nhave also gained popularity in neuro-symbolic AI, serving as a benchmark to\nevaluate general problem-solving and logical reasoning skills. The open-ended\nand conceptual nature of these visual programming tasks make them challenging,\nboth for state-of-the-art AI agents as well as for novice programmers. A\nnatural approach to providing assistance for problem-solving is breaking down a\ncomplex task into a progression of simpler subtasks; however, this is not\ntrivial given that the solution codes are typically nested and have non-linear\nexecution behavior. In this paper, we formalize the problem of synthesizing\nsuch a progression for a given reference block-based visual programming task.\nWe propose a novel synthesis algorithm that generates a progression of subtasks\nthat are high-quality, well-spaced in terms of their complexity, and solving\nthis progression leads to solving the reference task. We show the utility of\nour synthesis algorithm in improving the efficacy of AI agents (in this case,\nneural program synthesizers) for solving tasks in the Karel programming\nenvironment. Then, we conduct a user study to demonstrate that our synthesized\nprogression of subtasks can assist a novice programmer in solving tasks in the\nHour of Code: Maze Challenge by Code-dot-org.",
                "A Comprehensive Overview and Comparative Analysis on Deep Learning\n  Models: CNN, RNN, LSTM, GRU\nDeep learning (DL) has emerged as a powerful subset of machine learning (ML)\nand artificial intelligence (AI), outperforming traditional ML methods,\nespecially in handling unstructured and large datasets. Its impact spans across\nvarious domains, including speech recognition, healthcare, autonomous vehicles,\ncybersecurity, predictive analytics, and more. However, the complexity and\ndynamic nature of real-world problems present challenges in designing effective\ndeep learning models. Consequently, several deep learning models have been\ndeveloped to address different problems and applications. In this article, we\nconduct a comprehensive survey of various deep learning models, including\nConvolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs),\nGenerative Models, Deep Reinforcement Learning (DRL), and Deep Transfer\nLearning. We examine the structure, applications, benefits, and limitations of\neach model. Furthermore, we perform an analysis using three publicly available\ndatasets: IMDB, ARAS, and Fruit-360. We compare the performance of six renowned\ndeep learning models: CNN, Simple RNN, Long Short-Term Memory (LSTM),\nBidirectional LSTM, Gated Recurrent Unit (GRU), and Bidirectional GRU.",
                "AI Coach Assist: An Automated Approach for Call Recommendation in\n  Contact Centers for Agent Coaching\nIn recent years, the utilization of Artificial Intelligence (AI) in the\ncontact center industry is on the rise. One area where AI can have a\nsignificant impact is in the coaching of contact center agents. By analyzing\ncall transcripts using Natural Language Processing (NLP) techniques, it would\nbe possible to quickly determine which calls are most relevant for coaching\npurposes. In this paper, we present AI Coach Assist, which leverages the\npre-trained transformer-based language models to determine whether a given call\nis coachable or not based on the quality assurance (QA) questions asked by the\ncontact center managers or supervisors. The system was trained and evaluated on\na large dataset collected from real-world contact centers and provides an\neffective way to recommend calls to the contact center managers that are more\nlikely to contain coachable moments. Our experimental findings demonstrate the\npotential of AI Coach Assist to improve the coaching process, resulting in\nenhancing the performance of contact center agents."
            ],
            "interesting paper": 1
        },
        {
            "papers": [
                "What can Large Language Models do in chemistry? A comprehensive\n  benchmark on eight tasks\nLarge Language Models (LLMs) with strong abilities in natural language\nprocessing tasks have emerged and have been applied in various kinds of areas\nsuch as science, finance and software engineering. However, the capability of\nLLMs to advance the field of chemistry remains unclear. In this paper, rather\nthan pursuing state-of-the-art performance, we aim to evaluate capabilities of\nLLMs in a wide range of tasks across the chemistry domain. We identify three\nkey chemistry-related capabilities including understanding, reasoning and\nexplaining to explore in LLMs and establish a benchmark containing eight\nchemistry tasks. Our analysis draws on widely recognized datasets facilitating\na broad exploration of the capacities of LLMs within the context of practical\nchemistry. Five LLMs (GPT-4, GPT-3.5, Davinci-003, Llama and Galactica) are\nevaluated for each chemistry task in zero-shot and few-shot in-context learning\nsettings with carefully selected demonstration examples and specially crafted\nprompts. Our investigation found that GPT-4 outperformed other models and LLMs\nexhibit different competitive levels in eight chemistry tasks. In addition to\nthe key findings from the comprehensive benchmark analysis, our work provides\ninsights into the limitation of current LLMs and the impact of in-context\nlearning settings on LLMs' performance across various chemistry tasks. The code\nand datasets used in this study are available at\nhttps://github.com/ChemFoundationModels/ChemLLMBench.",
                "Probing reaction channels via reinforcement learning\nWe propose a reinforcement learning based method to identify important\nconfigurations that connect reactant and product states along chemical reaction\npaths. By shooting multiple trajectories from these configurations, we can\ngenerate an ensemble of configurations that concentrate on the transition path\nensemble. This configuration ensemble can be effectively employed in a neural\nnetwork-based partial differential equation solver to obtain an approximation\nsolution of a restricted Backward Kolmogorov equation, even when the dimension\nof the problem is very high. The resulting solution, known as the committor\nfunction, encodes mechanistic information for the reaction and can in turn be\nused to evaluate reaction rates.",
                "Counterfactual Formulation of Patient-Specific Root Causes of Disease\nRoot causes of disease intuitively correspond to root vertices that increase\nthe likelihood of a diagnosis. This description of a root cause nevertheless\nlacks the rigorous mathematical formulation needed for the development of\ncomputer algorithms designed to automatically detect root causes from data.\nPrior work defined patient-specific root causes of disease using an\ninterventionalist account that only climbs to the second rung of Pearl's Ladder\nof Causation. In this theoretical piece, we climb to the third rung by\nproposing a counterfactual definition matching clinical intuition based on\nfixed factual data alone. We then show how to assign a root causal contribution\nscore to each variable using Shapley values from explainable artificial\nintelligence. The proposed counterfactual formulation of patient-specific root\ncauses of disease accounts for noisy labels, adapts to disease prevalence and\nadmits fast computation without the need for counterfactual simulation.",
                "Diagnosing Transformers: Illuminating Feature Spaces for Clinical\n  Decision-Making\nPre-trained transformers are often fine-tuned to aid clinical decision-making\nusing limited clinical notes. Model interpretability is crucial, especially in\nhigh-stakes domains like medicine, to establish trust and ensure safety, which\nrequires human engagement. We introduce SUFO, a systematic framework that\nenhances interpretability of fine-tuned transformer feature spaces. SUFO\nutilizes a range of analytic and visualization techniques, including Supervised\nprobing, Unsupervised similarity analysis, Feature dynamics, and Outlier\nanalysis to address key questions about model trust and interpretability. We\nconduct a case study investigating the impact of pre-training data where we\nfocus on real-world pathology classification tasks, and validate our findings\non MedNLI. We evaluate five 110M-sized pre-trained transformer models,\ncategorized into general-domain (BERT, TNLR), mixed-domain (BioBERT, Clinical\nBioBERT), and domain-specific (PubMedBERT) groups. Our SUFO analyses reveal\nthat: (1) while PubMedBERT, the domain-specific model, contains valuable\ninformation for fine-tuning, it can overfit to minority classes when class\nimbalances exist. In contrast, mixed-domain models exhibit greater resistance\nto overfitting, suggesting potential improvements in domain-specific model\nrobustness; (2) in-domain pre-training accelerates feature disambiguation\nduring fine-tuning; and (3) feature spaces undergo significant sparsification\nduring this process, enabling clinicians to identify common outlier modes among\nfine-tuned models as demonstrated in this paper. These findings showcase the\nutility of SUFO in enhancing trust and safety when using transformers in\nmedicine, and we believe SUFO can aid practitioners in evaluating fine-tuned\nlanguage models for other applications in medicine and in more critical\ndomains.",
                "Distill Gold from Massive Ores: Bi-level Data Pruning towards Efficient\n  Dataset Distillation\nData-efficient learning has garnered significant attention, especially given\nthe current trend of large multi-modal models. Recently, dataset distillation\nhas become an effective approach by synthesizing data samples that are\nessential for network training. However, it remains to be explored which\nsamples are essential for the dataset distillation process itself. In this\nwork, we study the data efficiency and selection for the dataset distillation\ntask. By re-formulating the dynamics of distillation, we provide insight into\nthe inherent redundancy in the real dataset, both theoretically and\nempirically. We propose to use the empirical loss value as a static data\npruning criterion. To further compensate for the variation of the data value in\ntraining, we find the most contributing samples based on their causal effects\non the distillation. The proposed selection strategy can efficiently exploit\nthe training dataset, outperform the previous SOTA distillation algorithms, and\nconsistently enhance the distillation algorithms, even on much larger-scale and\nmore heterogeneous datasets, e.g., full ImageNet-1K and Kinetics-400. We\nbelieve this paradigm will open up new avenues in the dynamics of distillation\nand pave the way for efficient dataset distillation. Our code is available on\nhttps://github.com/silicx/GoldFromOres-BiLP.",
                "Emergent Modularity in Pre-trained Transformers\nThis work examines the presence of modularity in pre-trained Transformers, a\nfeature commonly found in human brains and thought to be vital for general\nintelligence. In analogy to human brains, we consider two main characteristics\nof modularity: (1) functional specialization of neurons: we evaluate whether\neach neuron is mainly specialized in a certain function, and find that the\nanswer is yes. (2) function-based neuron grouping: we explore finding a\nstructure that groups neurons into modules by function, and each module works\nfor its corresponding function. Given the enormous amount of possible\nstructures, we focus on Mixture-of-Experts as a promising candidate, which\npartitions neurons into experts and usually activates different experts for\ndifferent inputs. Experimental results show that there are functional experts,\nwhere clustered are the neurons specialized in a certain function. Moreover,\nperturbing the activations of functional experts significantly affects the\ncorresponding function. Finally, we study how modularity emerges during\npre-training, and find that the modular structure is stabilized at the early\nstage, which is faster than neuron stabilization. It suggests that Transformers\nfirst construct the modular structure and then learn fine-grained neuron\nfunctions. Our code and data are available at\nhttps://github.com/THUNLP/modularity-analysis."
            ],
            "interesting paper": 2
        },
        {
            "papers": [
                "State-Blocking Side-Channel Attacks and Autonomous Fault Detection in\n  Quantum Key Distribution\nSide-channel attacks allow an Eavesdropper to use insecurities in the\npractical implementation of QKD systems to gain an advantage that is not\nconsidered by security proofs that assume perfect implementations. In this work\nwe specify a side-channel capability for Eve that has yet to be considered,\nbefore then going on to discuss a scheme to autonomously detect such an attack\nduring an ongoing QKD session, and the limits as to how fast a detection can be\nmade. The side-channel capability is very general and covers a wide variety of\npossible implementations for the attack itself. We present how Alice and Bob\ncan put in place a countermeasure to continue use of the QKD system, once a\ndetection is made, regardless of the ongoing side-channel attack. This prevents\ndowntime of QKD systems, which in critical infrastructure could pose severe\nrisks. We then extend Eves side-channel capability and present a modified\nattack strategy. This strengthened attack can be detected under certain\nconditions by our scheme, however intelligent choices of parameters from Eve\nallow her strengthened attack to go undetected. From this, we discuss the\nimplications this has on Privacy Amplification, and therefore on the security\nof QKD as a whole. Finally, consideration is given as to how these types of\nattacks are analogous to certain types of faults in the QKD system, how our\ndetection scheme can also detect these faults, and therefore how this adds\nautonomous fault detection and redundancy to implementations of QKD.",
                "Universal Mechanical Polycomputation in Granular Matter\nUnconventional computing devices are increasingly of interest as they can\noperate in environments hostile to silicon-based electronics, or compute in\nways that traditional electronics cannot. Mechanical computers, wherein\ninformation processing is a material property emerging from the interaction of\ncomponents with the environment, are one such class of devices. This\ninformation processing can be manifested in various physical substrates, one of\nwhich is granular matter. In a granular assembly, vibration can be treated as\nthe information-bearing mode. This can be exploited to realize \"polycomputing\":\nmaterials can be evolved such that a single grain within them can report the\nresult of multiple logical operations simultaneously at different frequencies,\nwithout recourse to quantum effects. Here, we demonstrate the evolution of a\nmaterial in which one grain acts simultaneously as two different NAND gates at\ntwo different frequencies. NAND gates are of interest as any logical operations\ncan be built from them. Moreover, they are nonlinear thus demonstrating a step\ntoward general-purpose, computationally dense mechanical computers.\nPolycomputation was found to be distributed across each evolved material,\nsuggesting the material's robustness. With recent advances in material\nsciences, hardware realization of these materials may eventually provide\ndevices that challenge the computational density of traditional computers.",
                "Bayesian inference and neural estimation of acoustic wave propagation\nIn this work, we introduce a novel framework which combines physics and\nmachine learning methods to analyse acoustic signals. Three methods are\ndeveloped for this task: a Bayesian inference approach for inferring the\nspectral acoustics characteristics, a neural-physical model which equips a\nneural network with forward and backward physical losses, and the non-linear\nleast squares approach which serves as benchmark. The inferred propagation\ncoefficient leads to the room impulse response (RIR) quantity which can be used\nfor relocalisation with uncertainty. The simplicity and efficiency of this\nframework is empirically validated on simulated data.",
                "Re-imagining health and well-being in low resource African settings\n  using an augmented AI system and a 3D digital twin\nThis paper discusses and explores the potential and relevance of recent\ndevelopments in artificial intelligence (AI) and digital twins for health and\nwell-being in low-resource African countries. We use the case of public health\nemergency response to disease outbreaks and epidemic control. There is\npotential to take advantage of the increasing availability of data and\ndigitization to develop advanced AI methods for analysis and prediction. Using\nan AI systems perspective, we review emerging trends in AI systems and digital\ntwins and propose an initial augmented AI system architecture to illustrate how\nan AI system can work with a 3D digital twin to address public health goals. We\nhighlight scientific knowledge discovery, continual learning, pragmatic\ninteroperability, and interactive explanation and decision-making as essential\nresearch challenges for AI systems and digital twins.",
                "Deep Electron Cloud-activity and Field-activity Relationships\nChemists have been pursuing the general mathematical laws to explain and\npredict molecular properties for a long time. However, most of the traditional\nquantitative structure-activity relationship (QSAR) models have limited\napplication domains, e.g., they tend to have poor generalization performance\nwhen applied to molecules with parent structures different from those of the\ntrained molecules. This paper attempts to develop a new QSAR method that is\ntheoretically possible to predict various properties of molecules with diverse\nstructures. The proposed deep electron cloud-activity relationships (DECAR) and\ndeep field-activity relationships (DFAR) methods consist of three essentials:\n(1) A large number of molecule entities with activity data as training objects\nand responses; (2) three-dimensional electron cloud density (ECD) or related\nfield data by the accurate density functional theory methods as input\ndescriptors; (3) a deep learning model that is sufficiently flexible and\npowerful to learn the large data described above. DECAR and DFAR are used to\ndistinguish 977 sweet and 1965 non-sweet molecules (with 6-fold data\naugmentation) and the classification performance is demonstrated to be\nsignificantly better than the traditional least squares support vector machine\n(LS-SVM) models using traditional descriptors. DECAR and DFAR would provide a\npossible way to establish a widely applicable, cumulative, and shareable\nartificial intelligence-driven QSAR system. They are likely to promote the\ndevelopment of an interactive platform to collect and share the accurate ECD\nand field data of millions of molecules with annotated activities. With enough\ninput data, we envision the appearance of several deep networks trained for\nvarious molecular activities. Finally, we could anticipate a single DECAR or\nDFAR network to learn and infer various properties of interest for chemical\nmolecules.",
                "IoT based Personal Voice Assistant\nToday, technological advancement is increasing day by day. Earlier, there was\nonly a computer system in which we could only perform a few tasks. But now,\nmachine learning, artificial intelligence, deep learning, and a few more\ntechnologies have made computer systems so advanced that we can perform any\ntype of task. In this era of advancement, if people are still struggling to\ninteract using various input devices, then it's not worth it. For this reason,\nwe developed a voice assistant using Python that allows the user to run any\ntype of command in Linux without interaction with the keyboard. The main task\nof the voice assistant is to minimize the use of input devices like the\nkeyboard and mouse. It will also reduce hardware space and cost."
            ],
            "interesting paper": 1
        },
        {
            "papers": [
                "A Group Symmetric Stochastic Differential Equation Model for Molecule\n  Multi-modal Pretraining\nMolecule pretraining has quickly become the go-to schema to boost the\nperformance of AI-based drug discovery. Naturally, molecules can be represented\nas 2D topological graphs or 3D geometric point clouds. Although most existing\npertaining methods focus on merely the single modality, recent research has\nshown that maximizing the mutual information (MI) between such two modalities\nenhances the molecule representation ability. Meanwhile, existing molecule\nmulti-modal pretraining approaches approximate MI based on the representation\nspace encoded from the topology and geometry, thus resulting in the loss of\ncritical structural information of molecules. To address this issue, we propose\nMoleculeSDE. MoleculeSDE leverages group symmetric (e.g., SE(3)-equivariant and\nreflection-antisymmetric) stochastic differential equation models to generate\nthe 3D geometries from 2D topologies, and vice versa, directly in the input\nspace. It not only obtains tighter MI bound but also enables prosperous\ndownstream tasks than the previous work. By comparing with 17 pretraining\nbaselines, we empirically verify that MoleculeSDE can learn an expressive\nrepresentation with state-of-the-art performance on 26 out of 32 downstream\ntasks.",
                "Deep Electron Cloud-activity and Field-activity Relationships\nChemists have been pursuing the general mathematical laws to explain and\npredict molecular properties for a long time. However, most of the traditional\nquantitative structure-activity relationship (QSAR) models have limited\napplication domains, e.g., they tend to have poor generalization performance\nwhen applied to molecules with parent structures different from those of the\ntrained molecules. This paper attempts to develop a new QSAR method that is\ntheoretically possible to predict various properties of molecules with diverse\nstructures. The proposed deep electron cloud-activity relationships (DECAR) and\ndeep field-activity relationships (DFAR) methods consist of three essentials:\n(1) A large number of molecule entities with activity data as training objects\nand responses; (2) three-dimensional electron cloud density (ECD) or related\nfield data by the accurate density functional theory methods as input\ndescriptors; (3) a deep learning model that is sufficiently flexible and\npowerful to learn the large data described above. DECAR and DFAR are used to\ndistinguish 977 sweet and 1965 non-sweet molecules (with 6-fold data\naugmentation) and the classification performance is demonstrated to be\nsignificantly better than the traditional least squares support vector machine\n(LS-SVM) models using traditional descriptors. DECAR and DFAR would provide a\npossible way to establish a widely applicable, cumulative, and shareable\nartificial intelligence-driven QSAR system. They are likely to promote the\ndevelopment of an interactive platform to collect and share the accurate ECD\nand field data of millions of molecules with annotated activities. With enough\ninput data, we envision the appearance of several deep networks trained for\nvarious molecular activities. Finally, we could anticipate a single DECAR or\nDFAR network to learn and infer various properties of interest for chemical\nmolecules.",
                "Large Language Models, scientific knowledge and factuality: A systematic\n  analysis in antibiotic discovery\nInferring over and extracting information from Large Language Models (LLMs)\ntrained on a large corpus of scientific literature can potentially drive a new\nera in biomedical research, reducing the barriers for accessing existing\nmedical evidence. This work examines the potential of LLMs for dialoguing with\nbiomedical background knowledge, using the context of antibiotic discovery. The\nsystematic analysis is applied to ten state-of-the-art models, from models\nspecialised on biomedical scientific corpora to general models such as ChatGPT,\nGPT-4 and Llama 2 in two prompting-based tasks: chemical compound definition\ngeneration and chemical compound-fungus relation determination. The work\nprovides a systematic assessment on the ability of LLMs to encode and express\nthese relations, verifying for fluency, prompt-alignment, semantic coherence,\nfactual knowledge and specificity of generated responses. Results show that\nwhile recent models have improved in fluency, factual accuracy is still low and\nmodels are biased towards over-represented entities. The ability of LLMs to\nserve as biomedical knowledge bases is questioned, and the need for additional\nsystematic evaluation frameworks is highlighted. The best performing GPT-4\nproduced a factual definition for 70% of chemical compounds and 43.6% factual\nrelations to fungi, whereas the best open source model BioGPT-large 30% of the\ncompounds and 30% of the relations for the best-performing prompt. The results\nshow that while LLMs are currently not fit for purpose to be used as biomedical\nfactual knowledge bases, there is a promising emerging property in the\ndirection of factuality as the models become domain specialised, scale-up in\nsize and level of human feedback.",
                "Shift-Robust Molecular Relational Learning with Causal Substructure\nRecently, molecular relational learning, whose goal is to predict the\ninteraction behavior between molecular pairs, got a surge of interest in\nmolecular sciences due to its wide range of applications. In this work, we\npropose CMRL that is robust to the distributional shift in molecular relational\nlearning by detecting the core substructure that is causally related to\nchemical reactions. To do so, we first assume a causal relationship based on\nthe domain knowledge of molecular sciences and construct a structural causal\nmodel (SCM) that reveals the relationship between variables. Based on the SCM,\nwe introduce a novel conditional intervention framework whose intervention is\nconditioned on the paired molecule. With the conditional intervention\nframework, our model successfully learns from the causal substructure and\nalleviates the confounding effect of shortcut substructures that are spuriously\ncorrelated to chemical reactions. Extensive experiments on various tasks with\nreal-world and synthetic datasets demonstrate the superiority of CMRL over\nstate-of-the-art baseline models. Our code is available at\nhttps://github.com/Namkyeong/CMRL.",
                "Sequential Condition Evolved Interaction Knowledge Graph for Traditional\n  Chinese Medicine Recommendation\nTraditional Chinese Medicine (TCM) has a rich history of utilizing natural\nherbs to treat a diversity of illnesses. In practice, TCM diagnosis and\ntreatment are highly personalized and organically holistic, requiring\ncomprehensive consideration of the patient's state and symptoms over time.\nHowever, existing TCM recommendation approaches overlook the changes in patient\nstatus and only explore potential patterns between symptoms and prescriptions.\nIn this paper, we propose a novel Sequential Condition Evolved Interaction\nKnowledge Graph (SCEIKG), a framework that treats the model as a sequential\nprescription-making problem by considering the dynamics of the patient's\ncondition across multiple visits. In addition, we incorporate an interaction\nknowledge graph to enhance the accuracy of recommendations by considering the\ninteractions between different herbs and the patient's condition. Experimental\nresults on a real-world dataset demonstrate that our approach outperforms\nexisting TCM recommendation methods, achieving state-of-the-art performance.",
                "Re-imagining health and well-being in low resource African settings\n  using an augmented AI system and a 3D digital twin\nThis paper discusses and explores the potential and relevance of recent\ndevelopments in artificial intelligence (AI) and digital twins for health and\nwell-being in low-resource African countries. We use the case of public health\nemergency response to disease outbreaks and epidemic control. There is\npotential to take advantage of the increasing availability of data and\ndigitization to develop advanced AI methods for analysis and prediction. Using\nan AI systems perspective, we review emerging trends in AI systems and digital\ntwins and propose an initial augmented AI system architecture to illustrate how\nan AI system can work with a 3D digital twin to address public health goals. We\nhighlight scientific knowledge discovery, continual learning, pragmatic\ninteroperability, and interactive explanation and decision-making as essential\nresearch challenges for AI systems and digital twins."
            ],
            "interesting paper": 2
        },
        {
            "papers": [
                "Magnetic field regression using artificial neural networks for cold atom\n  experiments\nAccurately measuring magnetic fields is essential for magnetic-field\nsensitive experiments in fields like atomic, molecular, and optical physics,\ncondensed matter experiments, and other areas. However, since many experiments\nare conducted in an isolated vacuum environment that is inaccessible to\nexperimentalists, it can be challenging to accurately determine the magnetic\nfield. Here, we propose an efficient method for detecting magnetic fields with\nthe assistance of an artificial neural network (NN). Instead of measuring the\nmagnetic field directly at the desired location, we detect magnetic fields at\nseveral surrounding positions, and a trained NN can accurately predict the\nmagnetic field at the target location. After training, we achieve a relative\nerror of magnetic field magnitude (magnitude of error over the magnitude of\nmagnetic field) below 0.3$\\%$, and we successfully apply this method to our\nerbium quantum gas apparatus. This approach significantly simplifies the\nprocess of determining magnetic fields in isolated vacuum environments and can\nbe applied to various research fields across a wide range of magnetic field\nmagnitudes.",
                "Frame representations of qudit quantum mechanics\nThere exist many attempts to define a Wigner function for qudits, each of\nthem coming with its advantages and limitations. The existing finite versions\nhave simple definitions, but they are artificial in their construction and do\nnot allow an intuitive state analysis. The continuous versions have more\ncomplicated definitions, but they are similar to the original Wigner function\nand allow a visualization of the quantum states. The version based on the\nconcept of tight frame we present is finite, but it has certain properties and\napplications similar to those of continuous versions. Based on the frame\nrepresentation, we present several graphical representations of qubit states,\nand define two new parameters concerning them. We show that, from a\nmathematical point of view, the qubit is the orthogonal projection of qutrit.",
                "Low Precision Quantization-aware Training in Spiking Neural Networks\n  with Differentiable Quantization Function\nDeep neural networks have been proven to be highly effective tools in various\ndomains, yet their computational and memory costs restrict them from being\nwidely deployed on portable devices. The recent rapid increase of edge\ncomputing devices has led to an active search for techniques to address the\nabove-mentioned limitations of machine learning frameworks. The quantization of\nartificial neural networks (ANNs), which converts the full-precision synaptic\nweights into low-bit versions, emerged as one of the solutions. At the same\ntime, spiking neural networks (SNNs) have become an attractive alternative to\nconventional ANNs due to their temporal information processing capability,\nenergy efficiency, and high biological plausibility. Despite being driven by\nthe same motivation, the simultaneous utilization of both concepts has yet to\nbe thoroughly studied. Therefore, this work aims to bridge the gap between\nrecent progress in quantized neural networks and SNNs. It presents an extensive\nstudy on the performance of the quantization function, represented as a linear\ncombination of sigmoid functions, exploited in low-bit weight quantization in\nSNNs. The presented quantization function demonstrates the state-of-the-art\nperformance on four popular benchmarks, CIFAR10-DVS, DVS128 Gesture,\nN-Caltech101, and N-MNIST, for binary networks (64.05\\%, 95.45\\%, 68.71\\%, and\n99.43\\% respectively) with small accuracy drops and up to 31$\\times$ memory\nsavings, which outperforms existing methods.",
                "Phase Correction using Deep Learning for Satellite-to-Ground CV-QKD\nCoherent measurement of quantum signals used for continuous-variable (CV)\nquantum key distribution (QKD) across satellite-to-ground channels requires\ncompensation of phase wavefront distortions caused by atmospheric turbulence.\nOne compensation technique involves multiplexing classical reference pulses\n(RPs) and the quantum signal, with direct phase measurements on the RPs then\nused to modulate a real local oscillator (RLO) on the ground - a solution that\nalso removes some known attacks on CV-QKD. However, this is a cumbersome task\nin practice - requiring substantial complexity in equipment requirements and\ndeployment. As an alternative to this traditional practice, here we introduce a\nnew method for estimating phase corrections for an RLO by using only intensity\nmeasurements from RPs as input to a convolutional neural network, mitigating\ncompletely the necessity to measure phase wavefronts directly. Conventional\nwisdom dictates such an approach would likely be fruitless. However, we show\nthat the phase correction accuracy needed to provide for non-zero secure key\nrates through satellite-to-ground channels is achieved by our intensity-only\nmeasurements. Our work shows, for the first time, how artificial intelligence\nalgorithms can replace phase-measuring equipment in the context of CV-QKD\ndelivered from space, thereby delivering an alternate deployment paradigm for\nthis global quantum-communication application.",
                "Towards a Unifying Model of Rationality in Multiagent Systems\nMultiagent systems deployed in the real world need to cooperate with other\nagents (including humans) nearly as effectively as these agents cooperate with\none another. To design such AI, and provide guarantees of its effectiveness, we\nneed to clearly specify what types of agents our AI must be able to cooperate\nwith. In this work we propose a generic model of socially intelligent agents,\nwhich are individually rational learners that are also able to cooperate with\none another (in the sense that their joint behavior is Pareto efficient). We\ndefine rationality in terms of the regret incurred by each agent over its\nlifetime, and show how we can construct socially intelligent agents for\ndifferent forms of regret. We then discuss the implications of this model for\nthe development of \"robust\" MAS that can cooperate with a wide variety of\nsocially intelligent agents.",
                "Subequivariant Graph Reinforcement Learning in 3D Environments\nLearning a shared policy that guides the locomotion of different agents is of\ncore interest in Reinforcement Learning (RL), which leads to the study of\nmorphology-agnostic RL. However, existing benchmarks are highly restrictive in\nthe choice of starting point and target point, constraining the movement of the\nagents within 2D space. In this work, we propose a novel setup for\nmorphology-agnostic RL, dubbed Subequivariant Graph RL in 3D environments\n(3D-SGRL). Specifically, we first introduce a new set of more practical yet\nchallenging benchmarks in 3D space that allows the agent to have full\nDegree-of-Freedoms to explore in arbitrary directions starting from arbitrary\nconfigurations. Moreover, to optimize the policy over the enlarged state-action\nspace, we propose to inject geometric symmetry, i.e., subequivariance, into the\nmodeling of the policy and Q-function such that the policy can generalize to\nall directions, improving exploration efficiency. This goal is achieved by a\nnovel SubEquivariant Transformer (SET) that permits expressive message\nexchange. Finally, we evaluate the proposed method on the proposed benchmarks,\nwhere our method consistently and significantly outperforms existing approaches\non single-task, multi-task, and zero-shot generalization scenarios. Extensive\nablations are also conducted to verify our design. Code and videos are\navailable on our project page: https://alpc91.github.io/SGRL/."
            ],
            "interesting paper": 4
        },
        {
            "papers": [
                "ChatGPT-powered Conversational Drug Editing Using Retrieval and Domain\n  Feedback\nRecent advancements in conversational large language models (LLMs), such as\nChatGPT, have demonstrated remarkable promise in various domains, including\ndrug discovery. However, existing works mainly focus on investigating the\ncapabilities of conversational LLMs on chemical reaction and retrosynthesis.\nWhile drug editing, a critical task in the drug discovery pipeline, remains\nlargely unexplored. To bridge this gap, we propose ChatDrug, a framework to\nfacilitate the systematic investigation of drug editing using LLMs. ChatDrug\njointly leverages a prompt module, a retrieval and domain feedback (ReDF)\nmodule, and a conversation module to streamline effective drug editing. We\nempirically show that ChatDrug reaches the best performance on 33 out of 39\ndrug editing tasks, encompassing small molecules, peptides, and proteins. We\nfurther demonstrate, through 10 case studies, that ChatDrug can successfully\nidentify the key substructures (e.g., the molecule functional groups, peptide\nmotifs, and protein structures) for manipulation, generating diverse and valid\nsuggestions for drug editing. Promisingly, we also show that ChatDrug can offer\ninsightful explanations from a domain-specific perspective, enhancing\ninterpretability and enabling informed decision-making. This research sheds\nlight on the potential of ChatGPT and conversational LLMs for drug editing. It\npaves the way for a more efficient and collaborative drug discovery pipeline,\ncontributing to the advancement of pharmaceutical research and development.",
                "Contextual Bandits with Budgeted Information Reveal\nContextual bandit algorithms are commonly used in digital health to recommend\npersonalized treatments. However, to ensure the effectiveness of the\ntreatments, patients are often requested to take actions that have no immediate\nbenefit to them, which we refer to as pro-treatment actions. In practice,\nclinicians have a limited budget to encourage patients to take these actions\nand collect additional information. We introduce a novel optimization and\nlearning algorithm to address this problem. This algorithm effectively combines\nthe strengths of two algorithmic approaches in a seamless manner, including 1)\nan online primal-dual algorithm for deciding the optimal timing to reach out to\npatients, and 2) a contextual bandit learning algorithm to deliver personalized\ntreatment to the patient. We prove that this algorithm admits a sub-linear\nregret bound. We illustrate the usefulness of this algorithm on both synthetic\nand real-world data.",
                "Magnetic field regression using artificial neural networks for cold atom\n  experiments\nAccurately measuring magnetic fields is essential for magnetic-field\nsensitive experiments in fields like atomic, molecular, and optical physics,\ncondensed matter experiments, and other areas. However, since many experiments\nare conducted in an isolated vacuum environment that is inaccessible to\nexperimentalists, it can be challenging to accurately determine the magnetic\nfield. Here, we propose an efficient method for detecting magnetic fields with\nthe assistance of an artificial neural network (NN). Instead of measuring the\nmagnetic field directly at the desired location, we detect magnetic fields at\nseveral surrounding positions, and a trained NN can accurately predict the\nmagnetic field at the target location. After training, we achieve a relative\nerror of magnetic field magnitude (magnitude of error over the magnitude of\nmagnetic field) below 0.3$\\%$, and we successfully apply this method to our\nerbium quantum gas apparatus. This approach significantly simplifies the\nprocess of determining magnetic fields in isolated vacuum environments and can\nbe applied to various research fields across a wide range of magnetic field\nmagnitudes.",
                "Shapley Based Residual Decomposition for Instance Analysis\nIn this paper, we introduce the idea of decomposing the residuals of\nregression with respect to the data instances instead of features. This allows\nus to determine the effects of each individual instance on the model and each\nother, and in doing so makes for a model-agnostic method of identifying\ninstances of interest. In doing so, we can also determine the appropriateness\nof the model and data in the wider context of a given study. The paper focuses\non the possible applications that such a framework brings to the relatively\nunexplored field of instance analysis in the context of Explainable AI tasks.",
                "Understanding Predictive Coding as an Adaptive Trust-Region Method\nPredictive coding (PC) is a brain-inspired local learning algorithm that has\nrecently been suggested to provide advantages over backpropagation (BP) in\nbiologically relevant scenarios. While theoretical work has mainly focused on\nshowing how PC can approximate BP in various limits, the putative benefits of\n\"natural\" PC are less understood. Here we develop a theory of PC as an adaptive\ntrust-region (TR) algorithm that uses second-order information. We show that\nthe learning dynamics of PC can be interpreted as interpolating between BP's\nloss gradient direction and a TR direction found by the PC inference dynamics.\nOur theory suggests that PC should escape saddle points faster than BP, a\nprediction which we prove in a shallow linear model and support with\nexperiments on deeper networks. This work lays a foundation for understanding\nPC in deep and wide networks.",
                "On Diffusion Modeling for Anomaly Detection\nKnown for their impressive performance in generative modeling, diffusion\nmodels are attractive candidates for density-based anomaly detection. This\npaper investigates different variations of diffusion modeling for unsupervised\nand semi-supervised anomaly detection. In particular, we find that Denoising\nDiffusion Probability Models (DDPM) are performant on anomaly detection\nbenchmarks yet computationally expensive. By simplifying DDPM in application to\nanomaly detection, we are naturally led to an alternative approach called\nDiffusion Time Estimation (DTE). DTE estimates the distribution over diffusion\ntime for a given input and uses the mode or mean of this distribution as the\nanomaly score. We derive an analytical form for this density and leverage a\ndeep neural network to improve inference efficiency. Through empirical\nevaluations on the ADBench benchmark, we demonstrate that all diffusion-based\nanomaly detection methods perform competitively for both semi-supervised and\nunsupervised settings. Notably, DTE achieves orders of magnitude faster\ninference time than DDPM, while outperforming it on this benchmark. These\nresults establish diffusion-based anomaly detection as a scalable alternative\nto traditional methods and recent deep-learning techniques for standard\nunsupervised and semi-supervised anomaly detection settings."
            ],
            "interesting paper": 1
        },
        {
            "papers": [
                "Unified Information Dynamic Analysis of Quantum Decision-Making and\n  Search Algorithms: Computational Intelligence Measure\nThere are important algorithms built upon a mixture of basic techniques\ndescribed; for example, the Fast Fourier Transform (FFT) employs both\nDivide-and-Conquer and Transform-and-Conquer techniques. In this article, the\nevolution of a quantum algorithm (QA) is examined from an information theory\nviewpoint. The complex vector entering the quantum algorithmic gate - QAG is\nconsidered as an information source both from the classical and the quantum\nlevel. The analysis of the classical and quantum information flow in\nDeutsch-Jozsa, Shor and Grover algorithms is used. It is shown that QAG, based\non superposition of states, quantum entanglement and interference, when acting\non the input vector, stores information into the system state, minimizing the\ngap between classical Shannon entropy and quantum von Neumann entropy.\nMinimizing of the gap between Shannon and von Neumann entropies is considered\nas a termination criterion of QA computational intelligence measure.",
                "Scalable evaluation of incoherent infidelity in quantum devices\nQuantum processors can already execute tasks beyond the reach of classical\nsimulation, albeit for artificial problems. At this point, it is essential to\ndesign error metrics that test the experimental accuracy of quantum algorithms\nwith potential for a practical quantum advantage. The distinction between\ncoherent errors and incoherent errors is crucial, as they often involve\ndifferent error suppression tools. The first class encompasses miscalibrations\nof control signals and crosstalk, while the latter is usually related to\nstochastic events and unwanted interactions with the environment. We introduce\nthe incoherent infidelity as a measure of incoherent errors and present a\nscalable method for measuring it. This method is applicable to generic quantum\nevolutions subjected to time-dependent Markovian noise. Moreover, it provides\nan error quantifier for the target circuit, rather than an error averaged over\nmany circuits or quantum gates. The estimation of the incoherent infidelity is\nsuitable to assess circuits with sufficiently low error rates, regardless of\nthe circuit size, which is a natural requirement to run useful computations.",
                "IDToolkit: A Toolkit for Benchmarking and Developing Inverse Design\n  Algorithms in Nanophotonics\nAiding humans with scientific designs is one of the most exciting of\nartificial intelligence (AI) and machine learning (ML), due to their potential\nfor the discovery of new drugs, design of new materials and chemical compounds,\netc. However, scientific design typically requires complex domain knowledge\nthat is not familiar to AI researchers. Further, scientific studies involve\nprofessional skills to perform experiments and evaluations. These obstacles\nprevent AI researchers from developing specialized methods for scientific\ndesigns. To take a step towards easy-to-understand and reproducible research of\nscientific design, we propose a benchmark for the inverse design of\nnanophotonic devices, which can be verified computationally and accurately.\nSpecifically, we implemented three different nanophotonic design problems,\nnamely a radiative cooler, a selective emitter for thermophotovoltaics, and\nstructural color filters, all of which are different in design parameter\nspaces, complexity, and design targets. The benchmark environments are\nimplemented with an open-source simulator. We further implemented 10 different\ninverse design algorithms and compared them in a reproducible and fair\nframework. The results revealed the strengths and weaknesses of existing\nmethods, which shed light on several future directions for developing more\nefficient inverse design algorithms. Our benchmark can also serve as the\nstarting point for more challenging scientific design problems. The code of\nIDToolkit is available at https://github.com/ThyrixYang/IDToolkit.",
                "Bigger, Better, Faster: Human-level Atari with human-level efficiency\nWe introduce a value-based RL agent, which we call BBF, that achieves\nsuper-human performance in the Atari 100K benchmark. BBF relies on scaling the\nneural networks used for value estimation, as well as a number of other design\nchoices that enable this scaling in a sample-efficient manner. We conduct\nextensive analyses of these design choices and provide insights for future\nwork. We end with a discussion about updating the goalposts for\nsample-efficient RL research on the ALE. We make our code and data publicly\navailable at\nhttps://github.com/google-research/google-research/tree/master/bigger_better_faster.",
                "Uncovering multifunctional mechano-intelligence in and through phononic\n  metastructures harnessing physical reservoir computing\nThe recent advances in autonomous systems have prompted a strong demand for\nthe next generation of adaptive structures and materials to possess more\nbuilt-in intelligence in their mechanical domain, the so-called\nmechano-intelligence (MI). Previous MI attempts mainly focused on specific\ndesigns and case studies to realize limited aspects of MI, and there is a lack\nof a systematic foundation in constructing and integrating the different\nelements of intelligence in an effective and efficient manner. Here, we propose\na new approach to create the needed foundation in realizing integrated\nmultifunctional MI via a physical reservoir computing (PRC) framework. That is,\nto concurrently embody computing power and the various elements of\nintelligence, namely perception, decision-making, and commanding, directly in\nthe mechanical domain, advancing from conventional adaptive structures that\nrely solely on add-on digital computers and massive electronics to achieve\nintelligence. As an exemplar platform, we construct a mechanically intelligent\nphononic metastructure with the integrated elements of MI by harnessing the PRC\npower hidden in their high-degree-of-freedom nonlinear dynamics. Through\nanalyses and experimental investigations, we uncover multiple adaptive\nstructural functions ranging from self-tuning wave controls to wave-based logic\ngates. This research will provide the basis for creating future new structures\nthat would greatly surpass the state of the art - such as lower power\nconsumption, more direct interactions, and much better survivability in harsh\nenvironment or under cyberattacks. Moreover, it will enable the addition of new\nfunctions and autonomy to systems without overburdening the onboard computers.",
                "Probabilistic computation and uncertainty quantification with emerging\n  covariance\nBuilding robust, interpretable, and secure AI system requires quantifying and\nrepresenting uncertainty under a probabilistic perspective to mimic human\ncognitive abilities. However, probabilistic computation presents significant\nchallenges for most conventional artificial neural network, as they are\nessentially implemented in a deterministic manner. In this paper, we develop an\nefficient probabilistic computation framework by truncating the probabilistic\nrepresentation of neural activation up to its mean and covariance and construct\na moment neural network that encapsulates the nonlinear coupling between the\nmean and covariance of the underlying stochastic network. We reveal that when\nonly the mean but not the covariance is supervised during gradient-based\nlearning, the unsupervised covariance spontaneously emerges from its nonlinear\ncoupling with the mean and faithfully captures the uncertainty associated with\nmodel predictions. Our findings highlight the inherent simplicity of\nprobabilistic computation by seamlessly incorporating uncertainty into model\nprediction, paving the way for integrating it into large-scale AI systems."
            ],
            "interesting paper": 1
        },
        {
            "papers": [
                "Catalysis distillation neural network for the few shot open catalyst\n  challenge\nThe integration of artificial intelligence and science has resulted in\nsubstantial progress in computational chemistry methods for the design and\ndiscovery of novel catalysts. Nonetheless, the challenges of electrocatalytic\nreactions and developing a large-scale language model in catalysis persist, and\nthe recent success of ChatGPT's (Chat Generative Pre-trained Transformer)\nfew-shot methods surpassing BERT (Bidirectional Encoder Representation from\nTransformers) underscores the importance of addressing limited data, expensive\ncomputations, time constraints and structure-activity relationship in research.\nHence, the development of few-shot techniques for catalysis is critical and\nessential, regardless of present and future requirements. This paper introduces\nthe Few-Shot Open Catalyst Challenge 2023, a competition aimed at advancing the\napplication of machine learning technology for predicting catalytic reactions\non catalytic surfaces, with a specific focus on dual-atom catalysts in hydrogen\nperoxide electrocatalysis. To address the challenge of limited data in\ncatalysis, we propose a machine learning approach based on MLP-Like and a\nframework called Catalysis Distillation Graph Neural Network (CDGNN). Our\nresults demonstrate that CDGNN effectively learns embeddings from catalytic\nstructures, enabling the capture of structure-adsorption relationships. This\naccomplishment has resulted in the utmost advanced and efficient determination\nof the reaction pathway for hydrogen peroxide, surpassing the current graph\nneural network approach by 16.1%.. Consequently, CDGNN presents a promising\napproach for few-shot learning in catalysis.",
                "Graph Generation with $K^2$-trees\nGenerating graphs from a target distribution is a significant challenge\nacross many domains, including drug discovery and social network analysis. In\nthis work, we introduce a novel graph generation method leveraging $K^2$-tree\nrepresentation, originally designed for lossless graph compression. The\n$K^2$-tree representation {encompasses inherent hierarchy while enabling\ncompact graph generation}. In addition, we make contributions by (1) presenting\na sequential $K^2$-treerepresentation that incorporates pruning, flattening,\nand tokenization processes and (2) introducing a Transformer-based architecture\ndesigned to generate the sequence by incorporating a specialized tree\npositional encoding scheme. Finally, we extensively evaluate our algorithm on\nfour general and two molecular graph datasets to confirm its superiority for\ngraph generation.",
                "DKINet: Medication Recommendation via Domain Knowledge Informed Deep\n  Learning\nMedication recommendation is a fundamental yet crucial branch of healthcare\nthat presents opportunities to assist physicians in making more accurate\nmedication prescriptions for patients with complex health conditions. Previous\nstudies have primarily focused on learning patient representation from\nelectronic health records (EHR). While considering the clinical manifestations\nof the patient is important, incorporating domain-specific prior knowledge is\nequally significant in diagnosing the patient's health conditions. However,\neffectively integrating domain knowledge with the patient's clinical\nmanifestations can be challenging, particularly when dealing with complex\nclinical manifestations. Therefore, in this paper, we first identify\ncomprehensive domain-specific prior knowledge, namely the Unified Medical\nLanguage System (UMLS), which is a comprehensive repository of biomedical\nvocabularies and standards, for knowledge extraction. Subsequently, we propose\na knowledge injection module that addresses the effective integration of domain\nknowledge with complex clinical manifestations, enabling an effective\ncharacterization of the health conditions of the patient. Furthermore,\nconsidering the significant impact of a patient's medication history on their\ncurrent medication, we introduce a historical medication-aware patient\nrepresentation module to capture the longitudinal influence of historical\nmedication information on the representation of current patients. Extensive\nexperiments on three publicly benchmark datasets verify the superiority of our\nproposed method, which outperformed other methods by a significant margin. The\ncode is available at: https://github.com/sherry6247/DKINet.",
                "IDToolkit: A Toolkit for Benchmarking and Developing Inverse Design\n  Algorithms in Nanophotonics\nAiding humans with scientific designs is one of the most exciting of\nartificial intelligence (AI) and machine learning (ML), due to their potential\nfor the discovery of new drugs, design of new materials and chemical compounds,\netc. However, scientific design typically requires complex domain knowledge\nthat is not familiar to AI researchers. Further, scientific studies involve\nprofessional skills to perform experiments and evaluations. These obstacles\nprevent AI researchers from developing specialized methods for scientific\ndesigns. To take a step towards easy-to-understand and reproducible research of\nscientific design, we propose a benchmark for the inverse design of\nnanophotonic devices, which can be verified computationally and accurately.\nSpecifically, we implemented three different nanophotonic design problems,\nnamely a radiative cooler, a selective emitter for thermophotovoltaics, and\nstructural color filters, all of which are different in design parameter\nspaces, complexity, and design targets. The benchmark environments are\nimplemented with an open-source simulator. We further implemented 10 different\ninverse design algorithms and compared them in a reproducible and fair\nframework. The results revealed the strengths and weaknesses of existing\nmethods, which shed light on several future directions for developing more\nefficient inverse design algorithms. Our benchmark can also serve as the\nstarting point for more challenging scientific design problems. The code of\nIDToolkit is available at https://github.com/ThyrixYang/IDToolkit.",
                "Reliable Off-Policy Learning for Dosage Combinations\nDecision-making in personalized medicine such as cancer therapy or critical\ncare must often make choices for dosage combinations, i.e., multiple continuous\ntreatments. Existing work for this task has modeled the effect of multiple\ntreatments independently, while estimating the joint effect has received little\nattention but comes with non-trivial challenges. In this paper, we propose a\nnovel method for reliable off-policy learning for dosage combinations. Our\nmethod proceeds along three steps: (1) We develop a tailored neural network\nthat estimates the individualized dose-response function while accounting for\nthe joint effect of multiple dependent dosages. (2) We estimate the generalized\npropensity score using conditional normalizing flows in order to detect regions\nwith limited overlap in the shared covariate-treatment space. (3) We present a\ngradient-based learning algorithm to find the optimal, individualized dosage\ncombinations. Here, we ensure reliable estimation of the policy value by\navoiding regions with limited overlap. We finally perform an extensive\nevaluation of our method to show its effectiveness. To the best of our\nknowledge, ours is the first work to provide a method for reliable off-policy\nlearning for optimal dosage combinations.",
                "Less Likely Brainstorming: Using Language Models to Generate Alternative\n  Hypotheses\nA human decision-maker benefits the most from an AI assistant that corrects\nfor their biases. For problems such as generating interpretation of a radiology\nreport given findings, a system predicting only highly likely outcomes may be\nless useful, where such outcomes are already obvious to the user. To alleviate\nbiases in human decision-making, it is worth considering a broad differential\ndiagnosis, going beyond the most likely options. We introduce a new task, \"less\nlikely brainstorming,\" that asks a model to generate outputs that humans think\nare relevant but less likely to happen. We explore the task in two settings: a\nbrain MRI interpretation generation setting and an everyday commonsense\nreasoning setting. We found that a baseline approach of training with less\nlikely hypotheses as targets generates outputs that humans evaluate as either\nlikely or irrelevant nearly half of the time; standard MLE training is not\neffective. To tackle this problem, we propose a controlled text generation\nmethod that uses a novel contrastive learning strategy to encourage models to\ndifferentiate between generating likely and less likely outputs according to\nhumans. We compare our method with several state-of-the-art controlled text\ngeneration models via automatic and human evaluations and show that our models'\ncapability of generating less likely outputs is improved."
            ],
            "interesting paper": 1
        },
        {
            "papers": [
                "Shadows of quantum machine learning\nQuantum machine learning is often highlighted as one of the most promising\npractical applications for which quantum computers could provide a\ncomputational advantage. However, a major obstacle to the widespread use of\nquantum machine learning models in practice is that these models, even once\ntrained, still require access to a quantum computer in order to be evaluated on\nnew data. To solve this issue, we introduce a new class of quantum models where\nquantum resources are only required during training, while the deployment of\nthe trained model is classical. Specifically, the training phase of our models\nends with the generation of a 'shadow model' from which the classical\ndeployment becomes possible. We prove that: i) this class of models is\nuniversal for classically-deployed quantum machine learning; ii) it does have\nrestricted learning capacities compared to 'fully quantum' models, but\nnonetheless iii) it achieves a provable learning advantage over fully classical\nlearners, contingent on widely-believed assumptions in complexity theory. These\nresults provide compelling evidence that quantum machine learning can confer\nlearning advantages across a substantially broader range of scenarios, where\nquantum computers are exclusively employed during the training phase. By\nenabling classical deployment, our approach facilitates the implementation of\nquantum machine learning models in various practical contexts.",
                "Improved Financial Forecasting via Quantum Machine Learning\nQuantum algorithms have the potential to enhance machine learning across a\nvariety of domains and applications. In this work, we show how quantum machine\nlearning can be used to improve financial forecasting. First, we use classical\nand quantum Determinantal Point Processes to enhance Random Forest models for\nchurn prediction, improving precision by almost 6%. Second, we design quantum\nneural network architectures with orthogonal and compound layers for credit\nrisk assessment, which match classical performance with significantly fewer\nparameters. Our results demonstrate that leveraging quantum ideas can\neffectively enhance the performance of machine learning, both today as\nquantum-inspired classical ML solutions, and even more in the future, with the\nadvent of better quantum hardware.",
                "Cellular automata inspired multistable origami metamaterials for\n  mechanical learning\nRecent advances in multistable metamaterials reveal a link between structural\nconfiguration transition and Boolean logic, heralding a new generation of\ncomputationally capable intelligent materials. To enable higher-level\ncomputation, existing computational frameworks require the integration of\nlarge-scale networked logic gates, which places demanding requirements on the\nfabrication of materials counterparts and the propagation of signals. Inspired\nby cellular automata, we propose a novel computational framework based on\nmultistable origami metamaterials by incorporating reservoir computing, which\ncan accomplish high-level computation tasks without the need to construct a\nlogic gate network. This approach thus eleimates the demanding requirements for\nfabrication of materials and signal propagation when constructing large-scale\nnetworks for high-level computation in conventional mechano-logic. Using the\nmultistable stacked Miura-origami metamaterial as a validation platform, digit\nrecognition is successfully implemented through experiments by a single\nactuator. Moreover, complex tasks, such as handwriting recognition and 5-bit\nmemory tasks, are also shown to be feasible with the new computation framework.\nOur research represents a significant advancement in developing a new\ngeneration of intelligent materials with advanced computational capabilities.\nWith continued research and development, these materials could have a\ntransformative impact on a wide range of fields, from computational science to\nmaterial mechano-intelligence technology and beyond.",
                "Fast-SNN: Fast Spiking Neural Network by Converting Quantized ANN\nSpiking neural networks (SNNs) have shown advantages in computation and\nenergy efficiency over traditional artificial neural networks (ANNs) thanks to\ntheir event-driven representations. SNNs also replace weight multiplications in\nANNs with additions, which are more energy-efficient and less computationally\nintensive. However, it remains a challenge to train deep SNNs due to the\ndiscrete spike function. A popular approach to circumvent this challenge is\nANN-to-SNN conversion. However, due to the quantization error and accumulating\nerror, it often requires lots of time steps (high inference latency) to achieve\nhigh performance, which negates SNN's advantages. To this end, this paper\nproposes Fast-SNN that achieves high performance with low latency. We\ndemonstrate the equivalent mapping between temporal quantization in SNNs and\nspatial quantization in ANNs, based on which the minimization of the\nquantization error is transferred to quantized ANN training. With the\nminimization of the quantization error, we show that the sequential error is\nthe primary cause of the accumulating error, which is addressed by introducing\na signed IF neuron model and a layer-wise fine-tuning mechanism. Our method\nachieves state-of-the-art performance and low latency on various computer\nvision tasks, including image classification, object detection, and semantic\nsegmentation. Codes are available at: https://github.com/yangfan-hu/Fast-SNN.",
                "TorchRL: A data-driven decision-making library for PyTorch\nPyTorch has ascended as a premier machine learning framework, yet it lacks a\nnative and comprehensive library for decision and control tasks suitable for\nlarge development teams dealing with complex real-world data and environments.\nTo address this issue, we propose TorchRL, a generalistic control library for\nPyTorch that provides well-integrated, yet standalone components. We introduce\na new and flexible PyTorch primitive, the TensorDict, which facilitates\nstreamlined algorithm development across the many branches of Reinforcement\nLearning (RL) and control. We provide a detailed description of the building\nblocks and an extensive overview of the library across domains and tasks.\nFinally, we experimentally demonstrate its reliability and flexibility and show\ncomparative benchmarks to demonstrate its computational efficiency. TorchRL\nfosters long-term support and is publicly available on GitHub for greater\nreproducibility and collaboration within the research community. The code is\nopen-sourced on GitHub.",
                "BetaZero: Belief-State Planning for Long-Horizon POMDPs using Learned\n  Approximations\nReal-world planning problems, including autonomous driving and sustainable\nenergy applications like carbon storage and resource exploration, have recently\nbeen modeled as partially observable Markov decision processes (POMDPs) and\nsolved using approximate methods. To solve high-dimensional POMDPs in practice,\nstate-of-the-art methods use online planning with problem-specific heuristics\nto reduce planning horizons and make the problems tractable. Algorithms that\nlearn approximations to replace heuristics have recently found success in\nlarge-scale fully observable domains. The key insight is the combination of\nonline Monte Carlo tree search with offline neural network approximations of\nthe optimal policy and value function. In this work, we bring this insight to\npartially observable domains and propose BetaZero, a belief-state planning\nalgorithm for high-dimensional POMDPs. BetaZero learns offline approximations\nthat replace heuristics to enable online decision making in long-horizon\nproblems. We address several challenges inherent in large-scale partially\nobservable domains; namely challenges of transitioning in stochastic\nenvironments, prioritizing action branching with a limited search budget, and\nrepresenting beliefs as input to the network. To formalize the use of all\nlimited search information, we train against a novel $Q$-weighted visit counts\npolicy. We test BetaZero on various well-established POMDP benchmarks found in\nthe literature and a real-world problem of critical mineral exploration.\nExperiments show that BetaZero outperforms state-of-the-art POMDP solvers on a\nvariety of tasks."
            ],
            "interesting paper": 1
        },
        {
            "papers": [
                "AbODE: Ab Initio Antibody Design using Conjoined ODEs\nAntibodies are Y-shaped proteins that neutralize pathogens and constitute the\ncore of our adaptive immune system. De novo generation of new antibodies that\ntarget specific antigens holds the key to accelerating vaccine discovery.\nHowever, this co-design of the amino acid sequence and the 3D structure\nsubsumes and accentuates some central challenges from multiple tasks, including\nprotein folding (sequence to structure), inverse folding (structure to\nsequence), and docking (binding). We strive to surmount these challenges with a\nnew generative model AbODE that extends graph PDEs to accommodate both\ncontextual information and external interactions. Unlike existing approaches,\nAbODE uses a single round of full-shot decoding and elicits continuous\ndifferential attention that encapsulates and evolves with latent interactions\nwithin the antibody as well as those involving the antigen. We unravel\nfundamental connections between AbODE and temporal networks as well as\ngraph-matching networks. The proposed model significantly outperforms existing\nmethods on standard metrics across benchmarks.",
                "Knowledge Graph Embeddings in the Biomedical Domain: Are They Useful? A\n  Look at Link Prediction, Rule Learning, and Downstream Polypharmacy Tasks\nKnowledge graphs are powerful tools for representing and organising complex\nbiomedical data. Several knowledge graph embedding algorithms have been\nproposed to learn from and complete knowledge graphs. However, a recent study\ndemonstrates the limited efficacy of these embedding algorithms when applied to\nbiomedical knowledge graphs, raising the question of whether knowledge graph\nembeddings have limitations in biomedical settings. This study aims to apply\nstate-of-the-art knowledge graph embedding models in the context of a recent\nbiomedical knowledge graph, BioKG, and evaluate their performance and potential\ndownstream uses. We achieve a three-fold improvement in terms of performance\nbased on the HITS@10 score over previous work on the same biomedical knowledge\ngraph. Additionally, we provide interpretable predictions through a rule-based\nmethod. We demonstrate that knowledge graph embedding models are applicable in\npractice by evaluating the best-performing model on four tasks that represent\nreal-life polypharmacy situations. Results suggest that knowledge learnt from\nlarge biomedical knowledge graphs can be transferred to such downstream use\ncases. Our code is available at https://github.com/aryopg/biokge.",
                "Enrichment of the NLST and NSCLC-Radiomics computed tomography\n  collections with AI-derived annotations\nPublic imaging datasets are critical for the development and evaluation of\nautomated tools in cancer imaging. Unfortunately, many do not include\nannotations or image-derived features, complicating their downstream analysis.\nArtificial intelligence-based annotation tools have been shown to achieve\nacceptable performance and thus can be used to automatically annotate large\ndatasets. As part of the effort to enrich public data available within NCI\nImaging Data Commons (IDC), here we introduce AI-generated annotations for two\ncollections of computed tomography images of the chest, NSCLC-Radiomics, and\nthe National Lung Screening Trial. Using publicly available AI algorithms we\nderived volumetric annotations of thoracic organs at risk, their corresponding\nradiomics features, and slice-level annotations of anatomical landmarks and\nregions. The resulting annotations are publicly available within IDC, where the\nDICOM format is used to harmonize the data and achieve FAIR principles. The\nannotations are accompanied by cloud-enabled notebooks demonstrating their use.\nThis study reinforces the need for large, publicly accessible curated datasets\nand demonstrates how AI can be used to aid in cancer imaging.",
                "TorchRL: A data-driven decision-making library for PyTorch\nPyTorch has ascended as a premier machine learning framework, yet it lacks a\nnative and comprehensive library for decision and control tasks suitable for\nlarge development teams dealing with complex real-world data and environments.\nTo address this issue, we propose TorchRL, a generalistic control library for\nPyTorch that provides well-integrated, yet standalone components. We introduce\na new and flexible PyTorch primitive, the TensorDict, which facilitates\nstreamlined algorithm development across the many branches of Reinforcement\nLearning (RL) and control. We provide a detailed description of the building\nblocks and an extensive overview of the library across domains and tasks.\nFinally, we experimentally demonstrate its reliability and flexibility and show\ncomparative benchmarks to demonstrate its computational efficiency. TorchRL\nfosters long-term support and is publicly available on GitHub for greater\nreproducibility and collaboration within the research community. The code is\nopen-sourced on GitHub.",
                "Designing Closed-Loop Models for Task Allocation\nAutomatically assigning tasks to people is challenging because human\nperformance can vary across tasks for many reasons. This challenge is further\ncompounded in real-life settings in which no oracle exists to assess the\nquality of human decisions and task assignments made. Instead, we find\nourselves in a \"closed\" decision-making loop in which the same fallible human\ndecisions we rely on in practice must also be used to guide task allocation.\nHow can imperfect and potentially biased human decisions train an accurate\nallocation model? Our key insight is to exploit weak prior information on\nhuman-task similarity to bootstrap model training. We show that the use of such\na weak prior can improve task allocation accuracy, even when human\ndecision-makers are fallible and biased. We present both theoretical analysis\nand empirical evaluation over synthetic data and a social media toxicity\ndetection task. Results demonstrate the efficacy of our approach.",
                "Credit Card Fraud Detection Using Asexual Reproduction Optimization\nAs the number of credit card users has increased, detecting fraud in this\ndomain has become a vital issue. Previous literature has applied various\nsupervised and unsupervised machine learning methods to find an effective fraud\ndetection system. However, some of these methods require an enormous amount of\ntime to achieve reasonable accuracy. In this paper, an Asexual Reproduction\nOptimization (ARO) approach was employed, which is a supervised method to\ndetect credit card fraud. ARO refers to a kind of production in which one\nparent produces some offspring. By applying this method and sampling just from\nthe majority class, the effectiveness of the classification is increased. A\ncomparison to Artificial Immune Systems (AIS), which is one of the best methods\nimplemented on current datasets, has shown that the proposed method is able to\nremarkably reduce the required training time and at the same time increase the\nrecall that is important in fraud detection problems. The obtained results show\nthat ARO achieves the best cost in a short time, and consequently, it can be\nconsidered a real-time fraud detection system."
            ],
            "interesting paper": 1
        },
        {
            "papers": [
                "IQL-TD-MPC: Implicit Q-Learning for Hierarchical Model Predictive\n  Control\nModel-based reinforcement learning (RL) has shown great promise due to its\nsample efficiency, but still struggles with long-horizon sparse-reward tasks,\nespecially in offline settings where the agent learns from a fixed dataset. We\nhypothesize that model-based RL agents struggle in these environments due to a\nlack of long-term planning capabilities, and that planning in a temporally\nabstract model of the environment can alleviate this issue. In this paper, we\nmake two key contributions: 1) we introduce an offline model-based RL\nalgorithm, IQL-TD-MPC, that extends the state-of-the-art Temporal Difference\nLearning for Model Predictive Control (TD-MPC) with Implicit Q-Learning (IQL);\n2) we propose to use IQL-TD-MPC as a Manager in a hierarchical setting with any\noff-the-shelf offline RL algorithm as a Worker. More specifically, we pre-train\na temporally abstract IQL-TD-MPC Manager to predict \"intent embeddings\", which\nroughly correspond to subgoals, via planning. We empirically show that\naugmenting state representations with intent embeddings generated by an\nIQL-TD-MPC manager significantly improves off-the-shelf offline RL agents'\nperformance on some of the most challenging D4RL benchmark tasks. For instance,\nthe offline RL algorithms AWAC, TD3-BC, DT, and CQL all get zero or near-zero\nnormalized evaluation scores on the medium and large antmaze tasks, while our\nmodification gives an average score over 40.",
                "Deep Q-Learning versus Proximal Policy Optimization: Performance\n  Comparison in a Material Sorting Task\nThis paper presents a comparison between two well-known deep Reinforcement\nLearning (RL) algorithms: Deep Q-Learning (DQN) and Proximal Policy\nOptimization (PPO) in a simulated production system. We utilize a Petri Net\n(PN)-based simulation environment, which was previously proposed in related\nwork. The performance of the two algorithms is compared based on several\nevaluation metrics, including average percentage of correctly assembled and\nsorted products, average episode length, and percentage of successful episodes.\nThe results show that PPO outperforms DQN in terms of all evaluation metrics.\nThe study highlights the advantages of policy-based algorithms in problems with\nhigh-dimensional state and action spaces. The study contributes to the field of\ndeep RL in context of production systems by providing insights into the\neffectiveness of different algorithms and their suitability for different\ntasks.",
                "AI Liability Insurance With an Example in AI-Powered E-diagnosis System\nArtificial Intelligence (AI) has received an increasing amount of attention\nin multiple areas. The uncertainties and risks in AI-powered systems have\ncreated reluctance in their wild adoption. As an economic solution to\ncompensate for potential damages, AI liability insurance is a promising market\nto enhance the integration of AI into daily life. In this work, we use an\nAI-powered E-diagnosis system as an example to study AI liability insurance. We\nprovide a quantitative risk assessment model with evidence-based numerical\nanalysis. We discuss the insurability criteria for AI technologies and suggest\nnecessary adjustments to accommodate the features of AI products. We show that\nAI liability insurance can act as a regulatory mechanism to incentivize\ncompliant behaviors and serve as a certificate of high-quality AI systems.\nFurthermore, we suggest premium adjustment to reflect the dynamic evolution of\nthe inherent uncertainty in AI. Moral hazard problems are discussed and\nsuggestions for AI liability insurance are provided.",
                "An Overview on Generative AI at Scale with Edge-Cloud Computing\nAs a specific category of artificial intelligence (AI), generative artificial\nintelligence (GenAI) generates new content that resembles what is created by\nhumans. The rapid development of GenAI systems has created a huge amount of new\ndata on the Internet, posing new challenges to current computing and\ncommunication frameworks. Currently, GenAI services rely on the traditional\ncloud computing framework due to the need for large computation resources.\nHowever, such services will encounter high latency because of data transmission\nand a high volume of requests. On the other hand, edge-cloud computing can\nprovide adequate computation power and low latency at the same time through the\ncollaboration between edges and the cloud. Thus, it is attractive to build\nGenAI systems at scale by leveraging the edge-cloud computing paradigm. In this\noverview paper, we review recent developments in GenAI and edge-cloud\ncomputing, respectively. Then, we use two exemplary GenAI applications to\ndiscuss technical challenges in scaling up their solutions using edge-cloud\ncollaborative systems. Finally, we list design considerations for training and\ndeploying GenAI systems at scale and point out future research directions.",
                "Integrated Sensing-Communication-Computation for Edge Artificial\n  Intelligence\nEdge artificial intelligence (AI) has been a promising solution towards 6G to\nempower a series of advanced techniques such as digital twins, holographic\nprojection, semantic communications, and auto-driving, for achieving\nintelligence of everything. The performance of edge AI tasks, including edge\nlearning and edge AI inference, depends on the quality of three highly coupled\nprocesses, i.e., sensing for data acquisition, computation for information\nextraction, and communication for information transmission. However, these\nthree modules need to compete for network resources for enhancing their own\nquality-of-services. To this end, integrated sensing-communication-computation\n(ISCC) is of paramount significance for improving resource utilization as well\nas achieving the customized goals of edge AI tasks. By investigating the\ninterplay among the three modules, this article presents various kinds of ISCC\nschemes for federated edge learning tasks and edge AI inference tasks in both\napplication and physical layers.",
                "LLMatic: Neural Architecture Search via Large Language Models and\n  Quality Diversity Optimization\nLarge Language Models (LLMs) have emerged as powerful tools capable of\naccomplishing a broad spectrum of tasks. Their abilities span numerous areas,\nand one area where they have made a significant impact is in the domain of code\ngeneration. Here, we propose using the coding abilities of LLMs to introduce\nmeaningful variations to code defining neural networks. Meanwhile,\nQuality-Diversity (QD) algorithms are known to discover diverse and robust\nsolutions. By merging the code-generating abilities of LLMs with the diversity\nand robustness of QD solutions, we introduce \\texttt{LLMatic}, a Neural\nArchitecture Search (NAS) algorithm. While LLMs struggle to conduct NAS\ndirectly through prompts, \\texttt{LLMatic} uses a procedural approach,\nleveraging QD for prompts and network architecture to create diverse and\nhigh-performing networks. We test \\texttt{LLMatic} on the CIFAR-10 and\nNAS-bench-201 benchmarks, demonstrating that it can produce competitive\nnetworks while evaluating just $2,000$ candidates, even without prior knowledge\nof the benchmark domain or exposure to any previous top-performing models for\nthe benchmark. The open-sourced code is available in\n\\url{https://github.com/umair-nasir14/LLMatic}."
            ],
            "interesting paper": 1
        },
        {
            "papers": [
                "The ethical ambiguity of AI data enrichment: Measuring gaps in research\n  ethics norms and practices\nThe technical progression of artificial intelligence (AI) research has been\nbuilt on breakthroughs in fields such as computer science, statistics, and\nmathematics. However, in the past decade AI researchers have increasingly\nlooked to the social sciences, turning to human interactions to solve the\nchallenges of model development. Paying crowdsourcing workers to generate or\ncurate data, or data enrichment, has become indispensable for many areas of AI\nresearch, from natural language processing to reinforcement learning from human\nfeedback (RLHF). Other fields that routinely interact with crowdsourcing\nworkers, such as Psychology, have developed common governance requirements and\nnorms to ensure research is undertaken ethically. This study explores how, and\nto what extent, comparable research ethics requirements and norms have\ndeveloped for AI research and data enrichment. We focus on the approach taken\nby two leading conferences: ICLR and NeurIPS, and journal publisher Springer.\nIn a longitudinal study of accepted papers, and via a comparison with\nPsychology and CHI papers, this work finds that leading AI venues have begun to\nestablish protocols for human data collection, but these are are inconsistently\nfollowed by authors. Whilst Psychology papers engaging with crowdsourcing\nworkers frequently disclose ethics reviews, payment data, demographic data and\nother information, similar disclosures are far less common in leading AI venues\ndespite similar guidance. The work concludes with hypotheses to explain these\ngaps in research ethics practices and considerations for its implications.",
                "Cook-Gen: Robust Generative Modeling of Cooking Actions from Recipes\nAs people become more aware of their food choices, food computation models\nhave become increasingly popular in assisting people in maintaining healthy\neating habits. For example, food recommendation systems analyze recipe\ninstructions to assess nutritional contents and provide recipe recommendations.\nThe recent and remarkable successes of generative AI methods, such as\nauto-regressive large language models, can lead to robust methods for a more\ncomprehensive understanding of recipes for healthy food recommendations beyond\nsurface-level nutrition content assessments. In this study, we explore the use\nof generative AI methods to extend current food computation models, primarily\ninvolving the analysis of nutrition and ingredients, to also incorporate\ncooking actions (e.g., add salt, fry the meat, boil the vegetables, etc.).\nCooking actions are notoriously hard to model using statistical learning\nmethods due to irregular data patterns - significantly varying natural language\ndescriptions for the same action (e.g., marinate the meat vs. marinate the meat\nand leave overnight) and infrequently occurring patterns (e.g., add salt occurs\nfar more frequently than marinating the meat). The prototypical approach to\nhandling irregular data patterns is to increase the volume of data that the\nmodel ingests by orders of magnitude. Unfortunately, in the cooking domain,\nthese problems are further compounded with larger data volumes presenting a\nunique challenge that is not easily handled by simply scaling up. In this work,\nwe propose novel aggregation-based generative AI methods, Cook-Gen, that\nreliably generate cooking actions from recipes, despite difficulties with\nirregular data patterns, while also outperforming Large Language Models and\nother strong baselines.",
                "AI Liability Insurance With an Example in AI-Powered E-diagnosis System\nArtificial Intelligence (AI) has received an increasing amount of attention\nin multiple areas. The uncertainties and risks in AI-powered systems have\ncreated reluctance in their wild adoption. As an economic solution to\ncompensate for potential damages, AI liability insurance is a promising market\nto enhance the integration of AI into daily life. In this work, we use an\nAI-powered E-diagnosis system as an example to study AI liability insurance. We\nprovide a quantitative risk assessment model with evidence-based numerical\nanalysis. We discuss the insurability criteria for AI technologies and suggest\nnecessary adjustments to accommodate the features of AI products. We show that\nAI liability insurance can act as a regulatory mechanism to incentivize\ncompliant behaviors and serve as a certificate of high-quality AI systems.\nFurthermore, we suggest premium adjustment to reflect the dynamic evolution of\nthe inherent uncertainty in AI. Moral hazard problems are discussed and\nsuggestions for AI liability insurance are provided.",
                "From Temporal to Contemporaneous Iterative Causal Discovery in the\n  Presence of Latent Confounders\nWe present a constraint-based algorithm for learning causal structures from\nobservational time-series data, in the presence of latent confounders. We\nassume a discrete-time, stationary structural vector autoregressive process,\nwith both temporal and contemporaneous causal relations. One may ask if\ntemporal and contemporaneous relations should be treated differently. The\npresented algorithm gradually refines a causal graph by learning long-term\ntemporal relations before short-term ones, where contemporaneous relations are\nlearned last. This ordering of causal relations to be learnt leads to a\nreduction in the required number of statistical tests. We validate this\nreduction empirically and demonstrate that it leads to higher accuracy for\nsynthetic data and more plausible causal graphs for real-world data compared to\nstate-of-the-art algorithms.",
                "Rotating Features for Object Discovery\nThe binding problem in human cognition, concerning how the brain represents\nand connects objects within a fixed network of neural connections, remains a\nsubject of intense debate. Most machine learning efforts addressing this issue\nin an unsupervised setting have focused on slot-based methods, which may be\nlimiting due to their discrete nature and difficulty to express uncertainty.\nRecently, the Complex AutoEncoder was proposed as an alternative that learns\ncontinuous and distributed object-centric representations. However, it is only\napplicable to simple toy data. In this paper, we present Rotating Features, a\ngeneralization of complex-valued features to higher dimensions, and a new\nevaluation procedure for extracting objects from distributed representations.\nAdditionally, we show the applicability of our approach to pre-trained\nfeatures. Together, these advancements enable us to scale distributed\nobject-centric representations from simple toy to real-world data. We believe\nthis work advances a new paradigm for addressing the binding problem in machine\nlearning and has the potential to inspire further innovation in the field.",
                "Efficient Failure Pattern Identification of Predictive Algorithms\nGiven a (machine learning) classifier and a collection of unlabeled data, how\ncan we efficiently identify misclassification patterns presented in this\ndataset? To address this problem, we propose a human-machine collaborative\nframework that consists of a team of human annotators and a sequential\nrecommendation algorithm. The recommendation algorithm is conceptualized as a\nstochastic sampler that, in each round, queries the annotators a subset of\nsamples for their true labels and obtains the feedback information on whether\nthe samples are misclassified. The sampling mechanism needs to balance between\ndiscovering new patterns of misclassification (exploration) and confirming the\npotential patterns of classification (exploitation). We construct a\ndeterminantal point process, whose intensity balances the\nexploration-exploitation trade-off through the weighted update of the posterior\nat each round to form the generator of the stochastic sampler. The numerical\nresults empirically demonstrate the competitive performance of our framework on\nmultiple datasets at various signal-to-noise ratios."
            ],
            "interesting paper": 2
        },
        {
            "papers": [
                "Weight Bank Addition Photonic Accelerator for Artificial Intelligence\nNeural networks powered by artificial intelligence play a pivotal role in\ncurrent estimation and classification applications due to the escalating\ncomputational demands of evolving deep learning systems. The hindrances posed\nby existing computational limitations threaten to impede the further\nprogression of these neural networks. In response to these issues, we propose\nneuromorphic networks founded on photonics that offer superior processing speed\nthan electronic counterparts, thereby enhancing support for real time, three\ndimensional, and virtual reality applications. The weight bank, an integral\ncomponent of these networks has a direct bearing on their overall performance.\nOur study demonstrates the implementation of a weight bank utilizing parallelly\ncascaded micro ring resonators. We present our observations on neuromorphic\nnetworks based on silicon on insulators, where cascaded MRRs play a crucial\nrole in mitigating interchannel and intrachannel cross talk, a persistent issue\nin wavelength division multiplexing systems. Additionally, we design a standard\nsilicon photonic accelerator to perform weight addition. Optimized to offer\nincreased speed and reduced energy consumption, this photonic accelerator\nensures comparable processing power to electronic devices.",
                "Multichromatic Floquet engineering of quantum dissipation\nThe monochromatic driving of a quantum system is a successful technique in\nquantum simulations, well captured by an effective Hamiltonian approach, and\nwith applications in artificial gauge fields and topological engineering. In\nthis letter, we investigate the modeling of multichromatic Floquet driving for\nthe slow degrees of freedom. Within a well-defined range of parameters, we show\nthat the time coarse-grained dynamics of such a driven closed quantum system is\nencapsulated in an effective Master equation for the time-averaged density\nmatrix, that evolves under the action of an effective Hamiltonian and tunable\nLindblad-type dissipation/quantum gain terms. As an application, we emulate the\ndissipation induced by phase noise and incoherent emission/absorption processes\nin the bichromatic driving of a two-level system.",
                "Accelerating science with human-aware artificial intelligence\nArtificial intelligence (AI) models trained on published scientific findings\nhave been used to invent valuable materials and targeted therapies, but they\ntypically ignore the human scientists who continually alter the landscape of\ndiscovery. Here we show that incorporating the distribution of human expertise\nby training unsupervised models on simulated inferences cognitively accessible\nto experts dramatically improves (up to 400%) AI prediction of future\ndiscoveries beyond those focused on research content alone, especially when\nrelevant literature is sparse. These models succeed by predicting human\npredictions and the scientists who will make them. By tuning human-aware AI to\navoid the crowd, we can generate scientifically promising \"alien\" hypotheses\nunlikely to be imagined or pursued without intervention until the distant\nfuture, which hold promise to punctuate scientific advance beyond questions\ncurrently pursued. Accelerating human discovery or probing its blind spots,\nhuman-aware AI enables us to move toward and beyond the contemporary scientific\nfrontier.",
                "Training Terahertz Wireless Systems to Battle I/Q Imbalance\nDue to the non-ideality of analog components, transceivers experience high\nlevels of hardware imperfections, like in-phase and quadrature imbalance (IQI),\nwhich manifests itself as the mismatches of amplitude and phase between the I\nand Q branches. Unless proper mitigated, IQI has an important and negative\nimpact on the reliability and efficiency of high-frequency and high-data-rate\nsystems, such as terahertz wireless networks. Recognizing this, the current\npaper presents an intelligent transmitter (TX) and an intelligent receiver (RX)\narchitecture that by employing machine learning (ML) methodologies is capable\nto fully-mitigate the impact of IQI without performing IQI coefficients\nestimation. They key idea lies on co-training the TX mapper's and RX demapper\nin order to respectively design a constellation and detection scheme that takes\naccounts for IQI. Two training approaches are implemented, namely: i)\nconventional that requires a considerable amount of data for training, and ii)\na reinforcement learning based one, which demands a shorter dataset in\ncomparison to the former. The feasibility and efficiency of the proposed\narchitecture and training approaches are validated through respective Monte\nCarlo simulations.",
                "Physics-data-driven intelligent optimization for large-scale\n  meta-devices\nMeta-devices have gained significant attention and have been widely utilized\nin optical systems for focusing and imaging, owing to their lightweight,\nhigh-integration, and exceptional-flexibility capabilities. However, based on\nthe assumption of local phase approximation, traditional design method neglect\nthe local lattice coupling effect between adjacent meta-atoms, thus harming the\npractical performance of meta-devices. Using physics-driven or data-driven\noptimization algorithms can effectively solve the aforementioned problems.\nNevertheless, both of the methods either involve considerable time costs or\nrequire a substantial amount of data sets. Here, we propose a\nphysics-data-driven approach based \"intelligent optimizer\" that enables us to\nadaptively modify the sizes of the studied meta-atom according to the sizes of\nits surrounding ones. Such a scheme allows to mitigate the undesired local\nlattice coupling effect, and the proposed network model works well on thousands\nof datasets with a validation loss of 3*10-3. Experimental results show that\nthe 1-mm-diameter metalens designed with the \"intelligent optimizer\" possesses\na relative focusing efficiency of 93.4% (as compared to ideal focusing) and a\nStrehl ratio of 0.94. In contrast to the previous inverse design method, our\nmethod significantly boosts designing efficiency with five orders of magnitude\nreduction in time. Our design approach may sets a new paradigm for devising\nlarge-scale meta-devices.",
                "Enough With \"Human-AI Collaboration\"\nDescribing our interaction with Artificial Intelligence (AI) systems as\n'collaboration' is well-intentioned, but flawed. Not only is it misleading, but\nit also takes away the credit of AI 'labour' from the humans behind it, and\nerases and obscures an often exploitative arrangement between AI producers and\nconsumers. The AI 'collaboration' metaphor is merely the latest episode in a\nlong history of labour appropriation and credit reassignment that\ndisenfranchises labourers in the Global South. I propose that viewing AI as a\ntool or an instrument, rather than a collaborator, is more accurate, and\nultimately fairer."
            ],
            "interesting paper": 3
        },
        {
            "papers": [
                "Accelerating science with human-aware artificial intelligence\nArtificial intelligence (AI) models trained on published scientific findings\nhave been used to invent valuable materials and targeted therapies, but they\ntypically ignore the human scientists who continually alter the landscape of\ndiscovery. Here we show that incorporating the distribution of human expertise\nby training unsupervised models on simulated inferences cognitively accessible\nto experts dramatically improves (up to 400%) AI prediction of future\ndiscoveries beyond those focused on research content alone, especially when\nrelevant literature is sparse. These models succeed by predicting human\npredictions and the scientists who will make them. By tuning human-aware AI to\navoid the crowd, we can generate scientifically promising \"alien\" hypotheses\nunlikely to be imagined or pursued without intervention until the distant\nfuture, which hold promise to punctuate scientific advance beyond questions\ncurrently pursued. Accelerating human discovery or probing its blind spots,\nhuman-aware AI enables us to move toward and beyond the contemporary scientific\nfrontier.",
                "Balancing Exploration and Exploitation: Disentangled $\u03b2$-CVAE in De\n  Novo Drug Design\nDeep generative models have recently emerged as a promising de novo drug\ndesign method. In this respect, deep generative conditional variational\nautoencoder (CVAE) models are a powerful approach for generating novel\nmolecules with desired drug-like properties. However, molecular graph-based\nmodels with disentanglement and multivariate explicit latent conditioning have\nnot been fully elucidated. To address this, we proposed a molecular-graph\n$\\beta$-CVAE model for de novo drug design. Here, we empirically tuned the\nvalue of disentanglement and assessed its ability to generate molecules with\noptimised univariate- or-multivariate properties. In particular, we optimised\nthe octanol-water partition coefficient (ClogP), molar refractivity (CMR),\nquantitative estimate of drug-likeness (QED), and synthetic accessibility score\n(SAS). Results suggest that a lower $\\beta$ value increases the uniqueness of\ngenerated molecules (exploration). Univariate optimisation results showed our\nmodel generated molecular property averages of ClogP = 41.07% $\\pm$ 0.01% and\nCMR 66.76% $\\pm$ 0.01% by the Ghose filter. Multivariate property optimisation\nresults showed that our model generated an average of 30.07% $\\pm$ 0.01%\nmolecules for both desired properties. Furthermore, our model improved the QED\nand SAS (exploitation) of molecules generated. Together, these results suggest\nthat the $\\beta$-CVAE could balance exploration and exploitation through\ndisentanglement and is a promising model for de novo drug design, thus\nproviding a basis for future studies.",
                "Bi-level Contrastive Learning for Knowledge-Enhanced Molecule\n  Representations\nMolecule representation learning is crucial for various downstream\napplications, such as understanding and predicting molecular properties and\nside effects. In this paper, we propose a novel method called GODE, which takes\ninto account the two-level structure of individual molecules. We recognize that\nmolecules have an intrinsic graph structure as well as being a node in a larger\nmolecule knowledge graph. GODE integrates graph representations of individual\nmolecules with multidomain biochemical data from knowledge graphs. By\npre-training two graph neural networks (GNNs) on different graph structures,\ncombined with contrastive learning, GODE fuses molecular structures with their\ncorresponding knowledge graph substructures. This fusion results in a more\nrobust and informative representation, which enhances molecular property\nprediction by harnessing both chemical and biological information. When\nfine-tuned across 11 chemical property tasks, our model outperforms existing\nbenchmarks, registering an average ROC-AUC uplift of 13.8% for classification\ntasks and an average RMSE/MAE enhancement of 35.1% for regression tasks.\nImpressively, it surpasses the current leading model in molecule property\npredictions with average advancements of 2.1% in classification and 6.4% in\nregression tasks.",
                "XAI Renaissance: Redefining Interpretability in Medical Diagnostic\n  Models\nAs machine learning models become increasingly prevalent in medical\ndiagnostics, the need for interpretability and transparency becomes paramount.\nThe XAI Renaissance signifies a significant shift in the field, aiming to\nredefine the interpretability of medical diagnostic models. This paper explores\nthe innovative approaches and methodologies within the realm of Explainable AI\n(XAI) that are revolutionizing the interpretability of medical diagnostic\nmodels. By shedding light on the underlying decision-making process, XAI\ntechniques empower healthcare professionals to understand, trust, and\neffectively utilize these models for accurate and reliable medical diagnoses.\nThis review highlights the key advancements in XAI for medical diagnostics and\ntheir potential to transform the healthcare landscape, ultimately improving\npatient outcomes and fostering trust in AI-driven diagnostic systems.",
                "Deep Classifier Mimicry without Data Access\nAccess to pre-trained models has recently emerged as a standard across\nnumerous machine learning domains. Unfortunately, access to the original data\nthe models were trained on may not equally be granted. This makes it\ntremendously challenging to fine-tune, compress models, adapt continually, or\nto do any other type of data-driven update. We posit that original data access\nmay however not be required. Specifically, we propose Contrastive Abductive\nKnowledge Extraction (CAKE), a model-agnostic knowledge distillation procedure\nthat mimics deep classifiers without access to the original data. To this end,\nCAKE generates pairs of noisy synthetic samples and diffuses them contrastively\ntoward a model's decision boundary. We empirically corroborate CAKE's\neffectiveness using several benchmark datasets and various architectural\nchoices, paving the way for broad application.",
                "A Survey on Explainability of Graph Neural Networks\nGraph neural networks (GNNs) are powerful graph-based deep-learning models\nthat have gained significant attention and demonstrated remarkable performance\nin various domains, including natural language processing, drug discovery, and\nrecommendation systems. However, combining feature information and\ncombinatorial graph structures has led to complex non-linear GNN models.\nConsequently, this has increased the challenges of understanding the workings\nof GNNs and the underlying reasons behind their predictions. To address this,\nnumerous explainability methods have been proposed to shed light on the inner\nmechanism of the GNNs. Explainable GNNs improve their security and enhance\ntrust in their recommendations. This survey aims to provide a comprehensive\noverview of the existing explainability techniques for GNNs. We create a novel\ntaxonomy and hierarchy to categorize these methods based on their objective and\nmethodology. We also discuss the strengths, limitations, and application\nscenarios of each category. Furthermore, we highlight the key evaluation\nmetrics and datasets commonly used to assess the explainability of GNNs. This\nsurvey aims to assist researchers and practitioners in understanding the\nexisting landscape of explainability methods, identifying gaps, and fostering\nfurther advancements in interpretable graph-based machine learning."
            ],
            "interesting paper": 3
        }
    ],
    "Charlie Lopez": [
        {
            "papers": [
                "Classic machine learning methods\nIn this chapter, we present the main classic machine learning methods. A\nlarge part of the chapter is devoted to supervised learning techniques for\nclassification and regression, including nearest-neighbor methods, linear and\nlogistic regressions, support vector machines and tree-based algorithms. We\nalso describe the problem of overfitting as well as strategies to overcome it.\nWe finally provide a brief overview of unsupervised learning methods, namely\nfor clustering and dimensionality reduction.",
                "Trends and Challenges Towards an Effective Data-Driven Decision Making\n  in UK SMEs: Case Studies and Lessons Learnt from the Analysis of 85 SMEs\nThe adoption of data science brings vast benefits to Small and Medium-sized\nEnterprises (SMEs) including business productivity, economic growth, innovation\nand jobs creation. Data Science can support SMEs to optimise production\nprocesses, anticipate customers' needs, predict machinery failures and deliver\nefficient smart services. Businesses can also harness the power of Artificial\nIntelligence (AI) and Big Data and the smart use of digital technologies to\nenhance productivity and performance, paving the way for innovation. However,\nintegrating data science decisions into an SME requires both skills and IT\ninvestments. In most cases, such expenses are beyond the means of SMEs due to\nlimited resources and restricted access to financing. This paper presents\ntrends and challenges towards an effective data-driven decision making for\norganisations based on a case study of 85 SMEs, mostly from the West Midlands\nregion of England. The work is supported as part of a 3 years ERDF (European\nRegional Development Funded project) in the areas of big data management,\nanalytics and business intelligence. We present two case studies that\ndemonstrates the potential of Digitisation, AI and Machine Learning and use\nthese as examples to unveil challenges and showcase the wealth of current\navailable opportunities for SMEs.",
                "How to escape sharp minima with random perturbations\nModern machine learning applications have witnessed the remarkable success of\noptimization algorithms that are designed to find flat minima. Motivated by\nthis design choice, we undertake a formal study that (i) formulates the notion\nof flat minima, and (ii) studies the complexity of finding them. Specifically,\nwe adopt the trace of the Hessian of the cost function as a measure of\nflatness, and use it to formally define the notion of approximate flat minima.\nUnder this notion, we then analyze algorithms that find approximate flat minima\nefficiently. For general cost functions, we discuss a gradient-based algorithm\nthat finds an approximate flat local minimum efficiently. The main component of\nthe algorithm is to use gradients computed from randomly perturbed iterates to\nestimate a direction that leads to flatter minima. For the setting where the\ncost function is an empirical risk over training data, we present a faster\nalgorithm that is inspired by a recently proposed practical algorithm called\nsharpness-aware minimization, supporting its success in practice.",
                "Sharpness-Aware Minimization Revisited: Weighted Sharpness as a\n  Regularization Term\nDeep Neural Networks (DNNs) generalization is known to be closely related to\nthe flatness of minima, leading to the development of Sharpness-Aware\nMinimization (SAM) for seeking flatter minima and better generalization. In\nthis paper, we revisit the loss of SAM and propose a more general method,\ncalled WSAM, by incorporating sharpness as a regularization term. We prove its\ngeneralization bound through the combination of PAC and Bayes-PAC techniques,\nand evaluate its performance on various public datasets. The results\ndemonstrate that WSAM achieves improved generalization, or is at least highly\ncompetitive, compared to the vanilla optimizer, SAM and its variants. The code\nis available at\nhttps://github.com/intelligent-machine-learning/dlrover/tree/master/atorch/atorch/optimizers.",
                "Learning DAGs from Data with Few Root Causes\nWe present a novel perspective and algorithm for learning directed acyclic\ngraphs (DAGs) from data generated by a linear structural equation model (SEM).\nFirst, we show that a linear SEM can be viewed as a linear transform that, in\nprior work, computes the data from a dense input vector of random valued root\ncauses (as we will call them) associated with the nodes. Instead, we consider\nthe case of (approximately) few root causes and also introduce noise in the\nmeasurement of the data. Intuitively, this means that the DAG data is produced\nby few data-generating events whose effect percolates through the DAG. We prove\nidentifiability in this new setting and show that the true DAG is the global\nminimizer of the $L^0$-norm of the vector of root causes. For data with few\nroot causes, with and without noise, we show superior performance compared to\nprior DAG learning methods.",
                "Reliability Scores from Saliency Map Clusters for Improved Image-based\n  Harvest-Readiness Prediction in Cauliflower\nCauliflower is a hand-harvested crop that must fulfill high-quality standards\nin sales making the timing of harvest important. However, accurately\ndetermining harvest-readiness can be challenging due to the cauliflower head\nbeing covered by its canopy. While deep learning enables automated\nharvest-readiness estimation, errors can occur due to field-variability and\nlimited training data. In this paper, we analyze the reliability of a\nharvest-readiness classifier with interpretable machine learning. By\nidentifying clusters of saliency maps, we derive reliability scores for each\nclassification result using knowledge about the domain and the image\nproperties. For unseen data, the reliability can be used to (i) inform farmers\nto improve their decision-making and (ii) increase the model prediction\naccuracy. Using RGB images of single cauliflower plants at different\ndevelopmental stages from the GrowliFlower dataset, we investigate various\nsaliency mapping approaches and find that they result in different quality of\nreliability scores. With the most suitable interpretation tool, we adjust the\nclassification result and achieve a 15.72% improvement of the overall accuracy\nto 88.14% and a 15.44% improvement of the average class accuracy to 88.52% for\nthe GrowliFlower dataset."
            ],
            "interesting paper": 3
        },
        {
            "papers": [
                "How many samples are needed to leverage smoothness?\nA core principle in statistical learning is that smoothness of target\nfunctions allows to break the curse of dimensionality. However, learning a\nsmooth function seems to require enough samples close to one another to get\nmeaningful estimate of high-order derivatives, which would be hard in machine\nlearning problems where the ratio between number of data and input dimension is\nrelatively small. By deriving new lower bounds on the generalization error,\nthis paper formalizes such an intuition, before investigating the role of\nconstants and transitory regimes which are usually not depicted beyond\nclassical learning theory statements while they play a dominant role in\npractice.",
                "Leaving the Nest: Going Beyond Local Loss Functions for\n  Predict-Then-Optimize\nPredict-then-Optimize is a framework for using machine learning to perform\ndecision-making under uncertainty. The central research question it asks is,\n\"How can the structure of a decision-making task be used to tailor ML models\nfor that specific task?\" To this end, recent work has proposed learning\ntask-specific loss functions that capture this underlying structure. However,\ncurrent approaches make restrictive assumptions about the form of these losses\nand their impact on ML model behavior. These assumptions both lead to\napproaches with high computational cost, and when they are violated in\npractice, poor performance. In this paper, we propose solutions to these\nissues, avoiding the aforementioned assumptions and utilizing the ML model's\nfeatures to increase the sample efficiency of learning loss functions. We\nempirically show that our method achieves state-of-the-art results in four\ndomains from the literature, often requiring an order of magnitude fewer\nsamples than comparable methods from past work. Moreover, our approach\noutperforms the best existing method by nearly 200% when the localness\nassumption is broken.",
                "Distinguishing Human Generated Text From ChatGPT Generated Text Using\n  Machine Learning\nChatGPT is a conversational artificial intelligence that is a member of the\ngenerative pre-trained transformer of the large language model family. This\ntext generative model was fine-tuned by both supervised learning and\nreinforcement learning so that it can produce text documents that seem to be\nwritten by natural intelligence. Although there are numerous advantages of this\ngenerative model, it comes with some reasonable concerns as well. This paper\npresents a machine learning-based solution that can identify the ChatGPT\ndelivered text from the human written text along with the comparative analysis\nof a total of 11 machine learning and deep learning algorithms in the\nclassification process. We have tested the proposed model on a Kaggle dataset\nconsisting of 10,000 texts out of which 5,204 texts were written by humans and\ncollected from news and social media. On the corpus generated by GPT-3.5, the\nproposed algorithm presents an accuracy of 77%.",
                "Differentiable Random Partition Models\nPartitioning a set of elements into an unknown number of mutually exclusive\nsubsets is essential in many machine learning problems. However, assigning\nelements, such as samples in a dataset or neurons in a network layer, to an\nunknown and discrete number of subsets is inherently non-differentiable,\nprohibiting end-to-end gradient-based optimization of parameters. We overcome\nthis limitation by proposing a novel two-step method for inferring partitions,\nwhich allows its usage in variational inference tasks. This new approach\nenables reparameterized gradients with respect to the parameters of the new\nrandom partition model. Our method works by inferring the number of elements\nper subset and, second, by filling these subsets in a learned order. We\nhighlight the versatility of our general-purpose approach on three different\nchallenging experiments: variational clustering, inference of shared and\nindependent generative factors under weak supervision, and multitask learning.",
                "mldr.resampling: Efficient Reference Implementations of Multilabel\n  Resampling Algorithms\nResampling algorithms are a useful approach to deal with imbalanced learning\nin multilabel scenarios. These methods have to deal with singularities in the\nmultilabel data, such as the occurrence of frequent and infrequent labels in\nthe same instance. Implementations of these methods are sometimes limited to\nthe pseudocode provided by their authors in a paper. This Original Software\nPublication presents mldr.resampling, a software package that provides\nreference implementations for eleven multilabel resampling methods, with an\nemphasis on efficiency since these algorithms are usually time-consuming.",
                "PoCaPNet: A Novel Approach for Surgical Phase Recognition Using Speech\n  and X-Ray Images\nSurgical phase recognition is a challenging and necessary task for the\ndevelopment of context-aware intelligent systems that can support medical\npersonnel for better patient care and effective operating room management. In\nthis paper, we present a surgical phase recognition framework that employs a\nMulti-Stage Temporal Convolution Network using speech and X-Ray images for the\nfirst time. We evaluate our proposed approach using our dataset that comprises\n31 port-catheter placement operations and report 82.56 \\% frame-wise accuracy\nwith eight surgical phases. Additionally, we investigate the design choices in\nthe temporal model and solutions for the class-imbalance problem. Our\nexperiments demonstrate that speech and X-Ray data can be effectively utilized\nfor surgical phase recognition, providing a foundation for the development of\nspeech assistants in operating rooms of the future."
            ],
            "interesting paper": 2
        },
        {
            "papers": [
                "An Improved Model Ensembled of Different Hyper-parameter Tuned Machine\n  Learning Algorithms for Fetal Health Prediction\nFetal health is a critical concern during pregnancy as it can impact the\nwell-being of both the mother and the baby. Regular monitoring and timely\ninterventions are necessary to ensure the best possible outcomes. While there\nare various methods to monitor fetal health in the mother's womb, the use of\nartificial intelligence (AI) can improve the accuracy, efficiency, and speed of\ndiagnosis. In this study, we propose a robust ensemble model called ensemble of\ntuned Support Vector Machine and ExtraTrees (ETSE) for predicting fetal health.\nInitially, we employed various data preprocessing techniques such as outlier\nrejection, missing value imputation, data standardization, and data sampling.\nThen, seven machine learning (ML) classifiers including Support Vector Machine\n(SVM), XGBoost (XGB), Light Gradient Boosting Machine (LGBM), Decision Tree\n(DT), Random Forest (RF), ExtraTrees (ET), and K-Neighbors were implemented.\nThese models were evaluated and then optimized by hyperparameter tuning using\nthe grid search technique. Finally, we analyzed the performance of our proposed\nETSE model. The performance analysis of each model revealed that our proposed\nETSE model outperformed the other models with 100% precision, 100% recall, 100%\nF1-score, and 99.66% accuracy. This indicates that the ETSE model can\neffectively predict fetal health, which can aid in timely interventions and\nimprove outcomes for both the mother and the baby.",
                "Statistically Significant Concept-based Explanation of Image Classifiers\n  via Model Knockoffs\nA concept-based classifier can explain the decision process of a deep\nlearning model by human-understandable concepts in image classification\nproblems. However, sometimes concept-based explanations may cause false\npositives, which misregards unrelated concepts as important for the prediction\ntask. Our goal is to find the statistically significant concept for\nclassification to prevent misinterpretation. In this study, we propose a method\nusing a deep learning model to learn the image concept and then using the\nKnockoff samples to select the important concepts for prediction by controlling\nthe False Discovery Rate (FDR) under a certain value. We evaluate the proposed\nmethod in our synthetic and real data experiments. Also, it shows that our\nmethod can control the FDR properly while selecting highly interpretable\nconcepts to improve the trustworthiness of the model.",
                "A Framework For Refining Text Classification and Object Recognition from\n  Academic Articles\nWith the widespread use of the internet, it has become increasingly crucial\nto extract specific information from vast amounts of academic articles\nefficiently. Data mining techniques are generally employed to solve this issue.\nHowever, data mining for academic articles is challenging since it requires\nautomatically extracting specific patterns in complex and unstructured layout\ndocuments. Current data mining methods for academic articles employ\nrule-based(RB) or machine learning(ML) approaches. However, using rule-based\nmethods incurs a high coding cost for complex typesetting articles. On the\nother hand, simply using machine learning methods requires annotation work for\ncomplex content types within the paper, which can be costly. Furthermore, only\nusing machine learning can lead to cases where patterns easily recognized by\nrule-based methods are mistakenly extracted. To overcome these issues, from the\nperspective of analyzing the standard layout and typesetting used in the\nspecified publication, we emphasize implementing specific methods for specific\ncharacteristics in academic articles. We have developed a novel Text Block\nRefinement Framework (TBRF), a machine learning and rule-based scheme hybrid.\nWe used the well-known ACL proceeding articles as experimental data for the\nvalidation experiment. The experiment shows that our approach achieved over 95%\nclassification accuracy and 90% detection accuracy for tables and figures.",
                "Instance-based Max-margin for Practical Few-shot Recognition\nIn order to mimic the human few-shot learning (FSL) ability better and to\nmake FSL closer to real-world applications, this paper proposes a practical FSL\n(pFSL) setting. pFSL is based on unsupervised pretrained models (analogous to\nhuman prior knowledge) and recognizes many novel classes simultaneously.\nCompared to traditional FSL, pFSL is simpler in its formulation, easier to\nevaluate, more challenging and more practical. To cope with the rarity of\ntraining examples, this paper proposes IbM2, an instance-based max-margin\nmethod not only for the new pFSL setting, but also works well in traditional\nFSL scenarios. Based on the Gaussian Annulus Theorem, IbM2 converts random\nnoise applied to the instances into a mechanism to achieve maximum margin in\nthe many-way pFSL (or traditional FSL) recognition task. Experiments with\nvarious self-supervised pretraining methods and diverse many- or few-way FSL\ntasks show that IbM2 almost always leads to improvements compared to its\nrespective baseline methods, and in most cases the improvements are\nsignificant. With both the new pFSL setting and novel IbM2 method, this paper\nshows that practical few-shot learning is both viable and promising.",
                "Knowledge Extraction with Interval Temporal Logic Decision Trees\nMultivariate temporal, or time, series classification is, in a way, the\ntemporal generalization of (numeric) classification, as every instance is\ndescribed by multiple time series instead of multiple values. Symbolic\nclassification is the machine learning strategy to extract explicit knowledge\nfrom a data set, and the problem of symbolic classification of multivariate\ntemporal series requires the design, implementation, and test of ad-hoc machine\nlearning algorithms, such as, for example, algorithms for the extraction of\ntemporal versions of decision trees. One of the most well-known algorithms for\ndecision tree extraction from categorical data is Quinlan's ID3, which was\nlater extended to deal with numerical attributes, resulting in an algorithm\nknown as C4.5, and implemented in many open-sources data mining libraries,\nincluding the so-called Weka, which features an implementation of C4.5 called\nJ48. ID3 was recently generalized to deal with temporal data in form of\ntimelines, which can be seen as discrete (categorical) versions of multivariate\ntime series, and such a generalization, based on the interval temporal logic\nHS, is known as Temporal ID3. In this paper we introduce Temporal C4.5, that\nallows the extraction of temporal decision trees from undiscretized\nmultivariate time series, describe its implementation, called Temporal J48, and\ndiscuss the outcome of a set of experiments with the latter on a collection of\npublic data sets, comparing the results with those obtained by other,\nclassical, multivariate time series classification methods.",
                "A Framework for Incentivized Collaborative Learning\nCollaborations among various entities, such as companies, research labs, AI\nagents, and edge devices, have become increasingly crucial for achieving\nmachine learning tasks that cannot be accomplished by a single entity alone.\nThis is likely due to factors such as security constraints, privacy concerns,\nand limitations in computation resources. As a result, collaborative learning\n(CL) research has been gaining momentum. However, a significant challenge in\npractical applications of CL is how to effectively incentivize multiple\nentities to collaborate before any collaboration occurs. In this study, we\npropose ICL, a general framework for incentivized collaborative learning, and\nprovide insights into the critical issue of when and why incentives can improve\ncollaboration performance. Furthermore, we show the broad applicability of ICL\nto specific cases in federated learning, assisted learning, and multi-armed\nbandit with both theory and experimental results."
            ],
            "interesting paper": 4
        },
        {
            "papers": [
                "Data Minimization at Inference Time\nIn domains with high stakes such as law, recruitment, and healthcare,\nlearning models frequently rely on sensitive user data for inference,\nnecessitating the complete set of features. This not only poses significant\nprivacy risks for individuals but also demands substantial human effort from\norganizations to verify information accuracy. This paper asks whether it is\nnecessary to use \\emph{all} input features for accurate predictions at\ninference time. The paper demonstrates that, in a personalized setting,\nindividuals may only need to disclose a small subset of their features without\ncompromising decision-making accuracy. The paper also provides an efficient\nsequential algorithm to determine the appropriate attributes for each\nindividual to provide. Evaluations across various learning tasks show that\nindividuals can potentially report as little as 10\\% of their information while\nmaintaining the same accuracy level as a model that employs the full set of\nuser information.",
                "Distill Gold from Massive Ores: Bi-level Data Pruning towards Efficient\n  Dataset Distillation\nData-efficient learning has garnered significant attention, especially given\nthe current trend of large multi-modal models. Recently, dataset distillation\nhas become an effective approach by synthesizing data samples that are\nessential for network training. However, it remains to be explored which\nsamples are essential for the dataset distillation process itself. In this\nwork, we study the data efficiency and selection for the dataset distillation\ntask. By re-formulating the dynamics of distillation, we provide insight into\nthe inherent redundancy in the real dataset, both theoretically and\nempirically. We propose to use the empirical loss value as a static data\npruning criterion. To further compensate for the variation of the data value in\ntraining, we find the most contributing samples based on their causal effects\non the distillation. The proposed selection strategy can efficiently exploit\nthe training dataset, outperform the previous SOTA distillation algorithms, and\nconsistently enhance the distillation algorithms, even on much larger-scale and\nmore heterogeneous datasets, e.g., full ImageNet-1K and Kinetics-400. We\nbelieve this paradigm will open up new avenues in the dynamics of distillation\nand pave the way for efficient dataset distillation. Our code is available on\nhttps://github.com/silicx/GoldFromOres-BiLP.",
                "Optimization's Neglected Normative Commitments\nOptimization is offered as an objective approach to resolving complex,\nreal-world decisions involving uncertainty and conflicting interests. It drives\nbusiness strategies as well as public policies and, increasingly, lies at the\nheart of sophisticated machine learning systems. A paradigm used to approach\npotentially high-stakes decisions, optimization relies on abstracting the real\nworld to a set of decision(s), objective(s) and constraint(s). Drawing from the\nmodeling process and a range of actual cases, this paper describes the\nnormative choices and assumptions that are necessarily part of using\noptimization. It then identifies six emergent problems that may be neglected:\n1) Misspecified values can yield optimizations that omit certain imperatives\naltogether or incorporate them incorrectly as a constraint or as part of the\nobjective, 2) Problematic decision boundaries can lead to faulty modularity\nassumptions and feedback loops, 3) Failing to account for multiple agents'\ndivergent goals and decisions can lead to policies that serve only certain\nnarrow interests, 4) Mislabeling and mismeasurement can introduce bias and\nimprecision, 5) Faulty use of relaxation and approximation methods,\nunaccompanied by formal characterizations and guarantees, can severely impede\napplicability, and 6) Treating optimization as a justification for action,\nwithout specifying the necessary contextual information, can lead to ethically\ndubious or faulty decisions. Suggestions are given to further understand and\ncurb the harms that can arise when optimization is used wrongfully.",
                "A Comprehensive Overview and Comparative Analysis on Deep Learning\n  Models: CNN, RNN, LSTM, GRU\nDeep learning (DL) has emerged as a powerful subset of machine learning (ML)\nand artificial intelligence (AI), outperforming traditional ML methods,\nespecially in handling unstructured and large datasets. Its impact spans across\nvarious domains, including speech recognition, healthcare, autonomous vehicles,\ncybersecurity, predictive analytics, and more. However, the complexity and\ndynamic nature of real-world problems present challenges in designing effective\ndeep learning models. Consequently, several deep learning models have been\ndeveloped to address different problems and applications. In this article, we\nconduct a comprehensive survey of various deep learning models, including\nConvolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs),\nGenerative Models, Deep Reinforcement Learning (DRL), and Deep Transfer\nLearning. We examine the structure, applications, benefits, and limitations of\neach model. Furthermore, we perform an analysis using three publicly available\ndatasets: IMDB, ARAS, and Fruit-360. We compare the performance of six renowned\ndeep learning models: CNN, Simple RNN, Long Short-Term Memory (LSTM),\nBidirectional LSTM, Gated Recurrent Unit (GRU), and Bidirectional GRU.",
                "MLOps: A Step Forward to Enterprise Machine Learning\nMachine Learning Operations (MLOps) is becoming a highly crucial part of\nbusinesses looking to capitalize on the benefits of AI and ML models. This\nresearch presents a detailed review of MLOps, its benefits, difficulties,\nevolutions, and important underlying technologies such as MLOps frameworks,\nDocker, GitHub actions, and Kubernetes. The MLOps workflow, which includes\nmodel design, deployment, and operations, is explained in detail along with the\nvarious tools necessary for both model and data exploration and deployment.\nThis article also puts light on the end-to-end production of ML projects using\nvarious maturity levels of automated pipelines, with the least at no automation\nat all and the highest with complete CI/CD and CT capabilities. Furthermore, a\ndetailed example of an enterprise-level MLOps project for an object detection\nservice is used to explain the workflow of the technology in a real-world\nscenario. For this purpose, a web application hosting a pre-trained model from\nTensorFlow 2 Model Zoo is packaged and deployed to the internet making sure\nthat the system is scalable, reliable, and optimized for deployment at an\nenterprise level.",
                "Diagnosing Transformers: Illuminating Feature Spaces for Clinical\n  Decision-Making\nPre-trained transformers are often fine-tuned to aid clinical decision-making\nusing limited clinical notes. Model interpretability is crucial, especially in\nhigh-stakes domains like medicine, to establish trust and ensure safety, which\nrequires human engagement. We introduce SUFO, a systematic framework that\nenhances interpretability of fine-tuned transformer feature spaces. SUFO\nutilizes a range of analytic and visualization techniques, including Supervised\nprobing, Unsupervised similarity analysis, Feature dynamics, and Outlier\nanalysis to address key questions about model trust and interpretability. We\nconduct a case study investigating the impact of pre-training data where we\nfocus on real-world pathology classification tasks, and validate our findings\non MedNLI. We evaluate five 110M-sized pre-trained transformer models,\ncategorized into general-domain (BERT, TNLR), mixed-domain (BioBERT, Clinical\nBioBERT), and domain-specific (PubMedBERT) groups. Our SUFO analyses reveal\nthat: (1) while PubMedBERT, the domain-specific model, contains valuable\ninformation for fine-tuning, it can overfit to minority classes when class\nimbalances exist. In contrast, mixed-domain models exhibit greater resistance\nto overfitting, suggesting potential improvements in domain-specific model\nrobustness; (2) in-domain pre-training accelerates feature disambiguation\nduring fine-tuning; and (3) feature spaces undergo significant sparsification\nduring this process, enabling clinicians to identify common outlier modes among\nfine-tuned models as demonstrated in this paper. These findings showcase the\nutility of SUFO in enhancing trust and safety when using transformers in\nmedicine, and we believe SUFO can aid practitioners in evaluating fine-tuned\nlanguage models for other applications in medicine and in more critical\ndomains."
            ],
            "interesting paper": 2
        },
        {
            "papers": [
                "Full High-Dimensional Intelligible Learning In 2-D Lossless\n  Visualization Space\nThis study explores a new methodology for machine learning classification\ntasks in 2-dimensional visualization space (2-D ML) using Visual knowledge\nDiscovery in lossless General Line Coordinates. It is shown that this is a full\nmachine learning approach that does not require processing n-dimensional data\nin an abstract n-dimensional space. It enables discovering n-D patterns in 2-D\nspace without loss of n-D information using graph representations of n-D data\nin 2-D. Specifically, this study shows that it can be done with static and\ndynamic In-line Based Coordinates in different modifications, which are a\ncategory of General Line Coordinates. Based on these inline coordinates,\nclassification and regression methods were developed. The viability of the\nstrategy was shown by two case studies based on benchmark datasets (Wisconsin\nBreast Cancer and Page Block Classification datasets). The characteristics of\npage block classification data led to the development of an algorithm for\nimbalanced high-resolution data with multiple classes, which exploits the\ndecision trees as a model design facilitator producing a model, which is more\ngeneral than a decision tree. This work accelerates the ongoing consolidation\nof an emerging field of full 2-D machine learning and its methodology. Within\nthis methodology the end users can discover models and justify them as\nself-service. Providing interpretable ML models is another benefit of this\napproach.",
                "Deep Electron Cloud-activity and Field-activity Relationships\nChemists have been pursuing the general mathematical laws to explain and\npredict molecular properties for a long time. However, most of the traditional\nquantitative structure-activity relationship (QSAR) models have limited\napplication domains, e.g., they tend to have poor generalization performance\nwhen applied to molecules with parent structures different from those of the\ntrained molecules. This paper attempts to develop a new QSAR method that is\ntheoretically possible to predict various properties of molecules with diverse\nstructures. The proposed deep electron cloud-activity relationships (DECAR) and\ndeep field-activity relationships (DFAR) methods consist of three essentials:\n(1) A large number of molecule entities with activity data as training objects\nand responses; (2) three-dimensional electron cloud density (ECD) or related\nfield data by the accurate density functional theory methods as input\ndescriptors; (3) a deep learning model that is sufficiently flexible and\npowerful to learn the large data described above. DECAR and DFAR are used to\ndistinguish 977 sweet and 1965 non-sweet molecules (with 6-fold data\naugmentation) and the classification performance is demonstrated to be\nsignificantly better than the traditional least squares support vector machine\n(LS-SVM) models using traditional descriptors. DECAR and DFAR would provide a\npossible way to establish a widely applicable, cumulative, and shareable\nartificial intelligence-driven QSAR system. They are likely to promote the\ndevelopment of an interactive platform to collect and share the accurate ECD\nand field data of millions of molecules with annotated activities. With enough\ninput data, we envision the appearance of several deep networks trained for\nvarious molecular activities. Finally, we could anticipate a single DECAR or\nDFAR network to learn and infer various properties of interest for chemical\nmolecules.",
                "Employing Explainable Artificial Intelligence (XAI) Methodologies to\n  Analyze the Correlation between Input Variables and Tensile Strength in\n  Additively Manufactured Samples\nThis research paper explores the impact of various input parameters,\nincluding Infill percentage, Layer Height, Extrusion Temperature, and Print\nSpeed, on the resulting Tensile Strength in objects produced through additive\nmanufacturing. The main objective of this study is to enhance our understanding\nof the correlation between the input parameters and Tensile Strength, as well\nas to identify the key factors influencing the performance of the additive\nmanufacturing process. To achieve this objective, we introduced the utilization\nof Explainable Artificial Intelligence (XAI) techniques for the first time,\nwhich allowed us to analyze the data and gain valuable insights into the\nsystem's behavior. Specifically, we employed SHAP (SHapley Additive\nexPlanations), a widely adopted framework for interpreting machine learning\nmodel predictions, to provide explanations for the behavior of a machine\nlearning model trained on the data. Our findings reveal that the Infill\npercentage and Extrusion Temperature have the most significant influence on\nTensile Strength, while the impact of Layer Height and Print Speed is\nrelatively minor. Furthermore, we discovered that the relationship between the\ninput parameters and Tensile Strength is highly intricate and nonlinear, making\nit difficult to accurately describe using simple linear models.",
                "JutePestDetect: An Intelligent Approach for Jute Pest Identification\n  Using Fine-Tuned Transfer Learning\nIn certain Asian countries, Jute is one of the primary sources of income and\nGross Domestic Product (GDP) for the agricultural sector. Like many other\ncrops, Jute is prone to pest infestations, and its identification is typically\nmade visually in countries like Bangladesh, India, Myanmar, and China. In\naddition, this method is time-consuming, challenging, and somewhat imprecise,\nwhich poses a substantial financial risk. To address this issue, the study\nproposes a high-performing and resilient transfer learning (TL) based\nJutePestDetect model to identify jute pests at the early stage. Firstly, we\nprepared jute pest dataset containing 17 classes and around 380 photos per pest\nclass, which were evaluated after manual and automatic pre-processing and\ncleaning, such as background removal and resizing. Subsequently, five prominent\npre-trained models -DenseNet201, InceptionV3, MobileNetV2, VGG19, and ResNet50\nwere selected from a previous study to design the JutePestDetect model. Each\nmodel was revised by replacing the classification layer with a global average\npooling layer and incorporating a dropout layer for regularization. To evaluate\nthe models performance, various metrics such as precision, recall, F1 score,\nROC curve, and confusion matrix were employed. These analyses provided\nadditional insights for determining the efficacy of the models. Among them, the\ncustomized regularized DenseNet201-based proposed JutePestDetect model\noutperformed the others, achieving an impressive accuracy of 99%. As a result,\nour proposed method and strategy offer an enhanced approach to pest\nidentification in the case of Jute, which can significantly benefit farmers\nworldwide.",
                "Towards a Technology-Driven Adaptive Decision Support System for\n  Integrated Pavement and Maintenance strategies (TDADSS-IPM): focus on risk\n  assessment framework for climate change adaptation\nDecision Support Systems for pavement and maintenance strategies have\ntraditionally been designed as silos led to local optimum systems. Moreover,\nsince big data usage didn't exist as result of Industry 4.0 as of today, DSSs\nwere not initially designed adaptive to the sources of uncertainties led to\nrigid decisions. Motivated by the vulnerability of the road assets to the\nclimate phenomena, this paper takes a visionary step towards introducing a\nTechnology-Driven Adaptive Decision Support System for Integrated Pavement and\nMaintenance activities called TDADSS-IPM. As part of such DSS, a bottom-up risk\nassessment model is met via Bayesian Belief Networks (BBN) to realize the\nactual condition of the Danish roads due to weather condition. Such model fills\nthe gaps in the knowledge domain and develops a platform that can be trained\nover time, and applied in real-time to the actual event.",
                "Bayesian inference and neural estimation of acoustic wave propagation\nIn this work, we introduce a novel framework which combines physics and\nmachine learning methods to analyse acoustic signals. Three methods are\ndeveloped for this task: a Bayesian inference approach for inferring the\nspectral acoustics characteristics, a neural-physical model which equips a\nneural network with forward and backward physical losses, and the non-linear\nleast squares approach which serves as benchmark. The inferred propagation\ncoefficient leads to the room impulse response (RIR) quantity which can be used\nfor relocalisation with uncertainty. The simplicity and efficiency of this\nframework is empirically validated on simulated data."
            ],
            "interesting paper": 3
        },
        {
            "papers": [
                "Shapley Based Residual Decomposition for Instance Analysis\nIn this paper, we introduce the idea of decomposing the residuals of\nregression with respect to the data instances instead of features. This allows\nus to determine the effects of each individual instance on the model and each\nother, and in doing so makes for a model-agnostic method of identifying\ninstances of interest. In doing so, we can also determine the appropriateness\nof the model and data in the wider context of a given study. The paper focuses\non the possible applications that such a framework brings to the relatively\nunexplored field of instance analysis in the context of Explainable AI tasks.",
                "Parity Calibration\nIn a sequential regression setting, a decision-maker may be primarily\nconcerned with whether the future observation will increase or decrease\ncompared to the current one, rather than the actual value of the future\nobservation. In this context, we introduce the notion of parity calibration,\nwhich captures the goal of calibrated forecasting for the increase-decrease (or\n\"parity\") event in a timeseries. Parity probabilities can be extracted from a\nforecasted distribution for the output, but we show that such a strategy leads\nto theoretical unpredictability and poor practical performance. We then observe\nthat although the original task was regression, parity calibration can be\nexpressed as binary calibration. Drawing on this connection, we use an online\nbinary calibration method to achieve parity calibration. We demonstrate the\neffectiveness of our approach on real-world case studies in epidemiology,\nweather forecasting, and model-based control in nuclear fusion.",
                "Gaussian Process Probes (GPP) for Uncertainty-Aware Probing\nUnderstanding which concepts models can and cannot represent has been\nfundamental to many tasks: from effective and responsible use of models to\ndetecting out of distribution data. We introduce Gaussian process probes (GPP),\na unified and simple framework for probing and measuring uncertainty about\nconcepts represented by models. As a Bayesian extension of linear probing\nmethods, GPP asks what kind of distribution over classifiers (of concepts) is\ninduced by the model. This distribution can be used to measure both what the\nmodel represents and how confident the probe is about what the model\nrepresents. GPP can be applied to any pre-trained model with vector\nrepresentations of inputs (e.g., activations). It does not require access to\ntraining data, gradients, or the architecture. We validate GPP on datasets\ncontaining both synthetic and real images. Our experiments show it can (1)\nprobe a model's representations of concepts even with a very small number of\nexamples, (2) accurately measure both epistemic uncertainty (how confident the\nprobe is) and aleatory uncertainty (how fuzzy the concepts are to the model),\nand (3) detect out of distribution data using those uncertainty measures as\nwell as classic methods do. By using Gaussian processes to expand what probing\ncan offer, GPP provides a data-efficient, versatile and uncertainty-aware tool\nfor understanding and evaluating the capabilities of machine learning models.",
                "Prediction Error-based Classification for Class-Incremental Learning\nClass-incremental learning (CIL) is a particularly challenging variant of\ncontinual learning, where the goal is to learn to discriminate between all\nclasses presented in an incremental fashion. Existing approaches often suffer\nfrom excessive forgetting and imbalance of the scores assigned to classes that\nhave not been seen together during training. In this study, we introduce a\nnovel approach, Prediction Error-based Classification (PEC), which differs from\ntraditional discriminative and generative classification paradigms. PEC\ncomputes a class score by measuring the prediction error of a model trained to\nreplicate the outputs of a frozen random neural network on data from that\nclass. The method can be interpreted as approximating a classification rule\nbased on Gaussian Process posterior variance. PEC offers several practical\nadvantages, including sample efficiency, ease of tuning, and effectiveness even\nwhen data are presented one class at a time. Our empirical results show that\nPEC performs strongly in single-pass-through-data CIL, outperforming other\nrehearsal-free baselines in all cases and rehearsal-based methods with moderate\nreplay buffer size in most cases across multiple benchmarks.",
                "Beyond Confidence: Reliable Models Should Also Consider Atypicality\nWhile most machine learning models can provide confidence in their\npredictions, confidence is insufficient to understand a prediction's\nreliability. For instance, the model may have a low confidence prediction if\nthe input is not well-represented in the training dataset or if the input is\ninherently ambiguous. In this work, we investigate the relationship between how\natypical(rare) a sample or a class is and the reliability of a model's\npredictions. We first demonstrate that atypicality is strongly related to\nmiscalibration and accuracy. In particular, we empirically show that\npredictions for atypical inputs or atypical classes are more overconfident and\nhave lower accuracy. Using these insights, we show incorporating atypicality\nimproves uncertainty quantification and model performance for discriminative\nneural networks and large language models. In a case study, we show that using\natypicality improves the performance of a skin lesion classifier across\ndifferent skin tone groups without having access to the group attributes.\nOverall, we propose that models should use not only confidence but also\natypicality to improve uncertainty quantification and performance. Our results\ndemonstrate that simple post-hoc atypicality estimators can provide significant\nvalue.",
                "Identifying Spurious Biases Early in Training through the Lens of\n  Simplicity Bias\nNeural networks trained with (stochastic) gradient descent have an inductive\nbias towards learning simpler solutions. This makes them highly prone to\nlearning spurious correlations in the training data, that may not hold at test\ntime. In this work, we provide the first theoretical analysis of the effect of\nsimplicity bias on learning spurious correlations. Notably, we show that\nexamples with spurious features are provably separable based on the model's\noutput early in training. We further illustrate that if spurious features have\na small enough noise-to-signal ratio, the network's output on the majority of\nexamples is almost exclusively determined by the spurious features, leading to\npoor worst-group test accuracy. Finally, we propose SPARE, which identifies\nspurious correlations early in training and utilizes importance sampling to\nalleviate their effect. Empirically, we demonstrate that SPARE outperforms\nstate-of-the-art methods by up to 21.1% in worst-group accuracy, while being up\nto 12x faster. We also show that SPARE is a highly effective but lightweight\nmethod to discover spurious correlations."
            ],
            "interesting paper": 3
        },
        {
            "papers": [
                "Machine Learning Applications in Cascading Failure Analysis in Power\n  Systems: A Review\nCascading failures pose a significant threat to power grids and have garnered\nconsiderable research interest in the power system domain. The inherent\nuncertainty and severe impact associated with cascading failures have raised\nconcerns, prompting the development of various techniques to study these\ncomplex phenomena. In recent years, advancements in monitoring technologies and\nthe availability of large volumes of data from power systems, coupled with the\nemergence of intelligent algorithms, have made machine learning (ML) techniques\nincreasingly attractive for addressing cascading failure problems. This survey\nprovides a comprehensive overview of ML-based techniques for analyzing\ncascading failures in power systems. The survey categorizes these techniques\nbased on the evolutionary phases of the cascade process in power systems, as\nwell as studies focusing on cascade resiliency before the occurrence of\ncascades and problems related to cascades after their termination. By\norganizing and presenting these works into relevant categories, this survey\naims to offer insights and a systematic understanding the role of ML in\nmitigating cascading failures in power systems.",
                "Predicting Heart Disease and Reducing Survey Time Using Machine Learning\n  Algorithms\nCurrently, many researchers and analysts are working toward medical diagnosis\nenhancement for various diseases. Heart disease is one of the common diseases\nthat can be considered a significant cause of mortality worldwide. Early\ndetection of heart disease significantly helps in reducing the risk of heart\nfailure. Consequently, the Centers for Disease Control and Prevention (CDC)\nconducts a health-related telephone survey yearly from over 400,000\nparticipants. However, several concerns arise regarding the reliability of the\ndata in predicting heart disease and whether all of the survey questions are\nstrongly related. This study aims to utilize several machine learning\ntechniques, such as support vector machines and logistic regression, to\ninvestigate the accuracy of the CDC's heart disease survey in the United\nStates. Furthermore, we use various feature selection methods to identify the\nmost relevant subset of questions that can be utilized to forecast heart\nconditions. To reach a robust conclusion, we perform stability analysis by\nrandomly sampling the data 300 times. The experimental results show that the\nsurvey data can be useful up to 80% in terms of predicting heart disease, which\nsignificantly improves the diagnostic process before bloodwork and tests. In\naddition, the amount of time spent conducting the survey can be reduced by 77%\nwhile maintaining the same level of performance.",
                "Sensitivity Analysis of RF+clust for Leave-one-problem-out Performance\n  Prediction\nLeave-one-problem-out (LOPO) performance prediction requires machine learning\n(ML) models to extrapolate algorithms' performance from a set of training\nproblems to a previously unseen problem. LOPO is a very challenging task even\nfor state-of-the-art approaches. Models that work well in the easier\nleave-one-instance-out scenario often fail to generalize well to the LOPO\nsetting. To address the LOPO problem, recent work suggested enriching standard\nrandom forest (RF) performance regression models with a weighted average of\nalgorithms' performance on training problems that are considered similar to a\ntest problem. More precisely, in this RF+clust approach, the weights are chosen\nproportionally to the distances of the problems in some feature space. Here in\nthis work, we extend the RF+clust approach by adjusting the distance-based\nweights with the importance of the features for performance regression. That\nis, instead of considering cosine distance in the feature space, we consider a\nweighted distance measure, with weights depending on the relevance of the\nfeature for the regression model. Our empirical evaluation of the modified\nRF+clust approach on the CEC 2014 benchmark suite confirms its advantages over\nthe naive distance measure. However, we also observe room for improvement, in\nparticular with respect to more expressive feature portfolios.",
                "A rule-general abductive learning by rough sets\nIn real-world tasks, there is usually a large amount of unlabeled data and\nlabeled data. The task of combining the two to learn is known as\nsemi-supervised learning. Experts can use logical rules to label unlabeled\ndata, but this operation is costly. The combination of perception and reasoning\nhas a good effect in processing such semi-supervised tasks with domain\nknowledge. However, acquiring domain knowledge and the correction, reduction\nand generation of rules remain complex problems to be solved. Rough set theory\nis an important method for solving knowledge processing in information systems.\nIn this paper, we propose a rule general abductive learning by rough set\n(RS-ABL). By transforming the target concept and sub-concepts of rules into\ninformation tables, rough set theory is used to solve the acquisition of domain\nknowledge and the correction, reduction and generation of rules at a lower\ncost. This framework can also generate more extensive negative rules to enhance\nthe breadth of the knowledge base. Compared with the traditional\nsemi-supervised learning method, RS-ABL has higher accuracy in dealing with\nsemi-supervised tasks.",
                "FedCSD: A Federated Learning Based Approach for Code-Smell Detection\nThis paper proposes a Federated Learning Code Smell Detection (FedCSD)\napproach that allows organizations to collaboratively train federated ML models\nwhile preserving their data privacy. These assertions have been supported by\nthree experiments that have significantly leveraged three manually validated\ndatasets aimed at detecting and examining different code smell scenarios. In\nexperiment 1, which was concerned with a centralized training experiment,\ndataset two achieved the lowest accuracy (92.30%) with fewer smells, while\ndatasets one and three achieved the highest accuracy with a slight difference\n(98.90% and 99.5%, respectively). This was followed by experiment 2, which was\nconcerned with cross-evaluation, where each ML model was trained using one\ndataset, which was then evaluated over the other two datasets. Results from\nthis experiment show a significant drop in the model's accuracy (lowest\naccuracy: 63.80\\%) where fewer smells exist in the training dataset, which has\na noticeable reflection (technical debt) on the model's performance. Finally,\nthe last and third experiments evaluate our approach by splitting the dataset\ninto 10 companies. The ML model was trained on the company's site, then all\nmodel-updated weights were transferred to the server. Ultimately, an accuracy\nof 98.34% was achieved by the global model that has been trained using 10\ncompanies for 100 training rounds. The results reveal a slight difference in\nthe global model's accuracy compared to the highest accuracy of the centralized\nmodel, which can be ignored in favour of the global model's comprehensive\nknowledge, lower training cost, preservation of data privacy, and avoidance of\nthe technical debt problem.",
                "Active causal structure learning with advice\nWe introduce the problem of active causal structure learning with advice. In\nthe typical well-studied setting, the learning algorithm is given the essential\ngraph for the observational distribution and is asked to recover the underlying\ncausal directed acyclic graph (DAG) $G^*$ while minimizing the number of\ninterventions made. In our setting, we are additionally given side information\nabout $G^*$ as advice, e.g. a DAG $G$ purported to be $G^*$. We ask whether the\nlearning algorithm can benefit from the advice when it is close to being\ncorrect, while still having worst-case guarantees even when the advice is\narbitrarily bad. Our work is in the same space as the growing body of research\non algorithms with predictions. When the advice is a DAG $G$, we design an\nadaptive search algorithm to recover $G^*$ whose intervention cost is at most\n$O(\\max\\{1, \\log \\psi\\})$ times the cost for verifying $G^*$; here, $\\psi$ is a\ndistance measure between $G$ and $G^*$ that is upper bounded by the number of\nvariables $n$, and is exactly 0 when $G=G^*$. Our approximation factor matches\nthe state-of-the-art for the advice-less setting."
            ],
            "interesting paper": 3
        },
        {
            "papers": [
                "Assessing the Generalizability of a Performance Predictive Model\nA key component of automated algorithm selection and configuration, which in\nmost cases are performed using supervised machine learning (ML) methods is a\ngood-performing predictive model. The predictive model uses the feature\nrepresentation of a set of problem instances as input data and predicts the\nalgorithm performance achieved on them. Common machine learning models struggle\nto make predictions for instances with feature representations not covered by\nthe training data, resulting in poor generalization to unseen problems. In this\nstudy, we propose a workflow to estimate the generalizability of a predictive\nmodel for algorithm performance, trained on one benchmark suite to another. The\nworkflow has been tested by training predictive models across benchmark suites\nand the results show that generalizability patterns in the landscape feature\nspace are reflected in the performance space.",
                "Credit Card Fraud Detection Using Asexual Reproduction Optimization\nAs the number of credit card users has increased, detecting fraud in this\ndomain has become a vital issue. Previous literature has applied various\nsupervised and unsupervised machine learning methods to find an effective fraud\ndetection system. However, some of these methods require an enormous amount of\ntime to achieve reasonable accuracy. In this paper, an Asexual Reproduction\nOptimization (ARO) approach was employed, which is a supervised method to\ndetect credit card fraud. ARO refers to a kind of production in which one\nparent produces some offspring. By applying this method and sampling just from\nthe majority class, the effectiveness of the classification is increased. A\ncomparison to Artificial Immune Systems (AIS), which is one of the best methods\nimplemented on current datasets, has shown that the proposed method is able to\nremarkably reduce the required training time and at the same time increase the\nrecall that is important in fraud detection problems. The obtained results show\nthat ARO achieves the best cost in a short time, and consequently, it can be\nconsidered a real-time fraud detection system.",
                "Bias Mitigation Methods for Binary Classification Decision-Making\n  Systems: Survey and Recommendations\nBias mitigation methods for binary classification decision-making systems\nhave been widely researched due to the ever-growing importance of designing\nfair machine learning processes that are impartial and do not discriminate\nagainst individuals or groups based on protected personal characteristics. In\nthis paper, we present a structured overview of the research landscape for bias\nmitigation methods, report on their benefits and limitations, and provide\nrecommendations for the development of future bias mitigation methods for\nbinary classification.",
                "TorchRL: A data-driven decision-making library for PyTorch\nPyTorch has ascended as a premier machine learning framework, yet it lacks a\nnative and comprehensive library for decision and control tasks suitable for\nlarge development teams dealing with complex real-world data and environments.\nTo address this issue, we propose TorchRL, a generalistic control library for\nPyTorch that provides well-integrated, yet standalone components. We introduce\na new and flexible PyTorch primitive, the TensorDict, which facilitates\nstreamlined algorithm development across the many branches of Reinforcement\nLearning (RL) and control. We provide a detailed description of the building\nblocks and an extensive overview of the library across domains and tasks.\nFinally, we experimentally demonstrate its reliability and flexibility and show\ncomparative benchmarks to demonstrate its computational efficiency. TorchRL\nfosters long-term support and is publicly available on GitHub for greater\nreproducibility and collaboration within the research community. The code is\nopen-sourced on GitHub.",
                "One shot learning based drivers head movement identification using a\n  millimetre wave radar sensor\nConcentration of drivers on traffic is a vital safety issue; thus, monitoring\na driver being on road becomes an essential requirement. The key purpose of\nsupervision is to detect abnormal behaviours of the driver and promptly send\nwarnings to him her for avoiding incidents related to traffic accidents. In\nthis paper, to meet the requirement, based on radar sensors applications, the\nauthors first use a small sized millimetre wave radar installed at the steering\nwheel of the vehicle to collect signals from different head movements of the\ndriver. The received signals consist of the reflection patterns that change in\nresponse to the head movements of the driver. Then, in order to distinguish\nthese different movements, a classifier based on the measured signal of the\nradar sensor is designed. However, since the collected data set is not large,\nin this paper, the authors propose One shot learning to classify four cases of\ndriver's head movements. The experimental results indicate that the proposed\nmethod can classify the four types of cases according to the various head\nmovements of the driver with a high accuracy reaching up to 100. In addition,\nthe classification performance of the proposed method is significantly better\nthan that of the convolutional neural network model.",
                "Improved Financial Forecasting via Quantum Machine Learning\nQuantum algorithms have the potential to enhance machine learning across a\nvariety of domains and applications. In this work, we show how quantum machine\nlearning can be used to improve financial forecasting. First, we use classical\nand quantum Determinantal Point Processes to enhance Random Forest models for\nchurn prediction, improving precision by almost 6%. Second, we design quantum\nneural network architectures with orthogonal and compound layers for credit\nrisk assessment, which match classical performance with significantly fewer\nparameters. Our results demonstrate that leveraging quantum ideas can\neffectively enhance the performance of machine learning, both today as\nquantum-inspired classical ML solutions, and even more in the future, with the\nadvent of better quantum hardware."
            ],
            "interesting paper": 2
        },
        {
            "papers": [
                "Efficient Failure Pattern Identification of Predictive Algorithms\nGiven a (machine learning) classifier and a collection of unlabeled data, how\ncan we efficiently identify misclassification patterns presented in this\ndataset? To address this problem, we propose a human-machine collaborative\nframework that consists of a team of human annotators and a sequential\nrecommendation algorithm. The recommendation algorithm is conceptualized as a\nstochastic sampler that, in each round, queries the annotators a subset of\nsamples for their true labels and obtains the feedback information on whether\nthe samples are misclassified. The sampling mechanism needs to balance between\ndiscovering new patterns of misclassification (exploration) and confirming the\npotential patterns of classification (exploitation). We construct a\ndeterminantal point process, whose intensity balances the\nexploration-exploitation trade-off through the weighted update of the posterior\nat each round to form the generator of the stochastic sampler. The numerical\nresults empirically demonstrate the competitive performance of our framework on\nmultiple datasets at various signal-to-noise ratios.",
                "Heart Diseases Prediction Using Block-chain and Machine Learning\nMost people around the globe are dying due to heart disease. The main reason\nbehind the rapid increase in the death rate due to heart disease is that there\nis no infrastructure developed for the healthcare department that can provide a\nsecure way of data storage and transmission. Due to redundancy in the patient\ndata, it is difficult for cardiac Professionals to predict the disease early\non. This rapid increase in the death rate due to heart disease can be\ncontrolled by monitoring and eliminating some of the key attributes in the\nearly stages such as blood pressure, cholesterol level, body weight, and\naddiction to smoking. Patient data can be monitored by cardiac Professionals\n(Cp) by using the advanced framework in the healthcare departments. Blockchain\nis the world's most reliable provider. The use of advanced systems in the\nhealthcare departments providing new ways of dealing with diseases has been\ndeveloped as well. In this article Machine Learning (ML) algorithm known as a\nsine-cosine weighted k-nearest neighbor (SCA-WKNN) is used for predicting the\nHearth disease with the maximum accuracy among the existing approaches.\nBlockchain technology has been used in the research to secure the data\nthroughout the session and can give more accurate results using this\ntechnology. The performance of the system can be improved by using this\nalgorithm and the dataset proposed has been improved by using different\nresources as well.",
                "Prediction of Citrus Diseases Using Machine Learning And Deep Learning:\n  Classifier, Models SLR\nCitrus diseases have been major issues for citrus growing worldwide for many\nyears they can lead significantly reduce fruit quality. the most harmful citrus\ndiseases are citrus canker, citrus greening, citrus black spot, citrus leaf\nminer which can have significant economic losses of citrus industry in\nworldwide prevention and management strategies like chemical treatments. Citrus\ndiseases existing in all over the world where citrus is growing its effects the\ncitrus tree root, citrus tree leaf, citrus tree orange etc. Existing of citrus\ndiseases is highly impact on economic factor that can also produce low quality\nfruits and increased the rate for diseases management. Sanitation and routine\nmonitoring can be effective in managing certain citrus diseases, but others may\nrequire more intensive treatments like chemical or biological control methods.",
                "Federated Domain Generalization: A Survey\nMachine learning typically relies on the assumption that training and testing\ndistributions are identical and that data is centrally stored for training and\ntesting. However, in real-world scenarios, distributions may differ\nsignificantly and data is often distributed across different devices,\norganizations, or edge nodes. Consequently, it is imperative to develop models\nthat can effectively generalize to unseen distributions where data is\ndistributed across different domains. In response to this challenge, there has\nbeen a surge of interest in federated domain generalization (FDG) in recent\nyears. FDG combines the strengths of federated learning (FL) and domain\ngeneralization (DG) techniques to enable multiple source domains to\ncollaboratively learn a model capable of directly generalizing to unseen\ndomains while preserving data privacy. However, generalizing the federated\nmodel under domain shifts is a technically challenging problem that has\nreceived scant attention in the research area so far. This paper presents the\nfirst survey of recent advances in this area. Initially, we discuss the\ndevelopment process from traditional machine learning to domain adaptation and\ndomain generalization, leading to FDG as well as provide the corresponding\nformal definition. Then, we categorize recent methodologies into four classes:\nfederated domain alignment, data manipulation, learning strategies, and\naggregation optimization, and present suitable algorithms in detail for each\ncategory. Next, we introduce commonly used datasets, applications, evaluations,\nand benchmarks. Finally, we conclude this survey by providing some potential\nresearch topics for the future.",
                "Cook-Gen: Robust Generative Modeling of Cooking Actions from Recipes\nAs people become more aware of their food choices, food computation models\nhave become increasingly popular in assisting people in maintaining healthy\neating habits. For example, food recommendation systems analyze recipe\ninstructions to assess nutritional contents and provide recipe recommendations.\nThe recent and remarkable successes of generative AI methods, such as\nauto-regressive large language models, can lead to robust methods for a more\ncomprehensive understanding of recipes for healthy food recommendations beyond\nsurface-level nutrition content assessments. In this study, we explore the use\nof generative AI methods to extend current food computation models, primarily\ninvolving the analysis of nutrition and ingredients, to also incorporate\ncooking actions (e.g., add salt, fry the meat, boil the vegetables, etc.).\nCooking actions are notoriously hard to model using statistical learning\nmethods due to irregular data patterns - significantly varying natural language\ndescriptions for the same action (e.g., marinate the meat vs. marinate the meat\nand leave overnight) and infrequently occurring patterns (e.g., add salt occurs\nfar more frequently than marinating the meat). The prototypical approach to\nhandling irregular data patterns is to increase the volume of data that the\nmodel ingests by orders of magnitude. Unfortunately, in the cooking domain,\nthese problems are further compounded with larger data volumes presenting a\nunique challenge that is not easily handled by simply scaling up. In this work,\nwe propose novel aggregation-based generative AI methods, Cook-Gen, that\nreliably generate cooking actions from recipes, despite difficulties with\nirregular data patterns, while also outperforming Large Language Models and\nother strong baselines.",
                "Rotating Features for Object Discovery\nThe binding problem in human cognition, concerning how the brain represents\nand connects objects within a fixed network of neural connections, remains a\nsubject of intense debate. Most machine learning efforts addressing this issue\nin an unsupervised setting have focused on slot-based methods, which may be\nlimiting due to their discrete nature and difficulty to express uncertainty.\nRecently, the Complex AutoEncoder was proposed as an alternative that learns\ncontinuous and distributed object-centric representations. However, it is only\napplicable to simple toy data. In this paper, we present Rotating Features, a\ngeneralization of complex-valued features to higher dimensions, and a new\nevaluation procedure for extracting objects from distributed representations.\nAdditionally, we show the applicability of our approach to pre-trained\nfeatures. Together, these advancements enable us to scale distributed\nobject-centric representations from simple toy to real-world data. We believe\nthis work advances a new paradigm for addressing the binding problem in machine\nlearning and has the potential to inspire further innovation in the field."
            ],
            "interesting paper": 4
        },
        {
            "papers": [
                "XAI Renaissance: Redefining Interpretability in Medical Diagnostic\n  Models\nAs machine learning models become increasingly prevalent in medical\ndiagnostics, the need for interpretability and transparency becomes paramount.\nThe XAI Renaissance signifies a significant shift in the field, aiming to\nredefine the interpretability of medical diagnostic models. This paper explores\nthe innovative approaches and methodologies within the realm of Explainable AI\n(XAI) that are revolutionizing the interpretability of medical diagnostic\nmodels. By shedding light on the underlying decision-making process, XAI\ntechniques empower healthcare professionals to understand, trust, and\neffectively utilize these models for accurate and reliable medical diagnoses.\nThis review highlights the key advancements in XAI for medical diagnostics and\ntheir potential to transform the healthcare landscape, ultimately improving\npatient outcomes and fostering trust in AI-driven diagnostic systems.",
                "Deep Classifier Mimicry without Data Access\nAccess to pre-trained models has recently emerged as a standard across\nnumerous machine learning domains. Unfortunately, access to the original data\nthe models were trained on may not equally be granted. This makes it\ntremendously challenging to fine-tune, compress models, adapt continually, or\nto do any other type of data-driven update. We posit that original data access\nmay however not be required. Specifically, we propose Contrastive Abductive\nKnowledge Extraction (CAKE), a model-agnostic knowledge distillation procedure\nthat mimics deep classifiers without access to the original data. To this end,\nCAKE generates pairs of noisy synthetic samples and diffuses them contrastively\ntoward a model's decision boundary. We empirically corroborate CAKE's\neffectiveness using several benchmark datasets and various architectural\nchoices, paving the way for broad application.",
                "Simple Data Augmentation Techniques for Chinese Disease Normalization\nDisease name normalization is an important task in the medical domain. It\nclassifies disease names written in various formats into standardized names,\nserving as a fundamental component in smart healthcare systems for various\ndisease-related functions. Nevertheless, the most significant obstacle to\nexisting disease name normalization systems is the severe shortage of training\ndata. Consequently, we present a novel data augmentation approach that includes\na series of data augmentation techniques and some supporting modules to help\nmitigate the problem. Our proposed methods rely on the Structural Invariance\nproperty of disease names and the Hierarchy property of the disease\nclassification system. The goal is to equip the models with extensive\nunderstanding of the disease names and the hierarchical structure of the\ndisease name classification system. Through extensive experimentation, we\nillustrate that our proposed approach exhibits significant performance\nimprovements across various baseline models and training objectives,\nparticularly in scenarios with limited training data.",
                "Probabilistic Concept Bottleneck Models\nInterpretable models are designed to make decisions in a human-interpretable\nmanner. Representatively, Concept Bottleneck Models (CBM) follow a two-step\nprocess of concept prediction and class prediction based on the predicted\nconcepts. CBM provides explanations with high-level concepts derived from\nconcept predictions; thus, reliable concept predictions are important for\ntrustworthiness. In this study, we address the ambiguity issue that can harm\nreliability. While the existence of a concept can often be ambiguous in the\ndata, CBM predicts concepts deterministically without considering this\nambiguity. To provide a reliable interpretation against this ambiguity, we\npropose Probabilistic Concept Bottleneck Models (ProbCBM). By leveraging\nprobabilistic concept embeddings, ProbCBM models uncertainty in concept\nprediction and provides explanations based on the concept and its corresponding\nuncertainty. This uncertainty enhances the reliability of the explanations.\nFurthermore, as class uncertainty is derived from concept uncertainty in\nProbCBM, we can explain class uncertainty by means of concept uncertainty. Code\nis publicly available at https://github.com/ejkim47/prob-cbm.",
                "Publicly available datasets of breast histopathology H&E whole-slide\n  images: A scoping review\nAdvancements in digital pathology and computing resources have made a\nsignificant impact in the field of computational pathology for breast cancer\ndiagnosis and treatment. However, access to high-quality labeled\nhistopathological images of breast cancer is a big challenge that limits the\ndevelopment of accurate and robust deep learning models. In this scoping\nreview, we identified the publicly available datasets of breast H&E stained\nwhole-slide images (WSI) that can be used to develop deep learning algorithms.\nWe systematically searched nine scientific literature databases and nine\nresearch data repositories and found 17 publicly available datasets containing\n10385 H&E WSIs of breast cancer. Moreover, we reported image metadata and\ncharacteristics for each dataset to assist researchers in selecting proper\ndatasets for specific tasks in breast cancer computational pathology. In\naddition, we compiled two lists of breast H&E patches and private datasets as\nsupplementary resources for researchers. Notably, only 28% of the included\narticles utilized multiple datasets, and only 14% used an external validation\nset, suggesting that the performance of other developed models may be\nsusceptible to overestimation. The TCGA-BRCA was used in 52% of the selected\nstudies. This dataset has a considerable selection bias that can impact the\nrobustness and generalizability of the trained algorithms. There is also a lack\nof consistent metadata reporting of breast WSI datasets that can be an issue in\ndeveloping accurate deep learning models, indicating the necessity of\nestablishing explicit guidelines for documenting breast WSI dataset\ncharacteristics and metadata.",
                "AlerTiger: Deep Learning for AI Model Health Monitoring at LinkedIn\nData-driven companies use AI models extensively to develop products and\nintelligent business solutions, making the health of these models crucial for\nbusiness success. Model monitoring and alerting in industries pose unique\nchallenges, including a lack of clear model health metrics definition, label\nsparsity, and fast model iterations that result in short-lived models and\nfeatures. As a product, there are also requirements for scalability,\ngeneralizability, and explainability. To tackle these challenges, we propose\nAlerTiger, a deep-learning-based MLOps model monitoring system that helps AI\nteams across the company monitor their AI models' health by detecting anomalies\nin models' input features and output score over time. The system consists of\nfour major steps: model statistics generation, deep-learning-based anomaly\ndetection, anomaly post-processing, and user alerting. Our solution generates\nthree categories of statistics to indicate AI model health, offers a two-stage\ndeep anomaly detection solution to address label sparsity and attain the\ngeneralizability of monitoring new models, and provides holistic reports for\nactionable alerts. This approach has been deployed to most of LinkedIn's\nproduction AI models for over a year and has identified several model issues\nthat later led to significant business metric gains after fixing."
            ],
            "interesting paper": 2
        }
    ],
    "Drew Martinez": [
        {
            "papers": [
                "Distributed Online Rollout for Multivehicle Routing in Unmapped\n  Environments\nIn this work we consider a generalization of the well-known multivehicle\nrouting problem: given a network, a set of agents occupying a subset of its\nnodes, and a set of tasks, we seek a minimum cost sequence of movements subject\nto the constraint that each task is visited by some agent at least once. The\nclassical version of this problem assumes a central computational server that\nobserves the entire state of the system perfectly and directs individual agents\naccording to a centralized control scheme. In contrast, we assume that there is\nno centralized server and that each agent is an individual processor with no a\npriori knowledge of the underlying network (including task and agent\nlocations). Moreover, our agents possess strictly local communication and\nsensing capabilities (restricted to a fixed radius around their respective\nlocations), aligning more closely with several real-world multiagent\napplications. These restrictions introduce many challenges that are overcome\nthrough local information sharing and direct coordination between agents. We\npresent a fully distributed, online, and scalable reinforcement learning\nalgorithm for this problem whereby agents self-organize into local clusters and\nindependently apply a multiagent rollout scheme locally to each cluster. We\ndemonstrate empirically via extensive simulations that there exists a critical\nsensing radius beyond which the distributed rollout algorithm begins to improve\nover a greedy base policy. This critical sensing radius grows proportionally to\nthe $\\log^*$ function of the size of the network, and is, therefore, a small\nconstant for any relevant network. Our decentralized reinforcement learning\nalgorithm achieves approximately a factor of two cost improvement over the base\npolicy for a range of radii bounded from below and above by two and three times\nthe critical sensing radius, respectively.",
                "Cooperative Control of Multi-Channel Linear Systems with Self-Organizing\n  Private Agents\nCooperative behavior design for multi-agent systems with collective tasks is\na critical issue in promoting swarm intelligence. This paper investigates\ncooperative control for a multi-channel system, where each channel is managed\nby an agent expected to self-organize a controller to stabilize the system\ncollaboratively by communicating with neighbors in a network. Integrating a\nstate decomposition technique and a fusion approach, a fully distributed\nprivacy-preserving mechanism is proposed to shield agents' private information\nfrom neighbors' eavesdropping. Moreover, the cost of introducing the\nprivacy-preserving mechanism and the benefit of adding more channels to the\nsystem are quantitatively analyzed. Finally, comparative simulation examples\nare provided to demonstrate the effectiveness of the theoretical results.",
                "Discounting in Strategy Logic\nDiscounting is an important dimension in multi-agent systems as long as we\nwant to reason about strategies and time. It is a key aspect in economics as it\ncaptures the intuition that the far-away future is not as important as the near\nfuture. Traditional verification techniques allow to check whether there is a\nwinning strategy for a group of agents but they do not take into account the\nfact that satisfying a goal sooner is different from satisfying it after a long\nwait. In this paper, we augment Strategy Logic with future discounting over a\nset of discounted functions D, denoted SLdisc[D]. We consider \"until\" operators\nwith discounting functions: the satisfaction value of a specification in\nSLdisc[D] is a value in [0, 1], where the longer it takes to fulfill\nrequirements, the smaller the satisfaction value is. We motivate our approach\nwith classical examples from Game Theory and study the complexity of\nmodel-checking SLdisc[D]-formulas.",
                "SPRING: Studying the Paper and Reasoning to Play Games\nOpen-world survival games pose significant challenges for AI algorithms due\nto their multi-tasking, deep exploration, and goal prioritization requirements.\nDespite reinforcement learning (RL) being popular for solving games, its high\nsample complexity limits its effectiveness in complex open-world games like\nCrafter or Minecraft. We propose a novel approach, SPRING, to read the game's\noriginal academic paper and use the knowledge learned to reason and play the\ngame through a large language model (LLM). Prompted with the LaTeX source as\ngame context and a description of the agent's current observation, our SPRING\nframework employs a directed acyclic graph (DAG) with game-related questions as\nnodes and dependencies as edges. We identify the optimal action to take in the\nenvironment by traversing the DAG and calculating LLM responses for each node\nin topological order, with the LLM's answer to final node directly translating\nto environment actions. In our experiments, we study the quality of in-context\n\"reasoning\" induced by different forms of prompts under the setting of the\nCrafter open-world environment. Our experiments suggest that LLMs, when\nprompted with consistent chain-of-thought, have great potential in completing\nsophisticated high-level trajectories. Quantitatively, SPRING with GPT-4\noutperforms all state-of-the-art RL baselines, trained for 1M steps, without\nany training. Finally, we show the potential of games as a test bed for LLMs.",
                "Local control for the collective dynamics of self-propelled particles\nUtilizing a paradigmatic model for the motion of interacting self-propelled\nparticles, we demonstrate that local accelerations at the level of individual\nparticles can drive transitions between different collective dynamics, leading\nto a control process. We find that the ability to trigger such transitions is\nhierarchically distributed among the particles and can form distinctive spatial\npatterns within the collective. Chaotic dynamics occur during the transitions,\nwhich can be attributed to fractal basin boundaries mediating the control\nprocess. The particle hierarchies described in this study offer decentralized\ncapabilities for controlling artificial swarms.",
                "A Fast Algorithm for Consistency Checking Partially Ordered Time\nPartially ordered models of time occur naturally in applications where agents\nor processes cannot perfectly communicate with each other, and can be traced\nback to the seminal work of Lamport. In this paper we consider the problem of\ndeciding if a (likely incomplete) description of a system of events is\nconsistent, the network consistency problem for the point algebra of partially\nordered time (POT). While the classical complexity of this problem has been\nfully settled, comparably little is known of the fine-grained complexity of POT\nexcept that it can be solved in $O^*((0.368n)^n)$ time by enumerating ordered\npartitions. We construct a much faster algorithm with a run-time bounded by\n$O^*((0.26n)^n)$. This is achieved by a sophisticated enumeration of structures\nsimilar to total orders, which are then greedily expanded toward a solution.\nWhile similar ideas have been explored earlier for related problems it turns\nout that the analysis for POT is non-trivial and requires significant new\nideas."
            ],
            "interesting paper": 1
        },
        {
            "papers": [
                "Deep Learning and Ethics\nThis article appears as chapter 21 of Prince (2023, Understanding Deep\nLearning); a complete draft of the textbook is available here:\nhttp://udlbook.com. This chapter considers potential harms arising from the\ndesign and use of AI systems. These include algorithmic bias, lack of\nexplainability, data privacy violations, militarization, fraud, and\nenvironmental concerns. The aim is not to provide advice on being more ethical.\nInstead, the goal is to express ideas and start conversations in key areas that\nhave received attention in philosophy, political science, and the broader\nsocial sciences.",
                "Towards a Capability Assessment Model for the Comprehension and Adoption\n  of AI in Organisations\nThe comprehension and adoption of Artificial Intelligence (AI) are beset with\npractical and ethical problems. This article presents a 5-level AI Capability\nAssessment Model (AI-CAM) and a related AI Capabilities Matrix (AI-CM) to\nassist practitioners in AI comprehension and adoption. These practical tools\nwere developed with business executives, technologists, and other\norganisational stakeholders in mind. They are founded on a comprehensive\nconception of AI compared to those in other AI adoption models and are also\nopen-source artefacts. Thus, the AI-CAM and AI-CM present an accessible\nresource to help inform organisational decision-makers on the capability\nrequirements for (1) AI-based data analytics use cases based on machine\nlearning technologies; (2) Knowledge representation to engineer and represent\ndata, information and knowledge using semantic technologies; and (3) AI-based\nsolutions that seek to emulate human reasoning and decision-making. The AI-CAM\ncovers the core capability dimensions (business, data, technology,\norganisation, AI skills, risks, and ethical considerations) required at the\nfive capability maturity levels to achieve optimal use of AI in organisations.",
                "Trends and Challenges Towards an Effective Data-Driven Decision Making\n  in UK SMEs: Case Studies and Lessons Learnt from the Analysis of 85 SMEs\nThe adoption of data science brings vast benefits to Small and Medium-sized\nEnterprises (SMEs) including business productivity, economic growth, innovation\nand jobs creation. Data Science can support SMEs to optimise production\nprocesses, anticipate customers' needs, predict machinery failures and deliver\nefficient smart services. Businesses can also harness the power of Artificial\nIntelligence (AI) and Big Data and the smart use of digital technologies to\nenhance productivity and performance, paving the way for innovation. However,\nintegrating data science decisions into an SME requires both skills and IT\ninvestments. In most cases, such expenses are beyond the means of SMEs due to\nlimited resources and restricted access to financing. This paper presents\ntrends and challenges towards an effective data-driven decision making for\norganisations based on a case study of 85 SMEs, mostly from the West Midlands\nregion of England. The work is supported as part of a 3 years ERDF (European\nRegional Development Funded project) in the areas of big data management,\nanalytics and business intelligence. We present two case studies that\ndemonstrates the potential of Digitisation, AI and Machine Learning and use\nthese as examples to unveil challenges and showcase the wealth of current\navailable opportunities for SMEs.",
                "Applications of Intelligent Systems in Green Technology\nIntelligent Systems (ISs) are technologically advanced machines, which\nperceive and respond to the environment around them. They are usually of\nvarious forms ranging from software to hardware. ISs are generally the fusion\nof Artificial Intelligence (AI), robotics and Internet of things (IoT). In\norder to strengthen ISs, one of the key technologies is green technology (GT).\nIt refers to the continuously advancing methods and materials, which cover\ntechniques for producing energy to non-toxic cleaning products. It may also be\nbroadened to saving energy, and reducing toxic and waste materials in the\nenvironment. The motto of GT can be achieved by using the ISs. In this paper,\nwe present various applications of ISs in GT. Moreover, we discuss various\npossible solutions using ISs in order to overcome the on-going real-life\nproblems.",
                "Model evaluation for extreme risks\nCurrent approaches to building general-purpose AI systems tend to produce\nsystems with both beneficial and harmful capabilities. Further progress in AI\ndevelopment could lead to capabilities that pose extreme risks, such as\noffensive cyber capabilities or strong manipulation skills. We explain why\nmodel evaluation is critical for addressing extreme risks. Developers must be\nable to identify dangerous capabilities (through \"dangerous capability\nevaluations\") and the propensity of models to apply their capabilities for harm\n(through \"alignment evaluations\"). These evaluations will become critical for\nkeeping policymakers and other stakeholders informed, and for making\nresponsible decisions about model training, deployment, and security.",
                "Distributed Online Rollout for Multivehicle Routing in Unmapped\n  Environments\nIn this work we consider a generalization of the well-known multivehicle\nrouting problem: given a network, a set of agents occupying a subset of its\nnodes, and a set of tasks, we seek a minimum cost sequence of movements subject\nto the constraint that each task is visited by some agent at least once. The\nclassical version of this problem assumes a central computational server that\nobserves the entire state of the system perfectly and directs individual agents\naccording to a centralized control scheme. In contrast, we assume that there is\nno centralized server and that each agent is an individual processor with no a\npriori knowledge of the underlying network (including task and agent\nlocations). Moreover, our agents possess strictly local communication and\nsensing capabilities (restricted to a fixed radius around their respective\nlocations), aligning more closely with several real-world multiagent\napplications. These restrictions introduce many challenges that are overcome\nthrough local information sharing and direct coordination between agents. We\npresent a fully distributed, online, and scalable reinforcement learning\nalgorithm for this problem whereby agents self-organize into local clusters and\nindependently apply a multiagent rollout scheme locally to each cluster. We\ndemonstrate empirically via extensive simulations that there exists a critical\nsensing radius beyond which the distributed rollout algorithm begins to improve\nover a greedy base policy. This critical sensing radius grows proportionally to\nthe $\\log^*$ function of the size of the network, and is, therefore, a small\nconstant for any relevant network. Our decentralized reinforcement learning\nalgorithm achieves approximately a factor of two cost improvement over the base\npolicy for a range of radii bounded from below and above by two and three times\nthe critical sensing radius, respectively."
            ],
            "interesting paper": 6
        },
        {
            "papers": [
                "On Computing Universal Plans for Partially Observable Multi-Agent Path\n  Finding\nMulti-agent routing problems have drawn significant attention nowadays due to\ntheir broad industrial applications in, e.g., warehouse robots, logistics\nautomation, and traffic control. Conventionally, they are modelled as classical\nplanning problems. In this paper, we argue that it is beneficial to formulate\nthem as universal planning problems. We therefore propose universal plans, also\nknown as policies, as the solution concepts, and implement a system called\nASP-MAUPF (Answer Set Programming for Multi-Agent Universal Plan Finding) for\ncomputing them. Given an arbitrary two-dimensional map and a profile of goals\nfor the agents, the system finds a feasible universal plan for each agent that\nensures no collision with others. We use the system to conduct some\nexperiments, and make some observations on the types of goal profiles and\nenvironments that will have feasible policies, and how they may depend on\nagents' sensors. We also demonstrate how users can customize action preferences\nto compute more efficient policies, even (near-)optimal ones.",
                "Fine-Grained Complexity Analysis of Multi-Agent Path Finding on 2D Grids\nMulti-Agent Path Finding (MAPF) is a fundamental motion coordination problem\narising in multi-agent systems with a wide range of applications. The problem's\nintractability has led to extensive research on improving the scalability of\nsolvers for it. Since optimal solvers can struggle to scale, a major challenge\nthat arises is understanding what makes MAPF hard. We tackle this challenge\nthrough a fine-grained complexity analysis of time-optimal MAPF on 2D grids,\nthereby closing two gaps and identifying a new tractability frontier. First, we\nshow that 2-colored MAPF, i.e., where the agents are divided into two teams,\neach with its own set of targets, remains NP-hard. Second, for the flowtime\nobjective (also called sum-of-costs), we show that it remains NP-hard to find a\nsolution in which agents have an individually optimal cost, which we call an\nindividually optimal solution. The previously tightest results for these MAPF\nvariants are for (non-grid) planar graphs. We use a single hardness\nconstruction that replaces, strengthens, and unifies previous proofs. We\nbelieve that it is also simpler than previous proofs for the planar case as it\nemploys minimal gadgets that enable its full visualization in one figure.\nFinally, for the flowtime objective, we establish a tractability frontier based\non the number of directions agents can move in. Namely, we complement our\nhardness result, which holds for three directions, with an efficient algorithm\nfor finding an individually optimal solution if only two directions are\nallowed. This result sheds new light on the structure of optimal solutions,\nwhich may help guide algorithm design for the general problem.",
                "Individuality in Swarm Robots with the Case Study of Kilobots: Noise,\n  Bug, or Feature?\nInter-individual differences are studied in natural systems, such as fish,\nbees, and humans, as they contribute to the complexity of both individual and\ncollective behaviors. However, individuality in artificial systems, such as\nrobotic swarms, is undervalued or even overlooked. Agent-specific deviations\nfrom the norm in swarm robotics are usually understood as mere noise that can\nbe minimized, for example, by calibration. We observe that robots have\nconsistent deviations and argue that awareness and knowledge of these can be\nexploited to serve a task. We measure heterogeneity in robot swarms caused by\nindividual differences in how robots act, sense, and oscillate. Our use case is\nKilobots and we provide example behaviors where the performance of robots\nvaries depending on individual differences. We show a non-intuitive example of\nphototaxis with Kilobots where the non-calibrated Kilobots show better\nperformance than the calibrated supposedly ``ideal\" one. We measure the\ninter-individual variations for heterogeneity in sensing and oscillation, too.\nWe briefly discuss how these variations can enhance the complexity of\ncollective behaviors. We suggest that by recognizing and exploring this new\nperspective on individuality, and hence diversity, in robotic swarms, we can\ngain a deeper understanding of these systems and potentially unlock new\npossibilities for their design and implementation of applications.",
                "Almost Envy-Free Allocations of Indivisible Goods or Chores with\n  Entitlements\nWe here address the problem of fairly allocating indivisible goods or chores\nto $n$ agents with weights that define their entitlement to the set of\nindivisible resources. Stemming from well-studied fairness concepts such as\nenvy-freeness up to one good (EF1) and envy-freeness up to any good (EFX) for\nagents with equal entitlements, we present, in this study, the first set of\nimpossibility results alongside algorithmic guarantees for fairness among\nagents with unequal entitlements.\n  Within this paper, we expand the concept of envy-freeness up to any good or\nchore to the weighted context (WEFX and XWEF respectively), demonstrating that\nthese allocations are not guaranteed to exist for two or three agents. Despite\nthese negative results, we develop a WEFX procedure for two agents with integer\nweights, and furthermore, we devise an approximate WEFX procedure for two\nagents with normalized weights. We further present a polynomial-time algorithm\nthat guarantees a weighted envy-free allocation up to one chore (1WEF) for any\nnumber of agents with additive cost functions. Our work underscores the\nheightened complexity of the weighted fair division problem when compared to\nits unweighted counterpart.",
                "MULTIGAIN 2.0: MDP controller synthesis for multiple mean-payoff, LTL\n  and steady-state constraints\nWe present MULTIGAIN 2.0, a major extension to the controller synthesis tool\nMULTIGAIN, built on top of the probabilistic model checker PRISM. This new\nversion extends MULTIGAIN's multi-objective capabilities, by allowing for the\nformal verification and synthesis of controllers for probabilistic systems with\nmulti-dimensional long-run average reward structures, steady-state constraints,\nand linear temporal logic properties. Additionally, MULTIGAIN 2.0 can modify\nthe underlying linear program to prevent unbounded-memory and other unintuitive\nsolutions and visualizes Pareto curves, in the two- and three-dimensional\ncases, to facilitate trade-off analysis in multi-objective scenarios.",
                "Exploring the Adaptive Behaviors of Particle Lenia: A\n  Perturbation-Response Analysis for Computational Agency\nA firm cognitive subject or ``individual'' is presupposed for the emergence\nof mind. However, with the development of recent information technology, the\n``individual'' has become more dispersed in society and the cognitive subject\nhas become increasingly unstable and adaptive, necessitating an update in our\nunderstanding of the ``individual''. Autopoiesis serves as a model of the\ncognitive subject, which is unstable and requires effort to maintain itself to\nadapt to the environment. In this study, we evaluated adaptivity for a highly\nextensible multi-particle system model Particle Lenia through the response\nperturbation. As a result, we found that Particle Lenia has a particle\nconfiguration that is both temporally unstable and has multiple stable states.\nThis result suggests that Particle Lenia can express adaptive characteristics\nand is expected to be used as a computational model toward building an\nautopoietic cognitive agent."
            ],
            "interesting paper": 3
        },
        {
            "papers": [
                "Alert of the Second Decision-maker: An Introduction to Human-AI Conflict\nThe collaboration between humans and artificial intelligence (AI) is a\nsignificant feature in this digital age. However, humans and AI may have\nobservation, interpretation, and action conflicts when working synchronously.\nThis phenomenon is often masked by faults and, unfortunately, overlooked. This\npaper systematically introduces the human-AI conflict concept, causes,\nmeasurement methods, and risk assessment. The results highlight that there is a\npotential second decision-maker besides the human, which is the AI; the\nhuman-AI conflict is a unique and emerging risk in digitalized process systems;\nand this is an interdisciplinary field that needs to be distinguished from\ntraditional fault and failure analysis; the conflict risk is significant and\ncannot be ignored.",
                "AI Techniques in the Microservices Life-Cycle: A Survey\nMicroservices is a popular architectural style for the development of\ndistributed software, with an emphasis on modularity, scalability, and\nflexibility. Indeed, in microservice systems, functionalities are provided by\nloosely coupled, small services, each focusing on a specific business\ncapability. Building a system according to the microservices architectural\nstyle brings a number of challenges, mainly related to how the different\nmicroservices are deployed and coordinated and how they interact. In this\npaper, we provide a survey about how techniques in the area of Artificial\nIntelligence have been used to tackle these challenges.",
                "Attention Paper: How Generative AI Reshapes Digital Shadow Industry?\nThe rapid development of digital economy has led to the emergence of various\nblack and shadow internet industries, which pose potential risks that can be\nidentified and managed through digital risk management (DRM) that uses\ndifferent techniques such as machine learning and deep learning. The evolution\nof DRM architecture has been driven by changes in data forms. However, the\ndevelopment of AI-generated content (AIGC) technology, such as ChatGPT and\nStable Diffusion, has given black and shadow industries powerful tools to\npersonalize data and generate realistic images and conversations for fraudulent\nactivities. This poses a challenge for DRM systems to control risks from the\nsource of data generation and to respond quickly to the fast-changing risk\nenvironment. This paper aims to provide a technical analysis of the challenges\nand opportunities of AIGC from upstream, midstream, and downstream paths of\nblack/shadow industries and suggest future directions for improving existing\nrisk control systems. The paper will explore the new black and shadow\ntechniques triggered by generative AI technology and provide insights for\nbuilding the next-generation DRM system.",
                "On Computing Universal Plans for Partially Observable Multi-Agent Path\n  Finding\nMulti-agent routing problems have drawn significant attention nowadays due to\ntheir broad industrial applications in, e.g., warehouse robots, logistics\nautomation, and traffic control. Conventionally, they are modelled as classical\nplanning problems. In this paper, we argue that it is beneficial to formulate\nthem as universal planning problems. We therefore propose universal plans, also\nknown as policies, as the solution concepts, and implement a system called\nASP-MAUPF (Answer Set Programming for Multi-Agent Universal Plan Finding) for\ncomputing them. Given an arbitrary two-dimensional map and a profile of goals\nfor the agents, the system finds a feasible universal plan for each agent that\nensures no collision with others. We use the system to conduct some\nexperiments, and make some observations on the types of goal profiles and\nenvironments that will have feasible policies, and how they may depend on\nagents' sensors. We also demonstrate how users can customize action preferences\nto compute more efficient policies, even (near-)optimal ones.",
                "Enhancing Human Capabilities through Symbiotic Artificial Intelligence\n  with Shared Sensory Experiences\nThe merging of human intelligence and artificial intelligence has long been a\nsubject of interest in both science fiction and academia. In this paper, we\nintroduce a novel concept in Human-AI interaction called Symbiotic Artificial\nIntelligence with Shared Sensory Experiences (SAISSE), which aims to establish\na mutually beneficial relationship between AI systems and human users through\nshared sensory experiences. By integrating multiple sensory input channels and\nprocessing human experiences, SAISSE fosters a strong human-AI bond, enabling\nAI systems to learn from and adapt to individual users, providing personalized\nsupport, assistance, and enhancement. Furthermore, we discuss the incorporation\nof memory storage units for long-term growth and development of both the AI\nsystem and its human user. As we address user privacy and ethical guidelines\nfor responsible AI-human symbiosis, we also explore potential biases and\ninequalities in AI-human symbiosis and propose strategies to mitigate these\nchallenges. Our research aims to provide a comprehensive understanding of the\nSAISSE concept and its potential to effectively support and enhance individual\nhuman users through symbiotic AI systems. This position article aims at\ndiscussing poteintial AI-human interaction related topics within the scientific\ncommunity, rather than providing experimental or theoretical results.",
                "Applying Interdisciplinary Frameworks to Understand Algorithmic\n  Decision-Making\nWe argue that explanations for \"algorithmic decision-making\" (ADM) systems\ncan profit by adopting practices that are already used in the learning\nsciences. We shortly introduce the importance of explaining ADM systems, give a\nbrief overview of approaches drawing from other disciplines to improve\nexplanations, and present the results of our qualitative task-based study\nincorporating the \"six facets of understanding\" framework. We close with\nquestions guiding the discussion of how future studies can leverage an\ninterdisciplinary approach."
            ],
            "interesting paper": 3
        },
        {
            "papers": [
                "MADiff: Offline Multi-agent Learning with Diffusion Models\nDiffusion model (DM) recently achieved huge success in various scenarios\nincluding offline reinforcement learning, where the diffusion planner learn to\ngenerate desired trajectories during online evaluations. However, despite the\neffectiveness in single-agent learning, it remains unclear how DMs can operate\nin multi-agent problems, where agents can hardly complete teamwork without good\ncoordination by independently modeling each agent's trajectories. In this\npaper, we propose MADiff, a novel generative multi-agent learning framework to\ntackle this problem. MADiff is realized with an attention-based diffusion model\nto model the complex coordination among behaviors of multiple agents. To the\nbest of our knowledge, MADiff is the first diffusion-based multi-agent learning\nframework, which behaves as both a decentralized policy and a centralized\ncontroller. During decentralized executions, MADiff simultaneously performs\nteammate modeling, and the centralized controller can also be applied in\nmulti-agent trajectory predictions. Our experiments show the superior\nperformance of MADiff compared to baseline algorithms in a wide range of\nmulti-agent learning tasks, which emphasizes the effectiveness of MADiff in\nmodeling complex multi-agent interactions. Our code is available at\nhttps://github.com/zbzhu99/madiff.",
                "Formal Modelling for Multi-Robot Systems Under Uncertainty\nPurpose of Review: To effectively synthesise and analyse multi-robot\nbehaviour, we require formal task-level models which accurately capture\nmulti-robot execution. In this paper, we review modelling formalisms for\nmulti-robot systems under uncertainty, and discuss how they can be used for\nplanning, reinforcement learning, model checking, and simulation.\n  Recent Findings: Recent work has investigated models which more accurately\ncapture multi-robot execution by considering different forms of uncertainty,\nsuch as temporal uncertainty and partial observability, and modelling the\neffects of robot interactions on action execution. Other strands of work have\npresented approaches for reducing the size of multi-robot models to admit more\nefficient solution methods. This can be achieved by decoupling the robots under\nindependence assumptions, or reasoning over higher level macro actions.\n  Summary: Existing multi-robot models demonstrate a trade off between\naccurately capturing robot dependencies and uncertainty, and being small enough\nto tractably solve real world problems. Therefore, future research should\nexploit realistic assumptions over multi-robot behaviour to develop smaller\nmodels which retain accurate representations of uncertainty and robot\ninteractions; and exploit the structure of multi-robot problems, such as\nfactored state spaces, to develop scalable solution methods.",
                "Is Centralized Training with Decentralized Execution Framework\n  Centralized Enough for MARL?\nCentralized Training with Decentralized Execution (CTDE) has recently emerged\nas a popular framework for cooperative Multi-Agent Reinforcement Learning\n(MARL), where agents can use additional global state information to guide\ntraining in a centralized way and make their own decisions only based on\ndecentralized local policies. Despite the encouraging results achieved, CTDE\nmakes an independence assumption on agent policies, which limits agents to\nadopt global cooperative information from each other during centralized\ntraining. Therefore, we argue that existing CTDE methods cannot fully utilize\nglobal information for training, leading to an inefficient joint-policy\nexploration and even suboptimal results. In this paper, we introduce a novel\nCentralized Advising and Decentralized Pruning (CADP) framework for multi-agent\nreinforcement learning, that not only enables an efficacious message exchange\namong agents during training but also guarantees the independent policies for\nexecution. Firstly, CADP endows agents the explicit communication channel to\nseek and take advices from different agents for more centralized training. To\nfurther ensure the decentralized execution, we propose a smooth model pruning\nmechanism to progressively constraint the agent communication into a closed one\nwithout degradation in agent cooperation capability. Empirical evaluations on\nStarCraft II micromanagement and Google Research Football benchmarks\ndemonstrate that the proposed framework achieves superior performance compared\nwith the state-of-the-art counterparts. Our code will be made publicly\navailable.",
                "A Model-Based Solution to the Offline Multi-Agent Reinforcement Learning\n  Coordination Problem\nTraining multiple agents to coordinate is an essential problem with\napplications in robotics, game theory, economics, and social sciences. However,\nmost existing Multi-Agent Reinforcement Learning (MARL) methods are online and\nthus impractical for real-world applications in which collecting new\ninteractions is costly or dangerous. While these algorithms should leverage\noffline data when available, doing so gives rise to what we call the offline\ncoordination problem. Specifically, we identify and formalize the strategy\nagreement (SA) and the strategy fine-tuning (SFT) coordination challenges, two\nissues at which current offline MARL algorithms fail. Concretely, we reveal\nthat the prevalent model-free methods are severely deficient and cannot handle\ncoordination-intensive offline multi-agent tasks in either toy or MuJoCo\ndomains. To address this setback, we emphasize the importance of inter-agent\ninteractions and propose the very first model-based offline MARL method. Our\nresulting algorithm, Model-based Offline Multi-Agent Proximal Policy\nOptimization (MOMA-PPO) generates synthetic interaction data and enables agents\nto converge on a strategy while fine-tuning their policies accordingly. This\nsimple model-based solution solves the coordination-intensive offline tasks,\nsignificantly outperforming the prevalent model-free methods even under severe\npartial observability and with learned world models.",
                "Attention Schema in Neural Agents\nAttention has become a common ingredient in deep learning architectures. It\nadds a dynamical selection of information on top of the static selection of\ninformation supported by weights. In the same way, we can imagine a\nhigher-order informational filter built on top of attention: an Attention\nSchema (AS), namely, a descriptive and predictive model of attention. In\ncognitive neuroscience, Attention Schema Theory (AST) supports this idea of\ndistinguishing attention from AS. A strong prediction of this theory is that an\nagent can use its own AS to also infer the states of other agents' attention\nand consequently enhance coordination with other agents. As such, multi-agent\nreinforcement learning would be an ideal setting to experimentally test the\nvalidity of AST. We explore different ways in which attention and AS interact\nwith each other. Our preliminary results indicate that agents that implement\nthe AS as a recurrent internal control achieve the best performance. In\ngeneral, these exploratory experiments suggest that equipping artificial agents\nwith a model of attention can enhance their social intelligence.",
                "Reinforcement Learning With Reward Machines in Stochastic Games\nWe investigate multi-agent reinforcement learning for stochastic games with\ncomplex tasks, where the reward functions are non-Markovian. We utilize reward\nmachines to incorporate high-level knowledge of complex tasks. We develop an\nalgorithm called Q-learning with reward machines for stochastic games (QRM-SG),\nto learn the best-response strategy at Nash equilibrium for each agent. In\nQRM-SG, we define the Q-function at a Nash equilibrium in augmented state\nspace. The augmented state space integrates the state of the stochastic game\nand the state of reward machines. Each agent learns the Q-functions of all\nagents in the system. We prove that Q-functions learned in QRM-SG converge to\nthe Q-functions at a Nash equilibrium if the stage game at each time step\nduring learning has a global optimum point or a saddle point, and the agents\nupdate Q-functions based on the best-response strategy at this point. We use\nthe Lemke-Howson method to derive the best-response strategy given current\nQ-functions. The three case studies show that QRM-SG can learn the\nbest-response strategies effectively. QRM-SG learns the best-response\nstrategies after around 7500 episodes in Case Study I, 1000 episodes in Case\nStudy II, and 1500 episodes in Case Study III, while baseline methods such as\nNash Q-learning and MADDPG fail to converge to the Nash equilibrium in all\nthree case studies."
            ],
            "interesting paper": 1
        },
        {
            "papers": [
                "Towards Cognitive Bots: Architectural Research Challenges\nSoftware bots operating in multiple virtual digital platforms must understand\nthe platforms' affordances and behave like human users. Platform affordances or\nfeatures differ from one application platform to another or through a life\ncycle, requiring such bots to be adaptable. Moreover, bots in such platforms\ncould cooperate with humans or other software agents for work or to learn\nspecific behavior patterns. However, present-day bots, particularly chatbots,\nother than language processing and prediction, are far from reaching a human\nuser's behavior level within complex business information systems. They lack\nthe cognitive capabilities to sense and act in such virtual environments,\nrendering their development a challenge to artificial general intelligence\nresearch. In this study, we problematize and investigate assumptions in\nconceptualizing software bot architecture by directing attention to significant\narchitectural research challenges in developing cognitive bots endowed with\ncomplex behavior for operation on information systems. As an outlook, we\npropose alternate architectural assumptions to consider in future bot design\nand bot development frameworks.",
                "How Do UX Practitioners Communicate AI as a Design Material? Artifacts,\n  Conceptions, and Propositions\nUX practitioners (UXPs) face novel challenges when working with and\ncommunicating artificial intelligence (AI) as a design material. We explore how\nUXPs communicate AI concepts when given hands-on experience training and\nexperimenting with AI models. To do so, we conducted a task-based design study\nwith 27 UXPs in which they prototyped and created a design presentation for a\nAI-enabled interface while having access to a simple AI model training tool.\nThrough analyzing UXPs' design presentations and post-activity interviews, we\nfound that although UXPs struggled to clearly communicate some AI concepts,\ntinkering with AI broadened common ground when communicating with technical\nstakeholders. UXPs also identified key risks and benefits of AI in their\ndesigns, and proposed concrete next steps for both UX and AI work. We conclude\nwith a sensitizing concept and recommendations for design and AI tools to\nenhance multi-stakeholder communication and collaboration when crafting\nhuman-centered AI experiences.",
                "Mindstorms in Natural Language-Based Societies of Mind\nBoth Minsky's \"society of mind\" and Schmidhuber's \"learning to think\" inspire\ndiverse societies of large multimodal neural networks (NNs) that solve problems\nby interviewing each other in a \"mindstorm.\" Recent implementations of NN-based\nsocieties of minds consist of large language models (LLMs) and other NN-based\nexperts communicating through a natural language interface. In doing so, they\novercome the limitations of single LLMs, improving multimodal zero-shot\nreasoning. In these natural language-based societies of mind (NLSOMs), new\nagents -- all communicating through the same universal symbolic language -- are\neasily added in a modular fashion. To demonstrate the power of NLSOMs, we\nassemble and experiment with several of them (having up to 129 members),\nleveraging mindstorms in them to solve some practical AI tasks: visual question\nanswering, image captioning, text-to-image synthesis, 3D generation, egocentric\nretrieval, embodied AI, and general language-based task solving. We view this\nas a starting point towards much larger NLSOMs with billions of agents-some of\nwhich may be humans. And with this emergence of great societies of\nheterogeneous minds, many new research questions have suddenly become paramount\nto the future of artificial intelligence. What should be the social structure\nof an NLSOM? What would be the (dis)advantages of having a monarchical rather\nthan a democratic structure? How can principles of NN economies be used to\nmaximize the total reward of a reinforcement learning NLSOM? In this work, we\nidentify, discuss, and try to answer some of these questions.",
                "MADiff: Offline Multi-agent Learning with Diffusion Models\nDiffusion model (DM) recently achieved huge success in various scenarios\nincluding offline reinforcement learning, where the diffusion planner learn to\ngenerate desired trajectories during online evaluations. However, despite the\neffectiveness in single-agent learning, it remains unclear how DMs can operate\nin multi-agent problems, where agents can hardly complete teamwork without good\ncoordination by independently modeling each agent's trajectories. In this\npaper, we propose MADiff, a novel generative multi-agent learning framework to\ntackle this problem. MADiff is realized with an attention-based diffusion model\nto model the complex coordination among behaviors of multiple agents. To the\nbest of our knowledge, MADiff is the first diffusion-based multi-agent learning\nframework, which behaves as both a decentralized policy and a centralized\ncontroller. During decentralized executions, MADiff simultaneously performs\nteammate modeling, and the centralized controller can also be applied in\nmulti-agent trajectory predictions. Our experiments show the superior\nperformance of MADiff compared to baseline algorithms in a wide range of\nmulti-agent learning tasks, which emphasizes the effectiveness of MADiff in\nmodeling complex multi-agent interactions. Our code is available at\nhttps://github.com/zbzhu99/madiff.",
                "Attention Schema in Neural Agents\nAttention has become a common ingredient in deep learning architectures. It\nadds a dynamical selection of information on top of the static selection of\ninformation supported by weights. In the same way, we can imagine a\nhigher-order informational filter built on top of attention: an Attention\nSchema (AS), namely, a descriptive and predictive model of attention. In\ncognitive neuroscience, Attention Schema Theory (AST) supports this idea of\ndistinguishing attention from AS. A strong prediction of this theory is that an\nagent can use its own AS to also infer the states of other agents' attention\nand consequently enhance coordination with other agents. As such, multi-agent\nreinforcement learning would be an ideal setting to experimentally test the\nvalidity of AST. We explore different ways in which attention and AS interact\nwith each other. Our preliminary results indicate that agents that implement\nthe AS as a recurrent internal control achieve the best performance. In\ngeneral, these exploratory experiments suggest that equipping artificial agents\nwith a model of attention can enhance their social intelligence.",
                "A Categorical Representation Language and Computational System for\n  Knowledge-Based Planning\nClassical planning representation languages based on first-order logic have\npreliminarily been used to model and solve robotic task planning problems.\nWider adoption of these representation languages, however, is hindered by the\nlimitations present when managing implicit world changes with concise action\nmodels. To address this problem, we propose an alternative approach to\nrepresenting and managing updates to world states during planning. Based on the\ncategory-theoretic concepts of $\\mathsf{C}$-sets and double-pushout rewriting\n(DPO), our proposed representation can effectively handle structured knowledge\nabout world states that support domain abstractions at all levels. It\nformalizes the semantics of predicates according to a user-provided ontology\nand preserves the semantics when transitioning between world states. This\nmethod provides a formal semantics for using knowledge graphs and relational\ndatabases to model world states and updates in planning. In this paper, we\nconceptually compare our category-theoretic representation with the classical\nplanning representation. We show that our proposed representation has\nadvantages over the classical representation in terms of handling implicit\npreconditions and effects, and provides a more structured framework in which to\nmodel and solve planning problems."
            ],
            "interesting paper": 3
        },
        {
            "papers": [
                "On the Value of Myopic Behavior in Policy Reuse\nLeveraging learned strategies in unfamiliar scenarios is fundamental to human\nintelligence. In reinforcement learning, rationally reusing the policies\nacquired from other tasks or human experts is critical for tackling problems\nthat are difficult to learn from scratch. In this work, we present a framework\ncalled Selective Myopic bEhavior Control~(SMEC), which results from the insight\nthat the short-term behaviors of prior policies are sharable across tasks. By\nevaluating the behaviors of prior policies via a hybrid value function\narchitecture, SMEC adaptively aggregates the sharable short-term behaviors of\nprior policies and the long-term behaviors of the task policy, leading to\ncoordinated decisions. Empirical results on a collection of manipulation and\nlocomotion tasks demonstrate that SMEC outperforms existing methods, and\nvalidate the ability of SMEC to leverage related prior policies.",
                "Integrating Action Knowledge and LLMs for Task Planning and Situation\n  Handling in Open Worlds\nTask planning systems have been developed to help robots use human knowledge\n(about actions) to complete long-horizon tasks. Most of them have been\ndeveloped for \"closed worlds\" while assuming the robot is provided with\ncomplete world knowledge. However, the real world is generally open, and the\nrobots frequently encounter unforeseen situations that can potentially break\nthe planner's completeness. Could we leverage the recent advances on\npre-trained Large Language Models (LLMs) to enable classical planning systems\nto deal with novel situations?\n  This paper introduces a novel framework, called COWP, for open-world task\nplanning and situation handling. COWP dynamically augments the robot's action\nknowledge, including the preconditions and effects of actions, with\ntask-oriented commonsense knowledge. COWP embraces the openness from LLMs, and\nis grounded to specific domains via action knowledge. For systematic\nevaluations, we collected a dataset that includes 1,085 execution-time\nsituations. Each situation corresponds to a state instance wherein a robot is\npotentially unable to complete a task using a solution that normally works.\nExperimental results show that our approach outperforms competitive baselines\nfrom the literature in the success rate of service tasks. Additionally, we have\ndemonstrated COWP using a mobile manipulator. Supplementary materials are\navailable at: https://cowplanning.github.io/",
                "Probing reaction channels via reinforcement learning\nWe propose a reinforcement learning based method to identify important\nconfigurations that connect reactant and product states along chemical reaction\npaths. By shooting multiple trajectories from these configurations, we can\ngenerate an ensemble of configurations that concentrate on the transition path\nensemble. This configuration ensemble can be effectively employed in a neural\nnetwork-based partial differential equation solver to obtain an approximation\nsolution of a restricted Backward Kolmogorov equation, even when the dimension\nof the problem is very high. The resulting solution, known as the committor\nfunction, encodes mechanistic information for the reaction and can in turn be\nused to evaluate reaction rates.",
                "Observation Denoising in CYRUS Soccer Simulation 2D Team For RoboCup\n  2023\nThe RoboCup competitions hold various leagues, and the Soccer Simulation 2D\nLeague is a major one among them. Soccer Simulation 2D (SS2D) match involves\ntwo teams, including 11 players and a coach, competing against each other. The\nplayers can only communicate with the Soccer Simulation Server during the game.\nThis paper presents the latest research of the CYRUS soccer simulation 2D team,\nthe champion of RoboCup 2021. We will explain our denoising idea powered by\nlong short-term memory networks (LSTM) and deep neural networks (DNN). The\nCYRUS team uses the CYRUS2D base code that was developed based on the Helios\nand Gliders bases.",
                "Optimization's Neglected Normative Commitments\nOptimization is offered as an objective approach to resolving complex,\nreal-world decisions involving uncertainty and conflicting interests. It drives\nbusiness strategies as well as public policies and, increasingly, lies at the\nheart of sophisticated machine learning systems. A paradigm used to approach\npotentially high-stakes decisions, optimization relies on abstracting the real\nworld to a set of decision(s), objective(s) and constraint(s). Drawing from the\nmodeling process and a range of actual cases, this paper describes the\nnormative choices and assumptions that are necessarily part of using\noptimization. It then identifies six emergent problems that may be neglected:\n1) Misspecified values can yield optimizations that omit certain imperatives\naltogether or incorporate them incorrectly as a constraint or as part of the\nobjective, 2) Problematic decision boundaries can lead to faulty modularity\nassumptions and feedback loops, 3) Failing to account for multiple agents'\ndivergent goals and decisions can lead to policies that serve only certain\nnarrow interests, 4) Mislabeling and mismeasurement can introduce bias and\nimprecision, 5) Faulty use of relaxation and approximation methods,\nunaccompanied by formal characterizations and guarantees, can severely impede\napplicability, and 6) Treating optimization as a justification for action,\nwithout specifying the necessary contextual information, can lead to ethically\ndubious or faulty decisions. Suggestions are given to further understand and\ncurb the harms that can arise when optimization is used wrongfully.",
                "Assumption Generation for the Verification of Learning-Enabled\n  Autonomous Systems\nProviding safety guarantees for autonomous systems is difficult as these\nsystems operate in complex environments that require the use of\nlearning-enabled components, such as deep neural networks (DNNs) for visual\nperception. DNNs are hard to analyze due to their size (they can have thousands\nor millions of parameters), lack of formal specifications (DNNs are typically\nlearnt from labeled data, in the absence of any formal requirements), and\nsensitivity to small changes in the environment. We present an assume-guarantee\nstyle compositional approach for the formal verification of system-level safety\nproperties of such autonomous systems. Our insight is that we can analyze the\nsystem in the absence of the DNN perception components by automatically\nsynthesizing assumptions on the DNN behaviour that guarantee the satisfaction\nof the required safety properties. The synthesized assumptions are the weakest\nin the sense that they characterize the output sequences of all the possible\nDNNs that, plugged into the autonomous system, guarantee the required safety\nproperties. The assumptions can be leveraged as run-time monitors over a\ndeployed DNN to guarantee the safety of the overall system; they can also be\nmined to extract local specifications for use during training and testing of\nDNNs. We illustrate our approach on a case study taken from the autonomous\nairplanes domain that uses a complex DNN for perception."
            ],
            "interesting paper": 2
        },
        {
            "papers": [
                "AI Coach Assist: An Automated Approach for Call Recommendation in\n  Contact Centers for Agent Coaching\nIn recent years, the utilization of Artificial Intelligence (AI) in the\ncontact center industry is on the rise. One area where AI can have a\nsignificant impact is in the coaching of contact center agents. By analyzing\ncall transcripts using Natural Language Processing (NLP) techniques, it would\nbe possible to quickly determine which calls are most relevant for coaching\npurposes. In this paper, we present AI Coach Assist, which leverages the\npre-trained transformer-based language models to determine whether a given call\nis coachable or not based on the quality assurance (QA) questions asked by the\ncontact center managers or supervisors. The system was trained and evaluated on\na large dataset collected from real-world contact centers and provides an\neffective way to recommend calls to the contact center managers that are more\nlikely to contain coachable moments. Our experimental findings demonstrate the\npotential of AI Coach Assist to improve the coaching process, resulting in\nenhancing the performance of contact center agents.",
                "MLOps: A Step Forward to Enterprise Machine Learning\nMachine Learning Operations (MLOps) is becoming a highly crucial part of\nbusinesses looking to capitalize on the benefits of AI and ML models. This\nresearch presents a detailed review of MLOps, its benefits, difficulties,\nevolutions, and important underlying technologies such as MLOps frameworks,\nDocker, GitHub actions, and Kubernetes. The MLOps workflow, which includes\nmodel design, deployment, and operations, is explained in detail along with the\nvarious tools necessary for both model and data exploration and deployment.\nThis article also puts light on the end-to-end production of ML projects using\nvarious maturity levels of automated pipelines, with the least at no automation\nat all and the highest with complete CI/CD and CT capabilities. Furthermore, a\ndetailed example of an enterprise-level MLOps project for an object detection\nservice is used to explain the workflow of the technology in a real-world\nscenario. For this purpose, a web application hosting a pre-trained model from\nTensorFlow 2 Model Zoo is packaged and deployed to the internet making sure\nthat the system is scalable, reliable, and optimized for deployment at an\nenterprise level.",
                "Modeling Dynamic Environments with Scene Graph Memory\nEmbodied AI agents that search for objects in large environments such as\nhouseholds often need to make efficient decisions by predicting object\nlocations based on partial information. We pose this as a new type of link\nprediction problem: link prediction on partially observable dynamic graphs. Our\ngraph is a representation of a scene in which rooms and objects are nodes, and\ntheir relationships are encoded in the edges; only parts of the changing graph\nare known to the agent at each timestep. This partial observability poses a\nchallenge to existing link prediction approaches, which we address. We propose\na novel state representation -- Scene Graph Memory (SGM) -- with captures the\nagent's accumulated set of observations, as well as a neural net architecture\ncalled a Node Edge Predictor (NEP) that extracts information from the SGM to\nsearch efficiently. We evaluate our method in the Dynamic House Simulator, a\nnew benchmark that creates diverse dynamic graphs following the semantic\npatterns typically seen at homes, and show that NEP can be trained to predict\nthe locations of objects in a variety of environments with diverse object\nmovement dynamics, outperforming baselines both in terms of new scene\nadaptability and overall accuracy. The codebase and more can be found at\nhttps://www.scenegraphmemory.com.",
                "Integrating Action Knowledge and LLMs for Task Planning and Situation\n  Handling in Open Worlds\nTask planning systems have been developed to help robots use human knowledge\n(about actions) to complete long-horizon tasks. Most of them have been\ndeveloped for \"closed worlds\" while assuming the robot is provided with\ncomplete world knowledge. However, the real world is generally open, and the\nrobots frequently encounter unforeseen situations that can potentially break\nthe planner's completeness. Could we leverage the recent advances on\npre-trained Large Language Models (LLMs) to enable classical planning systems\nto deal with novel situations?\n  This paper introduces a novel framework, called COWP, for open-world task\nplanning and situation handling. COWP dynamically augments the robot's action\nknowledge, including the preconditions and effects of actions, with\ntask-oriented commonsense knowledge. COWP embraces the openness from LLMs, and\nis grounded to specific domains via action knowledge. For systematic\nevaluations, we collected a dataset that includes 1,085 execution-time\nsituations. Each situation corresponds to a state instance wherein a robot is\npotentially unable to complete a task using a solution that normally works.\nExperimental results show that our approach outperforms competitive baselines\nfrom the literature in the success rate of service tasks. Additionally, we have\ndemonstrated COWP using a mobile manipulator. Supplementary materials are\navailable at: https://cowplanning.github.io/",
                "On the Value of Myopic Behavior in Policy Reuse\nLeveraging learned strategies in unfamiliar scenarios is fundamental to human\nintelligence. In reinforcement learning, rationally reusing the policies\nacquired from other tasks or human experts is critical for tackling problems\nthat are difficult to learn from scratch. In this work, we present a framework\ncalled Selective Myopic bEhavior Control~(SMEC), which results from the insight\nthat the short-term behaviors of prior policies are sharable across tasks. By\nevaluating the behaviors of prior policies via a hybrid value function\narchitecture, SMEC adaptively aggregates the sharable short-term behaviors of\nprior policies and the long-term behaviors of the task policy, leading to\ncoordinated decisions. Empirical results on a collection of manipulation and\nlocomotion tasks demonstrate that SMEC outperforms existing methods, and\nvalidate the ability of SMEC to leverage related prior policies.",
                "Assumption Generation for the Verification of Learning-Enabled\n  Autonomous Systems\nProviding safety guarantees for autonomous systems is difficult as these\nsystems operate in complex environments that require the use of\nlearning-enabled components, such as deep neural networks (DNNs) for visual\nperception. DNNs are hard to analyze due to their size (they can have thousands\nor millions of parameters), lack of formal specifications (DNNs are typically\nlearnt from labeled data, in the absence of any formal requirements), and\nsensitivity to small changes in the environment. We present an assume-guarantee\nstyle compositional approach for the formal verification of system-level safety\nproperties of such autonomous systems. Our insight is that we can analyze the\nsystem in the absence of the DNN perception components by automatically\nsynthesizing assumptions on the DNN behaviour that guarantee the satisfaction\nof the required safety properties. The synthesized assumptions are the weakest\nin the sense that they characterize the output sequences of all the possible\nDNNs that, plugged into the autonomous system, guarantee the required safety\nproperties. The assumptions can be leveraged as run-time monitors over a\ndeployed DNN to guarantee the safety of the overall system; they can also be\nmined to extract local specifications for use during training and testing of\nDNNs. We illustrate our approach on a case study taken from the autonomous\nairplanes domain that uses a complex DNN for perception."
            ],
            "interesting paper": 3
        },
        {
            "papers": [
                "The Computational Complexity of Single-Player Imperfect-Recall Games\nWe study single-player extensive-form games with imperfect recall, such as\nthe Sleeping Beauty problem or the Absentminded Driver game. For such games,\ntwo natural equilibrium concepts have been proposed as alternative solution\nconcepts to ex-ante optimality. One equilibrium concept uses generalized double\nhalving (GDH) as a belief system and evidential decision theory (EDT), and\nanother one uses generalized thirding (GT) as a belief system and causal\ndecision theory (CDT). Our findings relate those three solution concepts of a\ngame to solution concepts of a polynomial maximization problem: global optima,\noptimal points with respect to subsets of variables and Karush-Kuhn-Tucker\n(KKT) points. Based on these correspondences, we are able to settle various\ncomplexity-theoretic questions on the computation of such strategies. For\nex-ante optimality and (EDT,GDH)-equilibria, we obtain NP-hardness and\ninapproximability, and for (CDT,GT)-equilibria we obtain CLS-completeness\nresults.",
                "Action valuation of on- and off-ball soccer players based on multi-agent\n  deep reinforcement learning\nAnalysis of invasive sports such as soccer is challenging because the game\nsituation changes continuously in time and space, and multiple agents\nindividually recognize the game situation and make decisions. Previous studies\nusing deep reinforcement learning have often considered teams as a single agent\nand valued the teams and players who hold the ball in each discrete event. Then\nit was challenging to value the actions of multiple players, including players\nfar from the ball, in a spatiotemporally continuous state space. In this paper,\nwe propose a method of valuing possible actions for on- and off-ball soccer\nplayers in a single holistic framework based on multi-agent deep reinforcement\nlearning. We consider a discrete action space in a continuous state space that\nmimics that of Google research football and leverages supervised learning for\nactions in reinforcement learning. In the experiment, we analyzed the\nrelationships with conventional indicators, season goals, and game ratings by\nexperts, and showed the effectiveness of the proposed method. Our approach can\nassess how multiple players move continuously throughout the game, which is\ndifficult to be discretized or labeled but vital for teamwork, scouting, and\nfan engagement.",
                "The Leximin Approach for a Sequence of Collective Decisions\nIn many situations, several agents need to make a sequence of decisions. For\nexample, a group of workers that needs to decide where their weekly meeting\nshould take place. In such situations, a decision-making mechanism must\nconsider fairness notions. In this paper, we analyze the fairness of three\nknown mechanisms: round-robin, maximum Nash welfare, and leximin. We consider\nboth offline and online settings, and concentrate on the fairness notion of\nproportionality and its relaxations. Specifically, in the offline setting, we\nshow that the three mechanisms fail to find a proportional or\napproximate-proportional outcome, even if such an outcome exists. We thus\nintroduce a new fairness property that captures this requirement, and show that\na variant of the leximin mechanism satisfies the new fairness property. In the\nonline setting, we show that it is impossible to guarantee proportionality or\nits relaxations. We thus consider a natural restriction on the agents'\npreferences, and show that the leximin mechanism guarantees the best possible\nadditive approximation to proportionality and satisfies all the relaxations of\nproportionality.",
                "Taming AI Bots: Controllability of Neural States in Large Language\n  Models\nWe tackle the question of whether an agent can, by suitable choice of\nprompts, control an AI bot to any state. To that end, we first introduce a\nformal definition of ``meaning'' that is amenable to analysis. Then, we\ncharacterize ``meaningful data'' on which large language models (LLMs) are\nostensibly trained, and ``well-trained LLMs'' through conditions that are\nlargely met by today's LLMs. While a well-trained LLM constructs an embedding\nspace of meanings that is Euclidean, meanings themselves do not form a vector\n(linear) subspace, but rather a quotient space within. We then characterize the\nsubset of meanings that can be reached by the state of the LLMs for some input\nprompt, and show that a well-trained bot can reach any meaning albeit with\nsmall probability. We then introduce a stronger notion of controllability as\n{\\em almost certain reachability}, and show that, when restricted to the space\nof meanings, an AI bot is controllable. We do so after introducing a functional\ncharacterization of attentive AI bots, and finally derive necessary and\nsufficient conditions for controllability. The fact that AI bots are\ncontrollable means that an adversary could steer them towards any state.\nHowever, the sampling process can be designed to counteract adverse actions and\navoid reaching undesirable regions of state space before their boundary is\ncrossed.",
                "Diffusion Model is an Effective Planner and Data Synthesizer for\n  Multi-Task Reinforcement Learning\nDiffusion models have demonstrated highly-expressive generative capabilities\nin vision and NLP. Recent studies in reinforcement learning (RL) have shown\nthat diffusion models are also powerful in modeling complex policies or\ntrajectories in offline datasets. However, these works have been limited to\nsingle-task settings where a generalist agent capable of addressing multi-task\npredicaments is absent. In this paper, we aim to investigate the effectiveness\nof a single diffusion model in modeling large-scale multi-task offline data,\nwhich can be challenging due to diverse and multimodal data distribution.\nSpecifically, we propose Multi-Task Diffusion Model (\\textsc{MTDiff}), a\ndiffusion-based method that incorporates Transformer backbones and prompt\nlearning for generative planning and data synthesis in multi-task offline\nsettings. \\textsc{MTDiff} leverages vast amounts of knowledge available in\nmulti-task data and performs implicit knowledge sharing among tasks. For\ngenerative planning, we find \\textsc{MTDiff} outperforms state-of-the-art\nalgorithms across 50 tasks on Meta-World and 8 maps on Maze2D. For data\nsynthesis, \\textsc{MTDiff} generates high-quality data for testing tasks given\na single demonstration as a prompt, which enhances the low-quality datasets for\neven unseen tasks.",
                "Safety of autonomous vehicles: A survey on Model-based vs. AI-based\n  approaches\nThe growing advancements in Autonomous Vehicles (AVs) have emphasized the\ncritical need to prioritize the absolute safety of AV maneuvers, especially in\ndynamic and unpredictable environments or situations. This objective becomes\neven more challenging due to the uniqueness of every traffic\nsituation/condition. To cope with all these very constrained and complex\nconfigurations, AVs must have appropriate control architectures with reliable\nand real-time Risk Assessment and Management Strategies (RAMS). These targeted\nRAMS must lead to reduce drastically the navigation risks. However, the lack of\nsafety guarantees proves, which is one of the key challenges to be addressed,\nlimit drastically the ambition to introduce more broadly AVs on our roads and\nrestrict the use of AVs to very limited use cases. Therefore, the focus and the\nambition of this paper is to survey research on autonomous vehicles while\nfocusing on the important topic of safety guarantee of AVs. For this purpose,\nit is proposed to review research on relevant methods and concepts defining an\noverall control architecture for AVs, with an emphasis on the safety assessment\nand decision-making systems composing these architectures. Moreover, it is\nintended through this reviewing process to highlight researches that use either\nmodel-based methods or AI-based approaches. This is performed while emphasizing\nthe strengths and weaknesses of each methodology and investigating the research\nthat proposes a comprehensive multi-modal design that combines model-based and\nAI approaches. This paper ends with discussions on the methods used to\nguarantee the safety of AVs namely: safety verification techniques and the\nstandardization/generalization of safety frameworks."
            ],
            "interesting paper": 2
        },
        {
            "papers": [
                "Re-imagining health and well-being in low resource African settings\n  using an augmented AI system and a 3D digital twin\nThis paper discusses and explores the potential and relevance of recent\ndevelopments in artificial intelligence (AI) and digital twins for health and\nwell-being in low-resource African countries. We use the case of public health\nemergency response to disease outbreaks and epidemic control. There is\npotential to take advantage of the increasing availability of data and\ndigitization to develop advanced AI methods for analysis and prediction. Using\nan AI systems perspective, we review emerging trends in AI systems and digital\ntwins and propose an initial augmented AI system architecture to illustrate how\nan AI system can work with a 3D digital twin to address public health goals. We\nhighlight scientific knowledge discovery, continual learning, pragmatic\ninteroperability, and interactive explanation and decision-making as essential\nresearch challenges for AI systems and digital twins.",
                "Towards Autonomous and Safe Last-mile Deliveries with AI-augmented\n  Self-driving Delivery Robots\nIn addition to its crucial impact on customer satisfaction, last-mile\ndelivery (LMD) is notorious for being the most time-consuming and costly stage\nof the shipping process. Pressing environmental concerns combined with the\nrecent surge of e-commerce sales have sparked renewed interest in automation\nand electrification of last-mile logistics. To address the hurdles faced by\nexisting robotic couriers, this paper introduces a customer-centric and\nsafety-conscious LMD system for small urban communities based on AI-assisted\nautonomous delivery robots. The presented framework enables end-to-end\nautomation and optimization of the logistic process while catering for\nreal-world imposed operational uncertainties, clients' preferred time\nschedules, and safety of pedestrians. To this end, the integrated optimization\ncomponent is modeled as a robust variant of the Cumulative Capacitated Vehicle\nRouting Problem with Time Windows, where routes are constructed under uncertain\ntravel times with an objective to minimize the total latency of deliveries\n(i.e., the overall waiting time of customers, which can negatively affect their\nsatisfaction). We demonstrate the proposed LMD system's utility through\nreal-world trials in a university campus with a single robotic courier.\nImplementation aspects as well as the findings and practical insights gained\nfrom the deployment are discussed in detail. Lastly, we round up the\ncontributions with numerical simulations to investigate the scalability of the\ndeveloped mathematical formulation with respect to the number of robotic\nvehicles and customers.",
                "The Digital Divide in Process Safety: Quantitative Risk Analysis of\n  Human-AI Collaboration\nDigital technologies have dramatically accelerated the digital transformation\nin process industries, boosted new industrial applications, upgraded the\nproduction system, and enhanced operational efficiency. In contrast, the\nchallenges and gaps between human and artificial intelligence (AI) have become\nmore and more prominent, whereas the digital divide in process safety is\naggregating. The study attempts to address the following questions: (i)What is\nAI in the process safety context? (ii)What is the difference between AI and\nhumans in process safety? (iii)How do AI and humans collaborate in process\nsafety? (iv)What are the challenges and gaps in human-AI collaboration? (v)How\nto quantify the risk of human-AI collaboration in process safety? Qualitative\nrisk analysis based on brainstorming and literature review, and quantitative\nrisk analysis based on layer of protection analysis (LOPA) and Bayesian network\n(BN), were applied to explore and model. The importance of human reliability\nshould be stressed in the digital age, not usually to increase the reliability\nof AI, and human-centered AI design in process safety needs to be propagated.",
                "Towards a Technology-Driven Adaptive Decision Support System for\n  Integrated Pavement and Maintenance strategies (TDADSS-IPM): focus on risk\n  assessment framework for climate change adaptation\nDecision Support Systems for pavement and maintenance strategies have\ntraditionally been designed as silos led to local optimum systems. Moreover,\nsince big data usage didn't exist as result of Industry 4.0 as of today, DSSs\nwere not initially designed adaptive to the sources of uncertainties led to\nrigid decisions. Motivated by the vulnerability of the road assets to the\nclimate phenomena, this paper takes a visionary step towards introducing a\nTechnology-Driven Adaptive Decision Support System for Integrated Pavement and\nMaintenance activities called TDADSS-IPM. As part of such DSS, a bottom-up risk\nassessment model is met via Bayesian Belief Networks (BBN) to realize the\nactual condition of the Danish roads due to weather condition. Such model fills\nthe gaps in the knowledge domain and develops a platform that can be trained\nover time, and applied in real-time to the actual event.",
                "AI Audit: A Card Game to Reflect on Everyday AI Systems\nAn essential element of K-12 AI literacy is educating learners about the\nethical and societal implications of AI systems. Previous work in AI ethics\nliteracy have developed curriculum and classroom activities that engage\nlearners in reflecting on the ethical implications of AI systems and developing\nresponsible AI. There is little work in using game-based learning methods in AI\nliteracy. Games are known to be compelling media to teach children about\ncomplex STEM concepts. In this work, we developed a competitive card game for\nmiddle and high school students called \"AI Audit\" where they play as AI\nstart-up founders building novel AI-powered technology. Players can challenge\nother players with potential harms of their technology or defend their own\nbusinesses by features that mitigate these harms. The game mechanics reward\nsystems that are ethically developed or that take steps to mitigate potential\nharms. In this paper, we present the game design, teacher resources for\nclassroom deployment and early playtesting results. We discuss our reflections\nabout using games as teaching tools for AI literacy in K-12 classrooms.",
                "IoT based Personal Voice Assistant\nToday, technological advancement is increasing day by day. Earlier, there was\nonly a computer system in which we could only perform a few tasks. But now,\nmachine learning, artificial intelligence, deep learning, and a few more\ntechnologies have made computer systems so advanced that we can perform any\ntype of task. In this era of advancement, if people are still struggling to\ninteract using various input devices, then it's not worth it. For this reason,\nwe developed a voice assistant using Python that allows the user to run any\ntype of command in Linux without interaction with the keyboard. The main task\nof the voice assistant is to minimize the use of input devices like the\nkeyboard and mouse. It will also reduce hardware space and cost."
            ],
            "interesting paper": 2
        },
        {
            "papers": [
                "Towards a Unifying Model of Rationality in Multiagent Systems\nMultiagent systems deployed in the real world need to cooperate with other\nagents (including humans) nearly as effectively as these agents cooperate with\none another. To design such AI, and provide guarantees of its effectiveness, we\nneed to clearly specify what types of agents our AI must be able to cooperate\nwith. In this work we propose a generic model of socially intelligent agents,\nwhich are individually rational learners that are also able to cooperate with\none another (in the sense that their joint behavior is Pareto efficient). We\ndefine rationality in terms of the regret incurred by each agent over its\nlifetime, and show how we can construct socially intelligent agents for\ndifferent forms of regret. We then discuss the implications of this model for\nthe development of \"robust\" MAS that can cooperate with a wide variety of\nsocially intelligent agents.",
                "Steering control of payoff-maximizing players in adaptive learning\n  dynamics\nEvolutionary game theory provides a mathematical foundation for\ncross-disciplinary fertilization, especially for integrating ideas from\nartificial intelligence and game theory. Such integration offers a transparent\nand rigorous approach to complex decision-making problems in a variety of\nimportant contexts, ranging from evolutionary computation to machine behavior.\nDespite the astronomically huge individual behavioral strategy space for\ninteractions in the iterated Prisoner's Dilemma (IPD) games, the so-called\nZero-Determinant (ZD) strategies is a set of rather simple memory-one\nstrategies yet can unilaterally set a linear payoff relationship between\nthemselves and their opponent. Although the witting of ZD strategies gives\nplayers an upper hand in the IPD games, we find and characterize unbending\nstrategies that can force ZD players to be fair in their own interest.\nMoreover, our analysis reveals the ubiquity of unbending properties in common\nIPD strategies which are previously overlooked. In this work, we demonstrate\nthe important steering role of unbending strategies in fostering fairness and\ncooperation in pairwise interactions. Our results will help bring a new\nperspective by means of combining game theory and multi-agent learning systems\nfor optimizing winning strategies that are robust to noises, errors, and\ndeceptions in non-zero-sum games.",
                "The Alternating-Time \u03bc-Calculus With Disjunctive Explicit Strategies\nAlternating-time temporal logic (ATL) and its extensions, including the\nalternating-time $\\mu$-calculus (AMC), serve the specification of the strategic\nabilities of coalitions of agents in concurrent game structures. The key\ningredient of the logic are path quantifiers specifying that some coalition of\nagents has a joint strategy to enforce a given goal. This basic setup has been\nextended to let some of the agents (revocably) commit to using certain named\nstrategies, as in ATL with explicit strategies (ATLES). In the present work, we\nextend ATLES with fixpoint operators and strategy disjunction, arriving at the\nalternating-time $\\mu$-calculus with disjunctive explicit strategies (AMCDES),\nwhich allows for a more flexible formulation of temporal properties (e.g.\nfairness) and, through strategy disjunction, a form of controlled\nnondeterminism in commitments. Our main result is an ExpTime upper bound for\nsatisfiability checking (which is thus ExpTime-complete). We also prove upper\nbounds QP (quasipolynomial time) and NP $\\cap$ coNP for model checking under\nfixed interpretations of explicit strategies, and NP under open interpretation.\nOur key technical tool is a treatment of the AMCDES within the generic\nframework of coalgebraic logic, which in particular reduces the analysis of\nmost reasoning tasks to the treatment of a very simple one-step logic featuring\nonly propositional operators and next-step operators without nesting; we give a\nnew model construction principle for this one-step logic that relies on a\nset-valued variant of first-order resolution.",
                "Split Federated Learning: Speed up Model Training in Resource-Limited\n  Wireless Networks\nIn this paper, we propose a novel distributed learning scheme, named\ngroup-based split federated learning (GSFL), to speed up artificial\nintelligence (AI) model training. Specifically, the GSFL operates in a\nsplit-then-federated manner, which consists of three steps: 1) Model\ndistribution, in which the access point (AP) splits the AI models and\ndistributes the client-side models to clients; 2) Model training, in which each\nclient executes forward propagation and transmit the smashed data to the edge\nserver. The edge server executes forward and backward propagation and then\nreturns the gradient to the clients for updating local client-side models; and\n3) Model aggregation, in which edge servers aggregate the server-side and\nclient-side models. Simulation results show that the GSFL outperforms vanilla\nsplit learning and federated learning schemes in terms of overall training\nlatency while achieving satisfactory accuracy.",
                "Subequivariant Graph Reinforcement Learning in 3D Environments\nLearning a shared policy that guides the locomotion of different agents is of\ncore interest in Reinforcement Learning (RL), which leads to the study of\nmorphology-agnostic RL. However, existing benchmarks are highly restrictive in\nthe choice of starting point and target point, constraining the movement of the\nagents within 2D space. In this work, we propose a novel setup for\nmorphology-agnostic RL, dubbed Subequivariant Graph RL in 3D environments\n(3D-SGRL). Specifically, we first introduce a new set of more practical yet\nchallenging benchmarks in 3D space that allows the agent to have full\nDegree-of-Freedoms to explore in arbitrary directions starting from arbitrary\nconfigurations. Moreover, to optimize the policy over the enlarged state-action\nspace, we propose to inject geometric symmetry, i.e., subequivariance, into the\nmodeling of the policy and Q-function such that the policy can generalize to\nall directions, improving exploration efficiency. This goal is achieved by a\nnovel SubEquivariant Transformer (SET) that permits expressive message\nexchange. Finally, we evaluate the proposed method on the proposed benchmarks,\nwhere our method consistently and significantly outperforms existing approaches\non single-task, multi-task, and zero-shot generalization scenarios. Extensive\nablations are also conducted to verify our design. Code and videos are\navailable on our project page: https://alpc91.github.io/SGRL/.",
                "Partially Personalized Federated Learning: Breaking the Curse of Data\n  Heterogeneity\nWe present a partially personalized formulation of Federated Learning (FL)\nthat strikes a balance between the flexibility of personalization and\ncooperativeness of global training. In our framework, we split the variables\ninto global parameters, which are shared across all clients, and individual\nlocal parameters, which are kept private. We prove that under the right split\nof parameters, it is possible to find global parameters that allow each client\nto fit their data perfectly, and refer to the obtained problem as\noverpersonalized. For instance, the shared global parameters can be used to\nlearn good data representations, whereas the personalized layers are fine-tuned\nfor a specific client. Moreover, we present a simple algorithm for the\npartially personalized formulation that offers significant benefits to all\nclients. In particular, it breaks the curse of data heterogeneity in several\nsettings, such as training with local steps, asynchronous training, and\nByzantine-robust training."
            ],
            "interesting paper": 3
        },
        {
            "papers": [
                "RE-centric Recommendations for the Development of Trustworthy(er)\n  Autonomous Systems\nComplying with the EU AI Act (AIA) guidelines while developing and\nimplementing AI systems will soon be mandatory within the EU. However,\npractitioners lack actionable instructions to operationalise ethics during AI\nsystems development. A literature review of different ethical guidelines\nrevealed inconsistencies in the principles addressed and the terminology used\nto describe them. Furthermore, requirements engineering (RE), which is\nidentified to foster trustworthiness in the AI development process from the\nearly stages was observed to be absent in a lot of frameworks that support the\ndevelopment of ethical and trustworthy AI. This incongruous phrasing combined\nwith a lack of concrete development practices makes trustworthy AI development\nharder. To address this concern, we formulated a comparison table for the\nterminology used and the coverage of the ethical AI principles in major ethical\nAI guidelines. We then examined the applicability of ethical AI development\nframeworks for performing effective RE during the development of trustworthy AI\nsystems. A tertiary review and meta-analysis of literature discussing ethical\nAI frameworks revealed their limitations when developing trustworthy AI. Based\non our findings, we propose recommendations to address such limitations during\nthe development of trustworthy AI.",
                "An Emergency Disposal Decision-making Method with Human--Machine\n  Collaboration\nRapid developments in artificial intelligence technology have led to unmanned\nsystems replacing human beings in many fields requiring high-precision\npredictions and decisions. In modern operational environments, all job plans\nare affected by emergency events such as equipment failures and resource\nshortages, making a quick resolution critical. The use of unmanned systems to\nassist decision-making can improve resolution efficiency, but their\ndecision-making is not interpretable and may make the wrong decisions. Current\nunmanned systems require human supervision and control. Based on this, we\npropose a collaborative human--machine method for resolving unplanned events\nusing two phases: task filtering and task scheduling. In the task filtering\nphase, we propose a human--machine collaborative decision-making algorithm for\ndynamic tasks. The GACRNN model is used to predict the state of the job nodes,\nlocate the key nodes, and generate a machine-predicted resolution task list. A\nhuman decision-maker supervises the list in real time and modifies and confirms\nthe machine-predicted list through the human--machine interface. In the task\nscheduling phase, we propose a scheduling algorithm that integrates human\nexperience constraints. The steps to resolve an event are inserted into the\nnormal job sequence to schedule the resolution. We propose several\nhuman--machine collaboration methods in each phase to generate steps to resolve\nan unplanned event while minimizing the impact on the original job plan.",
                "Towards a Unifying Model of Rationality in Multiagent Systems\nMultiagent systems deployed in the real world need to cooperate with other\nagents (including humans) nearly as effectively as these agents cooperate with\none another. To design such AI, and provide guarantees of its effectiveness, we\nneed to clearly specify what types of agents our AI must be able to cooperate\nwith. In this work we propose a generic model of socially intelligent agents,\nwhich are individually rational learners that are also able to cooperate with\none another (in the sense that their joint behavior is Pareto efficient). We\ndefine rationality in terms of the regret incurred by each agent over its\nlifetime, and show how we can construct socially intelligent agents for\ndifferent forms of regret. We then discuss the implications of this model for\nthe development of \"robust\" MAS that can cooperate with a wide variety of\nsocially intelligent agents.",
                "Development of a ROS-based Architecture for Intelligent Autonomous on\n  Demand Last Mile Delivery\nThis paper presents the development of the JKU-ITS Last Mile Delivery Robot.\nThe proposed approach utilizes a combination of one 3D LIDAR, RGB-D camera, IMU\nand GPS sensor on top of a mobile robot slope mower. An embedded computer,\nrunning ROS1, is utilized to process the sensor data streams to enable 2D and\n3D Simultaneous Localization and Mapping, 2D localization and object detection\nusing a convolutional neural network.",
                "Graph Neural Network for spatiotemporal data: methods and applications\nIn the era of big data, there has been a surge in the availability of data\ncontaining rich spatial and temporal information, offering valuable insights\ninto dynamic systems and processes for applications such as weather\nforecasting, natural disaster management, intelligent transport systems, and\nprecision agriculture. Graph neural networks (GNNs) have emerged as a powerful\ntool for modeling and understanding data with dependencies to each other such\nas spatial and temporal dependencies. There is a large amount of existing work\nthat focuses on addressing the complex spatial and temporal dependencies in\nspatiotemporal data using GNNs. However, the strong interdisciplinary nature of\nspatiotemporal data has created numerous GNNs variants specifically designed\nfor distinct application domains. Although the techniques are generally\napplicable across various domains, cross-referencing these methods remains\nessential yet challenging due to the absence of a comprehensive literature\nreview on GNNs for spatiotemporal data. This article aims to provide a\nsystematic and comprehensive overview of the technologies and applications of\nGNNs in the spatiotemporal domain. First, the ways of constructing graphs from\nspatiotemporal data are summarized to help domain experts understand how to\ngenerate graphs from various types of spatiotemporal data. Then, a systematic\ncategorization and summary of existing spatiotemporal GNNs are presented to\nenable domain experts to identify suitable techniques and to support model\ndevelopers in advancing their research. Moreover, a comprehensive overview of\nsignificant applications in the spatiotemporal domain is offered to introduce a\nbroader range of applications to model developers and domain experts, assisting\nthem in exploring potential research topics and enhancing the impact of their\nwork. Finally, open challenges and future directions are discussed.",
                "Generalized Disparate Impact for Configurable Fairness Solutions in ML\nWe make two contributions in the field of AI fairness over continuous\nprotected attributes. First, we show that the Hirschfeld-Gebelein-Renyi (HGR)\nindicator (the only one currently available for such a case) is valuable but\nsubject to a few crucial limitations regarding semantics, interpretability, and\nrobustness. Second, we introduce a family of indicators that are: 1)\ncomplementary to HGR in terms of semantics; 2) fully interpretable and\ntransparent; 3) robust over finite samples; 4) configurable to suit specific\napplications. Our approach also allows us to define fine-grained constraints to\npermit certain types of dependence and forbid others selectively. By expanding\nthe available options for continuous protected attributes, our approach\nrepresents a significant contribution to the area of fair artificial\nintelligence."
            ],
            "interesting paper": 5
        },
        {
            "papers": [
                "Intent-aligned AI systems deplete human agency: the need for agency\n  foundations research in AI safety\nThe rapid advancement of artificial intelligence (AI) systems suggests that\nartificial general intelligence (AGI) systems may soon arrive. Many researchers\nare concerned that AIs and AGIs will harm humans via intentional misuse\n(AI-misuse) or through accidents (AI-accidents). In respect of AI-accidents,\nthere is an increasing effort focused on developing algorithms and paradigms\nthat ensure AI systems are aligned to what humans intend, e.g. AI systems that\nyield actions or recommendations that humans might judge as consistent with\ntheir intentions and goals. Here we argue that alignment to human intent is\ninsufficient for safe AI systems and that preservation of long-term agency of\nhumans may be a more robust standard, and one that needs to be separated\nexplicitly and a priori during optimization. We argue that AI systems can\nreshape human intention and discuss the lack of biological and psychological\nmechanisms that protect humans from loss of agency. We provide the first formal\ndefinition of agency-preserving AI-human interactions which focuses on\nforward-looking agency evaluations and argue that AI systems - not humans -\nmust be increasingly tasked with making these evaluations. We show how agency\nloss can occur in simple environments containing embedded agents that use\ntemporal-difference learning to make action recommendations. Finally, we\npropose a new area of research called \"agency foundations\" and pose four\ninitial topics designed to improve our understanding of agency in AI-human\ninteractions: benevolent game theory, algorithmic foundations of human rights,\nmechanistic interpretability of agency representation in neural-networks and\nreinforcement learning from internal states.",
                "DHRL-FNMR: An Intelligent Multicast Routing Approach Based on Deep\n  Hierarchical Reinforcement Learning in SDN\nThe optimal multicast tree problem in the Software-Defined Networking (SDN)\nmulticast routing is an NP-hard combinatorial optimization problem. Although\nexisting SDN intelligent solution methods, which are based on deep\nreinforcement learning, can dynamically adapt to complex network link state\nchanges, these methods are plagued by problems such as redundant branches,\nlarge action space, and slow agent convergence. In this paper, an SDN\nintelligent multicast routing algorithm based on deep hierarchical\nreinforcement learning is proposed to circumvent the aforementioned problems.\nFirst, the multicast tree construction problem is decomposed into two\nsub-problems: the fork node selection problem and the construction of the\noptimal path from the fork node to the destination node. Second, based on the\ninformation characteristics of SDN global network perception, the multicast\ntree state matrix, link bandwidth matrix, link delay matrix, link packet loss\nrate matrix, and sub-goal matrix are designed as the state space of intrinsic\nand meta controllers. Then, in order to mitigate the excessive action space,\nour approach constructs different action spaces at the upper and lower levels.\nThe meta-controller generates an action space using network nodes to select the\nfork node, and the intrinsic controller uses the adjacent edges of the current\nnode as its action space, thus implementing four different action selection\nstrategies in the construction of the multicast tree. To facilitate the\nintelligent agent in constructing the optimal multicast tree with greater\nspeed, we developed alternative reward strategies that distinguish between\nsingle-step node actions and multi-step actions towards multiple destination\nnodes.",
                "Best of Both Distortion Worlds\nWe study the problem of designing voting rules that take as input the ordinal\npreferences of $n$ agents over a set of $m$ alternatives and output a single\nalternative, aiming to optimize the overall happiness of the agents. The input\nto the voting rule is each agent's ranking of the alternatives from most to\nleast preferred, yet the agents have more refined (cardinal) preferences that\ncapture the intensity with which they prefer one alternative over another. To\nquantify the extent to which voting rules can optimize over the cardinal\npreferences given access only to the ordinal ones, prior work has used the\ndistortion measure, i.e., the worst-case approximation ratio between a voting\nrule's performance and the best performance achievable given the cardinal\npreferences.\n  The work on the distortion of voting rules has been largely divided into two\nworlds: utilitarian distortion and metric distortion. In the former, the\ncardinal preferences of the agents correspond to general utilities and the goal\nis to maximize a normalized social welfare. In the latter, the agents' cardinal\npreferences correspond to costs given by distances in an underlying metric\nspace and the goal is to minimize the (unnormalized) social cost. Several\ndeterministic and randomized voting rules have been proposed and evaluated for\neach of these worlds separately, gradually improving the achievable distortion\nbounds, but none of the known voting rules perform well in both worlds\nsimultaneously.\n  In this work, we prove that one can achieve the best of both worlds by\ndesigning new voting rules, that simultaneously achieve near-optimal distortion\nguarantees in both distortion worlds. We also prove that this positive result\ndoes not generalize to the case where the voting rule is provided with the\nrankings of only the top-$t$ alternatives of each agent, for $t<m$.",
                "Strategic Reasoning with Language Models\nStrategic reasoning enables agents to cooperate, communicate, and compete\nwith other agents in diverse situations. Existing approaches to solving\nstrategic games rely on extensive training, yielding strategies that do not\ngeneralize to new scenarios or games without retraining. Large Language Models\n(LLMs), with their ability to comprehend and generate complex, context-rich\nlanguage, could prove powerful as tools for strategic gameplay. This paper\nintroduces an approach that uses pretrained LLMs with few-shot chain-of-thought\nexamples to enable strategic reasoning for AI agents. Our approach uses\nsystematically generated demonstrations of reasoning about states, values, and\nbeliefs to prompt the model. Using extensive variations of simple matrix games,\nwe show that strategies that are derived based on systematically generated\nprompts generalize almost perfectly to new game structures, alternate\nobjectives, and hidden information. Additionally, we demonstrate our approach\ncan lead to human-like negotiation strategies in realistic scenarios without\nany extra training or fine-tuning. Our results highlight the ability of LLMs,\nguided by systematic reasoning demonstrations, to adapt and excel in diverse\nstrategic scenarios.",
                "Bigger, Better, Faster: Human-level Atari with human-level efficiency\nWe introduce a value-based RL agent, which we call BBF, that achieves\nsuper-human performance in the Atari 100K benchmark. BBF relies on scaling the\nneural networks used for value estimation, as well as a number of other design\nchoices that enable this scaling in a sample-efficient manner. We conduct\nextensive analyses of these design choices and provide insights for future\nwork. We end with a discussion about updating the goalposts for\nsample-efficient RL research on the ALE. We make our code and data publicly\navailable at\nhttps://github.com/google-research/google-research/tree/master/bigger_better_faster.",
                "NetHack is Hard to Hack\nNeural policy learning methods have achieved remarkable results in various\ncontrol problems, ranging from Atari games to simulated locomotion. However,\nthese methods struggle in long-horizon tasks, especially in open-ended\nenvironments with multi-modal observations, such as the popular dungeon-crawler\ngame, NetHack. Intriguingly, the NeurIPS 2021 NetHack Challenge revealed that\nsymbolic agents outperformed neural approaches by over four times in median\ngame score. In this paper, we delve into the reasons behind this performance\ngap and present an extensive study on neural policy learning for NetHack. To\nconduct this study, we analyze the winning symbolic agent, extending its\ncodebase to track internal strategy selection in order to generate one of the\nlargest available demonstration datasets. Utilizing this dataset, we examine\n(i) the advantages of an action hierarchy; (ii) enhancements in neural\narchitecture; and (iii) the integration of reinforcement learning with\nimitation learning. Our investigations produce a state-of-the-art neural agent\nthat surpasses previous fully neural policies by 127% in offline settings and\n25% in online settings on median game score. However, we also demonstrate that\nmere scaling is insufficient to bridge the performance gap with the best\nsymbolic models or even the top human players."
            ],
            "interesting paper": 1
        },
        {
            "papers": [
                "Intent-aligned AI systems deplete human agency: the need for agency\n  foundations research in AI safety\nThe rapid advancement of artificial intelligence (AI) systems suggests that\nartificial general intelligence (AGI) systems may soon arrive. Many researchers\nare concerned that AIs and AGIs will harm humans via intentional misuse\n(AI-misuse) or through accidents (AI-accidents). In respect of AI-accidents,\nthere is an increasing effort focused on developing algorithms and paradigms\nthat ensure AI systems are aligned to what humans intend, e.g. AI systems that\nyield actions or recommendations that humans might judge as consistent with\ntheir intentions and goals. Here we argue that alignment to human intent is\ninsufficient for safe AI systems and that preservation of long-term agency of\nhumans may be a more robust standard, and one that needs to be separated\nexplicitly and a priori during optimization. We argue that AI systems can\nreshape human intention and discuss the lack of biological and psychological\nmechanisms that protect humans from loss of agency. We provide the first formal\ndefinition of agency-preserving AI-human interactions which focuses on\nforward-looking agency evaluations and argue that AI systems - not humans -\nmust be increasingly tasked with making these evaluations. We show how agency\nloss can occur in simple environments containing embedded agents that use\ntemporal-difference learning to make action recommendations. Finally, we\npropose a new area of research called \"agency foundations\" and pose four\ninitial topics designed to improve our understanding of agency in AI-human\ninteractions: benevolent game theory, algorithmic foundations of human rights,\nmechanistic interpretability of agency representation in neural-networks and\nreinforcement learning from internal states.",
                "Disentangling and Operationalizing AI Fairness at LinkedIn\nOperationalizing AI fairness at LinkedIn's scale is challenging not only\nbecause there are multiple mutually incompatible definitions of fairness but\nalso because determining what is fair depends on the specifics and context of\nthe product where AI is deployed. Moreover, AI practitioners need clarity on\nwhat fairness expectations need to be addressed at the AI level. In this paper,\nwe present the evolving AI fairness framework used at LinkedIn to address these\nthree challenges. The framework disentangles AI fairness by separating out\nequal treatment and equitable product expectations. Rather than imposing a\ntrade-off between these two commonly opposing interpretations of fairness, the\nframework provides clear guidelines for operationalizing equal AI treatment\ncomplemented with a product equity strategy. This paper focuses on the equal AI\ntreatment component of LinkedIn's AI fairness framework, shares the principles\nthat support it, and illustrates their application through a case study. We\nhope this paper will encourage other big tech companies to join us in sharing\ntheir approach to operationalizing AI fairness at scale, so that together we\ncan keep advancing this constantly evolving field.",
                "Evaluating GPT's Programming Capability through CodeWars' Katas\nIn the burgeoning field of artificial intelligence (AI), understanding the\ncapabilities and limitations of programming-oriented models is crucial. This\npaper presents a novel evaluation of the programming proficiency of Generative\nPretrained Transformer (GPT) models, specifically GPT-3.5 and GPT-4, against\ncoding problems of varying difficulty levels drawn from Codewars. The\nexperiments reveal a distinct boundary at the 3kyu level, beyond which these\nGPT models struggle to provide solutions. These findings led to the proposal of\na measure for coding problem complexity that incorporates both problem\ndifficulty and the time required for solution. The research emphasizes the need\nfor validation and creative thinking capabilities in AI models to better\nemulate human problem-solving techniques. Future work aims to refine this\nproposed complexity measure, enhance AI models with these suggested\ncapabilities, and develop an objective measure for programming problem\ndifficulty. The results of this research offer invaluable insights for\nimproving AI programming capabilities and advancing the frontier of AI\nproblem-solving abilities.",
                "Traffic Prediction using Artificial Intelligence: Review of Recent\n  Advances and Emerging Opportunities\nTraffic prediction plays a crucial role in alleviating traffic congestion\nwhich represents a critical problem globally, resulting in negative\nconsequences such as lost hours of additional travel time and increased fuel\nconsumption. Integrating emerging technologies into transportation systems\nprovides opportunities for improving traffic prediction significantly and\nbrings about new research problems. In order to lay the foundation for\nunderstanding the open research challenges in traffic prediction, this survey\naims to provide a comprehensive overview of traffic prediction methodologies.\nSpecifically, we focus on the recent advances and emerging research\nopportunities in Artificial Intelligence (AI)-based traffic prediction methods,\ndue to their recent success and potential in traffic prediction, with an\nemphasis on multivariate traffic time series modeling. We first provide a list\nand explanation of the various data types and resources used in the literature.\nNext, the essential data preprocessing methods within the traffic prediction\ncontext are categorized, and the prediction methods and applications are\nsubsequently summarized. Lastly, we present primary research challenges in\ntraffic prediction and discuss some directions for future research.",
                "Data and Knowledge for Overtaking Scenarios in Autonomous Driving\nAutonomous driving has become one of the most popular research topics within\nArtificial Intelligence. An autonomous vehicle is understood as a system that\ncombines perception, decision-making, planning, and control. All of those tasks\nrequire that the vehicle collects surrounding data in order to make a good\ndecision and action. In particular, the overtaking maneuver is one of the most\ncritical actions of driving. The process involves lane changes, acceleration\nand deceleration actions, and estimation of the speed and distance of the\nvehicle in front or in the lane in which it is moving. Despite the amount of\nwork available in the literature, just a few handle overtaking maneuvers and,\nbecause overtaking can be risky, no real-world dataset is available. This work\ncontributes in this area by presenting a new synthetic dataset whose focus is\nthe overtaking maneuver. We start by performing a thorough review of the state\nof the art in autonomous driving and then explore the main datasets found in\nthe literature (public and private, synthetic and real), highlighting their\nlimitations, and suggesting a new set of features whose focus is the overtaking\nmaneuver.",
                "Bigger, Better, Faster: Human-level Atari with human-level efficiency\nWe introduce a value-based RL agent, which we call BBF, that achieves\nsuper-human performance in the Atari 100K benchmark. BBF relies on scaling the\nneural networks used for value estimation, as well as a number of other design\nchoices that enable this scaling in a sample-efficient manner. We conduct\nextensive analyses of these design choices and provide insights for future\nwork. We end with a discussion about updating the goalposts for\nsample-efficient RL research on the ALE. We make our code and data publicly\navailable at\nhttps://github.com/google-research/google-research/tree/master/bigger_better_faster."
            ],
            "interesting paper": 1
        },
        {
            "papers": [
                "EMOTE: An Explainable architecture for Modelling the Other Through\n  Empathy\nWe can usually assume others have goals analogous to our own. This assumption\ncan also, at times, be applied to multi-agent games - e.g. Agent 1's attraction\nto green pellets is analogous to Agent 2's attraction to red pellets. This\n\"analogy\" assumption is tied closely to the cognitive process known as empathy.\nInspired by empathy, we design a simple and explainable architecture to model\nanother agent's action-value function. This involves learning an \"Imagination\nNetwork\" to transform the other agent's observed state in order to produce a\nhuman-interpretable \"empathetic state\" which, when presented to the learning\nagent, produces behaviours that mimic the other agent. Our approach is\napplicable to multi-agent scenarios consisting of a single learning agent and\nother (independent) agents acting according to fixed policies. This\narchitecture is particularly beneficial for (but not limited to) algorithms\nusing a composite value or reward function. We show our method produces better\nperformance in multi-agent games, where it robustly estimates the other's model\nin different environment configurations. Additionally, we show that the\nempathetic states are human interpretable, and thus verifiable.",
                "Environmental Memory Boosts Group Formation of Clueless Individuals\nThe formation of groups of interacting individuals improves performance and\nfitness in many decentralised systems, from micro-organisms to social insects,\nfrom robotic swarms to artificial intelligence algorithms. Often, group\nformation and high-level coordination in these systems emerge from individuals\nwith limited information-processing capabilities implementing low-level rules\nof communication to signal to each other. Here, we show that, even in a\ncommunity of clueless individuals incapable of processing information and\ncommunicating, a dynamic environment can coordinate group formation by\ntransiently storing memory of the earlier passage of individuals. Our results\nidentify a new mechanism of indirect coordination via shared memory that is\nprimarily promoted and reinforced by dynamic environmental factors, thus\novershadowing the need for any form of explicit signalling between individuals.\nWe expect this pathway to group formation to be relevant for understanding and\ncontrolling self-organisation and collective decision making in both living and\nartificial active matter in real-life environments.",
                "Human Control: Definitions and Algorithms\nHow can humans stay in control of advanced artificial intelligence systems?\nOne proposal is corrigibility, which requires the agent to follow the\ninstructions of a human overseer, without inappropriately influencing them. In\nthis paper, we formally define a variant of corrigibility called shutdown\ninstructability, and show that it implies appropriate shutdown behavior,\nretention of human autonomy, and avoidance of user harm. We also analyse the\nrelated concepts of non-obstruction and shutdown alignment, three previously\nproposed algorithms for human control, and one new algorithm.",
                "Provably Efficient Generalized Lagrangian Policy Optimization for Safe\n  Multi-Agent Reinforcement Learning\nWe examine online safe multi-agent reinforcement learning using constrained\nMarkov games in which agents compete by maximizing their expected total rewards\nunder a constraint on expected total utilities. Our focus is confined to an\nepisodic two-player zero-sum constrained Markov game with independent\ntransition functions that are unknown to agents, adversarial reward functions,\nand stochastic utility functions. For such a Markov game, we employ an approach\nbased on the occupancy measure to formulate it as an online constrained\nsaddle-point problem with an explicit constraint. We extend the Lagrange\nmultiplier method in constrained optimization to handle the constraint by\ncreating a generalized Lagrangian with minimax decision primal variables and a\ndual variable. Next, we develop an upper confidence reinforcement learning\nalgorithm to solve this Lagrangian problem while balancing exploration and\nexploitation. Our algorithm updates the minimax decision primal variables via\nonline mirror descent and the dual variable via projected gradient step and we\nprove that it enjoys sublinear rate $ O((|X|+|Y|) L \\sqrt{T(|A|+|B|)}))$ for\nboth regret and constraint violation after playing $T$ episodes of the game.\nHere, $L$ is the horizon of each episode, $(|X|,|A|)$ and $(|Y|,|B|)$ are the\nstate/action space sizes of the min-player and the max-player, respectively. To\nthe best of our knowledge, we provide the first provably efficient online safe\nreinforcement learning algorithm in constrained Markov games.",
                "Speaking the Language of Your Listener: Audience-Aware Adaptation via\n  Plug-and-Play Theory of Mind\nDialogue participants may have varying levels of knowledge about the topic\nunder discussion. In such cases, it is essential for speakers to adapt their\nutterances by taking their audience into account. Yet, it is an open question\nhow such adaptation can be modelled in computational agents. In this paper, we\nmodel a visually grounded referential game between a knowledgeable speaker and\na listener with more limited visual and linguistic experience. Inspired by\npsycholinguistic theories, we endow our speaker with the ability to adapt its\nreferring expressions via a simulation module that monitors the effectiveness\nof planned utterances from the listener's perspective. We propose an adaptation\nmechanism building on plug-and-play approaches to controlled language\ngeneration, where utterance generation is steered on the fly by the simulator\nwithout finetuning the speaker's underlying language model. Our results and\nanalyses show that our approach is effective: the speaker's utterances become\ncloser to the listener's domain of expertise, which leads to higher\ncommunicative success.",
                "Latent Exploration for Reinforcement Learning\nIn Reinforcement Learning, agents learn policies by exploring and interacting\nwith the environment. Due to the curse of dimensionality, learning policies\nthat map high-dimensional sensory input to motor output is particularly\nchallenging. During training, state of the art methods (SAC, PPO, etc.) explore\nthe environment by perturbing the actuation with independent Gaussian noise.\nWhile this unstructured exploration has proven successful in numerous tasks, it\ncan be suboptimal for overactuated systems. When multiple actuators, such as\nmotors or muscles, drive behavior, uncorrelated perturbations risk diminishing\neach other's effect, or modifying the behavior in a task-irrelevant way. While\nsolutions to introduce time correlation across action perturbations exist,\nintroducing correlation across actuators has been largely ignored. Here, we\npropose LATent TIme-Correlated Exploration (Lattice), a method to inject\ntemporally-correlated noise into the latent state of the policy network, which\ncan be seamlessly integrated with on- and off-policy algorithms. We demonstrate\nthat the noisy actions generated by perturbing the network's activations can be\nmodeled as a multivariate Gaussian distribution with a full covariance matrix.\nIn the PyBullet locomotion tasks, Lattice-SAC achieves state of the art\nresults, and reaches 18% higher reward than unstructured exploration in the\nHumanoid environment. In the musculoskeletal control environments of MyoSuite,\nLattice-PPO achieves higher reward in most reaching and object manipulation\ntasks, while also finding more energy-efficient policies with reductions of\n20-60%. Overall, we demonstrate the effectiveness of structured action noise in\ntime and actuator space for complex motor control tasks. The code is available\nat: https://github.com/amathislab/lattice."
            ],
            "interesting paper": 4
        },
        {
            "papers": [
                "Citizen Perspectives on Necessary Safeguards to the Use of AI by Law\n  Enforcement Agencies\nIn the light of modern technological advances, Artificial Intelligence (AI)\nis relied upon to enhance performance, increase efficiency, and maximize gains.\nFor Law Enforcement Agencies (LEAs), it can prove valuable in optimizing\nevidence analysis and establishing proactive prevention measures. Nevertheless,\ncitizens raise legitimate concerns around privacy invasions, biases,\ninequalities, and inaccurate decisions. This study explores the views of 111\ncitizens towards AI use by police through interviews, and integrates societal\nconcerns along with propositions of safeguards from negative effects of AI use\nby LEAs in the context of cybercrime and terrorism.",
                "Traffic Road Congestion System using by the internet of vehicles (IoV)\nTraffic problems have increased in modern life due to a huge number of\nvehicles, big cities, and ignoring the traffic rules. Vehicular ad hoc network\n(VANET) has improved the traffic system in previous some and plays a vital role\nin the best traffic control system in big cities. But due to some limitations,\nit is not enough to control some problems in specific conditions. Now a day\ninvention of new technologies of the Internet of Things (IoT) is used for\ncollaboratively and efficiently performing tasks. This technology was also\nintroduced in the transportation system which makes it an intelligent\ntransportation system (ITS), this is called the Internet of vehicles (IOV). We\nwill elaborate on traffic problems in the traditional system and elaborate on\nthe benefits, enhancements, and reasons to better IOV by Systematic Literature\nReview (SLR). This technique will be implemented by targeting needed papers\nthrough many search phrases. A systematic literature review is used for 121\narticles between 2014 and 2023. The IoV technologies and tools are required to\ncreate the IoV and resolve some traffic rules through SUMO (simulation of urban\nmobility) which is used for the design and simulation the road traffic. We have\ntried to contribute to the best model of the traffic control system. This paper\nwill analysis two vehicular congestion control models in term of select the\noptimized and efficient model and elaborate on the reasons for efficiency by\nsearching the solution SLR based questions. Due to some efficient features, we\nhave suggested the IOV based on vehicular clouds. These efficient features make\nthis model the best and most effective than the traditional model which is a\ngreat reason to enhance the network system.",
                "Responsible Design Patterns for Machine Learning Pipelines\nIntegrating ethical practices into the AI development process for artificial\nintelligence (AI) is essential to ensure safe, fair, and responsible operation.\nAI ethics involves applying ethical principles to the entire life cycle of AI\nsystems. This is essential to mitigate potential risks and harms associated\nwith AI, such as algorithm biases. To achieve this goal, responsible design\npatterns (RDPs) are critical for Machine Learning (ML) pipelines to guarantee\nethical and fair outcomes. In this paper, we propose a comprehensive framework\nincorporating RDPs into ML pipelines to mitigate risks and ensure the ethical\ndevelopment of AI systems. Our framework comprises new responsible AI design\npatterns for ML pipelines identified through a survey of AI ethics and data\nmanagement experts and validated through real-world scenarios with expert\nfeedback. The framework guides AI developers, data scientists, and\npolicy-makers to implement ethical practices in AI development and deploy\nresponsible AI systems in production.",
                "Human or Not? A Gamified Approach to the Turing Test\nWe present \"Human or Not?\", an online game inspired by the Turing test, that\nmeasures the capability of AI chatbots to mimic humans in dialog, and of humans\nto tell bots from other humans. Over the course of a month, the game was played\nby over 1.5 million users who engaged in anonymous two-minute chat sessions\nwith either another human or an AI language model which was prompted to behave\nlike humans. The task of the players was to correctly guess whether they spoke\nto a person or to an AI. This largest scale Turing-style test conducted to date\nrevealed some interesting facts. For example, overall users guessed the\nidentity of their partners correctly in only 68% of the games. In the subset of\nthe games in which users faced an AI bot, users had even lower correct guess\nrates of 60% (that is, not much higher than chance). This white paper details\nthe development, deployment, and results of this unique experiment. While this\nexperiment calls for many extensions and refinements, these findings already\nbegin to shed light on the inevitable near future which will commingle humans\nand AI.",
                "Human Control: Definitions and Algorithms\nHow can humans stay in control of advanced artificial intelligence systems?\nOne proposal is corrigibility, which requires the agent to follow the\ninstructions of a human overseer, without inappropriately influencing them. In\nthis paper, we formally define a variant of corrigibility called shutdown\ninstructability, and show that it implies appropriate shutdown behavior,\nretention of human autonomy, and avoidance of user harm. We also analyse the\nrelated concepts of non-obstruction and shutdown alignment, three previously\nproposed algorithms for human control, and one new algorithm.",
                "From Human-Centered to Social-Centered Artificial Intelligence:\n  Assessing ChatGPT's Impact through Disruptive Events\nLarge language models (LLMs) and dialogue agents have existed for years, but\nthe release of recent GPT models has been a watershed moment for artificial\nintelligence (AI) research and society at large. Immediately recognized for its\ngenerative capabilities and versatility, ChatGPT's impressive proficiency\nacross technical and creative domains led to its widespread adoption. While\nsociety grapples with the emerging cultural impacts of ChatGPT, critiques of\nChatGPT's impact within the machine learning community have coalesced around\nits performance or other conventional Responsible AI evaluations relating to\nbias, toxicity, and 'hallucination.' We argue that these latter critiques draw\nheavily on a particular conceptualization of the 'human-centered' framework,\nwhich tends to cast atomized individuals as the key recipients of both the\nbenefits and detriments of technology. In this article, we direct attention to\nanother dimension of LLMs and dialogue agents' impact: their effect on social\ngroups, institutions, and accompanying norms and practices. By illustrating\nChatGPT's social impact through three disruptive events, we challenge\nindividualistic approaches in AI development and contribute to ongoing debates\naround the ethical and responsible implementation of AI systems. We hope this\neffort will call attention to more comprehensive and longitudinal evaluation\ntools and compel technologists to go beyond human-centered thinking and ground\ntheir efforts through social-centered AI."
            ],
            "interesting paper": 2
        },
        {
            "papers": [
                "On the Existence of Information Bottlenecks in Living and Non-Living\n  Systems\nIn many complex systems, we observe that `interesting behaviour' is often the\nconsequence of a system exploiting the existence of an Information Bottleneck\n(IB). These bottlenecks can occur at different scales, between individuals or\ncomponents of a system, and sometimes within individuals themselves.\nOftentimes, we regard these bottlenecks negatively; as merely the limitations\nof the individual's physiology and something that ought to be overcome when\ndesigning and implementing artificial systems. However, we suggest instead that\nIBs may serve a purpose beyond merely providing a minimally-viable channel for\ncoordination in collective systems. More specifically, we suggest that\ninteresting or novel behaviour occurs when the individuals in a system are\nconstrained or limited in their ability to share information and must discover\nnovel ways to exploit existing mechanisms, which are inherently bottlenecked,\nrather than circumventing or otherwise avoiding those mechanisms entirely.",
                "An Architecture for Deploying Reinforcement Learning in Industrial\n  Environments\nIndustry 4.0 is driven by demands like shorter time-to-market, mass\ncustomization of products, and batch size one production. Reinforcement\nLearning (RL), a machine learning paradigm shown to possess a great potential\nin improving and surpassing human level performance in numerous complex tasks,\nallows coping with the mentioned demands. In this paper, we present an OPC UA\nbased Operational Technology (OT)-aware RL architecture, which extends the\nstandard RL setting, combining it with the setting of digital twins. Moreover,\nwe define an OPC UA information model allowing for a generalized plug-and-play\nlike approach for exchanging the RL agent used. In conclusion, we demonstrate\nand evaluate the architecture, by creating a proof of concept. By means of\nsolving a toy example, we show that this architecture can be used to determine\nthe optimal policy using a real control system.",
                "Federated Learning Games for Reconfigurable Intelligent Surfaces via\n  Causal Representations\nIn this paper, we investigate the problem of robust Reconfigurable\nIntelligent Surface (RIS) phase-shifts configuration over heterogeneous\ncommunication environments. The problem is formulated as a distributed learning\nproblem over different environments in a Federated Learning (FL) setting.\nEquivalently, this corresponds to a game played between multiple RISs, as\nlearning agents, in heterogeneous environments. Using Invariant Risk\nMinimization (IRM) and its FL equivalent, dubbed FL Games, we solve the RIS\nconfiguration problem by learning invariant causal representations across\nmultiple environments and then predicting the phases. The solution corresponds\nto playing according to Best Response Dynamics (BRD) which yields the Nash\nEquilibrium of the FL game. The representation learner and the phase predictor\nare modeled by two neural networks, and their performance is validated via\nsimulations against other benchmarks from the literature. Our results show that\ncausality-based learning yields a predictor that is 15% more accurate in unseen\nOut-of-Distribution (OoD) environments.",
                "Multi-Robot Path Planning Combining Heuristics and Multi-Agent\n  Reinforcement Learning\nMulti-robot path finding in dynamic environments is a highly challenging\nclassic problem. In the movement process, robots need to avoid collisions with\nother moving robots while minimizing their travel distance. Previous methods\nfor this problem either continuously replan paths using heuristic search\nmethods to avoid conflicts or choose appropriate collision avoidance strategies\nbased on learning approaches. The former may result in long travel distances\ndue to frequent replanning, while the latter may have low learning efficiency\ndue to low sample exploration and utilization, and causing high training costs\nfor the model. To address these issues, we propose a path planning method,\nMAPPOHR, which combines heuristic search, empirical rules, and multi-agent\nreinforcement learning. The method consists of two layers: a real-time planner\nbased on the multi-agent reinforcement learning algorithm, MAPPO, which embeds\nempirical rules in the action output layer and reward functions, and a\nheuristic search planner used to create a global guiding path. During movement,\nthe heuristic search planner replans new paths based on the instructions of the\nreal-time planner. We tested our method in 10 different conflict scenarios. The\nexperiments show that the planning performance of MAPPOHR is better than that\nof existing learning and heuristic methods. Due to the utilization of empirical\nknowledge and heuristic search, the learning efficiency of MAPPOHR is higher\nthan that of existing learning methods.",
                "Knowledge-based Reasoning and Learning under Partial Observability in Ad\n  Hoc Teamwork\nAd hoc teamwork refers to the problem of enabling an agent to collaborate\nwith teammates without prior coordination. Data-driven methods represent the\nstate of the art in ad hoc teamwork. They use a large labeled dataset of prior\nobservations to model the behavior of other agent types and to determine the ad\nhoc agent's behavior. These methods are computationally expensive, lack\ntransparency, and make it difficult to adapt to previously unseen changes,\ne.g., in team composition. Our recent work introduced an architecture that\ndetermined an ad hoc agent's behavior based on non-monotonic logical reasoning\nwith prior commonsense domain knowledge and predictive models of other agents'\nbehavior that were learned from limited examples. In this paper, we\nsubstantially expand the architecture's capabilities to support: (a) online\nselection, adaptation, and learning of the models that predict the other\nagents' behavior; and (b) collaboration with teammates in the presence of\npartial observability and limited communication. We illustrate and\nexperimentally evaluate the capabilities of our architecture in two simulated\nmultiagent benchmark domains for ad hoc teamwork: Fort Attack and Half Field\nOffense. We show that the performance of our architecture is comparable or\nbetter than state of the art data-driven baselines in both simple and complex\nscenarios, particularly in the presence of limited training data, partial\nobservability, and changes in team composition.",
                "chemSKI with tokens: world building and economy in the SKI universe\nchemSKI with tokens is a confluent graph rewrite system where all rewrites\nare local, which moreover can be used to do SKI calculus reductions. The graph\nrewrites of chemSKI are made conservative by the use of tokens. We thus achieve\nseveral goals: conservative rewrites in a chemical style, a solution to the\nproblem of new edge names in a distributed, decentralized graphical reduction\nand a new estimation of the cost of a combinatory calculus computation. This\nformalism can be used either as an artificial chemistry or as a model of a\nvirtual decentralized machine which performs only local reductions. A programs\nrepository and the same article with simulations are available at github at\nhttps://mbuliga.github.io/chemski/chemski-with-tokens.html"
            ],
            "interesting paper": 4
        },
        {
            "papers": [
                "AI Liability Insurance With an Example in AI-Powered E-diagnosis System\nArtificial Intelligence (AI) has received an increasing amount of attention\nin multiple areas. The uncertainties and risks in AI-powered systems have\ncreated reluctance in their wild adoption. As an economic solution to\ncompensate for potential damages, AI liability insurance is a promising market\nto enhance the integration of AI into daily life. In this work, we use an\nAI-powered E-diagnosis system as an example to study AI liability insurance. We\nprovide a quantitative risk assessment model with evidence-based numerical\nanalysis. We discuss the insurability criteria for AI technologies and suggest\nnecessary adjustments to accommodate the features of AI products. We show that\nAI liability insurance can act as a regulatory mechanism to incentivize\ncompliant behaviors and serve as a certificate of high-quality AI systems.\nFurthermore, we suggest premium adjustment to reflect the dynamic evolution of\nthe inherent uncertainty in AI. Moral hazard problems are discussed and\nsuggestions for AI liability insurance are provided.",
                "AI and the creative realm: A short review of current and future\n  applications\nThis study explores the concept of creativity and artificial intelligence\n(AI) and their recent integration. While AI has traditionally been perceived as\nincapable of generating new ideas or creating art, the development of more\nsophisticated AI models and the proliferation of human-computer interaction\ntools have opened up new possibilities for AI in artistic creation. This study\ninvestigates the various applications of AI in a creative context,\ndifferentiating between the type of art, language, and algorithms used. It also\nconsiders the philosophical implications of AI and creativity, questioning\nwhether consciousness can be researched in machines and AI's potential\ninterests and decision-making capabilities. Overall, we aim to stimulate a\nreflection on AI's use and ethical implications in creative contexts.",
                "Recent Advances in Graph-based Machine Learning for Applications in\n  Smart Urban Transportation Systems\nThe Intelligent Transportation System (ITS) is an important part of modern\ntransportation infrastructure, employing a combination of communication\ntechnology, information processing and control systems to manage transportation\nnetworks. This integration of various components such as roads, vehicles, and\ncommunication systems, is expected to improve efficiency and safety by\nproviding better information, services, and coordination of transportation\nmodes. In recent years, graph-based machine learning has become an increasingly\nimportant research focus in the field of ITS aiming at the development of\ncomplex, data-driven solutions to address various ITS-related challenges. This\nchapter presents background information on the key technical challenges for ITS\ndesign, along with a review of research methods ranging from classic\nstatistical approaches to modern machine learning and deep learning-based\napproaches. Specifically, we provide an in-depth review of graph-based machine\nlearning methods, including basic concepts of graphs, graph data\nrepresentation, graph neural network architectures and their relation to ITS\napplications. Additionally, two case studies of graph-based ITS applications\nproposed in our recent work are presented in detail to demonstrate the\npotential of graph-based machine learning in the ITS domain.",
                "An Overview on Generative AI at Scale with Edge-Cloud Computing\nAs a specific category of artificial intelligence (AI), generative artificial\nintelligence (GenAI) generates new content that resembles what is created by\nhumans. The rapid development of GenAI systems has created a huge amount of new\ndata on the Internet, posing new challenges to current computing and\ncommunication frameworks. Currently, GenAI services rely on the traditional\ncloud computing framework due to the need for large computation resources.\nHowever, such services will encounter high latency because of data transmission\nand a high volume of requests. On the other hand, edge-cloud computing can\nprovide adequate computation power and low latency at the same time through the\ncollaboration between edges and the cloud. Thus, it is attractive to build\nGenAI systems at scale by leveraging the edge-cloud computing paradigm. In this\noverview paper, we review recent developments in GenAI and edge-cloud\ncomputing, respectively. Then, we use two exemplary GenAI applications to\ndiscuss technical challenges in scaling up their solutions using edge-cloud\ncollaborative systems. Finally, we list design considerations for training and\ndeploying GenAI systems at scale and point out future research directions.",
                "The ethical ambiguity of AI data enrichment: Measuring gaps in research\n  ethics norms and practices\nThe technical progression of artificial intelligence (AI) research has been\nbuilt on breakthroughs in fields such as computer science, statistics, and\nmathematics. However, in the past decade AI researchers have increasingly\nlooked to the social sciences, turning to human interactions to solve the\nchallenges of model development. Paying crowdsourcing workers to generate or\ncurate data, or data enrichment, has become indispensable for many areas of AI\nresearch, from natural language processing to reinforcement learning from human\nfeedback (RLHF). Other fields that routinely interact with crowdsourcing\nworkers, such as Psychology, have developed common governance requirements and\nnorms to ensure research is undertaken ethically. This study explores how, and\nto what extent, comparable research ethics requirements and norms have\ndeveloped for AI research and data enrichment. We focus on the approach taken\nby two leading conferences: ICLR and NeurIPS, and journal publisher Springer.\nIn a longitudinal study of accepted papers, and via a comparison with\nPsychology and CHI papers, this work finds that leading AI venues have begun to\nestablish protocols for human data collection, but these are are inconsistently\nfollowed by authors. Whilst Psychology papers engaging with crowdsourcing\nworkers frequently disclose ethics reviews, payment data, demographic data and\nother information, similar disclosures are far less common in leading AI venues\ndespite similar guidance. The work concludes with hypotheses to explain these\ngaps in research ethics practices and considerations for its implications.",
                "Integrated Sensing-Communication-Computation for Edge Artificial\n  Intelligence\nEdge artificial intelligence (AI) has been a promising solution towards 6G to\nempower a series of advanced techniques such as digital twins, holographic\nprojection, semantic communications, and auto-driving, for achieving\nintelligence of everything. The performance of edge AI tasks, including edge\nlearning and edge AI inference, depends on the quality of three highly coupled\nprocesses, i.e., sensing for data acquisition, computation for information\nextraction, and communication for information transmission. However, these\nthree modules need to compete for network resources for enhancing their own\nquality-of-services. To this end, integrated sensing-communication-computation\n(ISCC) is of paramount significance for improving resource utilization as well\nas achieving the customized goals of edge AI tasks. By investigating the\ninterplay among the three modules, this article presents various kinds of ISCC\nschemes for federated edge learning tasks and edge AI inference tasks in both\napplication and physical layers."
            ],
            "interesting paper": 3
        }
    ],
    "Casey Martinez": [
        {
            "papers": [
                "Reverse Engineering Self-Supervised Learning\nSelf-supervised learning (SSL) is a powerful tool in machine learning, but\nunderstanding the learned representations and their underlying mechanisms\nremains a challenge. This paper presents an in-depth empirical analysis of\nSSL-trained representations, encompassing diverse models, architectures, and\nhyperparameters. Our study reveals an intriguing aspect of the SSL training\nprocess: it inherently facilitates the clustering of samples with respect to\nsemantic labels, which is surprisingly driven by the SSL objective's\nregularization term. This clustering process not only enhances downstream\nclassification but also compresses the data information. Furthermore, we\nestablish that SSL-trained representations align more closely with semantic\nclasses rather than random classes. Remarkably, we show that learned\nrepresentations align with semantic classes across various hierarchical levels,\nand this alignment increases during training and when moving deeper into the\nnetwork. Our findings provide valuable insights into SSL's representation\nlearning mechanisms and their impact on performance across different sets of\nclasses.",
                "Classic machine learning methods\nIn this chapter, we present the main classic machine learning methods. A\nlarge part of the chapter is devoted to supervised learning techniques for\nclassification and regression, including nearest-neighbor methods, linear and\nlogistic regressions, support vector machines and tree-based algorithms. We\nalso describe the problem of overfitting as well as strategies to overcome it.\nWe finally provide a brief overview of unsupervised learning methods, namely\nfor clustering and dimensionality reduction.",
                "Rethinking the Evaluation Protocol of Domain Generalization\nDomain generalization aims to solve the challenge of Out-of-Distribution\n(OOD) generalization by leveraging common knowledge learned from multiple\ntraining domains to generalize to unseen test domains. To accurately evaluate\nthe OOD generalization ability, it is required that test data information is\nunavailable. However, the current domain generalization protocol may still have\npotential test data information leakage. This paper examines the risks of test\ndata information leakage from two aspects of the current evaluation protocol:\nsupervised pretraining on ImageNet and oracle model selection. We propose\nmodifications to the current protocol that we should employ self-supervised\npretraining or train from scratch instead of employing the current supervised\npretraining, and we should use multiple test domains. These would result in a\nmore precise evaluation of OOD generalization ability. We also rerun the\nalgorithms with the modified protocol and introduce new leaderboards to\nencourage future research in domain generalization with a fairer comparison.",
                "Differentiable Clustering with Perturbed Spanning Forests\nWe introduce a differentiable clustering method based on stochastic\nperturbations of minimum-weight spanning forests. This allows us to include\nclustering in end-to-end trainable pipelines, with efficient gradients. We show\nthat our method performs well even in difficult settings, such as data sets\nwith high noise and challenging geometries. We also formulate an ad hoc loss to\nefficiently learn from partial clustering data using this operation. We\ndemonstrate its performance on several data sets for supervised and\nsemi-supervised tasks.",
                "How to escape sharp minima with random perturbations\nModern machine learning applications have witnessed the remarkable success of\noptimization algorithms that are designed to find flat minima. Motivated by\nthis design choice, we undertake a formal study that (i) formulates the notion\nof flat minima, and (ii) studies the complexity of finding them. Specifically,\nwe adopt the trace of the Hessian of the cost function as a measure of\nflatness, and use it to formally define the notion of approximate flat minima.\nUnder this notion, we then analyze algorithms that find approximate flat minima\nefficiently. For general cost functions, we discuss a gradient-based algorithm\nthat finds an approximate flat local minimum efficiently. The main component of\nthe algorithm is to use gradients computed from randomly perturbed iterates to\nestimate a direction that leads to flatter minima. For the setting where the\ncost function is an empirical risk over training data, we present a faster\nalgorithm that is inspired by a recently proposed practical algorithm called\nsharpness-aware minimization, supporting its success in practice.",
                "Generalizable Low-Resource Activity Recognition with Diverse and\n  Discriminative Representation Learning\nHuman activity recognition (HAR) is a time series classification task that\nfocuses on identifying the motion patterns from human sensor readings. Adequate\ndata is essential but a major bottleneck for training a generalizable HAR\nmodel, which assists customization and optimization of online web applications.\nHowever, it is costly in time and economy to collect large-scale labeled data\nin reality, i.e., the low-resource challenge. Meanwhile, data collected from\ndifferent persons have distribution shifts due to different living habits, body\nshapes, age groups, etc. The low-resource and distribution shift challenges are\ndetrimental to HAR when applying the trained model to new unseen subjects. In\nthis paper, we propose a novel approach called Diverse and Discriminative\nrepresentation Learning (DDLearn) for generalizable low-resource HAR. DDLearn\nsimultaneously considers diversity and discrimination learning. With the\nconstructed self-supervised learning task, DDLearn enlarges the data diversity\nand explores the latent activity properties. Then, we propose a diversity\npreservation module to preserve the diversity of learned features by enlarging\nthe distribution divergence between the original and augmented domains.\nMeanwhile, DDLearn also enhances semantic discrimination by learning\ndiscriminative representations with supervised contrastive learning. Extensive\nexperiments on three public HAR datasets demonstrate that our method\nsignificantly outperforms state-of-art methods by an average accuracy\nimprovement of 9.5% under the low-resource distribution shift scenarios, while\nbeing a generic, explainable, and flexible framework. Code is available at:\nhttps://github.com/microsoft/robustlearn."
            ],
            "interesting paper": 1
        },
        {
            "papers": [
                "Revisiting Structured Variational Autoencoders\nStructured variational autoencoders (SVAEs) combine probabilistic graphical\nmodel priors on latent variables, deep neural networks to link latent variables\nto observed data, and structure-exploiting algorithms for approximate posterior\ninference. These models are particularly appealing for sequential data, where\nthe prior can capture temporal dependencies. However, despite their conceptual\nelegance, SVAEs have proven difficult to implement, and more general approaches\nhave been favored in practice. Here, we revisit SVAEs using modern machine\nlearning tools and demonstrate their advantages over more general alternatives\nin terms of both accuracy and efficiency. First, we develop a modern\nimplementation for hardware acceleration, parallelization, and automatic\ndifferentiation of the message passing algorithms at the core of the SVAE.\nSecond, we show that by exploiting structure in the prior, the SVAE learns more\naccurate models and posterior distributions, which translate into improved\nperformance on prediction tasks. Third, we show how the SVAE can naturally\nhandle missing data, and we leverage this ability to develop a novel,\nself-supervised training approach. Altogether, these results show that the time\nis ripe to revisit structured variational autoencoders.",
                "Detecting Heart Disease from Multi-View Ultrasound Images via Supervised\n  Attention Multiple Instance Learning\nAortic stenosis (AS) is a degenerative valve condition that causes\nsubstantial morbidity and mortality. This condition is under-diagnosed and\nunder-treated. In clinical practice, AS is diagnosed with expert review of\ntransthoracic echocardiography, which produces dozens of ultrasound images of\nthe heart. Only some of these views show the aortic valve. To automate\nscreening for AS, deep networks must learn to mimic a human expert's ability to\nidentify views of the aortic valve then aggregate across these relevant images\nto produce a study-level diagnosis. We find previous approaches to AS detection\nyield insufficient accuracy due to relying on inflexible averages across\nimages. We further find that off-the-shelf attention-based multiple instance\nlearning (MIL) performs poorly. We contribute a new end-to-end MIL approach\nwith two key methodological innovations. First, a supervised attention\ntechnique guides the learned attention mechanism to favor relevant views.\nSecond, a novel self-supervised pretraining strategy applies contrastive\nlearning on the representation of the whole study instead of individual images\nas commonly done in prior literature. Experiments on an open-access dataset and\nan external validation set show that our approach yields higher accuracy while\nreducing model size.",
                "Scan and Snap: Understanding Training Dynamics and Token Composition in\n  1-layer Transformer\nTransformer architecture has shown impressive performance in multiple\nresearch domains and has become the backbone of many neural network models.\nHowever, there is limited understanding on how it works. In particular, with a\nsimple predictive loss, how the representation emerges from the gradient\n\\emph{training dynamics} remains a mystery. In this paper, for 1-layer\ntransformer with one self-attention layer plus one decoder layer, we analyze\nits SGD training dynamics for the task of next token prediction in a\nmathematically rigorous manner. We open the black box of the dynamic process of\nhow the self-attention layer combines input tokens, and reveal the nature of\nunderlying inductive bias. More specifically, with the assumption (a) no\npositional encoding, (b) long input sequence, and (c) the decoder layer learns\nfaster than the self-attention layer, we prove that self-attention acts as a\n\\emph{discriminative scanning algorithm}: starting from uniform attention, it\ngradually attends more to distinct key tokens for a specific next token to be\npredicted, and pays less attention to common key tokens that occur across\ndifferent next tokens. Among distinct tokens, it progressively drops attention\nweights, following the order of low to high co-occurrence between the key and\nthe query token in the training set. Interestingly, this procedure does not\nlead to winner-takes-all, but decelerates due to a \\emph{phase transition} that\nis controllable by the learning rates of the two layers, leaving (almost) fixed\ntoken combination. We verify this \\textbf{\\emph{scan and snap}} dynamics on\nsynthetic and real-world data (WikiText).",
                "Diversify Your Vision Datasets with Automatic Diffusion-Based\n  Augmentation\nMany fine-grained classification tasks, like rare animal identification, have\nlimited training data and consequently classifiers trained on these datasets\noften fail to generalize to variations in the domain like changes in weather or\nlocation. As such, we explore how natural language descriptions of the domains\nseen in training data can be used with large vision models trained on diverse\npretraining datasets to generate useful variations of the training data. We\nintroduce ALIA (Automated Language-guided Image Augmentation), a method which\nutilizes large vision and language models to automatically generate natural\nlanguage descriptions of a dataset's domains and augment the training data via\nlanguage-guided image editing. To maintain data integrity, a model trained on\nthe original dataset filters out minimal image edits and those which corrupt\nclass-relevant information. The resulting dataset is visually consistent with\nthe original training data and offers significantly enhanced diversity. We show\nthat ALIA is able to surpasses traditional data augmentation and text-to-image\ngenerated data on fine-grained classification tasks, including cases of domain\ngeneralization and contextual bias. Code is available at\nhttps://github.com/lisadunlap/ALIA.",
                "On convex decision regions in deep network representations\nCurrent work on human-machine alignment aims at understanding machine-learned\nlatent spaces and their correspondence to human representations.\nG{\\\"a}rdenfors' conceptual spaces is a prominent framework for understanding\nhuman representations. Convexity of object regions in conceptual spaces is\nargued to promote generalizability, few-shot learning, and interpersonal\nalignment. Based on these insights, we investigate the notion of convexity of\nconcept regions in machine-learned latent spaces. We develop a set of tools for\nmeasuring convexity in sampled data and evaluate emergent convexity in layered\nrepresentations of state-of-the-art deep networks. We show that convexity is\nrobust to basic re-parametrization and, hence, meaningful as a quality of\nmachine-learned latent spaces. We find that approximate convexity is pervasive\nin neural representations in multiple application domains, including models of\nimages, audio, human activity, text, and medical images. Generally, we observe\nthat fine-tuning increases the convexity of label regions. We find evidence\nthat pretraining convexity of class label regions predicts subsequent\nfine-tuning performance.",
                "Differentiable Random Partition Models\nPartitioning a set of elements into an unknown number of mutually exclusive\nsubsets is essential in many machine learning problems. However, assigning\nelements, such as samples in a dataset or neurons in a network layer, to an\nunknown and discrete number of subsets is inherently non-differentiable,\nprohibiting end-to-end gradient-based optimization of parameters. We overcome\nthis limitation by proposing a novel two-step method for inferring partitions,\nwhich allows its usage in variational inference tasks. This new approach\nenables reparameterized gradients with respect to the parameters of the new\nrandom partition model. Our method works by inferring the number of elements\nper subset and, second, by filling these subsets in a learned order. We\nhighlight the versatility of our general-purpose approach on three different\nchallenging experiments: variational clustering, inference of shared and\nindependent generative factors under weak supervision, and multitask learning."
            ],
            "interesting paper": 2
        },
        {
            "papers": [
                "Instance-based Max-margin for Practical Few-shot Recognition\nIn order to mimic the human few-shot learning (FSL) ability better and to\nmake FSL closer to real-world applications, this paper proposes a practical FSL\n(pFSL) setting. pFSL is based on unsupervised pretrained models (analogous to\nhuman prior knowledge) and recognizes many novel classes simultaneously.\nCompared to traditional FSL, pFSL is simpler in its formulation, easier to\nevaluate, more challenging and more practical. To cope with the rarity of\ntraining examples, this paper proposes IbM2, an instance-based max-margin\nmethod not only for the new pFSL setting, but also works well in traditional\nFSL scenarios. Based on the Gaussian Annulus Theorem, IbM2 converts random\nnoise applied to the instances into a mechanism to achieve maximum margin in\nthe many-way pFSL (or traditional FSL) recognition task. Experiments with\nvarious self-supervised pretraining methods and diverse many- or few-way FSL\ntasks show that IbM2 almost always leads to improvements compared to its\nrespective baseline methods, and in most cases the improvements are\nsignificant. With both the new pFSL setting and novel IbM2 method, this paper\nshows that practical few-shot learning is both viable and promising.",
                "Self-Supervised Reinforcement Learning that Transfers using Random\n  Features\nModel-free reinforcement learning algorithms have exhibited great potential\nin solving single-task sequential decision-making problems with\nhigh-dimensional observations and long horizons, but are known to be hard to\ngeneralize across tasks. Model-based RL, on the other hand, learns\ntask-agnostic models of the world that naturally enables transfer across\ndifferent reward functions, but struggles to scale to complex environments due\nto the compounding error. To get the best of both worlds, we propose a\nself-supervised reinforcement learning method that enables the transfer of\nbehaviors across tasks with different rewards, while circumventing the\nchallenges of model-based RL. In particular, we show self-supervised\npre-training of model-free reinforcement learning with a number of random\nfeatures as rewards allows implicit modeling of long-horizon environment\ndynamics. Then, planning techniques like model-predictive control using these\nimplicit models enable fast adaptation to problems with new reward functions.\nOur method is self-supervised in that it can be trained on offline datasets\nwithout reward labels, but can then be quickly deployed on new tasks. We\nvalidate that our proposed method enables transfer across tasks on a variety of\nmanipulation and locomotion domains in simulation, opening the door to\ngeneralist decision-making agents.",
                "Matrix Information Theory for Self-Supervised Learning\nThe maximum entropy encoding framework provides a unified perspective for\nmany non-contrastive learning methods like SimSiam, Barlow Twins, and MEC.\nInspired by this framework, we introduce Matrix-SSL, a novel approach that\nleverages matrix information theory to interpret the maximum entropy encoding\nloss as matrix uniformity loss. Furthermore, Matrix-SSL enhances the maximum\nentropy encoding method by seamlessly incorporating matrix alignment loss,\ndirectly aligning covariance matrices in different branches. Experimental\nresults reveal that Matrix-SSL outperforms state-of-the-art methods on the\nImageNet dataset under linear evaluation settings and on MS-COCO for transfer\nlearning tasks. Specifically, when performing transfer learning tasks on\nMS-COCO, our method outperforms previous SOTA methods such as MoCo v2 and BYOL\nup to 3.3% with only 400 epochs compared to 800 epochs pre-training. We also\ntry to introduce representation learning into the language modeling regime by\nfine-tuning a 7B model using matrix cross-entropy loss, with a margin of 3.1%\non the GSM8K dataset over the standard cross-entropy loss. Code available at\nhttps://github.com/yifanzhang-pro/Matrix-SSL.",
                "Statistically Significant Concept-based Explanation of Image Classifiers\n  via Model Knockoffs\nA concept-based classifier can explain the decision process of a deep\nlearning model by human-understandable concepts in image classification\nproblems. However, sometimes concept-based explanations may cause false\npositives, which misregards unrelated concepts as important for the prediction\ntask. Our goal is to find the statistically significant concept for\nclassification to prevent misinterpretation. In this study, we propose a method\nusing a deep learning model to learn the image concept and then using the\nKnockoff samples to select the important concepts for prediction by controlling\nthe False Discovery Rate (FDR) under a certain value. We evaluate the proposed\nmethod in our synthetic and real data experiments. Also, it shows that our\nmethod can control the FDR properly while selecting highly interpretable\nconcepts to improve the trustworthiness of the model.",
                "Visualizing Self-Regulated Learner Profiles in Dashboards: Design\n  Insights from Teachers\nFlipped Classrooms (FC) are a promising teaching strategy, where students\nengage with the learning material before attending face-to-face sessions. While\npre-class activities are critical for course success, many students struggle to\nengage effectively in them due to inadequate of self-regulated learning (SRL)\nskills. Thus, tools enabling teachers to monitor students' SRL and provide\npersonalized guidance have the potential to improve learning outcomes. However,\nexisting dashboards mostly focus on aggregated information, disregarding recent\nwork leveraging machine learning (ML) approaches that have identified\ncomprehensive, multi-dimensional SRL behaviors. Unfortunately, the complexity\nof such findings makes them difficult to communicate and act on. In this paper,\nwe follow a teacher-centered approach to study how to make thorough findings\naccessible to teachers. We design and implement FlippED, a dashboard for\nmonitoring students' SRL behavior. We evaluate the usability and actionability\nof the tool in semi-structured interviews with ten university teachers. We find\nthat communicating ML-based profiles spark a range of potential interventions\nfor students and course modifications.",
                "Causal Component Analysis\nIndependent Component Analysis (ICA) aims to recover independent latent\nvariables from observed mixtures thereof. Causal Representation Learning (CRL)\naims instead to infer causally related (thus often statistically dependent)\nlatent variables, together with the unknown graph encoding their causal\nrelationships. We introduce an intermediate problem termed Causal Component\nAnalysis (CauCA). CauCA can be viewed as a generalization of ICA, modelling the\ncausal dependence among the latent components, and as a special case of CRL. In\ncontrast to CRL, it presupposes knowledge of the causal graph, focusing solely\non learning the unmixing function and the causal mechanisms. Any impossibility\nresults regarding the recovery of the ground truth in CauCA also apply for CRL,\nwhile possibility results may serve as a stepping stone for extensions to CRL.\nWe characterize CauCA identifiability from multiple datasets generated through\ndifferent types of interventions on the latent causal variables. As a\ncorollary, this interventional perspective also leads to new identifiability\nresults for nonlinear ICA -- a special case of CauCA with an empty graph --\nrequiring strictly fewer datasets than previous results. We introduce a\nlikelihood-based approach using normalizing flows to estimate both the unmixing\nfunction and the causal mechanisms, and demonstrate its effectiveness through\nextensive synthetic experiments in the CauCA and ICA setting."
            ],
            "interesting paper": 3
        },
        {
            "papers": [
                "Data Minimization at Inference Time\nIn domains with high stakes such as law, recruitment, and healthcare,\nlearning models frequently rely on sensitive user data for inference,\nnecessitating the complete set of features. This not only poses significant\nprivacy risks for individuals but also demands substantial human effort from\norganizations to verify information accuracy. This paper asks whether it is\nnecessary to use \\emph{all} input features for accurate predictions at\ninference time. The paper demonstrates that, in a personalized setting,\nindividuals may only need to disclose a small subset of their features without\ncompromising decision-making accuracy. The paper also provides an efficient\nsequential algorithm to determine the appropriate attributes for each\nindividual to provide. Evaluations across various learning tasks show that\nindividuals can potentially report as little as 10\\% of their information while\nmaintaining the same accuracy level as a model that employs the full set of\nuser information.",
                "Distill Gold from Massive Ores: Bi-level Data Pruning towards Efficient\n  Dataset Distillation\nData-efficient learning has garnered significant attention, especially given\nthe current trend of large multi-modal models. Recently, dataset distillation\nhas become an effective approach by synthesizing data samples that are\nessential for network training. However, it remains to be explored which\nsamples are essential for the dataset distillation process itself. In this\nwork, we study the data efficiency and selection for the dataset distillation\ntask. By re-formulating the dynamics of distillation, we provide insight into\nthe inherent redundancy in the real dataset, both theoretically and\nempirically. We propose to use the empirical loss value as a static data\npruning criterion. To further compensate for the variation of the data value in\ntraining, we find the most contributing samples based on their causal effects\non the distillation. The proposed selection strategy can efficiently exploit\nthe training dataset, outperform the previous SOTA distillation algorithms, and\nconsistently enhance the distillation algorithms, even on much larger-scale and\nmore heterogeneous datasets, e.g., full ImageNet-1K and Kinetics-400. We\nbelieve this paradigm will open up new avenues in the dynamics of distillation\nand pave the way for efficient dataset distillation. Our code is available on\nhttps://github.com/silicx/GoldFromOres-BiLP.",
                "The Curse of Recursion: Training on Generated Data Makes Models Forget\nStable Diffusion revolutionised image creation from descriptive text. GPT-2,\nGPT-3(.5) and GPT-4 demonstrated astonishing performance across a variety of\nlanguage tasks. ChatGPT introduced such language models to the general public.\nIt is now clear that large language models (LLMs) are here to stay, and will\nbring about drastic change in the whole ecosystem of online text and images. In\nthis paper we consider what the future might hold. What will happen to GPT-{n}\nonce LLMs contribute much of the language found online? We find that use of\nmodel-generated content in training causes irreversible defects in the\nresulting models, where tails of the original content distribution disappear.\nWe refer to this effect as Model Collapse and show that it can occur in\nVariational Autoencoders, Gaussian Mixture Models and LLMs. We build\ntheoretical intuition behind the phenomenon and portray its ubiquity amongst\nall learned generative models. We demonstrate that it has to be taken seriously\nif we are to sustain the benefits of training from large-scale data scraped\nfrom the web. Indeed, the value of data collected about genuine human\ninteractions with systems will be increasingly valuable in the presence of\ncontent generated by LLMs in data crawled from the Internet.",
                "One Network, Many Masks: Towards More Parameter-Efficient Transfer\n  Learning\nFine-tuning pre-trained language models for multiple tasks tends to be\nexpensive in terms of storage. To mitigate this, parameter-efficient transfer\nlearning (PETL) methods have been proposed to address this issue, but they\nstill require a significant number of parameters and storage when being applied\nto broader ranges of tasks. To achieve even greater storage reduction, we\npropose PROPETL, a novel method that enables efficient sharing of a single PETL\nmodule which we call prototype network (e.g., adapter, LoRA, and prefix-tuning)\nacross layers and tasks. We then learn binary masks to select different\nsub-networks from the shared prototype network and apply them as PETL modules\ninto different layers. We find that the binary masks can determine crucial\ninformation from the network, which is often ignored in previous studies. Our\nwork can also be seen as a type of pruning method, where we find that\noverparameterization also exists in the seemingly small PETL modules. We\nevaluate PROPETL on various downstream tasks and show that it can outperform\nother PETL methods with approximately 10% of the parameter storage required by\nthe latter.",
                "Diagnosing Transformers: Illuminating Feature Spaces for Clinical\n  Decision-Making\nPre-trained transformers are often fine-tuned to aid clinical decision-making\nusing limited clinical notes. Model interpretability is crucial, especially in\nhigh-stakes domains like medicine, to establish trust and ensure safety, which\nrequires human engagement. We introduce SUFO, a systematic framework that\nenhances interpretability of fine-tuned transformer feature spaces. SUFO\nutilizes a range of analytic and visualization techniques, including Supervised\nprobing, Unsupervised similarity analysis, Feature dynamics, and Outlier\nanalysis to address key questions about model trust and interpretability. We\nconduct a case study investigating the impact of pre-training data where we\nfocus on real-world pathology classification tasks, and validate our findings\non MedNLI. We evaluate five 110M-sized pre-trained transformer models,\ncategorized into general-domain (BERT, TNLR), mixed-domain (BioBERT, Clinical\nBioBERT), and domain-specific (PubMedBERT) groups. Our SUFO analyses reveal\nthat: (1) while PubMedBERT, the domain-specific model, contains valuable\ninformation for fine-tuning, it can overfit to minority classes when class\nimbalances exist. In contrast, mixed-domain models exhibit greater resistance\nto overfitting, suggesting potential improvements in domain-specific model\nrobustness; (2) in-domain pre-training accelerates feature disambiguation\nduring fine-tuning; and (3) feature spaces undergo significant sparsification\nduring this process, enabling clinicians to identify common outlier modes among\nfine-tuned models as demonstrated in this paper. These findings showcase the\nutility of SUFO in enhancing trust and safety when using transformers in\nmedicine, and we believe SUFO can aid practitioners in evaluating fine-tuned\nlanguage models for other applications in medicine and in more critical\ndomains.",
                "USIM-DAL: Uncertainty-aware Statistical Image Modeling-based Dense\n  Active Learning for Super-resolution\nDense regression is a widely used approach in computer vision for tasks such\nas image super-resolution, enhancement, depth estimation, etc. However, the\nhigh cost of annotation and labeling makes it challenging to achieve accurate\nresults. We propose incorporating active learning into dense regression models\nto address this problem. Active learning allows models to select the most\ninformative samples for labeling, reducing the overall annotation cost while\nimproving performance. Despite its potential, active learning has not been\nwidely explored in high-dimensional computer vision regression tasks like\nsuper-resolution. We address this research gap and propose a new framework\ncalled USIM-DAL that leverages the statistical properties of colour images to\nlearn informative priors using probabilistic deep neural networks that model\nthe heteroscedastic predictive distribution allowing uncertainty\nquantification. Moreover, the aleatoric uncertainty from the network serves as\na proxy for error that is used for active learning. Our experiments on a wide\nvariety of datasets spanning applications in natural images (visual genome,\nBSD100), medical imaging (histopathology slides), and remote sensing (satellite\nimages) demonstrate the efficacy of the newly proposed USIM-DAL and superiority\nover several dense regression active learning methods."
            ],
            "interesting paper": 2
        },
        {
            "papers": [
                "Full High-Dimensional Intelligible Learning In 2-D Lossless\n  Visualization Space\nThis study explores a new methodology for machine learning classification\ntasks in 2-dimensional visualization space (2-D ML) using Visual knowledge\nDiscovery in lossless General Line Coordinates. It is shown that this is a full\nmachine learning approach that does not require processing n-dimensional data\nin an abstract n-dimensional space. It enables discovering n-D patterns in 2-D\nspace without loss of n-D information using graph representations of n-D data\nin 2-D. Specifically, this study shows that it can be done with static and\ndynamic In-line Based Coordinates in different modifications, which are a\ncategory of General Line Coordinates. Based on these inline coordinates,\nclassification and regression methods were developed. The viability of the\nstrategy was shown by two case studies based on benchmark datasets (Wisconsin\nBreast Cancer and Page Block Classification datasets). The characteristics of\npage block classification data led to the development of an algorithm for\nimbalanced high-resolution data with multiple classes, which exploits the\ndecision trees as a model design facilitator producing a model, which is more\ngeneral than a decision tree. This work accelerates the ongoing consolidation\nof an emerging field of full 2-D machine learning and its methodology. Within\nthis methodology the end users can discover models and justify them as\nself-service. Providing interpretable ML models is another benefit of this\napproach.",
                "Mitigating Label Biases for In-context Learning\nVarious design settings for in-context learning (ICL), such as the choice and\norder of the in-context examples, can bias a model toward a particular\nprediction without being reflective of an understanding of the task. While many\nstudies discuss these design choices, there have been few systematic\ninvestigations into categorizing them and mitigating their impact. In this\nwork, we define a typology for three types of label biases in ICL for text\nclassification: vanilla-label bias, context-label bias, and domain-label bias\n(which we conceptualize and detect for the first time).\n  Our analysis demonstrates that prior label bias calibration methods fall\nshort of addressing all three types of biases. Specifically, domain-label bias\nrestricts LLMs to random-level performance on many tasks regardless of the\nchoice of in-context examples. To mitigate the effect of these biases, we\npropose a simple bias calibration method that estimates a language model's\nlabel bias using random in-domain words from the task corpus. After controlling\nfor this estimated bias when making predictions, our novel domain-context\ncalibration significantly improves the ICL performance of GPT-J and GPT-3 on a\nwide range of tasks. The gain is substantial on tasks with large domain-label\nbias (up to 37% in Macro-F1). Furthermore, our results generalize to models\nwith different scales, pretraining methods, and manually-designed task\ninstructions, showing the prevalence of label biases in ICL.",
                "Learning to Learn from APIs: Black-Box Data-Free Meta-Learning\nData-free meta-learning (DFML) aims to enable efficient learning of new tasks\nby meta-learning from a collection of pre-trained models without access to the\ntraining data. Existing DFML work can only meta-learn from (i) white-box and\n(ii) small-scale pre-trained models (iii) with the same architecture,\nneglecting the more practical setting where the users only have inference\naccess to the APIs with arbitrary model architectures and model scale inside.\nTo solve this issue, we propose a Bi-level Data-free Meta Knowledge\nDistillation (BiDf-MKD) framework to transfer more general meta knowledge from\na collection of black-box APIs to one single meta model. Specifically, by just\nquerying APIs, we inverse each API to recover its training data via a\nzero-order gradient estimator and then perform meta-learning via a novel\nbi-level meta knowledge distillation structure, in which we design a boundary\nquery set recovery technique to recover a more informative query set near the\ndecision boundary. In addition, to encourage better generalization within the\nsetting of limited API budgets, we propose task memory replay to diversify the\nunderlying task distribution by covering more interpolated tasks. Extensive\nexperiments in various real-world scenarios show the superior performance of\nour BiDf-MKD framework.",
                "A Meta-learning Framework for Tuning Parameters of Protection Mechanisms\n  in Trustworthy Federated Learning\nTrustworthy Federated Learning (TFL) typically leverages protection\nmechanisms to guarantee privacy. However, protection mechanisms inevitably\nintroduce utility loss or efficiency reduction while protecting data privacy.\nTherefore, protection mechanisms and their parameters should be carefully\nchosen to strike an optimal tradeoff between \\textit{privacy leakage},\n\\textit{utility loss}, and \\textit{efficiency reduction}. To this end,\nfederated learning practitioners need tools to measure the three factors and\noptimize the tradeoff between them to choose the protection mechanism that is\nmost appropriate to the application at hand. Motivated by this requirement, we\npropose a framework that (1) formulates TFL as a problem of finding a\nprotection mechanism to optimize the tradeoff between privacy leakage, utility\nloss, and efficiency reduction and (2) formally defines bounded measurements of\nthe three factors. We then propose a meta-learning algorithm to approximate\nthis optimization problem and find optimal protection parameters for\nrepresentative protection mechanisms, including Randomization, Homomorphic\nEncryption, Secret Sharing, and Compression. We further design estimation\nalgorithms to quantify these found optimal protection parameters in a practical\nhorizontal federated learning setting and provide a theoretical analysis of the\nestimation error.",
                "JutePestDetect: An Intelligent Approach for Jute Pest Identification\n  Using Fine-Tuned Transfer Learning\nIn certain Asian countries, Jute is one of the primary sources of income and\nGross Domestic Product (GDP) for the agricultural sector. Like many other\ncrops, Jute is prone to pest infestations, and its identification is typically\nmade visually in countries like Bangladesh, India, Myanmar, and China. In\naddition, this method is time-consuming, challenging, and somewhat imprecise,\nwhich poses a substantial financial risk. To address this issue, the study\nproposes a high-performing and resilient transfer learning (TL) based\nJutePestDetect model to identify jute pests at the early stage. Firstly, we\nprepared jute pest dataset containing 17 classes and around 380 photos per pest\nclass, which were evaluated after manual and automatic pre-processing and\ncleaning, such as background removal and resizing. Subsequently, five prominent\npre-trained models -DenseNet201, InceptionV3, MobileNetV2, VGG19, and ResNet50\nwere selected from a previous study to design the JutePestDetect model. Each\nmodel was revised by replacing the classification layer with a global average\npooling layer and incorporating a dropout layer for regularization. To evaluate\nthe models performance, various metrics such as precision, recall, F1 score,\nROC curve, and confusion matrix were employed. These analyses provided\nadditional insights for determining the efficacy of the models. Among them, the\ncustomized regularized DenseNet201-based proposed JutePestDetect model\noutperformed the others, achieving an impressive accuracy of 99%. As a result,\nour proposed method and strategy offer an enhanced approach to pest\nidentification in the case of Jute, which can significantly benefit farmers\nworldwide.",
                "Dink-Net: Neural Clustering on Large Graphs\nDeep graph clustering, which aims to group the nodes of a graph into disjoint\nclusters with deep neural networks, has achieved promising progress in recent\nyears. However, the existing methods fail to scale to the large graph with\nmillion nodes. To solve this problem, a scalable deep graph clustering method\n(Dink-Net) is proposed with the idea of dilation and shrink. Firstly, by\ndiscriminating nodes, whether being corrupted by augmentations, representations\nare learned in a self-supervised manner. Meanwhile, the cluster centres are\ninitialized as learnable neural parameters. Subsequently, the clustering\ndistribution is optimized by minimizing the proposed cluster dilation loss and\ncluster shrink loss in an adversarial manner. By these settings, we unify the\ntwo-step clustering, i.e., representation learning and clustering optimization,\ninto an end-to-end framework, guiding the network to learn clustering-friendly\nfeatures. Besides, Dink-Net scales well to large graphs since the designed loss\nfunctions adopt the mini-batch data to optimize the clustering distribution\neven without performance drops. Both experimental results and theoretical\nanalyses demonstrate the superiority of our method. Compared to the runner-up,\nDink-Net achieves 9.62% NMI improvement on the ogbn-papers100M dataset with 111\nmillion nodes and 1.6 billion edges. The source code is released at\nhttps://github.com/yueliu1999/Dink-Net. Besides, a collection (papers, codes,\nand datasets) of deep graph clustering is shared at\nhttps://github.com/yueliu1999/Awesome-Deep-Graph-Clustering."
            ],
            "interesting paper": 3
        },
        {
            "papers": [
                "Learning Off-Road Terrain Traversability with Self-Supervisions Only\nEstimating the traversability of terrain should be reliable and accurate in\ndiverse conditions for autonomous driving in off-road environments. However,\nlearning-based approaches often yield unreliable results when confronted with\nunfamiliar contexts, and it is challenging to obtain manual annotations\nfrequently for new circumstances. In this paper, we introduce a method for\nlearning traversability from images that utilizes only self-supervision and no\nmanual labels, enabling it to easily learn traversability in new circumstances.\nTo this end, we first generate self-supervised traversability labels from past\ndriving trajectories by labeling regions traversed by the vehicle as highly\ntraversable. Using the self-supervised labels, we then train a neural network\nthat identifies terrains that are safe to traverse from an image using a\none-class classification algorithm. Additionally, we supplement the limitations\nof self-supervised labels by incorporating methods of self-supervised learning\nof visual representations. To conduct a comprehensive evaluation, we collect\ndata in a variety of driving environments and perceptual conditions and show\nthat our method produces reliable estimations in various environments. In\naddition, the experimental results validate that our method outperforms other\nself-supervised traversability estimation methods and achieves comparable\nperformances with supervised learning methods trained on manually labeled data.",
                "Partially Personalized Federated Learning: Breaking the Curse of Data\n  Heterogeneity\nWe present a partially personalized formulation of Federated Learning (FL)\nthat strikes a balance between the flexibility of personalization and\ncooperativeness of global training. In our framework, we split the variables\ninto global parameters, which are shared across all clients, and individual\nlocal parameters, which are kept private. We prove that under the right split\nof parameters, it is possible to find global parameters that allow each client\nto fit their data perfectly, and refer to the obtained problem as\noverpersonalized. For instance, the shared global parameters can be used to\nlearn good data representations, whereas the personalized layers are fine-tuned\nfor a specific client. Moreover, we present a simple algorithm for the\npartially personalized formulation that offers significant benefits to all\nclients. In particular, it breaks the curse of data heterogeneity in several\nsettings, such as training with local steps, asynchronous training, and\nByzantine-robust training.",
                "Task-Equivariant Graph Few-shot Learning\nAlthough Graph Neural Networks (GNNs) have been successful in node\nclassification tasks, their performance heavily relies on the availability of a\nsufficient number of labeled nodes per class. In real-world situations, not all\nclasses have many labeled nodes and there may be instances where the model\nneeds to classify new classes, making manual labeling difficult. To solve this\nproblem, it is important for GNNs to be able to classify nodes with a limited\nnumber of labeled nodes, known as few-shot node classification. Previous\nepisodic meta-learning based methods have demonstrated success in few-shot node\nclassification, but our findings suggest that optimal performance can only be\nachieved with a substantial amount of diverse training meta-tasks. To address\nthis challenge of meta-learning based few-shot learning (FSL), we propose a new\napproach, the Task-Equivariant Graph few-shot learning (TEG) framework. Our TEG\nframework enables the model to learn transferable task-adaptation strategies\nusing a limited number of training meta-tasks, allowing it to acquire\nmeta-knowledge for a wide range of meta-tasks. By incorporating equivariant\nneural networks, TEG can utilize their strong generalization abilities to learn\nhighly adaptable task-specific strategies. As a result, TEG achieves\nstate-of-the-art performance with limited training meta-tasks. Our experiments\non various benchmark datasets demonstrate TEG's superiority in terms of\naccuracy and generalization ability, even when using minimal meta-training\ndata, highlighting the effectiveness of our proposed approach in addressing the\nchallenges of meta-learning based few-shot node classification. Our code is\navailable at the following link: https://github.com/sung-won-kim/TEG",
                "Controllable Path of Destruction\nPath of Destruction (PoD) is a self-supervised method for learning iterative\ngenerators. The core idea is to produce a training set by destroying a set of\nartifacts, and for each destructive step create a training instance based on\nthe corresponding repair action. A generator trained on this dataset can then\ngenerate new artifacts by repairing from arbitrary states. The PoD method is\nvery data-efficient in terms of original training examples and well-suited to\nfunctional artifacts composed of categorical data, such as game levels and\ndiscrete 3D structures. In this paper, we extend the Path of Destruction method\nto allow designer control over aspects of the generated artifacts.\nControllability is introduced by adding conditional inputs to the state-action\npairs that make up the repair trajectories. We test the controllable PoD method\nin a 2D dungeon setting, as well as in the domain of small 3D Lego cars.",
                "Autoencoding Conditional Neural Processes for Representation Learning\nConditional neural processes (CNPs) are a flexible and efficient family of\nmodels that learn to learn a stochastic process from data. They have seen\nparticular application in contextual image completion - observing pixel values\nat some locations to predict a distribution over values at other unobserved\nlocations. However, the choice of pixels in learning CNPs is typically either\nrandom or derived from a simple statistical measure (e.g. pixel variance).\nHere, we turn the problem on its head and ask: which pixels would a CNP like to\nobserve - do they facilitate fitting better CNPs, and do such pixels tell us\nsomething meaningful about the underlying image? To this end we develop the\nPartial Pixel Space Variational Autoencoder (PPS-VAE), an amortised variational\nframework that casts CNP context as latent variables learnt simultaneously with\nthe CNP. We evaluate PPS-VAE over a number of tasks across different visual\ndata, and find that not only can it facilitate better-fit CNPs, but also that\nthe spatial arrangement and values meaningfully characterise image information\n- evaluated through the lens of classification on both within and out-of-data\ndistributions. Our model additionally allows for dynamic adaption of\ncontext-set size and the ability to scale-up to larger images, providing a\npromising avenue to explore learning meaningful and effective visual\nrepresentations.",
                "DMS: Differentiable Mean Shift for Dataset Agnostic Task Specific\n  Clustering Using Side Information\nWe present a novel approach, in which we learn to cluster data directly from\nside information, in the form of a small set of pairwise examples. Unlike\nprevious methods, with or without side information, we do not need to know the\nnumber of clusters, their centers or any kind of distance metric for\nsimilarity. Our method is able to divide the same data points in various ways\ndependant on the needs of a specific task, defined by the side information.\nContrastingly, other work generally finds only the intrinsic, most obvious,\nclusters. Inspired by the mean shift algorithm, we implement our new clustering\napproach using a custom iterative neural network to create Differentiable Mean\nShift (DMS), a state of the art, dataset agnostic, clustering method. We found\nthat it was possible to train a strong cluster definition without enforcing a\nconstraint that each cluster must be presented during training. DMS outperforms\ncurrent methods in both the intrinsic and non-intrinsic dataset tasks."
            ],
            "interesting paper": 1
        },
        {
            "papers": [
                "A Computational Account Of Self-Supervised Visual Learning From\n  Egocentric Object Play\nResearch in child development has shown that embodied experience handling\nphysical objects contributes to many cognitive abilities, including visual\nlearning. One characteristic of such experience is that the learner sees the\nsame object from several different viewpoints. In this paper, we study how\nlearning signals that equate different viewpoints -- e.g., assigning similar\nrepresentations to different views of a single object -- can support robust\nvisual learning. We use the Toybox dataset, which contains egocentric videos of\nhumans manipulating different objects, and conduct experiments using a computer\nvision framework for self-supervised contrastive learning. We find that\nrepresentations learned by equating different physical viewpoints of an object\nbenefit downstream image classification accuracy. Further experiments show that\nthis performance improvement is robust to variations in the gaps between\nviewpoints, and that the benefits transfer to several different image\nclassification tasks.",
                "Contextual Vision Transformers for Robust Representation Learning\nWe introduce Contextual Vision Transformers (ContextViT), a method designed\nto generate robust image representations for datasets experiencing shifts in\nlatent factors across various groups. Derived from the concept of in-context\nlearning, ContextViT incorporates an additional context token to encapsulate\ngroup-specific information. This integration allows the model to adjust the\nimage representation in accordance with the group-specific context.\nSpecifically, for a given input image, ContextViT maps images with identical\ngroup membership into this context token, which is appended to the input image\ntokens. Additionally, we introduce a context inference network to predict such\ntokens on-the-fly, given a batch of samples from the group. This enables\nContextViT to adapt to new testing distributions during inference time. We\ndemonstrate the efficacy of ContextViT across a wide range of applications. In\nsupervised fine-tuning, we show that augmenting pre-trained ViTs with our\nproposed context conditioning mechanism results in consistent improvements in\nout-of-distribution generalization on iWildCam and FMoW. We also investigate\nself-supervised representation learning with ContextViT. Our experiments on the\nCamelyon17 pathology imaging benchmark and the JUMP-CP microscopy imaging\nbenchmark demonstrate that ContextViT excels in learning stable image\nfeaturizations amidst distribution shift, consistently outperforming its ViT\ncounterpart.",
                "Spotlight Attention: Robust Object-Centric Learning With a Spatial\n  Locality Prior\nThe aim of object-centric vision is to construct an explicit representation\nof the objects in a scene. This representation is obtained via a set of\ninterchangeable modules called \\emph{slots} or \\emph{object files} that compete\nfor local patches of an image. The competition has a weak inductive bias to\npreserve spatial continuity; consequently, one slot may claim patches scattered\ndiffusely throughout the image. In contrast, the inductive bias of human vision\nis strong, to the degree that attention has classically been described with a\nspotlight metaphor. We incorporate a spatial-locality prior into\nstate-of-the-art object-centric vision models and obtain significant\nimprovements in segmenting objects in both synthetic and real-world datasets.\nSimilar to human visual attention, the combination of image content and spatial\nconstraints yield robust unsupervised object-centric learning, including less\nsensitivity to model hyperparameters.",
                "DENTEX: An Abnormal Tooth Detection with Dental Enumeration and\n  Diagnosis Benchmark for Panoramic X-rays\nPanoramic X-rays are frequently used in dentistry for treatment planning, but\ntheir interpretation can be both time-consuming and prone to error. Artificial\nintelligence (AI) has the potential to aid in the analysis of these X-rays,\nthereby improving the accuracy of dental diagnoses and treatment plans.\nNevertheless, designing automated algorithms for this purpose poses significant\nchallenges, mainly due to the scarcity of annotated data and variations in\nanatomical structure. To address these issues, the Dental Enumeration and\nDiagnosis on Panoramic X-rays Challenge (DENTEX) has been organized in\nassociation with the International Conference on Medical Image Computing and\nComputer-Assisted Intervention (MICCAI) in 2023. This challenge aims to promote\nthe development of algorithms for multi-label detection of abnormal teeth,\nusing three types of hierarchically annotated data: partially annotated\nquadrant data, partially annotated quadrant-enumeration data, and fully\nannotated quadrant-enumeration-diagnosis data, inclusive of four different\ndiagnoses. In this paper, we present the results of evaluating participant\nalgorithms on the fully annotated data, additionally investigating performance\nvariation for quadrant, enumeration, and diagnosis labels in the detection of\nabnormal teeth. The provision of this annotated dataset, alongside the results\nof this challenge, may lay the groundwork for the creation of AI-powered tools\nthat can offer more precise and efficient diagnosis and treatment planning in\nthe field of dentistry. The evaluation code and datasets can be accessed at\nhttps://github.com/ibrahimethemhamamci/DENTEX",
                "FedCSD: A Federated Learning Based Approach for Code-Smell Detection\nThis paper proposes a Federated Learning Code Smell Detection (FedCSD)\napproach that allows organizations to collaboratively train federated ML models\nwhile preserving their data privacy. These assertions have been supported by\nthree experiments that have significantly leveraged three manually validated\ndatasets aimed at detecting and examining different code smell scenarios. In\nexperiment 1, which was concerned with a centralized training experiment,\ndataset two achieved the lowest accuracy (92.30%) with fewer smells, while\ndatasets one and three achieved the highest accuracy with a slight difference\n(98.90% and 99.5%, respectively). This was followed by experiment 2, which was\nconcerned with cross-evaluation, where each ML model was trained using one\ndataset, which was then evaluated over the other two datasets. Results from\nthis experiment show a significant drop in the model's accuracy (lowest\naccuracy: 63.80\\%) where fewer smells exist in the training dataset, which has\na noticeable reflection (technical debt) on the model's performance. Finally,\nthe last and third experiments evaluate our approach by splitting the dataset\ninto 10 companies. The ML model was trained on the company's site, then all\nmodel-updated weights were transferred to the server. Ultimately, an accuracy\nof 98.34% was achieved by the global model that has been trained using 10\ncompanies for 100 training rounds. The results reveal a slight difference in\nthe global model's accuracy compared to the highest accuracy of the centralized\nmodel, which can be ignored in favour of the global model's comprehensive\nknowledge, lower training cost, preservation of data privacy, and avoidance of\nthe technical debt problem.",
                "A rule-general abductive learning by rough sets\nIn real-world tasks, there is usually a large amount of unlabeled data and\nlabeled data. The task of combining the two to learn is known as\nsemi-supervised learning. Experts can use logical rules to label unlabeled\ndata, but this operation is costly. The combination of perception and reasoning\nhas a good effect in processing such semi-supervised tasks with domain\nknowledge. However, acquiring domain knowledge and the correction, reduction\nand generation of rules remain complex problems to be solved. Rough set theory\nis an important method for solving knowledge processing in information systems.\nIn this paper, we propose a rule general abductive learning by rough set\n(RS-ABL). By transforming the target concept and sub-concepts of rules into\ninformation tables, rough set theory is used to solve the acquisition of domain\nknowledge and the correction, reduction and generation of rules at a lower\ncost. This framework can also generate more extensive negative rules to enhance\nthe breadth of the knowledge base. Compared with the traditional\nsemi-supervised learning method, RS-ABL has higher accuracy in dealing with\nsemi-supervised tasks."
            ],
            "interesting paper": 2
        },
        {
            "papers": [
                "Quantifying Representation Reliability in Self-Supervised Learning\n  Models\nSelf-supervised learning models extract general-purpose representations from\ndata. Quantifying the reliability of these representations is crucial, as many\ndownstream models rely on them as input for their own tasks. To this end, we\nintroduce a formal definition of representation reliability: the representation\nfor a given test point is considered to be reliable if the downstream models\nbuilt on top of that representation can consistently generate accurate\npredictions for that test point. However, accessing downstream data to quantify\nthe representation reliability is often infeasible or restricted due to privacy\nconcerns. We propose an ensemble-based method for estimating the representation\nreliability without knowing the downstream tasks a priori. Our method is based\non the concept of neighborhood consistency across distinct pre-trained\nrepresentation spaces. The key insight is to find shared neighboring points as\nanchors to align these representation spaces before comparing them. We\ndemonstrate through comprehensive numerical experiments that our method\neffectively captures the representation reliability with a high degree of\ncorrelation, achieving robust and favorable performance compared with baseline\nmethods.",
                "Doubly Robust Self-Training\nSelf-training is an important technique for solving semi-supervised learning\nproblems. It leverages unlabeled data by generating pseudo-labels and combining\nthem with a limited labeled dataset for training. The effectiveness of\nself-training heavily relies on the accuracy of these pseudo-labels. In this\npaper, we introduce doubly robust self-training, a novel semi-supervised\nalgorithm that provably balances between two extremes. When the pseudo-labels\nare entirely incorrect, our method reduces to a training process solely using\nlabeled data. Conversely, when the pseudo-labels are completely accurate, our\nmethod transforms into a training process utilizing all pseudo-labeled data and\nlabeled data, thus increasing the effective sample size. Through empirical\nevaluations on both the ImageNet dataset for image classification and the\nnuScenes autonomous driving dataset for 3D object detection, we demonstrate the\nsuperiority of the doubly robust loss over the standard self-training baseline.",
                "SSL-CPCD: Self-supervised learning with composite pretext-class\n  discrimination for improved generalisability in endoscopic image analysis\nData-driven methods have shown tremendous progress in medical image analysis.\nIn this context, deep learning-based supervised methods are widely popular.\nHowever, they require a large amount of training data and face issues in\ngeneralisability to unseen datasets that hinder clinical translation.\nEndoscopic imaging data incorporates large inter- and intra-patient variability\nthat makes these models more challenging to learn representative features for\ndownstream tasks. Thus, despite the publicly available datasets and datasets\nthat can be generated within hospitals, most supervised models still\nunderperform. While self-supervised learning has addressed this problem to some\nextent in natural scene data, there is a considerable performance gap in the\nmedical image domain. In this paper, we propose to explore patch-level\ninstance-group discrimination and penalisation of inter-class variation using\nadditive angular margin within the cosine similarity metrics. Our novel\napproach enables models to learn to cluster similar representative patches,\nthereby improving their ability to provide better separation between different\nclasses. Our results demonstrate significant improvement on all metrics over\nthe state-of-the-art (SOTA) methods on the test set from the same and diverse\ndatasets. We evaluated our approach for classification, detection, and\nsegmentation. SSL-CPCD achieves 79.77% on Top 1 accuracy for ulcerative colitis\nclassification, 88.62% on mAP for polyp detection, and 82.32% on dice\nsimilarity coefficient for segmentation tasks are nearly over 4%, 2%, and 3%,\nrespectively, compared to the baseline architectures. We also demonstrate that\nour method generalises better than all SOTA methods to unseen datasets,\nreporting nearly 7% improvement in our generalisability assessment.",
                "MERT: Acoustic Music Understanding Model with Large-Scale\n  Self-supervised Training\nSelf-supervised learning (SSL) has recently emerged as a promising paradigm\nfor training generalisable models on large-scale data in the fields of vision,\ntext, and speech. Although SSL has been proven effective in speech and audio,\nits application to music audio has yet to be thoroughly explored. This is\npartially due to the distinctive challenges associated with modelling musical\nknowledge, particularly tonal and pitched characteristics of music. To address\nthis research gap, we propose an acoustic Music undERstanding model with\nlarge-scale self-supervised Training (MERT), which incorporates teacher models\nto provide pseudo labels in the masked language modelling (MLM) style acoustic\npre-training. In our exploration, we identified an effective combination of\nteacher models, which outperforms conventional speech and audio approaches in\nterms of performance. This combination includes an acoustic teacher based on\nResidual Vector Quantisation - Variational AutoEncoder (RVQ-VAE) and a musical\nteacher based on the Constant-Q Transform (CQT). Furthermore, we explore a wide\nrange of settings to overcome the instability in acoustic language model\npre-training, which allows our designed paradigm to scale from 95M to 330M\nparameters. Experimental results indicate that our model can generalise and\nperform well on 14 music understanding tasks and attain state-of-the-art (SOTA)\noverall scores.",
                "Feature Learning in Image Hierarchies using Functional Maximal\n  Correlation\nThis paper proposes the Hierarchical Functional Maximal Correlation Algorithm\n(HFMCA), a hierarchical methodology that characterizes dependencies across two\nhierarchical levels in multiview systems. By framing view similarities as\ndependencies and ensuring contrastivity by imposing orthonormality, HFMCA\nachieves faster convergence and increased stability in self-supervised\nlearning. HFMCA defines and measures dependencies within image hierarchies,\nfrom pixels and patches to full images. We find that the network topology for\napproximating orthonormal basis functions aligns with a vanilla CNN, enabling\nthe decomposition of density ratios between neighboring layers of feature maps.\nThis approach provides powerful interpretability, revealing the resemblance\nbetween supervision and self-supervision through the lens of internal\nrepresentations.",
                "Primal-Attention: Self-attention through Asymmetric Kernel SVD in Primal\n  Representation\nRecently, a new line of works has emerged to understand and improve\nself-attention in Transformers by treating it as a kernel machine. However,\nexisting works apply the methods for symmetric kernels to the asymmetric\nself-attention, resulting in a nontrivial gap between the analytical\nunderstanding and numerical implementation. In this paper, we provide a new\nperspective to represent and optimize self-attention through asymmetric Kernel\nSingular Value Decomposition (KSVD), which is also motivated by the low-rank\nproperty of self-attention normally observed in deep layers. Through asymmetric\nKSVD, $i$) a primal-dual representation of self-attention is formulated, where\nthe optimization objective is cast to maximize the projection variances in the\nattention outputs; $ii$) a novel attention mechanism, i.e., Primal-Attention,\nis proposed via the primal representation of KSVD, avoiding explicit\ncomputation of the kernel matrix in the dual; $iii$) with KKT conditions, we\nprove that the stationary solution to the KSVD optimization in Primal-Attention\nyields a zero-value objective. In this manner, KSVD optimization can be\nimplemented by simply minimizing a regularization loss, so that low-rank\nproperty is promoted without extra decomposition. Numerical experiments show\nstate-of-the-art performance of our Primal-Attention with improved efficiency.\nMoreover, we demonstrate that the deployed KSVD optimization regularizes\nPrimal-Attention with a sharper singular value decay than that of the canonical\nself-attention, further verifying the great potential of our method. To the\nbest of our knowledge, this is the first work that provides a primal-dual\nrepresentation for the asymmetric kernel in self-attention and successfully\napplies it to modeling and optimization."
            ],
            "interesting paper": 3
        },
        {
            "papers": [
                "Scaling Up Semi-supervised Learning with Unconstrained Unlabelled Data\nWe propose UnMixMatch, a semi-supervised learning framework which can learn\neffective representations from unconstrained unlabelled data in order to scale\nup performance. Most existing semi-supervised methods rely on the assumption\nthat labelled and unlabelled samples are drawn from the same distribution,\nwhich limits the potential for improvement through the use of free-living\nunlabeled data. Consequently, the generalizability and scalability of\nsemi-supervised learning are often hindered by this assumption. Our method aims\nto overcome these constraints and effectively utilize unconstrained unlabelled\ndata in semi-supervised learning. UnMixMatch consists of three main components:\na supervised learner with hard augmentations that provides strong\nregularization, a contrastive consistency regularizer to learn underlying\nrepresentations from the unlabelled data, and a self-supervised loss to enhance\nthe representations that are learnt from the unlabelled data. We perform\nextensive experiments on 4 commonly used datasets and demonstrate superior\nperformance over existing semi-supervised methods with a performance boost of\n4.79%. Extensive ablation and sensitivity studies show the effectiveness and\nimpact of each of the proposed components of our method.",
                "Efficient Failure Pattern Identification of Predictive Algorithms\nGiven a (machine learning) classifier and a collection of unlabeled data, how\ncan we efficiently identify misclassification patterns presented in this\ndataset? To address this problem, we propose a human-machine collaborative\nframework that consists of a team of human annotators and a sequential\nrecommendation algorithm. The recommendation algorithm is conceptualized as a\nstochastic sampler that, in each round, queries the annotators a subset of\nsamples for their true labels and obtains the feedback information on whether\nthe samples are misclassified. The sampling mechanism needs to balance between\ndiscovering new patterns of misclassification (exploration) and confirming the\npotential patterns of classification (exploitation). We construct a\ndeterminantal point process, whose intensity balances the\nexploration-exploitation trade-off through the weighted update of the posterior\nat each round to form the generator of the stochastic sampler. The numerical\nresults empirically demonstrate the competitive performance of our framework on\nmultiple datasets at various signal-to-noise ratios.",
                "Rotating Features for Object Discovery\nThe binding problem in human cognition, concerning how the brain represents\nand connects objects within a fixed network of neural connections, remains a\nsubject of intense debate. Most machine learning efforts addressing this issue\nin an unsupervised setting have focused on slot-based methods, which may be\nlimiting due to their discrete nature and difficulty to express uncertainty.\nRecently, the Complex AutoEncoder was proposed as an alternative that learns\ncontinuous and distributed object-centric representations. However, it is only\napplicable to simple toy data. In this paper, we present Rotating Features, a\ngeneralization of complex-valued features to higher dimensions, and a new\nevaluation procedure for extracting objects from distributed representations.\nAdditionally, we show the applicability of our approach to pre-trained\nfeatures. Together, these advancements enable us to scale distributed\nobject-centric representations from simple toy to real-world data. We believe\nthis work advances a new paradigm for addressing the binding problem in machine\nlearning and has the potential to inspire further innovation in the field.",
                "Federated Domain Generalization: A Survey\nMachine learning typically relies on the assumption that training and testing\ndistributions are identical and that data is centrally stored for training and\ntesting. However, in real-world scenarios, distributions may differ\nsignificantly and data is often distributed across different devices,\norganizations, or edge nodes. Consequently, it is imperative to develop models\nthat can effectively generalize to unseen distributions where data is\ndistributed across different domains. In response to this challenge, there has\nbeen a surge of interest in federated domain generalization (FDG) in recent\nyears. FDG combines the strengths of federated learning (FL) and domain\ngeneralization (DG) techniques to enable multiple source domains to\ncollaboratively learn a model capable of directly generalizing to unseen\ndomains while preserving data privacy. However, generalizing the federated\nmodel under domain shifts is a technically challenging problem that has\nreceived scant attention in the research area so far. This paper presents the\nfirst survey of recent advances in this area. Initially, we discuss the\ndevelopment process from traditional machine learning to domain adaptation and\ndomain generalization, leading to FDG as well as provide the corresponding\nformal definition. Then, we categorize recent methodologies into four classes:\nfederated domain alignment, data manipulation, learning strategies, and\naggregation optimization, and present suitable algorithms in detail for each\ncategory. Next, we introduce commonly used datasets, applications, evaluations,\nand benchmarks. Finally, we conclude this survey by providing some potential\nresearch topics for the future.",
                "Learning Causally Disentangled Representations via the Principle of\n  Independent Causal Mechanisms\nLearning disentangled causal representations is a challenging problem that\nhas gained significant attention recently due to its implications for\nextracting meaningful information for downstream tasks. In this work, we define\na new notion of causal disentanglement from the perspective of independent\ncausal mechanisms. We propose ICM-VAE, a framework for learning causally\ndisentangled representations supervised by causally related observed labels. We\nmodel causal mechanisms using nonlinear learnable flow-based diffeomorphic\nfunctions to map noise variables to latent causal variables. Further, to\npromote the disentanglement of causal factors, we propose a causal\ndisentanglement prior learned from auxiliary labels and the latent causal\nstructure. We theoretically show the identifiability of causal factors and\nmechanisms up to permutation and elementwise reparameterization. We empirically\ndemonstrate that our framework induces highly disentangled causal factors,\nimproves interventional robustness, and is compatible with counterfactual\ngeneration.",
                "Joint Learning of Label and Environment Causal Independence for Graph\n  Out-of-Distribution Generalization\nWe tackle the problem of graph out-of-distribution (OOD) generalization.\nExisting graph OOD algorithms either rely on restricted assumptions or fail to\nexploit environment information in training data. In this work, we propose to\nsimultaneously incorporate label and environment causal independence (LECI) to\nfully make use of label and environment information, thereby addressing the\nchallenges faced by prior methods on identifying causal and invariant\nsubgraphs. We further develop an adversarial training strategy to jointly\noptimize these two properties for causal subgraph discovery with theoretical\nguarantees. Extensive experiments and analysis show that LECI significantly\noutperforms prior methods on both synthetic and real-world datasets,\nestablishing LECI as a practical and effective solution for graph OOD\ngeneralization.\n  Our code is available at https://github.com/divelab/LECI."
            ],
            "interesting paper": 3
        }
    ],
    "Jordan Smith": [
        {
            "papers": [
                "Local control for the collective dynamics of self-propelled particles\nUtilizing a paradigmatic model for the motion of interacting self-propelled\nparticles, we demonstrate that local accelerations at the level of individual\nparticles can drive transitions between different collective dynamics, leading\nto a control process. We find that the ability to trigger such transitions is\nhierarchically distributed among the particles and can form distinctive spatial\npatterns within the collective. Chaotic dynamics occur during the transitions,\nwhich can be attributed to fractal basin boundaries mediating the control\nprocess. The particle hierarchies described in this study offer decentralized\ncapabilities for controlling artificial swarms.",
                "Cooperative Control of Multi-Channel Linear Systems with Self-Organizing\n  Private Agents\nCooperative behavior design for multi-agent systems with collective tasks is\na critical issue in promoting swarm intelligence. This paper investigates\ncooperative control for a multi-channel system, where each channel is managed\nby an agent expected to self-organize a controller to stabilize the system\ncollaboratively by communicating with neighbors in a network. Integrating a\nstate decomposition technique and a fusion approach, a fully distributed\nprivacy-preserving mechanism is proposed to shield agents' private information\nfrom neighbors' eavesdropping. Moreover, the cost of introducing the\nprivacy-preserving mechanism and the benefit of adding more channels to the\nsystem are quantitatively analyzed. Finally, comparative simulation examples\nare provided to demonstrate the effectiveness of the theoretical results.",
                "Selection for short-term empowerment accelerates the evolution of\n  homeostatic neural cellular automata\nEmpowerment -- a domain independent, information-theoretic metric -- has\npreviously been shown to assist in the evolutionary search for neural cellular\nautomata (NCA) capable of homeostasis when employed as a fitness function. In\nour previous study, we successfully extended empowerment, defined as maximum\ntime-lagged mutual information between agents' actions and future sensations,\nto a distributed sensorimotor system embodied as an NCA. However, the\ntime-delay between actions and their corresponding sensations was arbitrarily\nchosen. Here, we expand upon previous work by exploring how the time scale at\nwhich empowerment operates impacts its efficacy as an auxiliary objective to\naccelerate the discovery of homeostatic NCAs. We show that shorter time delays\nresult in marked improvements over empowerment with longer delays, when\ncompared to evolutionary selection only for homeostasis. Moreover, we evaluate\nstability and adaptability of evolved NCAs, both hallmarks of living systems\nthat are of interest to replicate in artificial ones. We find that short-term\nempowered NCA are more stable and are capable of generalizing better to unseen\nhomeostatic challenges. Taken together, these findings motivate the use of\nempowerment during the evolution of other artifacts, and suggest how it should\nbe incorporated to accelerate evolution of desired behaviors for them. Source\ncode for the experiments in this paper can be found at:\nhttps://github.com/caitlingrasso/empowered-nca-II.",
                "Lucy-SKG: Learning to Play Rocket League Efficiently Using Deep\n  Reinforcement Learning\nA successful tactic that is followed by the scientific community for\nadvancing AI is to treat games as problems, which has been proven to lead to\nvarious breakthroughs. We adapt this strategy in order to study Rocket League,\na widely popular but rather under-explored 3D multiplayer video game with a\ndistinct physics engine and complex dynamics that pose a significant challenge\nin developing efficient and high-performance game-playing agents. In this\npaper, we present Lucy-SKG, a Reinforcement Learning-based model that learned\nhow to play Rocket League in a sample-efficient manner, outperforming by a\nnotable margin the two highest-ranking bots in this game, namely Necto (2022\nbot champion) and its successor Nexto, thus becoming a state-of-the-art agent.\nOur contributions include: a) the development of a reward analysis and\nvisualization library, b) novel parameterizable reward shape functions that\ncapture the utility of complex reward types via our proposed Kinesthetic Reward\nCombination (KRC) technique, and c) design of auxiliary neural architectures\nfor training on reward prediction and state representation tasks in an\non-policy fashion for enhanced efficiency in learning speed and performance. By\nperforming thorough ablation studies for each component of Lucy-SKG, we showed\ntheir independent effectiveness in overall performance. In doing so, we\ndemonstrate the prospects and challenges of using sample-efficient\nReinforcement Learning techniques for controlling complex dynamical systems\nunder competitive team-based multiplayer conditions.",
                "Distributed Online Rollout for Multivehicle Routing in Unmapped\n  Environments\nIn this work we consider a generalization of the well-known multivehicle\nrouting problem: given a network, a set of agents occupying a subset of its\nnodes, and a set of tasks, we seek a minimum cost sequence of movements subject\nto the constraint that each task is visited by some agent at least once. The\nclassical version of this problem assumes a central computational server that\nobserves the entire state of the system perfectly and directs individual agents\naccording to a centralized control scheme. In contrast, we assume that there is\nno centralized server and that each agent is an individual processor with no a\npriori knowledge of the underlying network (including task and agent\nlocations). Moreover, our agents possess strictly local communication and\nsensing capabilities (restricted to a fixed radius around their respective\nlocations), aligning more closely with several real-world multiagent\napplications. These restrictions introduce many challenges that are overcome\nthrough local information sharing and direct coordination between agents. We\npresent a fully distributed, online, and scalable reinforcement learning\nalgorithm for this problem whereby agents self-organize into local clusters and\nindependently apply a multiagent rollout scheme locally to each cluster. We\ndemonstrate empirically via extensive simulations that there exists a critical\nsensing radius beyond which the distributed rollout algorithm begins to improve\nover a greedy base policy. This critical sensing radius grows proportionally to\nthe $\\log^*$ function of the size of the network, and is, therefore, a small\nconstant for any relevant network. Our decentralized reinforcement learning\nalgorithm achieves approximately a factor of two cost improvement over the base\npolicy for a range of radii bounded from below and above by two and three times\nthe critical sensing radius, respectively.",
                "SPRING: Studying the Paper and Reasoning to Play Games\nOpen-world survival games pose significant challenges for AI algorithms due\nto their multi-tasking, deep exploration, and goal prioritization requirements.\nDespite reinforcement learning (RL) being popular for solving games, its high\nsample complexity limits its effectiveness in complex open-world games like\nCrafter or Minecraft. We propose a novel approach, SPRING, to read the game's\noriginal academic paper and use the knowledge learned to reason and play the\ngame through a large language model (LLM). Prompted with the LaTeX source as\ngame context and a description of the agent's current observation, our SPRING\nframework employs a directed acyclic graph (DAG) with game-related questions as\nnodes and dependencies as edges. We identify the optimal action to take in the\nenvironment by traversing the DAG and calculating LLM responses for each node\nin topological order, with the LLM's answer to final node directly translating\nto environment actions. In our experiments, we study the quality of in-context\n\"reasoning\" induced by different forms of prompts under the setting of the\nCrafter open-world environment. Our experiments suggest that LLMs, when\nprompted with consistent chain-of-thought, have great potential in completing\nsophisticated high-level trajectories. Quantitatively, SPRING with GPT-4\noutperforms all state-of-the-art RL baselines, trained for 1M steps, without\nany training. Finally, we show the potential of games as a test bed for LLMs."
            ],
            "interesting paper": 1
        },
        {
            "papers": [
                "Individuality in Swarm Robots with the Case Study of Kilobots: Noise,\n  Bug, or Feature?\nInter-individual differences are studied in natural systems, such as fish,\nbees, and humans, as they contribute to the complexity of both individual and\ncollective behaviors. However, individuality in artificial systems, such as\nrobotic swarms, is undervalued or even overlooked. Agent-specific deviations\nfrom the norm in swarm robotics are usually understood as mere noise that can\nbe minimized, for example, by calibration. We observe that robots have\nconsistent deviations and argue that awareness and knowledge of these can be\nexploited to serve a task. We measure heterogeneity in robot swarms caused by\nindividual differences in how robots act, sense, and oscillate. Our use case is\nKilobots and we provide example behaviors where the performance of robots\nvaries depending on individual differences. We show a non-intuitive example of\nphototaxis with Kilobots where the non-calibrated Kilobots show better\nperformance than the calibrated supposedly ``ideal\" one. We measure the\ninter-individual variations for heterogeneity in sensing and oscillation, too.\nWe briefly discuss how these variations can enhance the complexity of\ncollective behaviors. We suggest that by recognizing and exploring this new\nperspective on individuality, and hence diversity, in robotic swarms, we can\ngain a deeper understanding of these systems and potentially unlock new\npossibilities for their design and implementation of applications.",
                "AI Techniques in the Microservices Life-Cycle: A Survey\nMicroservices is a popular architectural style for the development of\ndistributed software, with an emphasis on modularity, scalability, and\nflexibility. Indeed, in microservice systems, functionalities are provided by\nloosely coupled, small services, each focusing on a specific business\ncapability. Building a system according to the microservices architectural\nstyle brings a number of challenges, mainly related to how the different\nmicroservices are deployed and coordinated and how they interact. In this\npaper, we provide a survey about how techniques in the area of Artificial\nIntelligence have been used to tackle these challenges.",
                "A Decentralized Spike-based Learning Framework for Sequential Capture in\n  Discrete Perimeter Defense Problem\nThis paper proposes a novel Decentralized Spike-based Learning (DSL)\nframework for the discrete Perimeter Defense Problem (d-PDP). A team of\ndefenders is operating on the perimeter to protect the circular territory from\nradially incoming intruders. At first, the d-PDP is formulated as a\nspatio-temporal multi-task assignment problem (STMTA). The problem of STMTA is\nthen converted into a multi-label learning problem to obtain labels of segments\nthat defenders have to visit in order to protect the perimeter. The DSL\nframework uses a Multi-Label Classifier using Synaptic Efficacy Function\nspiking neuRON (MLC-SEFRON) network for deterministic multi-label learning.\nEach defender contains a single MLC-SEFRON network. Each MLC-SEFRON network is\ntrained independently using input from its own perspective for decentralized\noperations. The input spikes to the MLC-SEFRON network can be directly obtained\nfrom the spatio-temporal information of defenders and intruders without any\nextra pre-processing step. The output of MLC-SEFRON contains the labels of\nsegments that a defender has to visit in order to protect the perimeter. Based\non the multi-label output from the MLC-SEFRON a trajectory is generated for a\ndefender using a Consensus-Based Bundle Algorithm (CBBA) in order to capture\nthe intruders. The target multi-label output for training MLC-SEFRON is\nobtained from an expert policy. Also, the MLC-SEFRON trained for a defender can\nbe directly used for obtaining labels of segments assigned to another defender\nwithout any retraining. The performance of MLC-SEFRON has been evaluated for\nfull observation and partial observation scenarios of the defender. The overall\nperformance of the DSL framework is then compared with expert policy along with\nother existing learning algorithms. The scalability of the DSL has been\nevaluated using an increasing number of defenders.",
                "Exploring the Adaptive Behaviors of Particle Lenia: A\n  Perturbation-Response Analysis for Computational Agency\nA firm cognitive subject or ``individual'' is presupposed for the emergence\nof mind. However, with the development of recent information technology, the\n``individual'' has become more dispersed in society and the cognitive subject\nhas become increasingly unstable and adaptive, necessitating an update in our\nunderstanding of the ``individual''. Autopoiesis serves as a model of the\ncognitive subject, which is unstable and requires effort to maintain itself to\nadapt to the environment. In this study, we evaluated adaptivity for a highly\nextensible multi-particle system model Particle Lenia through the response\nperturbation. As a result, we found that Particle Lenia has a particle\nconfiguration that is both temporally unstable and has multiple stable states.\nThis result suggests that Particle Lenia can express adaptive characteristics\nand is expected to be used as a computational model toward building an\nautopoietic cognitive agent.",
                "Ghost in the Minecraft: Generally Capable Agents for Open-World\n  Environments via Large Language Models with Text-based Knowledge and Memory\nThe captivating realm of Minecraft has attracted substantial research\ninterest in recent years, serving as a rich platform for developing intelligent\nagents capable of functioning in open-world environments. However, the current\nresearch landscape predominantly focuses on specific objectives, such as the\npopular \"ObtainDiamond\" task, and has not yet shown effective generalization to\na broader spectrum of tasks. Furthermore, the current leading success rate for\nthe \"ObtainDiamond\" task stands at around 20%, highlighting the limitations of\nReinforcement Learning (RL) based controllers used in existing methods. To\ntackle these challenges, we introduce Ghost in the Minecraft (GITM), a novel\nframework integrates Large Language Models (LLMs) with text-based knowledge and\nmemory, aiming to create Generally Capable Agents (GCAs) in Minecraft. These\nagents, equipped with the logic and common sense capabilities of LLMs, can\nskillfully navigate complex, sparse-reward environments with text-based\ninteractions. We develop a set of structured actions and leverage LLMs to\ngenerate action plans for the agents to execute. The resulting LLM-based agent\nmarkedly surpasses previous methods, achieving a remarkable improvement of\n+47.5% in success rate on the \"ObtainDiamond\" task, demonstrating superior\nrobustness compared to traditional RL-based controllers. Notably, our agent is\nthe first to procure all items in the Minecraft Overworld technology tree,\ndemonstrating its extensive capabilities. GITM does not need any GPU for\ntraining, but a single CPU node with 32 CPU cores is enough. This research\nshows the potential of LLMs in developing capable agents for handling\nlong-horizon, complex tasks and adapting to uncertainties in open-world\nenvironments. See the project website at https://github.com/OpenGVLab/GITM.",
                "On Computing Universal Plans for Partially Observable Multi-Agent Path\n  Finding\nMulti-agent routing problems have drawn significant attention nowadays due to\ntheir broad industrial applications in, e.g., warehouse robots, logistics\nautomation, and traffic control. Conventionally, they are modelled as classical\nplanning problems. In this paper, we argue that it is beneficial to formulate\nthem as universal planning problems. We therefore propose universal plans, also\nknown as policies, as the solution concepts, and implement a system called\nASP-MAUPF (Answer Set Programming for Multi-Agent Universal Plan Finding) for\ncomputing them. Given an arbitrary two-dimensional map and a profile of goals\nfor the agents, the system finds a feasible universal plan for each agent that\nensures no collision with others. We use the system to conduct some\nexperiments, and make some observations on the types of goal profiles and\nenvironments that will have feasible policies, and how they may depend on\nagents' sensors. We also demonstrate how users can customize action preferences\nto compute more efficient policies, even (near-)optimal ones."
            ],
            "interesting paper": 1
        },
        {
            "papers": [
                "Communication-Efficient Reinforcement Learning in Swarm Robotic Networks\n  for Maze Exploration\nSmooth coordination within a swarm robotic system is essential for the\neffective execution of collective robot missions. Having efficient\ncommunication is key to the successful coordination of swarm robots. This paper\nproposes a new communication-efficient decentralized cooperative reinforcement\nlearning algorithm for coordinating swarm robots. It is made efficient by\nhierarchically building on the use of local information exchanges. We consider\na case study application of maze solving through cooperation among a group of\nrobots, where the time and costs are minimized while avoiding inter-robot\ncollisions and path overlaps during exploration. With a solid theoretical\nbasis, we extensively analyze the algorithm with realistic CORE network\nsimulations and evaluate it against state-of-the-art solutions in terms of maze\ncoverage percentage and efficiency under communication-degraded environments.\nThe results demonstrate significantly higher coverage accuracy and efficiency\nwhile reducing costs and overlaps even in high packet loss and low\ncommunication range scenarios.",
                "MADiff: Offline Multi-agent Learning with Diffusion Models\nDiffusion model (DM) recently achieved huge success in various scenarios\nincluding offline reinforcement learning, where the diffusion planner learn to\ngenerate desired trajectories during online evaluations. However, despite the\neffectiveness in single-agent learning, it remains unclear how DMs can operate\nin multi-agent problems, where agents can hardly complete teamwork without good\ncoordination by independently modeling each agent's trajectories. In this\npaper, we propose MADiff, a novel generative multi-agent learning framework to\ntackle this problem. MADiff is realized with an attention-based diffusion model\nto model the complex coordination among behaviors of multiple agents. To the\nbest of our knowledge, MADiff is the first diffusion-based multi-agent learning\nframework, which behaves as both a decentralized policy and a centralized\ncontroller. During decentralized executions, MADiff simultaneously performs\nteammate modeling, and the centralized controller can also be applied in\nmulti-agent trajectory predictions. Our experiments show the superior\nperformance of MADiff compared to baseline algorithms in a wide range of\nmulti-agent learning tasks, which emphasizes the effectiveness of MADiff in\nmodeling complex multi-agent interactions. Our code is available at\nhttps://github.com/zbzhu99/madiff.",
                "Mindstorms in Natural Language-Based Societies of Mind\nBoth Minsky's \"society of mind\" and Schmidhuber's \"learning to think\" inspire\ndiverse societies of large multimodal neural networks (NNs) that solve problems\nby interviewing each other in a \"mindstorm.\" Recent implementations of NN-based\nsocieties of minds consist of large language models (LLMs) and other NN-based\nexperts communicating through a natural language interface. In doing so, they\novercome the limitations of single LLMs, improving multimodal zero-shot\nreasoning. In these natural language-based societies of mind (NLSOMs), new\nagents -- all communicating through the same universal symbolic language -- are\neasily added in a modular fashion. To demonstrate the power of NLSOMs, we\nassemble and experiment with several of them (having up to 129 members),\nleveraging mindstorms in them to solve some practical AI tasks: visual question\nanswering, image captioning, text-to-image synthesis, 3D generation, egocentric\nretrieval, embodied AI, and general language-based task solving. We view this\nas a starting point towards much larger NLSOMs with billions of agents-some of\nwhich may be humans. And with this emergence of great societies of\nheterogeneous minds, many new research questions have suddenly become paramount\nto the future of artificial intelligence. What should be the social structure\nof an NLSOM? What would be the (dis)advantages of having a monarchical rather\nthan a democratic structure? How can principles of NN economies be used to\nmaximize the total reward of a reinforcement learning NLSOM? In this work, we\nidentify, discuss, and try to answer some of these questions.",
                "Attention Schema in Neural Agents\nAttention has become a common ingredient in deep learning architectures. It\nadds a dynamical selection of information on top of the static selection of\ninformation supported by weights. In the same way, we can imagine a\nhigher-order informational filter built on top of attention: an Attention\nSchema (AS), namely, a descriptive and predictive model of attention. In\ncognitive neuroscience, Attention Schema Theory (AST) supports this idea of\ndistinguishing attention from AS. A strong prediction of this theory is that an\nagent can use its own AS to also infer the states of other agents' attention\nand consequently enhance coordination with other agents. As such, multi-agent\nreinforcement learning would be an ideal setting to experimentally test the\nvalidity of AST. We explore different ways in which attention and AS interact\nwith each other. Our preliminary results indicate that agents that implement\nthe AS as a recurrent internal control achieve the best performance. In\ngeneral, these exploratory experiments suggest that equipping artificial agents\nwith a model of attention can enhance their social intelligence.",
                "A Model-Based Solution to the Offline Multi-Agent Reinforcement Learning\n  Coordination Problem\nTraining multiple agents to coordinate is an essential problem with\napplications in robotics, game theory, economics, and social sciences. However,\nmost existing Multi-Agent Reinforcement Learning (MARL) methods are online and\nthus impractical for real-world applications in which collecting new\ninteractions is costly or dangerous. While these algorithms should leverage\noffline data when available, doing so gives rise to what we call the offline\ncoordination problem. Specifically, we identify and formalize the strategy\nagreement (SA) and the strategy fine-tuning (SFT) coordination challenges, two\nissues at which current offline MARL algorithms fail. Concretely, we reveal\nthat the prevalent model-free methods are severely deficient and cannot handle\ncoordination-intensive offline multi-agent tasks in either toy or MuJoCo\ndomains. To address this setback, we emphasize the importance of inter-agent\ninteractions and propose the very first model-based offline MARL method. Our\nresulting algorithm, Model-based Offline Multi-Agent Proximal Policy\nOptimization (MOMA-PPO) generates synthetic interaction data and enables agents\nto converge on a strategy while fine-tuning their policies accordingly. This\nsimple model-based solution solves the coordination-intensive offline tasks,\nsignificantly outperforming the prevalent model-free methods even under severe\npartial observability and with learned world models.",
                "Exploiting Large Neuroimaging Datasets to Create Connectome-Constrained\n  Approaches for more Robust, Efficient, and Adaptable Artificial Intelligence\nDespite the progress in deep learning networks, efficient learning at the\nedge (enabling adaptable, low-complexity machine learning solutions) remains a\ncritical need for defense and commercial applications. We envision a pipeline\nto utilize large neuroimaging datasets, including maps of the brain which\ncapture neuron and synapse connectivity, to improve machine learning\napproaches. We have pursued different approaches within this pipeline\nstructure. First, as a demonstration of data-driven discovery, the team has\ndeveloped a technique for discovery of repeated subcircuits, or motifs. These\nwere incorporated into a neural architecture search approach to evolve network\narchitectures. Second, we have conducted analysis of the heading direction\ncircuit in the fruit fly, which performs fusion of visual and angular velocity\nfeatures, to explore augmenting existing computational models with new insight.\nOur team discovered a novel pattern of connectivity, implemented a new model,\nand demonstrated sensor fusion on a robotic platform. Third, the team analyzed\ncircuitry for memory formation in the fruit fly connectome, enabling the design\nof a novel generative replay approach. Finally, the team has begun analysis of\nconnectivity in mammalian cortex to explore potential improvements to\ntransformer networks. These constraints increased network robustness on the\nmost challenging examples in the CIFAR-10-C computer vision robustness\nbenchmark task, while reducing learnable attention parameters by over an order\nof magnitude. Taken together, these results demonstrate multiple potential\napproaches to utilize insight from neural systems for developing robust and\nefficient machine learning techniques."
            ],
            "interesting paper": 1
        },
        {
            "papers": [
                "On the Value of Myopic Behavior in Policy Reuse\nLeveraging learned strategies in unfamiliar scenarios is fundamental to human\nintelligence. In reinforcement learning, rationally reusing the policies\nacquired from other tasks or human experts is critical for tackling problems\nthat are difficult to learn from scratch. In this work, we present a framework\ncalled Selective Myopic bEhavior Control~(SMEC), which results from the insight\nthat the short-term behaviors of prior policies are sharable across tasks. By\nevaluating the behaviors of prior policies via a hybrid value function\narchitecture, SMEC adaptively aggregates the sharable short-term behaviors of\nprior policies and the long-term behaviors of the task policy, leading to\ncoordinated decisions. Empirical results on a collection of manipulation and\nlocomotion tasks demonstrate that SMEC outperforms existing methods, and\nvalidate the ability of SMEC to leverage related prior policies.",
                "Probing reaction channels via reinforcement learning\nWe propose a reinforcement learning based method to identify important\nconfigurations that connect reactant and product states along chemical reaction\npaths. By shooting multiple trajectories from these configurations, we can\ngenerate an ensemble of configurations that concentrate on the transition path\nensemble. This configuration ensemble can be effectively employed in a neural\nnetwork-based partial differential equation solver to obtain an approximation\nsolution of a restricted Backward Kolmogorov equation, even when the dimension\nof the problem is very high. The resulting solution, known as the committor\nfunction, encodes mechanistic information for the reaction and can in turn be\nused to evaluate reaction rates.",
                "Emergent Modularity in Pre-trained Transformers\nThis work examines the presence of modularity in pre-trained Transformers, a\nfeature commonly found in human brains and thought to be vital for general\nintelligence. In analogy to human brains, we consider two main characteristics\nof modularity: (1) functional specialization of neurons: we evaluate whether\neach neuron is mainly specialized in a certain function, and find that the\nanswer is yes. (2) function-based neuron grouping: we explore finding a\nstructure that groups neurons into modules by function, and each module works\nfor its corresponding function. Given the enormous amount of possible\nstructures, we focus on Mixture-of-Experts as a promising candidate, which\npartitions neurons into experts and usually activates different experts for\ndifferent inputs. Experimental results show that there are functional experts,\nwhere clustered are the neurons specialized in a certain function. Moreover,\nperturbing the activations of functional experts significantly affects the\ncorresponding function. Finally, we study how modularity emerges during\npre-training, and find that the modular structure is stabilized at the early\nstage, which is faster than neuron stabilization. It suggests that Transformers\nfirst construct the modular structure and then learn fine-grained neuron\nfunctions. Our code and data are available at\nhttps://github.com/THUNLP/modularity-analysis.",
                "Observation Denoising in CYRUS Soccer Simulation 2D Team For RoboCup\n  2023\nThe RoboCup competitions hold various leagues, and the Soccer Simulation 2D\nLeague is a major one among them. Soccer Simulation 2D (SS2D) match involves\ntwo teams, including 11 players and a coach, competing against each other. The\nplayers can only communicate with the Soccer Simulation Server during the game.\nThis paper presents the latest research of the CYRUS soccer simulation 2D team,\nthe champion of RoboCup 2021. We will explain our denoising idea powered by\nlong short-term memory networks (LSTM) and deep neural networks (DNN). The\nCYRUS team uses the CYRUS2D base code that was developed based on the Helios\nand Gliders bases.",
                "Integrating Action Knowledge and LLMs for Task Planning and Situation\n  Handling in Open Worlds\nTask planning systems have been developed to help robots use human knowledge\n(about actions) to complete long-horizon tasks. Most of them have been\ndeveloped for \"closed worlds\" while assuming the robot is provided with\ncomplete world knowledge. However, the real world is generally open, and the\nrobots frequently encounter unforeseen situations that can potentially break\nthe planner's completeness. Could we leverage the recent advances on\npre-trained Large Language Models (LLMs) to enable classical planning systems\nto deal with novel situations?\n  This paper introduces a novel framework, called COWP, for open-world task\nplanning and situation handling. COWP dynamically augments the robot's action\nknowledge, including the preconditions and effects of actions, with\ntask-oriented commonsense knowledge. COWP embraces the openness from LLMs, and\nis grounded to specific domains via action knowledge. For systematic\nevaluations, we collected a dataset that includes 1,085 execution-time\nsituations. Each situation corresponds to a state instance wherein a robot is\npotentially unable to complete a task using a solution that normally works.\nExperimental results show that our approach outperforms competitive baselines\nfrom the literature in the success rate of service tasks. Additionally, we have\ndemonstrated COWP using a mobile manipulator. Supplementary materials are\navailable at: https://cowplanning.github.io/",
                "Synthesizing a Progression of Subtasks for Block-Based Visual\n  Programming Tasks\nBlock-based visual programming environments play an increasingly important\nrole in introducing computing concepts to K-12 students. In recent years, they\nhave also gained popularity in neuro-symbolic AI, serving as a benchmark to\nevaluate general problem-solving and logical reasoning skills. The open-ended\nand conceptual nature of these visual programming tasks make them challenging,\nboth for state-of-the-art AI agents as well as for novice programmers. A\nnatural approach to providing assistance for problem-solving is breaking down a\ncomplex task into a progression of simpler subtasks; however, this is not\ntrivial given that the solution codes are typically nested and have non-linear\nexecution behavior. In this paper, we formalize the problem of synthesizing\nsuch a progression for a given reference block-based visual programming task.\nWe propose a novel synthesis algorithm that generates a progression of subtasks\nthat are high-quality, well-spaced in terms of their complexity, and solving\nthis progression leads to solving the reference task. We show the utility of\nour synthesis algorithm in improving the efficacy of AI agents (in this case,\nneural program synthesizers) for solving tasks in the Karel programming\nenvironment. Then, we conduct a user study to demonstrate that our synthesized\nprogression of subtasks can assist a novice programmer in solving tasks in the\nHour of Code: Maze Challenge by Code-dot-org."
            ],
            "interesting paper": 3
        },
        {
            "papers": [
                "A Hybrid Framework of Reinforcement Learning and Convex Optimization for\n  UAV-Based Autonomous Metaverse Data Collection\nUnmanned aerial vehicles (UAVs) are promising for providing communication\nservices due to their advantages in cost and mobility, especially in the\ncontext of the emerging Metaverse and Internet of Things (IoT). This paper\nconsiders a UAV-assisted Metaverse network, in which UAVs extend the coverage\nof the base station (BS) to collect the Metaverse data generated at roadside\nunits (RSUs). Specifically, to improve the data collection efficiency, resource\nallocation and trajectory control are integrated into the system model. The\ntime-dependent nature of the optimization problem makes it non-trivial to be\nsolved by traditional convex optimization methods. Based on the proposed\nUAV-assisted Metaverse network system model, we design a hybrid framework with\nreinforcement learning and convex optimization to {cooperatively} solve the\ntime-sequential optimization problem. Simulation results show that the proposed\nframework is able to reduce the mission completion time with a given\ntransmission power resource.",
                "Taming AI Bots: Controllability of Neural States in Large Language\n  Models\nWe tackle the question of whether an agent can, by suitable choice of\nprompts, control an AI bot to any state. To that end, we first introduce a\nformal definition of ``meaning'' that is amenable to analysis. Then, we\ncharacterize ``meaningful data'' on which large language models (LLMs) are\nostensibly trained, and ``well-trained LLMs'' through conditions that are\nlargely met by today's LLMs. While a well-trained LLM constructs an embedding\nspace of meanings that is Euclidean, meanings themselves do not form a vector\n(linear) subspace, but rather a quotient space within. We then characterize the\nsubset of meanings that can be reached by the state of the LLMs for some input\nprompt, and show that a well-trained bot can reach any meaning albeit with\nsmall probability. We then introduce a stronger notion of controllability as\n{\\em almost certain reachability}, and show that, when restricted to the space\nof meanings, an AI bot is controllable. We do so after introducing a functional\ncharacterization of attentive AI bots, and finally derive necessary and\nsufficient conditions for controllability. The fact that AI bots are\ncontrollable means that an adversary could steer them towards any state.\nHowever, the sampling process can be designed to counteract adverse actions and\navoid reaching undesirable regions of state space before their boundary is\ncrossed.",
                "Diffusion Model is an Effective Planner and Data Synthesizer for\n  Multi-Task Reinforcement Learning\nDiffusion models have demonstrated highly-expressive generative capabilities\nin vision and NLP. Recent studies in reinforcement learning (RL) have shown\nthat diffusion models are also powerful in modeling complex policies or\ntrajectories in offline datasets. However, these works have been limited to\nsingle-task settings where a generalist agent capable of addressing multi-task\npredicaments is absent. In this paper, we aim to investigate the effectiveness\nof a single diffusion model in modeling large-scale multi-task offline data,\nwhich can be challenging due to diverse and multimodal data distribution.\nSpecifically, we propose Multi-Task Diffusion Model (\\textsc{MTDiff}), a\ndiffusion-based method that incorporates Transformer backbones and prompt\nlearning for generative planning and data synthesis in multi-task offline\nsettings. \\textsc{MTDiff} leverages vast amounts of knowledge available in\nmulti-task data and performs implicit knowledge sharing among tasks. For\ngenerative planning, we find \\textsc{MTDiff} outperforms state-of-the-art\nalgorithms across 50 tasks on Meta-World and 8 maps on Maze2D. For data\nsynthesis, \\textsc{MTDiff} generates high-quality data for testing tasks given\na single demonstration as a prompt, which enhances the low-quality datasets for\neven unseen tasks.",
                "Beyond the Meta: Leveraging Game Design Parameters for Patch-Agnostic\n  Esport Analytics\nEsport games comprise a sizeable fraction of the global games market, and is\nthe fastest growing segment in games. This has given rise to the domain of\nesports analytics, which uses telemetry data from games to inform players,\ncoaches, broadcasters and other stakeholders. Compared to traditional sports,\nesport titles change rapidly, in terms of mechanics as well as rules. Due to\nthese frequent changes to the parameters of the game, esport analytics models\ncan have a short life-spam, a problem which is largely ignored within the\nliterature. This paper extracts information from game design (i.e. patch notes)\nand utilises clustering techniques to propose a new form of character\nrepresentation. As a case study, a neural network model is trained to predict\nthe number of kills in a Dota 2 match utilising this novel character\nrepresentation technique. The performance of this model is then evaluated\nagainst two distinct baselines, including conventional techniques. Not only did\nthe model significantly outperform the baselines in terms of accuracy (85%\nAUC), but the model also maintains the accuracy in two newer iterations of the\ngame that introduced one new character and a brand new character type. These\nchanges introduced to the design of the game would typically break conventional\ntechniques that are commonly used within the literature. Therefore, the\nproposed methodology for representing characters can increase the life-spam of\nmachine learning models as well as contribute to a higher performance when\ncompared to traditional techniques typically employed within the literature.",
                "Continual Task Allocation in Meta-Policy Network via Sparse Prompting\nHow to train a generalizable meta-policy by continually learning a sequence\nof tasks? It is a natural human skill yet challenging to achieve by current\nreinforcement learning: the agent is expected to quickly adapt to new tasks\n(plasticity) meanwhile retaining the common knowledge from previous tasks\n(stability). We address it by \"Continual Task Allocation via Sparse Prompting\n(CoTASP)\", which learns over-complete dictionaries to produce sparse masks as\nprompts extracting a sub-network for each task from a meta-policy network.\nCoTASP trains a policy for each task by optimizing the prompts and the\nsub-network weights alternatively. The dictionary is then updated to align the\noptimized prompts with tasks' embedding, thereby capturing tasks' semantic\ncorrelations. Hence, relevant tasks share more neurons in the meta-policy\nnetwork due to similar prompts while cross-task interference causing forgetting\nis effectively restrained. Given a meta-policy and dictionaries trained on\nprevious tasks, new task adaptation reduces to highly efficient sparse\nprompting and sub-network finetuning. In experiments, CoTASP achieves a\npromising plasticity-stability trade-off without storing or replaying any past\ntasks' experiences. It outperforms existing continual and multi-task RL methods\non all seen tasks, forgetting reduction, and generalization to unseen tasks.",
                "Determinantal Point Process Attention Over Grid Cell Code Supports Out\n  of Distribution Generalization\nDeep neural networks have made tremendous gains in emulating human-like\nintelligence, and have been used increasingly as ways of understanding how the\nbrain may solve the complex computational problems on which this relies.\nHowever, these still fall short of, and therefore fail to provide insight into\nhow the brain supports strong forms of generalization of which humans are\ncapable. One such case is out-of-distribution (OOD) generalization-successful\nperformance on test examples that lie outside the distribution of the training\nset. Here, we identify properties of processing in the brain that may\ncontribute to this ability. We describe a two-part algorithm that draws on\nspecific features of neural computation to achieve OOD generalization, and\nprovide a proof of concept by evaluating performance on two challenging\ncognitive tasks. First we draw on the fact that the mammalian brain represents\nmetric spaces using grid cell code (e.g., in the entorhinal cortex): abstract\nrepresentations of relational structure, organized in recurring motifs that\ncover the representational space. Second, we propose an attentional mechanism\nthat operates over the grid cell code using Determinantal Point Process (DPP),\nthat we call DPP attention (DPP-A) -- a transformation that ensures maximum\nsparseness in the coverage of that space. We show that a loss function that\ncombines standard task-optimized error with DPP-A can exploit the recurring\nmotifs in the grid cell code, and can be integrated with common architectures\nto achieve strong OOD generalization performance on analogy and arithmetic\ntasks. This provides both an interpretation of how the grid cell code in the\nmammalian brain may contribute to generalization performance, and at the same\ntime a potential means for improving such capabilities in artificial neural\nnetworks."
            ],
            "interesting paper": 3
        },
        {
            "papers": [
                "Towards a Unifying Model of Rationality in Multiagent Systems\nMultiagent systems deployed in the real world need to cooperate with other\nagents (including humans) nearly as effectively as these agents cooperate with\none another. To design such AI, and provide guarantees of its effectiveness, we\nneed to clearly specify what types of agents our AI must be able to cooperate\nwith. In this work we propose a generic model of socially intelligent agents,\nwhich are individually rational learners that are also able to cooperate with\none another (in the sense that their joint behavior is Pareto efficient). We\ndefine rationality in terms of the regret incurred by each agent over its\nlifetime, and show how we can construct socially intelligent agents for\ndifferent forms of regret. We then discuss the implications of this model for\nthe development of \"robust\" MAS that can cooperate with a wide variety of\nsocially intelligent agents.",
                "Steering control of payoff-maximizing players in adaptive learning\n  dynamics\nEvolutionary game theory provides a mathematical foundation for\ncross-disciplinary fertilization, especially for integrating ideas from\nartificial intelligence and game theory. Such integration offers a transparent\nand rigorous approach to complex decision-making problems in a variety of\nimportant contexts, ranging from evolutionary computation to machine behavior.\nDespite the astronomically huge individual behavioral strategy space for\ninteractions in the iterated Prisoner's Dilemma (IPD) games, the so-called\nZero-Determinant (ZD) strategies is a set of rather simple memory-one\nstrategies yet can unilaterally set a linear payoff relationship between\nthemselves and their opponent. Although the witting of ZD strategies gives\nplayers an upper hand in the IPD games, we find and characterize unbending\nstrategies that can force ZD players to be fair in their own interest.\nMoreover, our analysis reveals the ubiquity of unbending properties in common\nIPD strategies which are previously overlooked. In this work, we demonstrate\nthe important steering role of unbending strategies in fostering fairness and\ncooperation in pairwise interactions. Our results will help bring a new\nperspective by means of combining game theory and multi-agent learning systems\nfor optimizing winning strategies that are robust to noises, errors, and\ndeceptions in non-zero-sum games.",
                "An Emergency Disposal Decision-making Method with Human--Machine\n  Collaboration\nRapid developments in artificial intelligence technology have led to unmanned\nsystems replacing human beings in many fields requiring high-precision\npredictions and decisions. In modern operational environments, all job plans\nare affected by emergency events such as equipment failures and resource\nshortages, making a quick resolution critical. The use of unmanned systems to\nassist decision-making can improve resolution efficiency, but their\ndecision-making is not interpretable and may make the wrong decisions. Current\nunmanned systems require human supervision and control. Based on this, we\npropose a collaborative human--machine method for resolving unplanned events\nusing two phases: task filtering and task scheduling. In the task filtering\nphase, we propose a human--machine collaborative decision-making algorithm for\ndynamic tasks. The GACRNN model is used to predict the state of the job nodes,\nlocate the key nodes, and generate a machine-predicted resolution task list. A\nhuman decision-maker supervises the list in real time and modifies and confirms\nthe machine-predicted list through the human--machine interface. In the task\nscheduling phase, we propose a scheduling algorithm that integrates human\nexperience constraints. The steps to resolve an event are inserted into the\nnormal job sequence to schedule the resolution. We propose several\nhuman--machine collaboration methods in each phase to generate steps to resolve\nan unplanned event while minimizing the impact on the original job plan.",
                "Controllable Path of Destruction\nPath of Destruction (PoD) is a self-supervised method for learning iterative\ngenerators. The core idea is to produce a training set by destroying a set of\nartifacts, and for each destructive step create a training instance based on\nthe corresponding repair action. A generator trained on this dataset can then\ngenerate new artifacts by repairing from arbitrary states. The PoD method is\nvery data-efficient in terms of original training examples and well-suited to\nfunctional artifacts composed of categorical data, such as game levels and\ndiscrete 3D structures. In this paper, we extend the Path of Destruction method\nto allow designer control over aspects of the generated artifacts.\nControllability is introduced by adding conditional inputs to the state-action\npairs that make up the repair trajectories. We test the controllable PoD method\nin a 2D dungeon setting, as well as in the domain of small 3D Lego cars.",
                "Split Federated Learning: Speed up Model Training in Resource-Limited\n  Wireless Networks\nIn this paper, we propose a novel distributed learning scheme, named\ngroup-based split federated learning (GSFL), to speed up artificial\nintelligence (AI) model training. Specifically, the GSFL operates in a\nsplit-then-federated manner, which consists of three steps: 1) Model\ndistribution, in which the access point (AP) splits the AI models and\ndistributes the client-side models to clients; 2) Model training, in which each\nclient executes forward propagation and transmit the smashed data to the edge\nserver. The edge server executes forward and backward propagation and then\nreturns the gradient to the clients for updating local client-side models; and\n3) Model aggregation, in which edge servers aggregate the server-side and\nclient-side models. Simulation results show that the GSFL outperforms vanilla\nsplit learning and federated learning schemes in terms of overall training\nlatency while achieving satisfactory accuracy.",
                "Magnetic field regression using artificial neural networks for cold atom\n  experiments\nAccurately measuring magnetic fields is essential for magnetic-field\nsensitive experiments in fields like atomic, molecular, and optical physics,\ncondensed matter experiments, and other areas. However, since many experiments\nare conducted in an isolated vacuum environment that is inaccessible to\nexperimentalists, it can be challenging to accurately determine the magnetic\nfield. Here, we propose an efficient method for detecting magnetic fields with\nthe assistance of an artificial neural network (NN). Instead of measuring the\nmagnetic field directly at the desired location, we detect magnetic fields at\nseveral surrounding positions, and a trained NN can accurately predict the\nmagnetic field at the target location. After training, we achieve a relative\nerror of magnetic field magnitude (magnitude of error over the magnitude of\nmagnetic field) below 0.3$\\%$, and we successfully apply this method to our\nerbium quantum gas apparatus. This approach significantly simplifies the\nprocess of determining magnetic fields in isolated vacuum environments and can\nbe applied to various research fields across a wide range of magnetic field\nmagnitudes."
            ],
            "interesting paper": 3
        },
        {
            "papers": [
                "NetHack is Hard to Hack\nNeural policy learning methods have achieved remarkable results in various\ncontrol problems, ranging from Atari games to simulated locomotion. However,\nthese methods struggle in long-horizon tasks, especially in open-ended\nenvironments with multi-modal observations, such as the popular dungeon-crawler\ngame, NetHack. Intriguingly, the NeurIPS 2021 NetHack Challenge revealed that\nsymbolic agents outperformed neural approaches by over four times in median\ngame score. In this paper, we delve into the reasons behind this performance\ngap and present an extensive study on neural policy learning for NetHack. To\nconduct this study, we analyze the winning symbolic agent, extending its\ncodebase to track internal strategy selection in order to generate one of the\nlargest available demonstration datasets. Utilizing this dataset, we examine\n(i) the advantages of an action hierarchy; (ii) enhancements in neural\narchitecture; and (iii) the integration of reinforcement learning with\nimitation learning. Our investigations produce a state-of-the-art neural agent\nthat surpasses previous fully neural policies by 127% in offline settings and\n25% in online settings on median game score. However, we also demonstrate that\nmere scaling is insufficient to bridge the performance gap with the best\nsymbolic models or even the top human players.",
                "Uncovering multifunctional mechano-intelligence in and through phononic\n  metastructures harnessing physical reservoir computing\nThe recent advances in autonomous systems have prompted a strong demand for\nthe next generation of adaptive structures and materials to possess more\nbuilt-in intelligence in their mechanical domain, the so-called\nmechano-intelligence (MI). Previous MI attempts mainly focused on specific\ndesigns and case studies to realize limited aspects of MI, and there is a lack\nof a systematic foundation in constructing and integrating the different\nelements of intelligence in an effective and efficient manner. Here, we propose\na new approach to create the needed foundation in realizing integrated\nmultifunctional MI via a physical reservoir computing (PRC) framework. That is,\nto concurrently embody computing power and the various elements of\nintelligence, namely perception, decision-making, and commanding, directly in\nthe mechanical domain, advancing from conventional adaptive structures that\nrely solely on add-on digital computers and massive electronics to achieve\nintelligence. As an exemplar platform, we construct a mechanically intelligent\nphononic metastructure with the integrated elements of MI by harnessing the PRC\npower hidden in their high-degree-of-freedom nonlinear dynamics. Through\nanalyses and experimental investigations, we uncover multiple adaptive\nstructural functions ranging from self-tuning wave controls to wave-based logic\ngates. This research will provide the basis for creating future new structures\nthat would greatly surpass the state of the art - such as lower power\nconsumption, more direct interactions, and much better survivability in harsh\nenvironment or under cyberattacks. Moreover, it will enable the addition of new\nfunctions and autonomy to systems without overburdening the onboard computers.",
                "Symmetry-Aware Robot Design with Structured Subgroups\nRobot design aims at learning to create robots that can be easily controlled\nand perform tasks efficiently. Previous works on robot design have proven its\nability to generate robots for various tasks. However, these works searched the\nrobots directly from the vast design space and ignored common structures,\nresulting in abnormal robots and poor performance. To tackle this problem, we\npropose a Symmetry-Aware Robot Design (SARD) framework that exploits the\nstructure of the design space by incorporating symmetry searching into the\nrobot design process. Specifically, we represent symmetries with the subgroups\nof the dihedral group and search for the optimal symmetry in structured\nsubgroups. Then robots are designed under the searched symmetry. In this way,\nSARD can design efficient symmetric robots while covering the original design\nspace, which is theoretically analyzed. We further empirically evaluate SARD on\nvarious tasks, and the results show its superior efficiency and\ngeneralizability.",
                "Bigger, Better, Faster: Human-level Atari with human-level efficiency\nWe introduce a value-based RL agent, which we call BBF, that achieves\nsuper-human performance in the Atari 100K benchmark. BBF relies on scaling the\nneural networks used for value estimation, as well as a number of other design\nchoices that enable this scaling in a sample-efficient manner. We conduct\nextensive analyses of these design choices and provide insights for future\nwork. We end with a discussion about updating the goalposts for\nsample-efficient RL research on the ALE. We make our code and data publicly\navailable at\nhttps://github.com/google-research/google-research/tree/master/bigger_better_faster.",
                "Intent-aligned AI systems deplete human agency: the need for agency\n  foundations research in AI safety\nThe rapid advancement of artificial intelligence (AI) systems suggests that\nartificial general intelligence (AGI) systems may soon arrive. Many researchers\nare concerned that AIs and AGIs will harm humans via intentional misuse\n(AI-misuse) or through accidents (AI-accidents). In respect of AI-accidents,\nthere is an increasing effort focused on developing algorithms and paradigms\nthat ensure AI systems are aligned to what humans intend, e.g. AI systems that\nyield actions or recommendations that humans might judge as consistent with\ntheir intentions and goals. Here we argue that alignment to human intent is\ninsufficient for safe AI systems and that preservation of long-term agency of\nhumans may be a more robust standard, and one that needs to be separated\nexplicitly and a priori during optimization. We argue that AI systems can\nreshape human intention and discuss the lack of biological and psychological\nmechanisms that protect humans from loss of agency. We provide the first formal\ndefinition of agency-preserving AI-human interactions which focuses on\nforward-looking agency evaluations and argue that AI systems - not humans -\nmust be increasingly tasked with making these evaluations. We show how agency\nloss can occur in simple environments containing embedded agents that use\ntemporal-difference learning to make action recommendations. Finally, we\npropose a new area of research called \"agency foundations\" and pose four\ninitial topics designed to improve our understanding of agency in AI-human\ninteractions: benevolent game theory, algorithmic foundations of human rights,\nmechanistic interpretability of agency representation in neural-networks and\nreinforcement learning from internal states.",
                "DHRL-FNMR: An Intelligent Multicast Routing Approach Based on Deep\n  Hierarchical Reinforcement Learning in SDN\nThe optimal multicast tree problem in the Software-Defined Networking (SDN)\nmulticast routing is an NP-hard combinatorial optimization problem. Although\nexisting SDN intelligent solution methods, which are based on deep\nreinforcement learning, can dynamically adapt to complex network link state\nchanges, these methods are plagued by problems such as redundant branches,\nlarge action space, and slow agent convergence. In this paper, an SDN\nintelligent multicast routing algorithm based on deep hierarchical\nreinforcement learning is proposed to circumvent the aforementioned problems.\nFirst, the multicast tree construction problem is decomposed into two\nsub-problems: the fork node selection problem and the construction of the\noptimal path from the fork node to the destination node. Second, based on the\ninformation characteristics of SDN global network perception, the multicast\ntree state matrix, link bandwidth matrix, link delay matrix, link packet loss\nrate matrix, and sub-goal matrix are designed as the state space of intrinsic\nand meta controllers. Then, in order to mitigate the excessive action space,\nour approach constructs different action spaces at the upper and lower levels.\nThe meta-controller generates an action space using network nodes to select the\nfork node, and the intrinsic controller uses the adjacent edges of the current\nnode as its action space, thus implementing four different action selection\nstrategies in the construction of the multicast tree. To facilitate the\nintelligent agent in constructing the optimal multicast tree with greater\nspeed, we developed alternative reward strategies that distinguish between\nsingle-step node actions and multi-step actions towards multiple destination\nnodes."
            ],
            "interesting paper": 3
        },
        {
            "papers": [
                "Environmental Memory Boosts Group Formation of Clueless Individuals\nThe formation of groups of interacting individuals improves performance and\nfitness in many decentralised systems, from micro-organisms to social insects,\nfrom robotic swarms to artificial intelligence algorithms. Often, group\nformation and high-level coordination in these systems emerge from individuals\nwith limited information-processing capabilities implementing low-level rules\nof communication to signal to each other. Here, we show that, even in a\ncommunity of clueless individuals incapable of processing information and\ncommunicating, a dynamic environment can coordinate group formation by\ntransiently storing memory of the earlier passage of individuals. Our results\nidentify a new mechanism of indirect coordination via shared memory that is\nprimarily promoted and reinforced by dynamic environmental factors, thus\novershadowing the need for any form of explicit signalling between individuals.\nWe expect this pathway to group formation to be relevant for understanding and\ncontrolling self-organisation and collective decision making in both living and\nartificial active matter in real-life environments.",
                "Space Net Optimization\nMost metaheuristic algorithms rely on a few searched solutions to guide later\nsearches during the convergence process for a simple reason: the limited\ncomputing resource of a computer makes it impossible to retain all the searched\nsolutions. This also reveals that each search of most metaheuristic algorithms\nis just like a ballpark guess. To help address this issue, we present a novel\nmetaheuristic algorithm called space net optimization (SNO). It is equipped\nwith a new mechanism called space net; thus, making it possible for a\nmetaheuristic algorithm to use most information provided by all searched\nsolutions to depict the landscape of the solution space. With the space net, a\nmetaheuristic algorithm is kind of like having a ``vision'' on the solution\nspace. Simulation results show that SNO outperforms all the other metaheuristic\nalgorithms compared in this study for a set of well-known single objective\nbound constrained problems in most cases.",
                "EMOTE: An Explainable architecture for Modelling the Other Through\n  Empathy\nWe can usually assume others have goals analogous to our own. This assumption\ncan also, at times, be applied to multi-agent games - e.g. Agent 1's attraction\nto green pellets is analogous to Agent 2's attraction to red pellets. This\n\"analogy\" assumption is tied closely to the cognitive process known as empathy.\nInspired by empathy, we design a simple and explainable architecture to model\nanother agent's action-value function. This involves learning an \"Imagination\nNetwork\" to transform the other agent's observed state in order to produce a\nhuman-interpretable \"empathetic state\" which, when presented to the learning\nagent, produces behaviours that mimic the other agent. Our approach is\napplicable to multi-agent scenarios consisting of a single learning agent and\nother (independent) agents acting according to fixed policies. This\narchitecture is particularly beneficial for (but not limited to) algorithms\nusing a composite value or reward function. We show our method produces better\nperformance in multi-agent games, where it robustly estimates the other's model\nin different environment configurations. Additionally, we show that the\nempathetic states are human interpretable, and thus verifiable.",
                "A Virtual-Force Based Swarm Algorithm for Balanced Circular Bin Packing\n  Problems\nBalanced circular bin packing problems consist in positioning a given number\nof weighted circles in order to minimize the radius of a circular container\nwhile satisfying equilibrium constraints. These problems are NP-hard, highly\nconstrained and dimensional. This paper describes a swarm algorithm based on a\nvirtual-force system in order to solve balanced circular bin packing problems.\nIn the proposed approach, a system of forces is applied to each component\nallowing to take into account the constraints and minimizing the objective\nfunction using the fundamental principle of dynamics. The proposed algorithm is\nexperimented and validated on benchmarks of various balanced circular bin\npacking problems with up to 300 circles. The reported results allow to assess\nthe effectiveness of the proposed approach compared to existing results from\nthe literature.",
                "Human Control: Definitions and Algorithms\nHow can humans stay in control of advanced artificial intelligence systems?\nOne proposal is corrigibility, which requires the agent to follow the\ninstructions of a human overseer, without inappropriately influencing them. In\nthis paper, we formally define a variant of corrigibility called shutdown\ninstructability, and show that it implies appropriate shutdown behavior,\nretention of human autonomy, and avoidance of user harm. We also analyse the\nrelated concepts of non-obstruction and shutdown alignment, three previously\nproposed algorithms for human control, and one new algorithm.",
                "Human or Not? A Gamified Approach to the Turing Test\nWe present \"Human or Not?\", an online game inspired by the Turing test, that\nmeasures the capability of AI chatbots to mimic humans in dialog, and of humans\nto tell bots from other humans. Over the course of a month, the game was played\nby over 1.5 million users who engaged in anonymous two-minute chat sessions\nwith either another human or an AI language model which was prompted to behave\nlike humans. The task of the players was to correctly guess whether they spoke\nto a person or to an AI. This largest scale Turing-style test conducted to date\nrevealed some interesting facts. For example, overall users guessed the\nidentity of their partners correctly in only 68% of the games. In the subset of\nthe games in which users faced an AI bot, users had even lower correct guess\nrates of 60% (that is, not much higher than chance). This white paper details\nthe development, deployment, and results of this unique experiment. While this\nexperiment calls for many extensions and refinements, these findings already\nbegin to shed light on the inevitable near future which will commingle humans\nand AI."
            ],
            "interesting paper": 1
        },
        {
            "papers": [
                "Federated Learning Games for Reconfigurable Intelligent Surfaces via\n  Causal Representations\nIn this paper, we investigate the problem of robust Reconfigurable\nIntelligent Surface (RIS) phase-shifts configuration over heterogeneous\ncommunication environments. The problem is formulated as a distributed learning\nproblem over different environments in a Federated Learning (FL) setting.\nEquivalently, this corresponds to a game played between multiple RISs, as\nlearning agents, in heterogeneous environments. Using Invariant Risk\nMinimization (IRM) and its FL equivalent, dubbed FL Games, we solve the RIS\nconfiguration problem by learning invariant causal representations across\nmultiple environments and then predicting the phases. The solution corresponds\nto playing according to Best Response Dynamics (BRD) which yields the Nash\nEquilibrium of the FL game. The representation learner and the phase predictor\nare modeled by two neural networks, and their performance is validated via\nsimulations against other benchmarks from the literature. Our results show that\ncausality-based learning yields a predictor that is 15% more accurate in unseen\nOut-of-Distribution (OoD) environments.",
                "Knowledge-based Reasoning and Learning under Partial Observability in Ad\n  Hoc Teamwork\nAd hoc teamwork refers to the problem of enabling an agent to collaborate\nwith teammates without prior coordination. Data-driven methods represent the\nstate of the art in ad hoc teamwork. They use a large labeled dataset of prior\nobservations to model the behavior of other agent types and to determine the ad\nhoc agent's behavior. These methods are computationally expensive, lack\ntransparency, and make it difficult to adapt to previously unseen changes,\ne.g., in team composition. Our recent work introduced an architecture that\ndetermined an ad hoc agent's behavior based on non-monotonic logical reasoning\nwith prior commonsense domain knowledge and predictive models of other agents'\nbehavior that were learned from limited examples. In this paper, we\nsubstantially expand the architecture's capabilities to support: (a) online\nselection, adaptation, and learning of the models that predict the other\nagents' behavior; and (b) collaboration with teammates in the presence of\npartial observability and limited communication. We illustrate and\nexperimentally evaluate the capabilities of our architecture in two simulated\nmultiagent benchmark domains for ad hoc teamwork: Fort Attack and Half Field\nOffense. We show that the performance of our architecture is comparable or\nbetter than state of the art data-driven baselines in both simple and complex\nscenarios, particularly in the presence of limited training data, partial\nobservability, and changes in team composition.",
                "On the Existence of Information Bottlenecks in Living and Non-Living\n  Systems\nIn many complex systems, we observe that `interesting behaviour' is often the\nconsequence of a system exploiting the existence of an Information Bottleneck\n(IB). These bottlenecks can occur at different scales, between individuals or\ncomponents of a system, and sometimes within individuals themselves.\nOftentimes, we regard these bottlenecks negatively; as merely the limitations\nof the individual's physiology and something that ought to be overcome when\ndesigning and implementing artificial systems. However, we suggest instead that\nIBs may serve a purpose beyond merely providing a minimally-viable channel for\ncoordination in collective systems. More specifically, we suggest that\ninteresting or novel behaviour occurs when the individuals in a system are\nconstrained or limited in their ability to share information and must discover\nnovel ways to exploit existing mechanisms, which are inherently bottlenecked,\nrather than circumventing or otherwise avoiding those mechanisms entirely.",
                "Multi-Robot Path Planning Combining Heuristics and Multi-Agent\n  Reinforcement Learning\nMulti-robot path finding in dynamic environments is a highly challenging\nclassic problem. In the movement process, robots need to avoid collisions with\nother moving robots while minimizing their travel distance. Previous methods\nfor this problem either continuously replan paths using heuristic search\nmethods to avoid conflicts or choose appropriate collision avoidance strategies\nbased on learning approaches. The former may result in long travel distances\ndue to frequent replanning, while the latter may have low learning efficiency\ndue to low sample exploration and utilization, and causing high training costs\nfor the model. To address these issues, we propose a path planning method,\nMAPPOHR, which combines heuristic search, empirical rules, and multi-agent\nreinforcement learning. The method consists of two layers: a real-time planner\nbased on the multi-agent reinforcement learning algorithm, MAPPO, which embeds\nempirical rules in the action output layer and reward functions, and a\nheuristic search planner used to create a global guiding path. During movement,\nthe heuristic search planner replans new paths based on the instructions of the\nreal-time planner. We tested our method in 10 different conflict scenarios. The\nexperiments show that the planning performance of MAPPOHR is better than that\nof existing learning and heuristic methods. Due to the utilization of empirical\nknowledge and heuristic search, the learning efficiency of MAPPOHR is higher\nthan that of existing learning methods.",
                "Egocentric Planning for Scalable Embodied Task Achievement\nEmbodied agents face significant challenges when tasked with performing\nactions in diverse environments, particularly in generalizing across object\ntypes and executing suitable actions to accomplish tasks. Furthermore, agents\nshould exhibit robustness, minimizing the execution of illegal actions. In this\nwork, we present Egocentric Planning, an innovative approach that combines\nsymbolic planning and Object-oriented POMDPs to solve tasks in complex\nenvironments, harnessing existing models for visual perception and natural\nlanguage processing. We evaluated our approach in ALFRED, a simulated\nenvironment designed for domestic tasks, and demonstrated its high scalability,\nachieving an impressive 36.07% unseen success rate in the ALFRED benchmark and\nwinning the ALFRED challenge at CVPR Embodied AI workshop. Our method requires\nreliable perception and the specification or learning of a symbolic description\nof the preconditions and effects of the agent's actions, as well as what object\ntypes reveal information about others. It is capable of naturally scaling to\nsolve new tasks beyond ALFRED, as long as they can be solved using the\navailable skills. This work offers a solid baseline for studying end-to-end and\nhybrid methods that aim to generalize to new tasks, including recent approaches\nrelying on LLMs, but often struggle to scale to long sequences of actions or\nproduce robust plans for novel tasks.",
                "An Architecture for Deploying Reinforcement Learning in Industrial\n  Environments\nIndustry 4.0 is driven by demands like shorter time-to-market, mass\ncustomization of products, and batch size one production. Reinforcement\nLearning (RL), a machine learning paradigm shown to possess a great potential\nin improving and surpassing human level performance in numerous complex tasks,\nallows coping with the mentioned demands. In this paper, we present an OPC UA\nbased Operational Technology (OT)-aware RL architecture, which extends the\nstandard RL setting, combining it with the setting of digital twins. Moreover,\nwe define an OPC UA information model allowing for a generalized plug-and-play\nlike approach for exchanging the RL agent used. In conclusion, we demonstrate\nand evaluate the architecture, by creating a proof of concept. By means of\nsolving a toy example, we show that this architecture can be used to determine\nthe optimal policy using a real control system."
            ],
            "interesting paper": 2
        }
    ],
    "Bailey Martinez": [
        {
            "papers": [
                "FollowNet: A Comprehensive Benchmark for Car-Following Behavior Modeling\nCar-following is a control process in which a following vehicle (FV) adjusts\nits acceleration to keep a safe distance from the lead vehicle (LV). Recently,\nthere has been a booming of data-driven models that enable more accurate\nmodeling of car-following through real-world driving datasets. Although there\nare several public datasets available, their formats are not always consistent,\nmaking it challenging to determine the state-of-the-art models and how well a\nnew model performs compared to existing ones. In contrast, research fields such\nas image recognition and object detection have benchmark datasets like\nImageNet, Microsoft COCO, and KITTI. To address this gap and promote the\ndevelopment of microscopic traffic flow modeling, we establish a public\nbenchmark dataset for car-following behavior modeling. The benchmark consists\nof more than 80K car-following events extracted from five public driving\ndatasets using the same criteria. These events cover diverse situations\nincluding different road types, various weather conditions, and mixed traffic\nflows with autonomous vehicles. Moreover, to give an overview of current\nprogress in car-following modeling, we implemented and tested representative\nbaseline models with the benchmark. Results show that the deep deterministic\npolicy gradient (DDPG) based model performs competitively with a lower MSE for\nspacing compared to traditional intelligent driver model (IDM) and\nGazis-Herman-Rothery (GHR) models, and a smaller collision rate compared to\nfully connected neural network (NN) and long short-term memory (LSTM) models in\nmost datasets. The established benchmark will provide researchers with\nconsistent data formats and metrics for cross-comparing different car-following\nmodels, promoting the development of more accurate models. We open-source our\ndataset and implementation code in\nhttps://github.com/HKUST-DRIVE-AI-LAB/FollowNet.",
                "Using Models Based on Cognitive Theory to Predict Human Behavior in\n  Traffic: A Case Study\nThe development of automated vehicles has the potential to revolutionize\ntransportation, but they are currently unable to ensure a safe and\ntime-efficient driving style. Reliable models predicting human behavior are\nessential for overcoming this issue. While data-driven models are commonly used\nto this end, they can be vulnerable in safety-critical edge cases. This has led\nto an interest in models incorporating cognitive theory, but as such models are\ncommonly developed for explanatory purposes, this approach's effectiveness in\nbehavior prediction has remained largely untested so far. In this article, we\ninvestigate the usefulness of the \\emph{Commotions} model -- a novel\ncognitively plausible model incorporating the latest theories of human\nperception, decision-making, and motor control -- for predicting human behavior\nin gap acceptance scenarios, which entail many important traffic interactions\nsuch as lane changes and intersections. We show that this model can compete\nwith or even outperform well-established data-driven prediction models across\nseveral naturalistic datasets. These results demonstrate the promise of\nincorporating cognitive theory in behavior prediction models for automated\nvehicles.",
                "Lucy-SKG: Learning to Play Rocket League Efficiently Using Deep\n  Reinforcement Learning\nA successful tactic that is followed by the scientific community for\nadvancing AI is to treat games as problems, which has been proven to lead to\nvarious breakthroughs. We adapt this strategy in order to study Rocket League,\na widely popular but rather under-explored 3D multiplayer video game with a\ndistinct physics engine and complex dynamics that pose a significant challenge\nin developing efficient and high-performance game-playing agents. In this\npaper, we present Lucy-SKG, a Reinforcement Learning-based model that learned\nhow to play Rocket League in a sample-efficient manner, outperforming by a\nnotable margin the two highest-ranking bots in this game, namely Necto (2022\nbot champion) and its successor Nexto, thus becoming a state-of-the-art agent.\nOur contributions include: a) the development of a reward analysis and\nvisualization library, b) novel parameterizable reward shape functions that\ncapture the utility of complex reward types via our proposed Kinesthetic Reward\nCombination (KRC) technique, and c) design of auxiliary neural architectures\nfor training on reward prediction and state representation tasks in an\non-policy fashion for enhanced efficiency in learning speed and performance. By\nperforming thorough ablation studies for each component of Lucy-SKG, we showed\ntheir independent effectiveness in overall performance. In doing so, we\ndemonstrate the prospects and challenges of using sample-efficient\nReinforcement Learning techniques for controlling complex dynamical systems\nunder competitive team-based multiplayer conditions.",
                "A Mini Review on the utilization of Reinforcement Learning with OPC UA\nReinforcement Learning (RL) is a powerful machine learning paradigm that has\nbeen applied in various fields such as robotics, natural language processing\nand game playing achieving state-of-the-art results. Targeted to solve\nsequential decision making problems, it is by design able to learn from\nexperience and therefore adapt to changing dynamic environments. These\ncapabilities make it a prime candidate for controlling and optimizing complex\nprocesses in industry. The key to fully exploiting this potential is the\nseamless integration of RL into existing industrial systems. The industrial\ncommunication standard Open Platform Communications UnifiedArchitecture (OPC\nUA) could bridge this gap. However, since RL and OPC UA are from different\nfields,there is a need for researchers to bridge the gap between the two\ntechnologies. This work serves to bridge this gap by providing a brief\ntechnical overview of both technologies and carrying out a semi-exhaustive\nliterature review to gain insights on how RL and OPC UA are applied in\ncombination. With this survey, three main research topics have been identified,\nfollowing the intersection of RL with OPC UA. The results of the literature\nreview show that RL is a promising technology for the control and optimization\nof industrial processes, but does not yet have the necessary standardized\ninterfaces to be deployed in real-world scenarios with reasonably low effort.",
                "Deep Learning and Ethics\nThis article appears as chapter 21 of Prince (2023, Understanding Deep\nLearning); a complete draft of the textbook is available here:\nhttp://udlbook.com. This chapter considers potential harms arising from the\ndesign and use of AI systems. These include algorithmic bias, lack of\nexplainability, data privacy violations, militarization, fraud, and\nenvironmental concerns. The aim is not to provide advice on being more ethical.\nInstead, the goal is to express ideas and start conversations in key areas that\nhave received attention in philosophy, political science, and the broader\nsocial sciences.",
                "Distributed Online Rollout for Multivehicle Routing in Unmapped\n  Environments\nIn this work we consider a generalization of the well-known multivehicle\nrouting problem: given a network, a set of agents occupying a subset of its\nnodes, and a set of tasks, we seek a minimum cost sequence of movements subject\nto the constraint that each task is visited by some agent at least once. The\nclassical version of this problem assumes a central computational server that\nobserves the entire state of the system perfectly and directs individual agents\naccording to a centralized control scheme. In contrast, we assume that there is\nno centralized server and that each agent is an individual processor with no a\npriori knowledge of the underlying network (including task and agent\nlocations). Moreover, our agents possess strictly local communication and\nsensing capabilities (restricted to a fixed radius around their respective\nlocations), aligning more closely with several real-world multiagent\napplications. These restrictions introduce many challenges that are overcome\nthrough local information sharing and direct coordination between agents. We\npresent a fully distributed, online, and scalable reinforcement learning\nalgorithm for this problem whereby agents self-organize into local clusters and\nindependently apply a multiagent rollout scheme locally to each cluster. We\ndemonstrate empirically via extensive simulations that there exists a critical\nsensing radius beyond which the distributed rollout algorithm begins to improve\nover a greedy base policy. This critical sensing radius grows proportionally to\nthe $\\log^*$ function of the size of the network, and is, therefore, a small\nconstant for any relevant network. Our decentralized reinforcement learning\nalgorithm achieves approximately a factor of two cost improvement over the base\npolicy for a range of radii bounded from below and above by two and three times\nthe critical sensing radius, respectively."
            ],
            "interesting paper": 3
        },
        {
            "papers": [
                "On Computing Universal Plans for Partially Observable Multi-Agent Path\n  Finding\nMulti-agent routing problems have drawn significant attention nowadays due to\ntheir broad industrial applications in, e.g., warehouse robots, logistics\nautomation, and traffic control. Conventionally, they are modelled as classical\nplanning problems. In this paper, we argue that it is beneficial to formulate\nthem as universal planning problems. We therefore propose universal plans, also\nknown as policies, as the solution concepts, and implement a system called\nASP-MAUPF (Answer Set Programming for Multi-Agent Universal Plan Finding) for\ncomputing them. Given an arbitrary two-dimensional map and a profile of goals\nfor the agents, the system finds a feasible universal plan for each agent that\nensures no collision with others. We use the system to conduct some\nexperiments, and make some observations on the types of goal profiles and\nenvironments that will have feasible policies, and how they may depend on\nagents' sensors. We also demonstrate how users can customize action preferences\nto compute more efficient policies, even (near-)optimal ones.",
                "A Semi-Automated Corner Case Detection and Evaluation Pipeline\nIn order to deploy automated vehicles to the public, it has to be proven that\nthe vehicle can safely and robustly handle traffic in many different scenarios.\nOne important component of automated vehicles is the perception system that\ncaptures and processes the environment around the vehicle. Perception systems\nrequire large datasets for training their deep neural network. Knowing which\nparts of the data in these datasets describe a corner case is an advantage\nduring training or testing of the network. These corner cases describe\nsituations that are rare and potentially challenging for the network. We\npropose a pipeline that converts collective expert knowledge descriptions into\nthe extended KI Absicherung ontology. The ontology is used to describe scenes\nand scenarios that can be mapped to perception datasets. The corner cases can\nthen be extracted from the datasets. In addition, the pipeline enables the\nevaluation of the detection networks against the extracted corner cases to\nmeasure their performance.",
                "Learning Safety Constraints from Demonstrations with Unknown Rewards\nWe propose Convex Constraint Learning for Reinforcement Learning (CoCoRL), a\nnovel approach for inferring shared constraints in a Constrained Markov\nDecision Process (CMDP) from a set of safe demonstrations with possibly\ndifferent reward functions. While previous work is limited to demonstrations\nwith known rewards or fully known environment dynamics, CoCoRL can learn\nconstraints from demonstrations with different unknown rewards without\nknowledge of the environment dynamics. CoCoRL constructs a convex safe set\nbased on demonstrations, which provably guarantees safety even for potentially\nsub-optimal (but safe) demonstrations. For near-optimal demonstrations, CoCoRL\nconverges to the true safe set with no policy regret. We evaluate CoCoRL in\ngridworld environments and a driving simulation with multiple constraints.\nCoCoRL learns constraints that lead to safe driving behavior. Importantly, we\ncan safely transfer the learned constraints to different tasks and\nenvironments. In contrast, alternative methods based on Inverse Reinforcement\nLearning (IRL) often exhibit poor performance and learn unsafe policies.",
                "Trust-Aware Resilient Control and Coordination of Connected and\n  Automated Vehicles\nWe address the security of a network of Connected and Automated Vehicles\n(CAVs) cooperating to navigate through a conflict area. Adversarial attacks\nsuch as Sybil attacks can cause safety violations resulting in collisions and\ntraffic jams. In addition, uncooperative (but not necessarily adversarial) CAVs\ncan also induce similar adversarial effects on the traffic network. We propose\na decentralized resilient control and coordination scheme that mitigates the\neffects of adversarial attacks and uncooperative CAVs by utilizing a trust\nframework. Our trust-aware scheme can guarantee safe collision free\ncoordination and mitigate traffic jams. Simulation results validate the\ntheoretical guarantee of our proposed scheme, and demonstrate that it can\neffectively mitigate adversarial effects across different traffic scenarios.",
                "Pedestrian Trajectory Forecasting Using Deep Ensembles Under Sensing\n  Uncertainty\nOne of the fundamental challenges in the prediction of dynamic agents is\nrobustness. Usually, most predictions are deterministic estimates of future\nstates which are over-confident and prone to error. Recently, few works have\naddressed capturing uncertainty during forecasting of future states. However,\nthese probabilistic estimation methods fail to account for the upstream noise\nin perception data during tracking. Sensors always have noise and state\nestimation becomes even more difficult under adverse weather conditions and\nocclusion. Traditionally, Bayes filters have been used to fuse information from\nnoisy sensors to update states with associated belief. But, they fail to\naddress non-linearities and long-term predictions. Therefore, we propose an\nend-to-end estimator that can take noisy sensor measurements and make robust\nfuture state predictions with uncertainty bounds while simultaneously taking\ninto consideration the upstream perceptual uncertainty. For the current\nresearch, we consider an encoder-decoder based deep ensemble network for\ncapturing both perception and predictive uncertainty simultaneously. We\ncompared the current model to other approximate Bayesian inference methods.\nOverall, deep ensembles provided more robust predictions and the consideration\nof upstream uncertainty further increased the estimation accuracy for the\nmodel.",
                "Alert of the Second Decision-maker: An Introduction to Human-AI Conflict\nThe collaboration between humans and artificial intelligence (AI) is a\nsignificant feature in this digital age. However, humans and AI may have\nobservation, interpretation, and action conflicts when working synchronously.\nThis phenomenon is often masked by faults and, unfortunately, overlooked. This\npaper systematically introduces the human-AI conflict concept, causes,\nmeasurement methods, and risk assessment. The results highlight that there is a\npotential second decision-maker besides the human, which is the AI; the\nhuman-AI conflict is a unique and emerging risk in digitalized process systems;\nand this is an interdisciplinary field that needs to be distinguished from\ntraditional fault and failure analysis; the conflict risk is significant and\ncannot be ignored."
            ],
            "interesting paper": 2
        },
        {
            "papers": [
                "Robust Lane Detection through Self Pre-training with Masked Sequential\n  Autoencoders and Fine-tuning with Customized PolyLoss\nLane detection is crucial for vehicle localization which makes it the\nfoundation for automated driving and many intelligent and advanced driving\nassistant systems. Available vision-based lane detection methods do not make\nfull use of the valuable features and aggregate contextual information,\nespecially the interrelationships between lane lines and other regions of the\nimages in continuous frames. To fill this research gap and upgrade lane\ndetection performance, this paper proposes a pipeline consisting of self\npre-training with masked sequential autoencoders and fine-tuning with\ncustomized PolyLoss for the end-to-end neural network models using\nmulti-continuous image frames. The masked sequential autoencoders are adopted\nto pre-train the neural network models with reconstructing the missing pixels\nfrom a random masked image as the objective. Then, in the fine-tuning\nsegmentation phase where lane detection segmentation is performed, the\ncontinuous image frames are served as the inputs, and the pre-trained model\nweights are transferred and further updated using the backpropagation mechanism\nwith customized PolyLoss calculating the weighted errors between the output\nlane detection results and the labeled ground truth. Extensive experiment\nresults demonstrate that, with the proposed pipeline, the lane detection model\nperformance on both normal and challenging scenes can be advanced beyond the\nstate-of-the-art, delivering the best testing accuracy (98.38%), precision\n(0.937), and F1-measure (0.924) on the normal scene testing set, together with\nthe best overall accuracy (98.36%) and precision (0.844) in the challenging\nscene test set, while the training time can be substantially shortened.",
                "Formal Modelling for Multi-Robot Systems Under Uncertainty\nPurpose of Review: To effectively synthesise and analyse multi-robot\nbehaviour, we require formal task-level models which accurately capture\nmulti-robot execution. In this paper, we review modelling formalisms for\nmulti-robot systems under uncertainty, and discuss how they can be used for\nplanning, reinforcement learning, model checking, and simulation.\n  Recent Findings: Recent work has investigated models which more accurately\ncapture multi-robot execution by considering different forms of uncertainty,\nsuch as temporal uncertainty and partial observability, and modelling the\neffects of robot interactions on action execution. Other strands of work have\npresented approaches for reducing the size of multi-robot models to admit more\nefficient solution methods. This can be achieved by decoupling the robots under\nindependence assumptions, or reasoning over higher level macro actions.\n  Summary: Existing multi-robot models demonstrate a trade off between\naccurately capturing robot dependencies and uncertainty, and being small enough\nto tractably solve real world problems. Therefore, future research should\nexploit realistic assumptions over multi-robot behaviour to develop smaller\nmodels which retain accurate representations of uncertainty and robot\ninteractions; and exploit the structure of multi-robot problems, such as\nfactored state spaces, to develop scalable solution methods.",
                "Learning Interpretable Models of Aircraft Handling Behaviour by\n  Reinforcement Learning from Human Feedback\nWe propose a method to capture the handling abilities of fast jet pilots in a\nsoftware model via reinforcement learning (RL) from human preference feedback.\nWe use pairwise preferences over simulated flight trajectories to learn an\ninterpretable rule-based model called a reward tree, which enables the\nautomated scoring of trajectories alongside an explanatory rationale. We train\nan RL agent to execute high-quality handling behaviour by using the reward tree\nas the objective, and thereby generate data for iterative preference collection\nand further refinement of both tree and agent. Experiments with synthetic\npreferences show reward trees to be competitive with uninterpretable neural\nnetwork reward models on quantitative and qualitative evaluations.",
                "Moral Machine or Tyranny of the Majority?\nWith Artificial Intelligence systems increasingly applied in consequential\ndomains, researchers have begun to ask how these systems ought to act in\nethically charged situations where even humans lack consensus. In the Moral\nMachine project, researchers crowdsourced answers to \"Trolley Problems\"\nconcerning autonomous vehicles. Subsequently, Noothigattu et al. (2018)\nproposed inferring linear functions that approximate each individual's\npreferences and aggregating these linear models by averaging parameters across\nthe population. In this paper, we examine this averaging mechanism, focusing on\nfairness concerns in the presence of strategic effects. We investigate a simple\nsetting where the population consists of two groups, with the minority\nconstituting an {\\alpha} < 0.5 share of the population. To simplify the\nanalysis, we consider the extreme case in which within-group preferences are\nhomogeneous. Focusing on the fraction of contested cases where the minority\ngroup prevails, we make the following observations: (a) even when all parties\nreport their preferences truthfully, the fraction of disputes where the\nminority prevails is less than proportionate in {\\alpha}; (b) the degree of\nsub-proportionality grows more severe as the level of disagreement between the\ngroups increases; (c) when parties report preferences strategically, pure\nstrategy equilibria do not always exist; and (d) whenever a pure strategy\nequilibrium exists, the majority group prevails 100% of the time. These\nfindings raise concerns about stability and fairness of preference vector\naveraging as a mechanism for aggregating diverging voices. Finally, we discuss\nalternatives, including randomized dictatorship and median-based mechanisms.",
                "MADiff: Offline Multi-agent Learning with Diffusion Models\nDiffusion model (DM) recently achieved huge success in various scenarios\nincluding offline reinforcement learning, where the diffusion planner learn to\ngenerate desired trajectories during online evaluations. However, despite the\neffectiveness in single-agent learning, it remains unclear how DMs can operate\nin multi-agent problems, where agents can hardly complete teamwork without good\ncoordination by independently modeling each agent's trajectories. In this\npaper, we propose MADiff, a novel generative multi-agent learning framework to\ntackle this problem. MADiff is realized with an attention-based diffusion model\nto model the complex coordination among behaviors of multiple agents. To the\nbest of our knowledge, MADiff is the first diffusion-based multi-agent learning\nframework, which behaves as both a decentralized policy and a centralized\ncontroller. During decentralized executions, MADiff simultaneously performs\nteammate modeling, and the centralized controller can also be applied in\nmulti-agent trajectory predictions. Our experiments show the superior\nperformance of MADiff compared to baseline algorithms in a wide range of\nmulti-agent learning tasks, which emphasizes the effectiveness of MADiff in\nmodeling complex multi-agent interactions. Our code is available at\nhttps://github.com/zbzhu99/madiff.",
                "NavGPT: Explicit Reasoning in Vision-and-Language Navigation with Large\n  Language Models\nTrained with an unprecedented scale of data, large language models (LLMs)\nlike ChatGPT and GPT-4 exhibit the emergence of significant reasoning abilities\nfrom model scaling. Such a trend underscored the potential of training LLMs\nwith unlimited language data, advancing the development of a universal embodied\nagent. In this work, we introduce the NavGPT, a purely LLM-based\ninstruction-following navigation agent, to reveal the reasoning capability of\nGPT models in complex embodied scenes by performing zero-shot sequential action\nprediction for vision-and-language navigation (VLN). At each step, NavGPT takes\nthe textual descriptions of visual observations, navigation history, and future\nexplorable directions as inputs to reason the agent's current status, and makes\nthe decision to approach the target. Through comprehensive experiments, we\ndemonstrate NavGPT can explicitly perform high-level planning for navigation,\nincluding decomposing instruction into sub-goal, integrating commonsense\nknowledge relevant to navigation task resolution, identifying landmarks from\nobserved scenes, tracking navigation progress, and adapting to exceptions with\nplan adjustment. Furthermore, we show that LLMs is capable of generating\nhigh-quality navigational instructions from observations and actions along a\npath, as well as drawing accurate top-down metric trajectory given the agent's\nnavigation history. Despite the performance of using NavGPT to zero-shot R2R\ntasks still falling short of trained models, we suggest adapting multi-modality\ninputs for LLMs to use as visual navigation agents and applying the explicit\nreasoning of LLMs to benefit learning-based models."
            ],
            "interesting paper": 1
        },
        {
            "papers": [
                "Assumption Generation for the Verification of Learning-Enabled\n  Autonomous Systems\nProviding safety guarantees for autonomous systems is difficult as these\nsystems operate in complex environments that require the use of\nlearning-enabled components, such as deep neural networks (DNNs) for visual\nperception. DNNs are hard to analyze due to their size (they can have thousands\nor millions of parameters), lack of formal specifications (DNNs are typically\nlearnt from labeled data, in the absence of any formal requirements), and\nsensitivity to small changes in the environment. We present an assume-guarantee\nstyle compositional approach for the formal verification of system-level safety\nproperties of such autonomous systems. Our insight is that we can analyze the\nsystem in the absence of the DNN perception components by automatically\nsynthesizing assumptions on the DNN behaviour that guarantee the satisfaction\nof the required safety properties. The synthesized assumptions are the weakest\nin the sense that they characterize the output sequences of all the possible\nDNNs that, plugged into the autonomous system, guarantee the required safety\nproperties. The assumptions can be leveraged as run-time monitors over a\ndeployed DNN to guarantee the safety of the overall system; they can also be\nmined to extract local specifications for use during training and testing of\nDNNs. We illustrate our approach on a case study taken from the autonomous\nairplanes domain that uses a complex DNN for perception.",
                "Integrating Action Knowledge and LLMs for Task Planning and Situation\n  Handling in Open Worlds\nTask planning systems have been developed to help robots use human knowledge\n(about actions) to complete long-horizon tasks. Most of them have been\ndeveloped for \"closed worlds\" while assuming the robot is provided with\ncomplete world knowledge. However, the real world is generally open, and the\nrobots frequently encounter unforeseen situations that can potentially break\nthe planner's completeness. Could we leverage the recent advances on\npre-trained Large Language Models (LLMs) to enable classical planning systems\nto deal with novel situations?\n  This paper introduces a novel framework, called COWP, for open-world task\nplanning and situation handling. COWP dynamically augments the robot's action\nknowledge, including the preconditions and effects of actions, with\ntask-oriented commonsense knowledge. COWP embraces the openness from LLMs, and\nis grounded to specific domains via action knowledge. For systematic\nevaluations, we collected a dataset that includes 1,085 execution-time\nsituations. Each situation corresponds to a state instance wherein a robot is\npotentially unable to complete a task using a solution that normally works.\nExperimental results show that our approach outperforms competitive baselines\nfrom the literature in the success rate of service tasks. Additionally, we have\ndemonstrated COWP using a mobile manipulator. Supplementary materials are\navailable at: https://cowplanning.github.io/",
                "Modeling Dynamic Environments with Scene Graph Memory\nEmbodied AI agents that search for objects in large environments such as\nhouseholds often need to make efficient decisions by predicting object\nlocations based on partial information. We pose this as a new type of link\nprediction problem: link prediction on partially observable dynamic graphs. Our\ngraph is a representation of a scene in which rooms and objects are nodes, and\ntheir relationships are encoded in the edges; only parts of the changing graph\nare known to the agent at each timestep. This partial observability poses a\nchallenge to existing link prediction approaches, which we address. We propose\na novel state representation -- Scene Graph Memory (SGM) -- with captures the\nagent's accumulated set of observations, as well as a neural net architecture\ncalled a Node Edge Predictor (NEP) that extracts information from the SGM to\nsearch efficiently. We evaluate our method in the Dynamic House Simulator, a\nnew benchmark that creates diverse dynamic graphs following the semantic\npatterns typically seen at homes, and show that NEP can be trained to predict\nthe locations of objects in a variety of environments with diverse object\nmovement dynamics, outperforming baselines both in terms of new scene\nadaptability and overall accuracy. The codebase and more can be found at\nhttps://www.scenegraphmemory.com.",
                "A Comprehensive Overview and Comparative Analysis on Deep Learning\n  Models: CNN, RNN, LSTM, GRU\nDeep learning (DL) has emerged as a powerful subset of machine learning (ML)\nand artificial intelligence (AI), outperforming traditional ML methods,\nespecially in handling unstructured and large datasets. Its impact spans across\nvarious domains, including speech recognition, healthcare, autonomous vehicles,\ncybersecurity, predictive analytics, and more. However, the complexity and\ndynamic nature of real-world problems present challenges in designing effective\ndeep learning models. Consequently, several deep learning models have been\ndeveloped to address different problems and applications. In this article, we\nconduct a comprehensive survey of various deep learning models, including\nConvolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs),\nGenerative Models, Deep Reinforcement Learning (DRL), and Deep Transfer\nLearning. We examine the structure, applications, benefits, and limitations of\neach model. Furthermore, we perform an analysis using three publicly available\ndatasets: IMDB, ARAS, and Fruit-360. We compare the performance of six renowned\ndeep learning models: CNN, Simple RNN, Long Short-Term Memory (LSTM),\nBidirectional LSTM, Gated Recurrent Unit (GRU), and Bidirectional GRU.",
                "On the Value of Myopic Behavior in Policy Reuse\nLeveraging learned strategies in unfamiliar scenarios is fundamental to human\nintelligence. In reinforcement learning, rationally reusing the policies\nacquired from other tasks or human experts is critical for tackling problems\nthat are difficult to learn from scratch. In this work, we present a framework\ncalled Selective Myopic bEhavior Control~(SMEC), which results from the insight\nthat the short-term behaviors of prior policies are sharable across tasks. By\nevaluating the behaviors of prior policies via a hybrid value function\narchitecture, SMEC adaptively aggregates the sharable short-term behaviors of\nprior policies and the long-term behaviors of the task policy, leading to\ncoordinated decisions. Empirical results on a collection of manipulation and\nlocomotion tasks demonstrate that SMEC outperforms existing methods, and\nvalidate the ability of SMEC to leverage related prior policies.",
                "Synthesizing a Progression of Subtasks for Block-Based Visual\n  Programming Tasks\nBlock-based visual programming environments play an increasingly important\nrole in introducing computing concepts to K-12 students. In recent years, they\nhave also gained popularity in neuro-symbolic AI, serving as a benchmark to\nevaluate general problem-solving and logical reasoning skills. The open-ended\nand conceptual nature of these visual programming tasks make them challenging,\nboth for state-of-the-art AI agents as well as for novice programmers. A\nnatural approach to providing assistance for problem-solving is breaking down a\ncomplex task into a progression of simpler subtasks; however, this is not\ntrivial given that the solution codes are typically nested and have non-linear\nexecution behavior. In this paper, we formalize the problem of synthesizing\nsuch a progression for a given reference block-based visual programming task.\nWe propose a novel synthesis algorithm that generates a progression of subtasks\nthat are high-quality, well-spaced in terms of their complexity, and solving\nthis progression leads to solving the reference task. We show the utility of\nour synthesis algorithm in improving the efficacy of AI agents (in this case,\nneural program synthesizers) for solving tasks in the Karel programming\nenvironment. Then, we conduct a user study to demonstrate that our synthesized\nprogression of subtasks can assist a novice programmer in solving tasks in the\nHour of Code: Maze Challenge by Code-dot-org."
            ],
            "interesting paper": 3
        },
        {
            "papers": [
                "Safety of autonomous vehicles: A survey on Model-based vs. AI-based\n  approaches\nThe growing advancements in Autonomous Vehicles (AVs) have emphasized the\ncritical need to prioritize the absolute safety of AV maneuvers, especially in\ndynamic and unpredictable environments or situations. This objective becomes\neven more challenging due to the uniqueness of every traffic\nsituation/condition. To cope with all these very constrained and complex\nconfigurations, AVs must have appropriate control architectures with reliable\nand real-time Risk Assessment and Management Strategies (RAMS). These targeted\nRAMS must lead to reduce drastically the navigation risks. However, the lack of\nsafety guarantees proves, which is one of the key challenges to be addressed,\nlimit drastically the ambition to introduce more broadly AVs on our roads and\nrestrict the use of AVs to very limited use cases. Therefore, the focus and the\nambition of this paper is to survey research on autonomous vehicles while\nfocusing on the important topic of safety guarantee of AVs. For this purpose,\nit is proposed to review research on relevant methods and concepts defining an\noverall control architecture for AVs, with an emphasis on the safety assessment\nand decision-making systems composing these architectures. Moreover, it is\nintended through this reviewing process to highlight researches that use either\nmodel-based methods or AI-based approaches. This is performed while emphasizing\nthe strengths and weaknesses of each methodology and investigating the research\nthat proposes a comprehensive multi-modal design that combines model-based and\nAI approaches. This paper ends with discussions on the methods used to\nguarantee the safety of AVs namely: safety verification techniques and the\nstandardization/generalization of safety frameworks.",
                "Towards Autonomous and Safe Last-mile Deliveries with AI-augmented\n  Self-driving Delivery Robots\nIn addition to its crucial impact on customer satisfaction, last-mile\ndelivery (LMD) is notorious for being the most time-consuming and costly stage\nof the shipping process. Pressing environmental concerns combined with the\nrecent surge of e-commerce sales have sparked renewed interest in automation\nand electrification of last-mile logistics. To address the hurdles faced by\nexisting robotic couriers, this paper introduces a customer-centric and\nsafety-conscious LMD system for small urban communities based on AI-assisted\nautonomous delivery robots. The presented framework enables end-to-end\nautomation and optimization of the logistic process while catering for\nreal-world imposed operational uncertainties, clients' preferred time\nschedules, and safety of pedestrians. To this end, the integrated optimization\ncomponent is modeled as a robust variant of the Cumulative Capacitated Vehicle\nRouting Problem with Time Windows, where routes are constructed under uncertain\ntravel times with an objective to minimize the total latency of deliveries\n(i.e., the overall waiting time of customers, which can negatively affect their\nsatisfaction). We demonstrate the proposed LMD system's utility through\nreal-world trials in a university campus with a single robotic courier.\nImplementation aspects as well as the findings and practical insights gained\nfrom the deployment are discussed in detail. Lastly, we round up the\ncontributions with numerical simulations to investigate the scalability of the\ndeveloped mathematical formulation with respect to the number of robotic\nvehicles and customers.",
                "Pedestrian detection with high-resolution event camera\nDespite the dynamic development of computer vision algorithms, the\nimplementation of perception and control systems for autonomous vehicles such\nas drones and self-driving cars still poses many challenges. A video stream\ncaptured by traditional cameras is often prone to problems such as motion blur\nor degraded image quality due to challenging lighting conditions. In addition,\nthe frame rate - typically 30 or 60 frames per second - can be a limiting\nfactor in certain scenarios. Event cameras (DVS -- Dynamic Vision Sensor) are a\npotentially interesting technology to address the above mentioned problems. In\nthis paper, we compare two methods of processing event data by means of deep\nlearning for the task of pedestrian detection. We used a representation in the\nform of video frames, convolutional neural networks and asynchronous sparse\nconvolutional neural networks. The results obtained illustrate the potential of\nevent cameras and allow the evaluation of the accuracy and efficiency of the\nmethods used for high-resolution (1280 x 720 pixels) footage.",
                "An Efficient Safety-oriented Car-following Model for Connected Automated\n  Vehicles Considering Discrete Signals\nWith the rapid development of Connected and Automated Vehicle (CAV)\ntechnology, limited self-driving vehicles have been commercially available in\ncertain leading intelligent transportation system countries. When formulating\nthe car-following model for CAVs, safety is usually the basic constraint.\nSafety-oriented car-following models seek to specify a safe following distance\nthat can guarantee safety if the preceding vehicle were to brake hard suddenly.\nThe discrete signals of CAVs bring a series of phenomena, including discrete\ndecision-making, phase difference, and discretely distributed communication\ndelay. The influences of these phenomena on the car-following safety of CAVs\nare rarely considered in the literature. This paper proposes an efficient\nsafety-oriented car-following model for CAVs considering the impact of discrete\nsignals. The safety constraints during both normal driving and a sudden hard\nbrake are incorporated into one integrated model to eliminate possible\ncollisions during the whole driving process. The mechanical delay information\nof the preceding vehicle is used to improve car-following efficiency. Four\nmodules are designed to enhance driving comfort and string stability in case of\nheavy packet losses. Simulations of a platoon with diversified vehicle types\ndemonstrate the safety, efficiency, and string stability of the proposed model.\nTests with different packet loss rates imply that the model could guarantee\nsafety and driving comfort in even poor communication environments.",
                "Re-imagining health and well-being in low resource African settings\n  using an augmented AI system and a 3D digital twin\nThis paper discusses and explores the potential and relevance of recent\ndevelopments in artificial intelligence (AI) and digital twins for health and\nwell-being in low-resource African countries. We use the case of public health\nemergency response to disease outbreaks and epidemic control. There is\npotential to take advantage of the increasing availability of data and\ndigitization to develop advanced AI methods for analysis and prediction. Using\nan AI systems perspective, we review emerging trends in AI systems and digital\ntwins and propose an initial augmented AI system architecture to illustrate how\nan AI system can work with a 3D digital twin to address public health goals. We\nhighlight scientific knowledge discovery, continual learning, pragmatic\ninteroperability, and interactive explanation and decision-making as essential\nresearch challenges for AI systems and digital twins.",
                "FMM-X3D: FPGA-based modeling and mapping of X3D for Human Action\n  Recognition\n3D Convolutional Neural Networks are gaining increasing attention from\nresearchers and practitioners and have found applications in many domains, such\nas surveillance systems, autonomous vehicles, human monitoring systems, and\nvideo retrieval. However, their widespread adoption is hindered by their high\ncomputational and memory requirements, especially when resource-constrained\nsystems are targeted. This paper addresses the problem of mapping X3D, a\nstate-of-the-art model in Human Action Recognition that achieves accuracy of\n95.5\\% in the UCF101 benchmark, onto any FPGA device. The proposed toolflow\ngenerates an optimised stream-based hardware system, taking into account the\navailable resources and off-chip memory characteristics of the FPGA device. The\ngenerated designs push further the current performance-accuracy pareto front,\nand enable for the first time the targeting of such complex model architectures\nfor the Human Action Recognition task."
            ],
            "interesting paper": 4
        },
        {
            "papers": [
                "Development of a ROS-based Architecture for Intelligent Autonomous on\n  Demand Last Mile Delivery\nThis paper presents the development of the JKU-ITS Last Mile Delivery Robot.\nThe proposed approach utilizes a combination of one 3D LIDAR, RGB-D camera, IMU\nand GPS sensor on top of a mobile robot slope mower. An embedded computer,\nrunning ROS1, is utilized to process the sensor data streams to enable 2D and\n3D Simultaneous Localization and Mapping, 2D localization and object detection\nusing a convolutional neural network.",
                "RLAD: Reinforcement Learning from Pixels for Autonomous Driving in Urban\n  Environments\nCurrent approaches of Reinforcement Learning (RL) applied in urban Autonomous\nDriving (AD) focus on decoupling the perception training from the driving\npolicy training. The main reason is to avoid training a convolution encoder\nalongside a policy network, which is known to have issues related to sample\nefficiency, degenerated feature representations, and catastrophic\nself-overfitting. However, this paradigm can lead to representations of the\nenvironment that are not aligned with the downstream task, which may result in\nsuboptimal performances. To address this limitation, this paper proposes RLAD,\nthe first Reinforcement Learning from Pixels (RLfP) method applied in the urban\nAD domain. We propose several techniques to enhance the performance of an RLfP\nalgorithm in this domain, including: i) an image encoder that leverages both\nimage augmentations and Adaptive Local Signal Mixing (A-LIX) layers; ii)\nWayConv1D, which is a waypoint encoder that harnesses the 2D geometrical\ninformation of the waypoints using 1D convolutions; and iii) an auxiliary loss\nto increase the significance of the traffic lights in the latent representation\nof the environment. Experimental results show that RLAD significantly\noutperforms all state-of-the-art RLfP methods on the NoCrash benchmark. We also\npresent an infraction analysis on the NoCrash-regular benchmark, which\nindicates that RLAD performs better than all other methods in terms of both\ncollision rate and red light infractions.",
                "Towards a Unifying Model of Rationality in Multiagent Systems\nMultiagent systems deployed in the real world need to cooperate with other\nagents (including humans) nearly as effectively as these agents cooperate with\none another. To design such AI, and provide guarantees of its effectiveness, we\nneed to clearly specify what types of agents our AI must be able to cooperate\nwith. In this work we propose a generic model of socially intelligent agents,\nwhich are individually rational learners that are also able to cooperate with\none another (in the sense that their joint behavior is Pareto efficient). We\ndefine rationality in terms of the regret incurred by each agent over its\nlifetime, and show how we can construct socially intelligent agents for\ndifferent forms of regret. We then discuss the implications of this model for\nthe development of \"robust\" MAS that can cooperate with a wide variety of\nsocially intelligent agents.",
                "Experience Filter: Using Past Experiences on Unseen Tasks or\n  Environments\nOne of the bottlenecks of training autonomous vehicle (AV) agents is the\nvariability of training environments. Since learning optimal policies for\nunseen environments is often very costly and requires substantial data\ncollection, it becomes computationally intractable to train the agent on every\npossible environment or task the AV may encounter. This paper introduces a\nzero-shot filtering approach to interpolate learned policies of past\nexperiences to generalize to unseen ones. We use an experience kernel to\ncorrelate environments. These correlations are then exploited to produce\npolicies for new tasks or environments from learned policies. We demonstrate\nour methods on an autonomous vehicle driving through T-intersections with\ndifferent characteristics, where its behavior is modeled as a partially\nobservable Markov decision process (POMDP). We first construct compact\nrepresentations of learned policies for POMDPs with unknown transition\nfunctions given a dataset of sequential actions and observations. Then, we\nfilter parameterized policies of previously visited environments to generate\npolicies to new, unseen environments. We demonstrate our approaches on both an\nactual AV and a high-fidelity simulator. Results indicate that our experience\nfilter offers a fast, low-effort, and near-optimal solution to create policies\nfor tasks or environments never seen before. Furthermore, the generated new\npolicies outperform the policy learned using the entire data collected from\npast environments, suggesting that the correlation among different environments\ncan be exploited and irrelevant ones can be filtered out.",
                "Learning Off-Road Terrain Traversability with Self-Supervisions Only\nEstimating the traversability of terrain should be reliable and accurate in\ndiverse conditions for autonomous driving in off-road environments. However,\nlearning-based approaches often yield unreliable results when confronted with\nunfamiliar contexts, and it is challenging to obtain manual annotations\nfrequently for new circumstances. In this paper, we introduce a method for\nlearning traversability from images that utilizes only self-supervision and no\nmanual labels, enabling it to easily learn traversability in new circumstances.\nTo this end, we first generate self-supervised traversability labels from past\ndriving trajectories by labeling regions traversed by the vehicle as highly\ntraversable. Using the self-supervised labels, we then train a neural network\nthat identifies terrains that are safe to traverse from an image using a\none-class classification algorithm. Additionally, we supplement the limitations\nof self-supervised labels by incorporating methods of self-supervised learning\nof visual representations. To conduct a comprehensive evaluation, we collect\ndata in a variety of driving environments and perceptual conditions and show\nthat our method produces reliable estimations in various environments. In\naddition, the experimental results validate that our method outperforms other\nself-supervised traversability estimation methods and achieves comparable\nperformances with supervised learning methods trained on manually labeled data.",
                "RE-centric Recommendations for the Development of Trustworthy(er)\n  Autonomous Systems\nComplying with the EU AI Act (AIA) guidelines while developing and\nimplementing AI systems will soon be mandatory within the EU. However,\npractitioners lack actionable instructions to operationalise ethics during AI\nsystems development. A literature review of different ethical guidelines\nrevealed inconsistencies in the principles addressed and the terminology used\nto describe them. Furthermore, requirements engineering (RE), which is\nidentified to foster trustworthiness in the AI development process from the\nearly stages was observed to be absent in a lot of frameworks that support the\ndevelopment of ethical and trustworthy AI. This incongruous phrasing combined\nwith a lack of concrete development practices makes trustworthy AI development\nharder. To address this concern, we formulated a comparison table for the\nterminology used and the coverage of the ethical AI principles in major ethical\nAI guidelines. We then examined the applicability of ethical AI development\nframeworks for performing effective RE during the development of trustworthy AI\nsystems. A tertiary review and meta-analysis of literature discussing ethical\nAI frameworks revealed their limitations when developing trustworthy AI. Based\non our findings, we propose recommendations to address such limitations during\nthe development of trustworthy AI."
            ],
            "interesting paper": 2
        },
        {
            "papers": [
                "Data and Knowledge for Overtaking Scenarios in Autonomous Driving\nAutonomous driving has become one of the most popular research topics within\nArtificial Intelligence. An autonomous vehicle is understood as a system that\ncombines perception, decision-making, planning, and control. All of those tasks\nrequire that the vehicle collects surrounding data in order to make a good\ndecision and action. In particular, the overtaking maneuver is one of the most\ncritical actions of driving. The process involves lane changes, acceleration\nand deceleration actions, and estimation of the speed and distance of the\nvehicle in front or in the lane in which it is moving. Despite the amount of\nwork available in the literature, just a few handle overtaking maneuvers and,\nbecause overtaking can be risky, no real-world dataset is available. This work\ncontributes in this area by presenting a new synthetic dataset whose focus is\nthe overtaking maneuver. We start by performing a thorough review of the state\nof the art in autonomous driving and then explore the main datasets found in\nthe literature (public and private, synthetic and real), highlighting their\nlimitations, and suggesting a new set of features whose focus is the overtaking\nmaneuver.",
                "Intent-aligned AI systems deplete human agency: the need for agency\n  foundations research in AI safety\nThe rapid advancement of artificial intelligence (AI) systems suggests that\nartificial general intelligence (AGI) systems may soon arrive. Many researchers\nare concerned that AIs and AGIs will harm humans via intentional misuse\n(AI-misuse) or through accidents (AI-accidents). In respect of AI-accidents,\nthere is an increasing effort focused on developing algorithms and paradigms\nthat ensure AI systems are aligned to what humans intend, e.g. AI systems that\nyield actions or recommendations that humans might judge as consistent with\ntheir intentions and goals. Here we argue that alignment to human intent is\ninsufficient for safe AI systems and that preservation of long-term agency of\nhumans may be a more robust standard, and one that needs to be separated\nexplicitly and a priori during optimization. We argue that AI systems can\nreshape human intention and discuss the lack of biological and psychological\nmechanisms that protect humans from loss of agency. We provide the first formal\ndefinition of agency-preserving AI-human interactions which focuses on\nforward-looking agency evaluations and argue that AI systems - not humans -\nmust be increasingly tasked with making these evaluations. We show how agency\nloss can occur in simple environments containing embedded agents that use\ntemporal-difference learning to make action recommendations. Finally, we\npropose a new area of research called \"agency foundations\" and pose four\ninitial topics designed to improve our understanding of agency in AI-human\ninteractions: benevolent game theory, algorithmic foundations of human rights,\nmechanistic interpretability of agency representation in neural-networks and\nreinforcement learning from internal states.",
                "Bigger, Better, Faster: Human-level Atari with human-level efficiency\nWe introduce a value-based RL agent, which we call BBF, that achieves\nsuper-human performance in the Atari 100K benchmark. BBF relies on scaling the\nneural networks used for value estimation, as well as a number of other design\nchoices that enable this scaling in a sample-efficient manner. We conduct\nextensive analyses of these design choices and provide insights for future\nwork. We end with a discussion about updating the goalposts for\nsample-efficient RL research on the ALE. We make our code and data publicly\navailable at\nhttps://github.com/google-research/google-research/tree/master/bigger_better_faster.",
                "GAN-MPC: Training Model Predictive Controllers with Parameterized Cost\n  Functions using Demonstrations from Non-identical Experts\nModel predictive control (MPC) is a popular approach for trajectory\noptimization in practical robotics applications. MPC policies can optimize\ntrajectory parameters under kinodynamic and safety constraints and provide\nguarantees on safety, optimality, generalizability, interpretability, and\nexplainability. However, some behaviors are complex and it is difficult to\nhand-craft an MPC objective function. A special class of MPC policies called\nLearnable-MPC addresses this difficulty using imitation learning from expert\ndemonstrations. However, they require the demonstrator and the imitator agents\nto be identical which is hard to satisfy in many real world applications of\nrobotics. In this paper, we address the practical problem of training\nLearnable-MPC policies when the demonstrator and the imitator do not share the\nsame dynamics and their state spaces may have a partial overlap. We propose a\nnovel approach that uses a generative adversarial network (GAN) to minimize the\nJensen-Shannon divergence between the state-trajectory distributions of the\ndemonstrator and the imitator. We evaluate our approach on a variety of\nsimulated robotics tasks of DeepMind Control suite and demonstrate the efficacy\nof our approach at learning the demonstrator's behavior without having to copy\ntheir actions.",
                "SO(2)-Equivariant Downwash Models for Close Proximity Flight\nMultirotors flying in close proximity induce aerodynamic wake effects on each\nother through propeller downwash. Conventional methods have fallen short of\nproviding adequate 3D force-based models that can be incorporated into robust\ncontrol paradigms for deploying dense formations. Thus, learning a model for\nthese downwash patterns presents an attractive solution. In this paper, we\npresent a novel learning-based approach for modelling the downwash forces that\nexploits the latent geometries (i.e. symmetries) present in the problem. We\ndemonstrate that when trained with only 5 minutes of real-world flight data,\nour geometry-aware model outperforms state-of-the-art baseline models trained\nwith more than 15 minutes of data. In dense real-world flights with two\nvehicles, deploying our model online improves 3D trajectory tracking by nearly\n36% on average (and vertical tracking by 56%).",
                "Probabilistic computation and uncertainty quantification with emerging\n  covariance\nBuilding robust, interpretable, and secure AI system requires quantifying and\nrepresenting uncertainty under a probabilistic perspective to mimic human\ncognitive abilities. However, probabilistic computation presents significant\nchallenges for most conventional artificial neural network, as they are\nessentially implemented in a deterministic manner. In this paper, we develop an\nefficient probabilistic computation framework by truncating the probabilistic\nrepresentation of neural activation up to its mean and covariance and construct\na moment neural network that encapsulates the nonlinear coupling between the\nmean and covariance of the underlying stochastic network. We reveal that when\nonly the mean but not the covariance is supervised during gradient-based\nlearning, the unsupervised covariance spontaneously emerges from its nonlinear\ncoupling with the mean and faithfully captures the uncertainty associated with\nmodel predictions. Our findings highlight the inherent simplicity of\nprobabilistic computation by seamlessly incorporating uncertainty into model\nprediction, paving the way for integrating it into large-scale AI systems."
            ],
            "interesting paper": 5
        },
        {
            "papers": [
                "Efficient Learning of Urban Driving Policies Using Bird's-Eye-View State\n  Representations\nAutonomous driving involves complex decision-making in highly interactive\nenvironments, requiring thoughtful negotiation with other traffic participants.\nWhile reinforcement learning provides a way to learn such interaction behavior,\nefficient learning critically depends on scalable state representations.\nContrary to imitation learning methods, high-dimensional state representations\nstill constitute a major bottleneck for deep reinforcement learning methods in\nautonomous driving. In this paper, we study the challenges of constructing\nbird's-eye-view representations for autonomous driving and propose a recurrent\nlearning architecture for long-horizon driving. Our PPO-based approach, called\nRecurrDriveNet, is demonstrated on a simulated autonomous driving task in\nCARLA, where it outperforms traditional frame-stacking methods while only\nrequiring one million experiences for efficient training. RecurrDriveNet causes\nless than one infraction per driven kilometer by interacting safely with other\nroad users.",
                "EMOTE: An Explainable architecture for Modelling the Other Through\n  Empathy\nWe can usually assume others have goals analogous to our own. This assumption\ncan also, at times, be applied to multi-agent games - e.g. Agent 1's attraction\nto green pellets is analogous to Agent 2's attraction to red pellets. This\n\"analogy\" assumption is tied closely to the cognitive process known as empathy.\nInspired by empathy, we design a simple and explainable architecture to model\nanother agent's action-value function. This involves learning an \"Imagination\nNetwork\" to transform the other agent's observed state in order to produce a\nhuman-interpretable \"empathetic state\" which, when presented to the learning\nagent, produces behaviours that mimic the other agent. Our approach is\napplicable to multi-agent scenarios consisting of a single learning agent and\nother (independent) agents acting according to fixed policies. This\narchitecture is particularly beneficial for (but not limited to) algorithms\nusing a composite value or reward function. We show our method produces better\nperformance in multi-agent games, where it robustly estimates the other's model\nin different environment configurations. Additionally, we show that the\nempathetic states are human interpretable, and thus verifiable.",
                "Human Control: Definitions and Algorithms\nHow can humans stay in control of advanced artificial intelligence systems?\nOne proposal is corrigibility, which requires the agent to follow the\ninstructions of a human overseer, without inappropriately influencing them. In\nthis paper, we formally define a variant of corrigibility called shutdown\ninstructability, and show that it implies appropriate shutdown behavior,\nretention of human autonomy, and avoidance of user harm. We also analyse the\nrelated concepts of non-obstruction and shutdown alignment, three previously\nproposed algorithms for human control, and one new algorithm.",
                "Traffic Road Congestion System using by the internet of vehicles (IoV)\nTraffic problems have increased in modern life due to a huge number of\nvehicles, big cities, and ignoring the traffic rules. Vehicular ad hoc network\n(VANET) has improved the traffic system in previous some and plays a vital role\nin the best traffic control system in big cities. But due to some limitations,\nit is not enough to control some problems in specific conditions. Now a day\ninvention of new technologies of the Internet of Things (IoT) is used for\ncollaboratively and efficiently performing tasks. This technology was also\nintroduced in the transportation system which makes it an intelligent\ntransportation system (ITS), this is called the Internet of vehicles (IOV). We\nwill elaborate on traffic problems in the traditional system and elaborate on\nthe benefits, enhancements, and reasons to better IOV by Systematic Literature\nReview (SLR). This technique will be implemented by targeting needed papers\nthrough many search phrases. A systematic literature review is used for 121\narticles between 2014 and 2023. The IoV technologies and tools are required to\ncreate the IoV and resolve some traffic rules through SUMO (simulation of urban\nmobility) which is used for the design and simulation the road traffic. We have\ntried to contribute to the best model of the traffic control system. This paper\nwill analysis two vehicular congestion control models in term of select the\noptimized and efficient model and elaborate on the reasons for efficiency by\nsearching the solution SLR based questions. Due to some efficient features, we\nhave suggested the IOV based on vehicular clouds. These efficient features make\nthis model the best and most effective than the traditional model which is a\ngreat reason to enhance the network system.",
                "Decision-Oriented Dialogue for Human-AI Collaboration\nWe describe a class of tasks called decision-oriented dialogues, in which AI\nassistants such as large language models (LMs) must collaborate with one or\nmore humans via natural language to help them make complex decisions. We\nformalize three domains in which users face everyday decisions: (1) choosing an\nassignment of reviewers to conference papers, (2) planning a multi-step\nitinerary in a city, and (3) negotiating travel plans for a group of friends.\nIn each of these settings, AI assistants and users have disparate abilities\nthat they must combine to arrive at the best decision: assistants can access\nand process large amounts of information, while users have preferences and\nconstraints external to the system. For each task, we build a dialogue\nenvironment where agents receive a reward based on the quality of the final\ndecision they reach. We evaluate LMs in self-play and in collaboration with\nhumans and find that they fall short compared to human assistants, achieving\nmuch lower rewards despite engaging in longer dialogues. We highlight a number\nof challenges models face in decision-oriented dialogues, ranging from\ngoal-directed behavior to reasoning and optimization, and release our\nenvironments as a testbed for future work.",
                "BetaZero: Belief-State Planning for Long-Horizon POMDPs using Learned\n  Approximations\nReal-world planning problems, including autonomous driving and sustainable\nenergy applications like carbon storage and resource exploration, have recently\nbeen modeled as partially observable Markov decision processes (POMDPs) and\nsolved using approximate methods. To solve high-dimensional POMDPs in practice,\nstate-of-the-art methods use online planning with problem-specific heuristics\nto reduce planning horizons and make the problems tractable. Algorithms that\nlearn approximations to replace heuristics have recently found success in\nlarge-scale fully observable domains. The key insight is the combination of\nonline Monte Carlo tree search with offline neural network approximations of\nthe optimal policy and value function. In this work, we bring this insight to\npartially observable domains and propose BetaZero, a belief-state planning\nalgorithm for high-dimensional POMDPs. BetaZero learns offline approximations\nthat replace heuristics to enable online decision making in long-horizon\nproblems. We address several challenges inherent in large-scale partially\nobservable domains; namely challenges of transitioning in stochastic\nenvironments, prioritizing action branching with a limited search budget, and\nrepresenting beliefs as input to the network. To formalize the use of all\nlimited search information, we train against a novel $Q$-weighted visit counts\npolicy. We test BetaZero on various well-established POMDP benchmarks found in\nthe literature and a real-world problem of critical mineral exploration.\nExperiments show that BetaZero outperforms state-of-the-art POMDP solvers on a\nvariety of tasks."
            ],
            "interesting paper": 1
        },
        {
            "papers": [
                "AI Liability Insurance With an Example in AI-Powered E-diagnosis System\nArtificial Intelligence (AI) has received an increasing amount of attention\nin multiple areas. The uncertainties and risks in AI-powered systems have\ncreated reluctance in their wild adoption. As an economic solution to\ncompensate for potential damages, AI liability insurance is a promising market\nto enhance the integration of AI into daily life. In this work, we use an\nAI-powered E-diagnosis system as an example to study AI liability insurance. We\nprovide a quantitative risk assessment model with evidence-based numerical\nanalysis. We discuss the insurability criteria for AI technologies and suggest\nnecessary adjustments to accommodate the features of AI products. We show that\nAI liability insurance can act as a regulatory mechanism to incentivize\ncompliant behaviors and serve as a certificate of high-quality AI systems.\nFurthermore, we suggest premium adjustment to reflect the dynamic evolution of\nthe inherent uncertainty in AI. Moral hazard problems are discussed and\nsuggestions for AI liability insurance are provided.",
                "Multi-Robot Path Planning Combining Heuristics and Multi-Agent\n  Reinforcement Learning\nMulti-robot path finding in dynamic environments is a highly challenging\nclassic problem. In the movement process, robots need to avoid collisions with\nother moving robots while minimizing their travel distance. Previous methods\nfor this problem either continuously replan paths using heuristic search\nmethods to avoid conflicts or choose appropriate collision avoidance strategies\nbased on learning approaches. The former may result in long travel distances\ndue to frequent replanning, while the latter may have low learning efficiency\ndue to low sample exploration and utilization, and causing high training costs\nfor the model. To address these issues, we propose a path planning method,\nMAPPOHR, which combines heuristic search, empirical rules, and multi-agent\nreinforcement learning. The method consists of two layers: a real-time planner\nbased on the multi-agent reinforcement learning algorithm, MAPPO, which embeds\nempirical rules in the action output layer and reward functions, and a\nheuristic search planner used to create a global guiding path. During movement,\nthe heuristic search planner replans new paths based on the instructions of the\nreal-time planner. We tested our method in 10 different conflict scenarios. The\nexperiments show that the planning performance of MAPPOHR is better than that\nof existing learning and heuristic methods. Due to the utilization of empirical\nknowledge and heuristic search, the learning efficiency of MAPPOHR is higher\nthan that of existing learning methods.",
                "Egocentric Planning for Scalable Embodied Task Achievement\nEmbodied agents face significant challenges when tasked with performing\nactions in diverse environments, particularly in generalizing across object\ntypes and executing suitable actions to accomplish tasks. Furthermore, agents\nshould exhibit robustness, minimizing the execution of illegal actions. In this\nwork, we present Egocentric Planning, an innovative approach that combines\nsymbolic planning and Object-oriented POMDPs to solve tasks in complex\nenvironments, harnessing existing models for visual perception and natural\nlanguage processing. We evaluated our approach in ALFRED, a simulated\nenvironment designed for domestic tasks, and demonstrated its high scalability,\nachieving an impressive 36.07% unseen success rate in the ALFRED benchmark and\nwinning the ALFRED challenge at CVPR Embodied AI workshop. Our method requires\nreliable perception and the specification or learning of a symbolic description\nof the preconditions and effects of the agent's actions, as well as what object\ntypes reveal information about others. It is capable of naturally scaling to\nsolve new tasks beyond ALFRED, as long as they can be solved using the\navailable skills. This work offers a solid baseline for studying end-to-end and\nhybrid methods that aim to generalize to new tasks, including recent approaches\nrelying on LLMs, but often struggle to scale to long sequences of actions or\nproduce robust plans for novel tasks.",
                "LIV: Language-Image Representations and Rewards for Robotic Control\nWe present Language-Image Value learning (LIV), a unified objective for\nvision-language representation and reward learning from action-free videos with\ntext annotations. Exploiting a novel connection between dual reinforcement\nlearning and mutual information contrastive learning, the LIV objective trains\na multi-modal representation that implicitly encodes a universal value function\nfor tasks specified as language or image goals. We use LIV to pre-train the\nfirst control-centric vision-language representation from large human video\ndatasets such as EpicKitchen. Given only a language or image goal, the\npre-trained LIV model can assign dense rewards to each frame in videos of\nunseen robots or humans attempting that task in unseen environments. Further,\nwhen some target domain-specific data is available, the same objective can be\nused to fine-tune and improve LIV and even other pre-trained representations\nfor robotic control and reward specification in that domain. In our experiments\non several simulated and real-world robot environments, LIV models consistently\noutperform the best prior input state representations for imitation learning,\nas well as reward specification methods for policy synthesis. Our results\nvalidate the advantages of joint vision-language representation and reward\nlearning within the unified, compact LIV framework.",
                "Interpretable and Explainable Logical Policies via Neurally Guided\n  Symbolic Abstraction\nThe limited priors required by neural networks make them the dominating\nchoice to encode and learn policies using reinforcement learning (RL). However,\nthey are also black-boxes, making it hard to understand the agent's behaviour,\nespecially when working on the image level. Therefore, neuro-symbolic RL aims\nat creating policies that are interpretable in the first place. Unfortunately,\ninterpretability is not explainability. To achieve both, we introduce Neurally\ngUided Differentiable loGic policiEs (NUDGE). NUDGE exploits trained neural\nnetwork-based agents to guide the search of candidate-weighted logic rules,\nthen uses differentiable logic to train the logic agents. Our experimental\nevaluation demonstrates that NUDGE agents can induce interpretable and\nexplainable policies while outperforming purely neural ones and showing good\nflexibility to environments of different initial states and problem sizes.",
                "An Architecture for Deploying Reinforcement Learning in Industrial\n  Environments\nIndustry 4.0 is driven by demands like shorter time-to-market, mass\ncustomization of products, and batch size one production. Reinforcement\nLearning (RL), a machine learning paradigm shown to possess a great potential\nin improving and surpassing human level performance in numerous complex tasks,\nallows coping with the mentioned demands. In this paper, we present an OPC UA\nbased Operational Technology (OT)-aware RL architecture, which extends the\nstandard RL setting, combining it with the setting of digital twins. Moreover,\nwe define an OPC UA information model allowing for a generalized plug-and-play\nlike approach for exchanging the RL agent used. In conclusion, we demonstrate\nand evaluate the architecture, by creating a proof of concept. By means of\nsolving a toy example, we show that this architecture can be used to determine\nthe optimal policy using a real control system."
            ],
            "interesting paper": 2
        }
    ],
    "Bailey Johnson": [
        {
            "papers": [
                "READ: Recurrent Adaptation of Large Transformers\nFine-tuning large-scale Transformers has led to the explosion of many AI\napplications across Natural Language Processing and Computer Vision tasks.\nHowever, fine-tuning all pre-trained model parameters becomes impractical as\nthe model size and number of tasks increase. Parameter-efficient transfer\nlearning (PETL) methods aim to address these challenges. While effective in\nreducing the number of trainable parameters, PETL methods still require\nsignificant energy and computational resources to fine-tune. In this paper, we\nintroduce \\textbf{RE}current \\textbf{AD}aption (READ) -- a lightweight and\nmemory-efficient fine-tuning method -- to overcome the limitations of the\ncurrent PETL approaches. Specifically, READ inserts a small RNN network\nalongside the backbone model so that the model does not have to back-propagate\nthrough the large backbone network. Through comprehensive empirical evaluation\nof the GLUE benchmark, we demonstrate READ can achieve a $56\\%$ reduction in\nthe training memory consumption and an $84\\%$ reduction in the GPU energy usage\nwhile retraining high model quality compared to full-tuning. Additionally, the\nmodel size of READ does not grow with the backbone model size, making it a\nhighly scalable solution for fine-tuning large Transformers.",
                "TLNets: Transformation Learning Networks for long-range time-series\n  prediction\nTime series prediction is a prevalent issue across various disciplines, such\nas meteorology, traffic surveillance, investment, and energy production and\nconsumption. Many statistical and machine-learning strategies have been\ndeveloped to tackle this problem. However, these approaches either lack\nexplainability or exhibit less satisfactory performance when the prediction\nhorizon increases. To this end, we propose a novel plan for the designing of\nnetworks' architecture based on transformations, possessing the potential to\nachieve an enhanced receptive field in learning which brings benefits to fuse\nfeatures across scales. In this context, we introduce four different\ntransformation mechanisms as bases to construct the learning model including\nFourier Transform (FT), Singular Value Decomposition (SVD), matrix\nmultiplication and Conv block. Hence, we develop four learning models based on\nthe above building blocks, namely, FT-Matrix, FT-SVD, FT-Conv, and Conv-SVD.\nNote that the FT and SVD blocks are capable of learning global information,\nwhile the Conv blocks focus on learning local information. The matrix block is\nsparsely designed to learn both global and local information simultaneously.\nThe above Transformation Learning Networks (TLNets) have been extensively\ntested and compared with multiple baseline models based on several real-world\ndatasets and showed clear potential in long-range time-series forecasting.",
                "PDE+: Enhancing Generalization via PDE with Adaptive Distributional\n  Diffusion\nThe generalization of neural networks is a central challenge in machine\nlearning, especially concerning the performance under distributions that differ\nfrom training ones. Current methods, mainly based on the data-driven paradigm\nsuch as data augmentation, adversarial training, and noise injection, may\nencounter limited generalization due to model non-smoothness. In this paper, we\npropose to investigate generalization from a Partial Differential Equation\n(PDE) perspective, aiming to enhance it directly through the underlying\nfunction of neural networks, rather than focusing on adjusting input data.\nSpecifically, we first establish the connection between neural network\ngeneralization and the smoothness of the solution to a specific PDE, namely\n\"transport equation\". Building upon this, we propose a general framework that\nintroduces adaptive distributional diffusion into transport equation to enhance\nthe smoothness of its solution, thereby improving generalization. In the\ncontext of neural networks, we put this theoretical framework into practice as\n$\\textbf{PDE+}$ ($\\textbf{PDE}$ with $\\textbf{A}$daptive\n$\\textbf{D}$istributional $\\textbf{D}$iffusion) which diffuses each sample into\na distribution covering semantically similar inputs. This enables better\ncoverage of potentially unobserved distributions in training, thus improving\ngeneralization beyond merely data-driven methods. The effectiveness of PDE+ is\nvalidated through extensive experimental settings, demonstrating its superior\nperformance compared to SOTA methods.",
                "Unpaired Image-to-Image Translation via Neural Schr\u00f6dinger Bridge\nDiffusion models are a powerful class of generative models which simulate\nstochastic differential equations (SDEs) to generate data from noise. While\ndiffusion models have achieved remarkable progress, they have limitations in\nunpaired image-to-image (I2I) translation tasks due to the Gaussian prior\nassumption. Schr\\\"{o}dinger Bridge (SB), which learns an SDE to translate\nbetween two arbitrary distributions, have risen as an attractive solution to\nthis problem. Yet, to our best knowledge, none of SB models so far have been\nsuccessful at unpaired translation between high-resolution images. In this\nwork, we propose Unpaired Neural Schr\\\"{o}dinger Bridge (UNSB), which expresses\nthe SB problem as a sequence of adversarial learning problems. This allows us\nto incorporate advanced discriminators and regularization to learn a SB between\nunpaired data. We show that UNSB is scalable and successfully solves various\nunpaired I2I translation tasks. Code: \\url{https://github.com/cyclomon/UNSB}",
                "Meta Adaptive Task Sampling for Few-Domain Generalization\nTo ensure the out-of-distribution (OOD) generalization performance,\ntraditional domain generalization (DG) methods resort to training on data from\nmultiple sources with different underlying distributions. And the success of\nthose DG methods largely depends on the fact that there are diverse training\ndistributions. However, it usually needs great efforts to obtain enough\nheterogeneous data due to the high expenses, privacy issues or the scarcity of\ndata. Thus an interesting yet seldom investigated problem arises: how to\nimprove the OOD generalization performance when the perceived heterogeneity is\nlimited. In this paper, we instantiate a new framework called few-domain\ngeneralization (FDG), which aims to learn a generalizable model from very few\ndomains of novel tasks with the knowledge acquired from previous learning\nexperiences on base tasks. Moreover, we propose a Meta Adaptive Task Sampling\n(MATS) procedure to differentiate base tasks according to their semantic and\ndomain-shift similarity to the novel task. Empirically, we show that the newly\nintroduced FDG framework can substantially improve the OOD generalization\nperformance on the novel task and further combining MATS with episodic training\ncould outperform several state-of-the-art DG baselines on widely used\nbenchmarks like PACS and DomainNet.",
                "Knowledge Diffusion for Distillation\nThe representation gap between teacher and student is an emerging topic in\nknowledge distillation (KD). To reduce the gap and improve the performance,\ncurrent methods often resort to complicated training schemes, loss functions,\nand feature alignments, which are task-specific and feature-specific. In this\npaper, we state that the essence of these methods is to discard the noisy\ninformation and distill the valuable information in the feature, and propose a\nnovel KD method dubbed DiffKD, to explicitly denoise and match features using\ndiffusion models. Our approach is based on the observation that student\nfeatures typically contain more noises than teacher features due to the smaller\ncapacity of student model. To address this, we propose to denoise student\nfeatures using a diffusion model trained by teacher features. This allows us to\nperform better distillation between the refined clean feature and teacher\nfeature. Additionally, we introduce a light-weight diffusion model with a\nlinear autoencoder to reduce the computation cost and an adaptive noise\nmatching module to improve the denoising performance. Extensive experiments\ndemonstrate that DiffKD is effective across various types of features and\nachieves state-of-the-art performance consistently on image classification,\nobject detection, and semantic segmentation tasks. Code is available at\nhttps://github.com/hunto/DiffKD."
            ],
            "interesting paper": 1
        },
        {
            "papers": [
                "Balanced Supervised Contrastive Learning for Few-Shot Class-Incremental\n  Learning\nFew-shot class-incremental learning (FSCIL) presents the primary challenge of\nbalancing underfitting to a new session's task and forgetting the tasks from\nprevious sessions. To address this challenge, we develop a simple yet powerful\nlearning scheme that integrates effective methods for each core component of\nthe FSCIL network, including the feature extractor, base session classifiers,\nand incremental session classifiers. In feature extractor training, our goal is\nto obtain balanced generic representations that benefit both current viewable\nand unseen or past classes. To achieve this, we propose a balanced supervised\ncontrastive loss that effectively balances these two objectives. In terms of\nclassifiers, we analyze and emphasize the importance of unifying initialization\nmethods for both the base and incremental session classifiers. Our method\ndemonstrates outstanding ability for new task learning and preventing\nforgetting on CUB200, CIFAR100, and miniImagenet datasets, with significant\nimprovements over previous state-of-the-art methods across diverse metrics. We\nconduct experiments to analyze the significance and rationale behind our\napproach and visualize the effectiveness of our representations on new tasks.\nFurthermore, we conduct diverse ablation studies to analyze the effects of each\nmodule.",
                "Dynamic Inter-treatment Information Sharing for Individualized Treatment\n  Effects Estimation\nEstimation of individualized treatment effects (ITE) from observational\nstudies is a fundamental problem in causal inference and holds significant\nimportance across domains, including healthcare. However, limited observational\ndatasets pose challenges in reliable ITE estimation as data have to be split\namong treatment groups to train an ITE learner. While information sharing among\ntreatment groups can partially alleviate the problem, there is currently no\ngeneral framework for end-to-end information sharing in ITE estimation. To\ntackle this problem, we propose a deep learning framework based on\n`\\textit{soft weight sharing}' to train ITE learners, enabling \\textit{dynamic\nend-to-end} information sharing among treatment groups. The proposed framework\ncomplements existing ITE learners, and introduces a new class of ITE learners,\nreferred to as \\textit{HyperITE}. We extend state-of-the-art ITE learners with\n\\textit{HyperITE} versions and evaluate them on IHDP, ACIC-2016, and Twins\nbenchmarks. Our experimental results show that the proposed framework improves\nITE estimation error, with increasing effectiveness for smaller datasets.",
                "LANISTR: Multimodal Learning from Structured and Unstructured Data\nMultimodal large-scale pretraining has shown impressive performance for\nunstructured data such as language and image. However, a prevalent real-world\nscenario involves structured data types, tabular and time-series, along with\nunstructured data. Such scenarios have been understudied. To bridge this gap,\nwe propose LANISTR, an attention-based framework to learn from LANguage, Image,\nand STRuctured data. The core of LANISTR's methodology is rooted in\n\\textit{masking-based} training applied across both unimodal and multimodal\nlevels. In particular, we introduce a new similarity-based multimodal masking\nloss that enables it to learn cross-modal relations from large-scale multimodal\ndata with missing modalities. On two real-world datasets, MIMIC-IV (from\nhealthcare) and Amazon Product Review (from retail), LANISTR demonstrates\nremarkable improvements, 6.6\\% (in AUROC) and 14\\% (in accuracy) when\nfine-tuned with 0.1\\% and 0.01\\% of labeled data, respectively, compared to the\nstate-of-the-art alternatives. Notably, these improvements are observed even\nwith very high ratio of samples (35.7\\% and 99.8\\% respectively) not containing\nall modalities, underlining the robustness of LANISTR to practical missing\nmodality challenge. Our code and models will be available at\nhttps://github.com/google-research/lanistr",
                "Differentiable Random Partition Models\nPartitioning a set of elements into an unknown number of mutually exclusive\nsubsets is essential in many machine learning problems. However, assigning\nelements, such as samples in a dataset or neurons in a network layer, to an\nunknown and discrete number of subsets is inherently non-differentiable,\nprohibiting end-to-end gradient-based optimization of parameters. We overcome\nthis limitation by proposing a novel two-step method for inferring partitions,\nwhich allows its usage in variational inference tasks. This new approach\nenables reparameterized gradients with respect to the parameters of the new\nrandom partition model. Our method works by inferring the number of elements\nper subset and, second, by filling these subsets in a learned order. We\nhighlight the versatility of our general-purpose approach on three different\nchallenging experiments: variational clustering, inference of shared and\nindependent generative factors under weak supervision, and multitask learning.",
                "On convex decision regions in deep network representations\nCurrent work on human-machine alignment aims at understanding machine-learned\nlatent spaces and their correspondence to human representations.\nG{\\\"a}rdenfors' conceptual spaces is a prominent framework for understanding\nhuman representations. Convexity of object regions in conceptual spaces is\nargued to promote generalizability, few-shot learning, and interpersonal\nalignment. Based on these insights, we investigate the notion of convexity of\nconcept regions in machine-learned latent spaces. We develop a set of tools for\nmeasuring convexity in sampled data and evaluate emergent convexity in layered\nrepresentations of state-of-the-art deep networks. We show that convexity is\nrobust to basic re-parametrization and, hence, meaningful as a quality of\nmachine-learned latent spaces. We find that approximate convexity is pervasive\nin neural representations in multiple application domains, including models of\nimages, audio, human activity, text, and medical images. Generally, we observe\nthat fine-tuning increases the convexity of label regions. We find evidence\nthat pretraining convexity of class label regions predicts subsequent\nfine-tuning performance.",
                "Diversify Your Vision Datasets with Automatic Diffusion-Based\n  Augmentation\nMany fine-grained classification tasks, like rare animal identification, have\nlimited training data and consequently classifiers trained on these datasets\noften fail to generalize to variations in the domain like changes in weather or\nlocation. As such, we explore how natural language descriptions of the domains\nseen in training data can be used with large vision models trained on diverse\npretraining datasets to generate useful variations of the training data. We\nintroduce ALIA (Automated Language-guided Image Augmentation), a method which\nutilizes large vision and language models to automatically generate natural\nlanguage descriptions of a dataset's domains and augment the training data via\nlanguage-guided image editing. To maintain data integrity, a model trained on\nthe original dataset filters out minimal image edits and those which corrupt\nclass-relevant information. The resulting dataset is visually consistent with\nthe original training data and offers significantly enhanced diversity. We show\nthat ALIA is able to surpasses traditional data augmentation and text-to-image\ngenerated data on fine-grained classification tasks, including cases of domain\ngeneralization and contextual bias. Code is available at\nhttps://github.com/lisadunlap/ALIA."
            ],
            "interesting paper": 1
        },
        {
            "papers": [
                "Matrix Information Theory for Self-Supervised Learning\nThe maximum entropy encoding framework provides a unified perspective for\nmany non-contrastive learning methods like SimSiam, Barlow Twins, and MEC.\nInspired by this framework, we introduce Matrix-SSL, a novel approach that\nleverages matrix information theory to interpret the maximum entropy encoding\nloss as matrix uniformity loss. Furthermore, Matrix-SSL enhances the maximum\nentropy encoding method by seamlessly incorporating matrix alignment loss,\ndirectly aligning covariance matrices in different branches. Experimental\nresults reveal that Matrix-SSL outperforms state-of-the-art methods on the\nImageNet dataset under linear evaluation settings and on MS-COCO for transfer\nlearning tasks. Specifically, when performing transfer learning tasks on\nMS-COCO, our method outperforms previous SOTA methods such as MoCo v2 and BYOL\nup to 3.3% with only 400 epochs compared to 800 epochs pre-training. We also\ntry to introduce representation learning into the language modeling regime by\nfine-tuning a 7B model using matrix cross-entropy loss, with a margin of 3.1%\non the GSM8K dataset over the standard cross-entropy loss. Code available at\nhttps://github.com/yifanzhang-pro/Matrix-SSL.",
                "Vision Transformers for Small Histological Datasets Learned through\n  Knowledge Distillation\nComputational Pathology (CPATH) systems have the potential to automate\ndiagnostic tasks. However, the artifacts on the digitized histological glass\nslides, known as Whole Slide Images (WSIs), may hamper the overall performance\nof CPATH systems. Deep Learning (DL) models such as Vision Transformers (ViTs)\nmay detect and exclude artifacts before running the diagnostic algorithm. A\nsimple way to develop robust and generalized ViTs is to train them on massive\ndatasets. Unfortunately, acquiring large medical datasets is expensive and\ninconvenient, prompting the need for a generalized artifact detection method\nfor WSIs. In this paper, we present a student-teacher recipe to improve the\nclassification performance of ViT for the air bubbles detection task. ViT,\ntrained under the student-teacher framework, boosts its performance by\ndistilling existing knowledge from the high-capacity teacher model. Our\nbest-performing ViT yields 0.961 and 0.911 F1-score and MCC, respectively,\nobserving a 7% gain in MCC against stand-alone training. The proposed method\npresents a new perspective of leveraging knowledge distillation over transfer\nlearning to encourage the use of customized transformers for efficient\npreprocessing pipelines in the CPATH systems.",
                "MixCE: Training Autoregressive Language Models by Mixing Forward and\n  Reverse Cross-Entropies\nAutoregressive language models are trained by minimizing the cross-entropy of\nthe model distribution Q relative to the data distribution P -- that is,\nminimizing the forward cross-entropy, which is equivalent to maximum likelihood\nestimation (MLE). We have observed that models trained in this way may\n\"over-generalize\", in the sense that they produce non-human-like text.\nMoreover, we believe that reverse cross-entropy, i.e., the cross-entropy of P\nrelative to Q, is a better reflection of how a human would evaluate text\ngenerated by a model. Hence, we propose learning with MixCE, an objective that\nmixes the forward and reverse cross-entropies. We evaluate models trained with\nthis objective on synthetic data settings (where P is known) and real data, and\nshow that the resulting models yield better generated text without complex\ndecoding strategies. Our code and models are publicly available at\nhttps://github.com/bloomberg/mixce-acl2023",
                "Meta-prediction Model for Distillation-Aware NAS on Unseen Datasets\nDistillation-aware Neural Architecture Search (DaNAS) aims to search for an\noptimal student architecture that obtains the best performance and/or\nefficiency when distilling the knowledge from a given teacher model. Previous\nDaNAS methods have mostly tackled the search for the neural architecture for\nfixed datasets and the teacher, which are not generalized well on a new task\nconsisting of an unseen dataset and an unseen teacher, thus need to perform a\ncostly search for any new combination of the datasets and the teachers. For\nstandard NAS tasks without KD, meta-learning-based computationally efficient\nNAS methods have been proposed, which learn the generalized search process over\nmultiple tasks (datasets) and transfer the knowledge obtained over those tasks\nto a new task. However, since they assume learning from scratch without KD from\na teacher, they might not be ideal for DaNAS scenarios. To eliminate the\nexcessive computational cost of DaNAS methods and the sub-optimality of rapid\nNAS methods, we propose a distillation-aware meta accuracy prediction model,\nDaSS (Distillation-aware Student Search), which can predict a given\narchitecture's final performances on a dataset when performing KD with a given\nteacher, without having actually to train it on the target task. The\nexperimental results demonstrate that our proposed meta-prediction model\nsuccessfully generalizes to multiple unseen datasets for DaNAS tasks, largely\noutperforming existing meta-NAS methods and rapid NAS baselines. Code is\navailable at https://github.com/CownowAn/DaSS",
                "Self-Supervised Reinforcement Learning that Transfers using Random\n  Features\nModel-free reinforcement learning algorithms have exhibited great potential\nin solving single-task sequential decision-making problems with\nhigh-dimensional observations and long horizons, but are known to be hard to\ngeneralize across tasks. Model-based RL, on the other hand, learns\ntask-agnostic models of the world that naturally enables transfer across\ndifferent reward functions, but struggles to scale to complex environments due\nto the compounding error. To get the best of both worlds, we propose a\nself-supervised reinforcement learning method that enables the transfer of\nbehaviors across tasks with different rewards, while circumventing the\nchallenges of model-based RL. In particular, we show self-supervised\npre-training of model-free reinforcement learning with a number of random\nfeatures as rewards allows implicit modeling of long-horizon environment\ndynamics. Then, planning techniques like model-predictive control using these\nimplicit models enable fast adaptation to problems with new reward functions.\nOur method is self-supervised in that it can be trained on offline datasets\nwithout reward labels, but can then be quickly deployed on new tasks. We\nvalidate that our proposed method enables transfer across tasks on a variety of\nmanipulation and locomotion domains in simulation, opening the door to\ngeneralist decision-making agents.",
                "Learning from Integral Losses in Physics Informed Neural Networks\nThis work proposes a solution for the problem of training physics-informed\nnetworks under partial integro-differential equations. These equations require\nan infinite or a large number of neural evaluations to construct a single\nresidual for training. As a result, accurate evaluation may be impractical, and\nwe show that naive approximations at replacing these integrals with unbiased\nestimates lead to biased loss functions and solutions. To overcome this bias,\nwe investigate three types of potential solutions: the deterministic sampling\napproaches, the double-sampling trick, and the delayed target method. We\nconsider three classes of PDEs for benchmarking; one defining Poisson problems\nwith singular charges and weak solutions of up to 10 dimensions, another\ninvolving weak solutions on electro-magnetic fields and a Maxwell equation, and\na third one defining a Smoluchowski coagulation problem. Our numerical results\nconfirm the existence of the aforementioned bias in practice and also show that\nour proposed delayed target approach can lead to accurate solutions with\ncomparable quality to ones estimated with a large sample size integral. Our\nimplementation is open-source and available at\nhttps://github.com/ehsansaleh/btspinn."
            ],
            "interesting paper": 1
        },
        {
            "papers": [
                "One Network, Many Masks: Towards More Parameter-Efficient Transfer\n  Learning\nFine-tuning pre-trained language models for multiple tasks tends to be\nexpensive in terms of storage. To mitigate this, parameter-efficient transfer\nlearning (PETL) methods have been proposed to address this issue, but they\nstill require a significant number of parameters and storage when being applied\nto broader ranges of tasks. To achieve even greater storage reduction, we\npropose PROPETL, a novel method that enables efficient sharing of a single PETL\nmodule which we call prototype network (e.g., adapter, LoRA, and prefix-tuning)\nacross layers and tasks. We then learn binary masks to select different\nsub-networks from the shared prototype network and apply them as PETL modules\ninto different layers. We find that the binary masks can determine crucial\ninformation from the network, which is often ignored in previous studies. Our\nwork can also be seen as a type of pruning method, where we find that\noverparameterization also exists in the seemingly small PETL modules. We\nevaluate PROPETL on various downstream tasks and show that it can outperform\nother PETL methods with approximately 10% of the parameter storage required by\nthe latter.",
                "Distill Gold from Massive Ores: Bi-level Data Pruning towards Efficient\n  Dataset Distillation\nData-efficient learning has garnered significant attention, especially given\nthe current trend of large multi-modal models. Recently, dataset distillation\nhas become an effective approach by synthesizing data samples that are\nessential for network training. However, it remains to be explored which\nsamples are essential for the dataset distillation process itself. In this\nwork, we study the data efficiency and selection for the dataset distillation\ntask. By re-formulating the dynamics of distillation, we provide insight into\nthe inherent redundancy in the real dataset, both theoretically and\nempirically. We propose to use the empirical loss value as a static data\npruning criterion. To further compensate for the variation of the data value in\ntraining, we find the most contributing samples based on their causal effects\non the distillation. The proposed selection strategy can efficiently exploit\nthe training dataset, outperform the previous SOTA distillation algorithms, and\nconsistently enhance the distillation algorithms, even on much larger-scale and\nmore heterogeneous datasets, e.g., full ImageNet-1K and Kinetics-400. We\nbelieve this paradigm will open up new avenues in the dynamics of distillation\nand pave the way for efficient dataset distillation. Our code is available on\nhttps://github.com/silicx/GoldFromOres-BiLP.",
                "A Comprehensive Overview and Comparative Analysis on Deep Learning\n  Models: CNN, RNN, LSTM, GRU\nDeep learning (DL) has emerged as a powerful subset of machine learning (ML)\nand artificial intelligence (AI), outperforming traditional ML methods,\nespecially in handling unstructured and large datasets. Its impact spans across\nvarious domains, including speech recognition, healthcare, autonomous vehicles,\ncybersecurity, predictive analytics, and more. However, the complexity and\ndynamic nature of real-world problems present challenges in designing effective\ndeep learning models. Consequently, several deep learning models have been\ndeveloped to address different problems and applications. In this article, we\nconduct a comprehensive survey of various deep learning models, including\nConvolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs),\nGenerative Models, Deep Reinforcement Learning (DRL), and Deep Transfer\nLearning. We examine the structure, applications, benefits, and limitations of\neach model. Furthermore, we perform an analysis using three publicly available\ndatasets: IMDB, ARAS, and Fruit-360. We compare the performance of six renowned\ndeep learning models: CNN, Simple RNN, Long Short-Term Memory (LSTM),\nBidirectional LSTM, Gated Recurrent Unit (GRU), and Bidirectional GRU.",
                "Emergent Modularity in Pre-trained Transformers\nThis work examines the presence of modularity in pre-trained Transformers, a\nfeature commonly found in human brains and thought to be vital for general\nintelligence. In analogy to human brains, we consider two main characteristics\nof modularity: (1) functional specialization of neurons: we evaluate whether\neach neuron is mainly specialized in a certain function, and find that the\nanswer is yes. (2) function-based neuron grouping: we explore finding a\nstructure that groups neurons into modules by function, and each module works\nfor its corresponding function. Given the enormous amount of possible\nstructures, we focus on Mixture-of-Experts as a promising candidate, which\npartitions neurons into experts and usually activates different experts for\ndifferent inputs. Experimental results show that there are functional experts,\nwhere clustered are the neurons specialized in a certain function. Moreover,\nperturbing the activations of functional experts significantly affects the\ncorresponding function. Finally, we study how modularity emerges during\npre-training, and find that the modular structure is stabilized at the early\nstage, which is faster than neuron stabilization. It suggests that Transformers\nfirst construct the modular structure and then learn fine-grained neuron\nfunctions. Our code and data are available at\nhttps://github.com/THUNLP/modularity-analysis.",
                "PuMer: Pruning and Merging Tokens for Efficient Vision Language Models\nLarge-scale vision language (VL) models use Transformers to perform\ncross-modal interactions between the input text and image. These cross-modal\ninteractions are computationally expensive and memory-intensive due to the\nquadratic complexity of processing the input image and text. We present PuMer:\na token reduction framework that uses text-informed Pruning and modality-aware\nMerging strategies to progressively reduce the tokens of input image and text,\nimproving model inference speed and reducing memory footprint. PuMer learns to\nkeep salient image tokens related to the input text and merges similar textual\nand visual tokens by adding lightweight token reducer modules at several\ncross-modal layers in the VL model. Training PuMer is mostly the same as\nfinetuning the original VL model but faster. Our evaluation for two vision\nlanguage models on four downstream VL tasks shows PuMer increases inference\nthroughput by up to 2x and reduces memory footprint by over 50% while incurring\nless than a 1% accuracy drop.",
                "Graph Inductive Biases in Transformers without Message Passing\nTransformers for graph data are increasingly widely studied and successful in\nnumerous learning tasks. Graph inductive biases are crucial for Graph\nTransformers, and previous works incorporate them using message-passing modules\nand/or positional encodings. However, Graph Transformers that use\nmessage-passing inherit known issues of message-passing, and differ\nsignificantly from Transformers used in other domains, thus making transfer of\nresearch advances more difficult. On the other hand, Graph Transformers without\nmessage-passing often perform poorly on smaller datasets, where inductive\nbiases are more crucial. To bridge this gap, we propose the Graph Inductive\nbias Transformer (GRIT) -- a new Graph Transformer that incorporates graph\ninductive biases without using message passing. GRIT is based on several\narchitectural changes that are each theoretically and empirically justified,\nincluding: learned relative positional encodings initialized with random walk\nprobabilities, a flexible attention mechanism that updates node and node-pair\nrepresentations, and injection of degree information in each layer. We prove\nthat GRIT is expressive -- it can express shortest path distances and various\ngraph propagation matrices. GRIT achieves state-of-the-art empirical\nperformance across a variety of graph datasets, thus showing the power that\nGraph Transformers without message-passing can deliver."
            ],
            "interesting paper": 1
        },
        {
            "papers": [
                "JutePestDetect: An Intelligent Approach for Jute Pest Identification\n  Using Fine-Tuned Transfer Learning\nIn certain Asian countries, Jute is one of the primary sources of income and\nGross Domestic Product (GDP) for the agricultural sector. Like many other\ncrops, Jute is prone to pest infestations, and its identification is typically\nmade visually in countries like Bangladesh, India, Myanmar, and China. In\naddition, this method is time-consuming, challenging, and somewhat imprecise,\nwhich poses a substantial financial risk. To address this issue, the study\nproposes a high-performing and resilient transfer learning (TL) based\nJutePestDetect model to identify jute pests at the early stage. Firstly, we\nprepared jute pest dataset containing 17 classes and around 380 photos per pest\nclass, which were evaluated after manual and automatic pre-processing and\ncleaning, such as background removal and resizing. Subsequently, five prominent\npre-trained models -DenseNet201, InceptionV3, MobileNetV2, VGG19, and ResNet50\nwere selected from a previous study to design the JutePestDetect model. Each\nmodel was revised by replacing the classification layer with a global average\npooling layer and incorporating a dropout layer for regularization. To evaluate\nthe models performance, various metrics such as precision, recall, F1 score,\nROC curve, and confusion matrix were employed. These analyses provided\nadditional insights for determining the efficacy of the models. Among them, the\ncustomized regularized DenseNet201-based proposed JutePestDetect model\noutperformed the others, achieving an impressive accuracy of 99%. As a result,\nour proposed method and strategy offer an enhanced approach to pest\nidentification in the case of Jute, which can significantly benefit farmers\nworldwide.",
                "Visual Affordance Prediction for Guiding Robot Exploration\nMotivated by the intuitive understanding humans have about the space of\npossible interactions, and the ease with which they can generalize this\nunderstanding to previously unseen scenes, we develop an approach for learning\nvisual affordances for guiding robot exploration. Given an input image of a\nscene, we infer a distribution over plausible future states that can be\nachieved via interactions with it. We use a Transformer-based model to learn a\nconditional distribution in the latent embedding space of a VQ-VAE and show\nthat these models can be trained using large-scale and diverse passive data,\nand that the learned models exhibit compositional generalization to diverse\nobjects beyond the training distribution. We show how the trained affordance\nmodel can be used for guiding exploration by acting as a goal-sampling\ndistribution, during visual goal-conditioned policy learning in robotic\nmanipulation.",
                "ADAPTERMIX: Exploring the Efficacy of Mixture of Adapters for\n  Low-Resource TTS Adaptation\nThere are significant challenges for speaker adaptation in text-to-speech for\nlanguages that are not widely spoken or for speakers with accents or dialects\nthat are not well-represented in the training data. To address this issue, we\npropose the use of the \"mixture of adapters\" method. This approach involves\nadding multiple adapters within a backbone-model layer to learn the unique\ncharacteristics of different speakers. Our approach outperforms the baseline,\nwith a noticeable improvement of 5% observed in speaker preference tests when\nusing only one minute of data for each new speaker. Moreover, following the\nadapter paradigm, we fine-tune only the adapter parameters (11% of the total\nmodel parameters). This is a significant achievement in parameter-efficient\nspeaker adaptation, and one of the first models of its kind. Overall, our\nproposed approach offers a promising solution to the speech synthesis\ntechniques, particularly for adapting to speakers from diverse backgrounds.",
                "A machine learning approach to the prediction of heat-transfer\n  coefficients in micro-channels\nThe accurate prediction of the two-phase heat transfer coefficient (HTC) as a\nfunction of working fluids, channel geometries and process conditions is key to\nthe optimal design and operation of compact heat exchangers. Advances in\nartificial intelligence research have recently boosted the application of\nmachine learning (ML) algorithms to obtain data-driven surrogate models for the\nHTC. For most supervised learning algorithms, the task is that of a nonlinear\nregression problem. Despite the fact that these models have been proven capable\nof outperforming traditional empirical correlations, they have key limitations\nsuch as overfitting the data, the lack of uncertainty estimation, and\ninterpretability of the results. To address these limitations, in this paper,\nwe use a multi-output Gaussian process regression (GPR) to estimate the HTC in\nmicrochannels as a function of the mass flow rate, heat flux, system pressure\nand channel diameter and length. The model is trained using the Brunel\nTwo-Phase Flow database of high-fidelity experimental data. The advantages of\nGPR are data efficiency, the small number of hyperparameters to be trained\n(typically of the same order of the number of input dimensions), and the\nautomatic trade-off between data fit and model complexity guaranteed by the\nmaximization of the marginal likelihood (Bayesian approach). Our paper proposes\nresearch directions to improve the performance of the GPR-based model in\nextrapolation.",
                "Intelligent gradient amplification for deep neural networks\nDeep learning models offer superior performance compared to other machine\nlearning techniques for a variety of tasks and domains, but pose their own\nchallenges. In particular, deep learning models require larger training times\nas the depth of a model increases, and suffer from vanishing gradients. Several\nsolutions address these problems independently, but there have been minimal\nefforts to identify an integrated solution that improves the performance of a\nmodel by addressing vanishing gradients, as well as accelerates the training\nprocess to achieve higher performance at larger learning rates. In this work,\nwe intelligently determine which layers of a deep learning model to apply\ngradient amplification to, using a formulated approach that analyzes gradient\nfluctuations of layers during training. Detailed experiments are performed for\nsimpler and deeper neural networks using two different intelligent measures and\ntwo different thresholds that determine the amplification layers, and a\ntraining strategy where gradients are amplified only during certain epochs.\nResults show that our amplification offers better performance compared to the\noriginal models, and achieves accuracy improvement of around 2.5% on CIFAR- 10\nand around 4.5% on CIFAR-100 datasets, even when the models are trained with\nhigher learning rates.",
                "Learning to Learn from APIs: Black-Box Data-Free Meta-Learning\nData-free meta-learning (DFML) aims to enable efficient learning of new tasks\nby meta-learning from a collection of pre-trained models without access to the\ntraining data. Existing DFML work can only meta-learn from (i) white-box and\n(ii) small-scale pre-trained models (iii) with the same architecture,\nneglecting the more practical setting where the users only have inference\naccess to the APIs with arbitrary model architectures and model scale inside.\nTo solve this issue, we propose a Bi-level Data-free Meta Knowledge\nDistillation (BiDf-MKD) framework to transfer more general meta knowledge from\na collection of black-box APIs to one single meta model. Specifically, by just\nquerying APIs, we inverse each API to recover its training data via a\nzero-order gradient estimator and then perform meta-learning via a novel\nbi-level meta knowledge distillation structure, in which we design a boundary\nquery set recovery technique to recover a more informative query set near the\ndecision boundary. In addition, to encourage better generalization within the\nsetting of limited API budgets, we propose task memory replay to diversify the\nunderlying task distribution by covering more interpolated tasks. Extensive\nexperiments in various real-world scenarios show the superior performance of\nour BiDf-MKD framework."
            ],
            "interesting paper": 1
        },
        {
            "papers": [
                "Task-Equivariant Graph Few-shot Learning\nAlthough Graph Neural Networks (GNNs) have been successful in node\nclassification tasks, their performance heavily relies on the availability of a\nsufficient number of labeled nodes per class. In real-world situations, not all\nclasses have many labeled nodes and there may be instances where the model\nneeds to classify new classes, making manual labeling difficult. To solve this\nproblem, it is important for GNNs to be able to classify nodes with a limited\nnumber of labeled nodes, known as few-shot node classification. Previous\nepisodic meta-learning based methods have demonstrated success in few-shot node\nclassification, but our findings suggest that optimal performance can only be\nachieved with a substantial amount of diverse training meta-tasks. To address\nthis challenge of meta-learning based few-shot learning (FSL), we propose a new\napproach, the Task-Equivariant Graph few-shot learning (TEG) framework. Our TEG\nframework enables the model to learn transferable task-adaptation strategies\nusing a limited number of training meta-tasks, allowing it to acquire\nmeta-knowledge for a wide range of meta-tasks. By incorporating equivariant\nneural networks, TEG can utilize their strong generalization abilities to learn\nhighly adaptable task-specific strategies. As a result, TEG achieves\nstate-of-the-art performance with limited training meta-tasks. Our experiments\non various benchmark datasets demonstrate TEG's superiority in terms of\naccuracy and generalization ability, even when using minimal meta-training\ndata, highlighting the effectiveness of our proposed approach in addressing the\nchallenges of meta-learning based few-shot node classification. Our code is\navailable at the following link: https://github.com/sung-won-kim/TEG",
                "Understanding Predictive Coding as an Adaptive Trust-Region Method\nPredictive coding (PC) is a brain-inspired local learning algorithm that has\nrecently been suggested to provide advantages over backpropagation (BP) in\nbiologically relevant scenarios. While theoretical work has mainly focused on\nshowing how PC can approximate BP in various limits, the putative benefits of\n\"natural\" PC are less understood. Here we develop a theory of PC as an adaptive\ntrust-region (TR) algorithm that uses second-order information. We show that\nthe learning dynamics of PC can be interpreted as interpolating between BP's\nloss gradient direction and a TR direction found by the PC inference dynamics.\nOur theory suggests that PC should escape saddle points faster than BP, a\nprediction which we prove in a shallow linear model and support with\nexperiments on deeper networks. This work lays a foundation for understanding\nPC in deep and wide networks.",
                "Vector-based Representation is the Key: A Study on Disentanglement and\n  Compositional Generalization\nRecognizing elementary underlying concepts from observations\n(disentanglement) and generating novel combinations of these concepts\n(compositional generalization) are fundamental abilities for humans to support\nrapid knowledge learning and generalize to new tasks, with which the deep\nlearning models struggle. Towards human-like intelligence, various works on\ndisentangled representation learning have been proposed, and recently some\nstudies on compositional generalization have been presented. However, few works\nstudy the relationship between disentanglement and compositional\ngeneralization, and the observed results are inconsistent. In this paper, we\nstudy several typical disentangled representation learning works in terms of\nboth disentanglement and compositional generalization abilities, and we provide\nan important insight: vector-based representation (using a vector instead of a\nscalar to represent a concept) is the key to empower both good disentanglement\nand strong compositional generalization. This insight also resonates the\nneuroscience research that the brain encodes information in neuron population\nactivity rather than individual neurons. Motivated by this observation, we\nfurther propose a method to reform the scalar-based disentanglement works\n($\\beta$-TCVAE and FactorVAE) to be vector-based to increase both capabilities.\nWe investigate the impact of the dimensions of vector-based representation and\none important question: whether better disentanglement indicates higher\ncompositional generalization. In summary, our study demonstrates that it is\npossible to achieve both good concept recognition and novel concept\ncomposition, contributing an important step towards human-like intelligence.",
                "Prediction Error-based Classification for Class-Incremental Learning\nClass-incremental learning (CIL) is a particularly challenging variant of\ncontinual learning, where the goal is to learn to discriminate between all\nclasses presented in an incremental fashion. Existing approaches often suffer\nfrom excessive forgetting and imbalance of the scores assigned to classes that\nhave not been seen together during training. In this study, we introduce a\nnovel approach, Prediction Error-based Classification (PEC), which differs from\ntraditional discriminative and generative classification paradigms. PEC\ncomputes a class score by measuring the prediction error of a model trained to\nreplicate the outputs of a frozen random neural network on data from that\nclass. The method can be interpreted as approximating a classification rule\nbased on Gaussian Process posterior variance. PEC offers several practical\nadvantages, including sample efficiency, ease of tuning, and effectiveness even\nwhen data are presented one class at a time. Our empirical results show that\nPEC performs strongly in single-pass-through-data CIL, outperforming other\nrehearsal-free baselines in all cases and rehearsal-based methods with moderate\nreplay buffer size in most cases across multiple benchmarks.",
                "What is Essential for Unseen Goal Generalization of Offline\n  Goal-conditioned RL?\nOffline goal-conditioned RL (GCRL) offers a way to train general-purpose\nagents from fully offline datasets. In addition to being conservative within\nthe dataset, the generalization ability to achieve unseen goals is another\nfundamental challenge for offline GCRL. However, to the best of our knowledge,\nthis problem has not been well studied yet. In this paper, we study\nout-of-distribution (OOD) generalization of offline GCRL both theoretically and\nempirically to identify factors that are important. In a number of experiments,\nwe observe that weighted imitation learning enjoys better generalization than\npessimism-based offline RL method. Based on this insight, we derive a theory\nfor OOD generalization, which characterizes several important design choices.\nWe then propose a new offline GCRL method, Generalizable Offline\ngoAl-condiTioned RL (GOAT), by combining the findings from our theoretical and\nempirical studies. On a new benchmark containing 9 independent identically\ndistributed (IID) tasks and 17 OOD tasks, GOAT outperforms current\nstate-of-the-art methods by a large margin.",
                "Generating Behaviorally Diverse Policies with Latent Diffusion Models\nRecent progress in Quality Diversity Reinforcement Learning (QD-RL) has\nenabled learning a collection of behaviorally diverse, high performing\npolicies. However, these methods typically involve storing thousands of\npolicies, which results in high space-complexity and poor scaling to additional\nbehaviors. Condensing the archive into a single model while retaining the\nperformance and coverage of the original collection of policies has proved\nchallenging. In this work, we propose using diffusion models to distill the\narchive into a single generative model over policy parameters. We show that our\nmethod achieves a compression ratio of 13x while recovering 98% of the original\nrewards and 89% of the original coverage. Further, the conditioning mechanism\nof diffusion models allows for flexibly selecting and sequencing behaviors,\nincluding using language. Project website:\nhttps://sites.google.com/view/policydiffusion/home"
            ],
            "interesting paper": 1
        },
        {
            "papers": [
                "Multi-source adversarial transfer learning based on similar source\n  domains with local features\nTransfer learning leverages knowledge from other domains and has been\nsuccessful in many applications. Transfer learning methods rely on the overall\nsimilarity of the source and target domains. However, in some cases, it is\nimpossible to provide an overall similar source domain, and only some source\ndomains with similar local features can be provided. Can transfer learning be\nachieved? In this regard, we propose a multi-source adversarial transfer\nlearning method based on local feature similarity to the source domain to\nhandle transfer scenarios where the source and target domains have only local\nsimilarities. This method extracts transferable local features between a single\nsource domain and the target domain through a sub-network. Specifically, the\nfeature extractor of the sub-network is induced by the domain discriminator to\nlearn transferable knowledge between the source domain and the target domain.\nThe extracted features are then weighted by an attention module to suppress\nnon-transferable local features while enhancing transferable local features. In\norder to ensure that the data from the target domain in different sub-networks\nin the same batch is exactly the same, we designed a multi-source domain\nindependent strategy to provide the possibility for later local feature fusion\nto complete the key features required. In order to verify the effectiveness of\nthe method, we made the dataset \"Local Carvana Image Masking Dataset\". Applying\nthe proposed method to the image segmentation task of the proposed dataset\nachieves better transfer performance than other multi-source transfer learning\nmethods. It is shown that the designed transfer learning method is feasible for\ntransfer scenarios where the source and target domains have only local\nsimilarities.",
                "Multi-source adversarial transfer learning for ultrasound image\n  segmentation with limited similarity\nLesion segmentation of ultrasound medical images based on deep learning\ntechniques is a widely used method for diagnosing diseases. Although there is a\nlarge amount of ultrasound image data in medical centers and other places,\nlabeled ultrasound datasets are a scarce resource, and it is likely that no\ndatasets are available for new tissues/organs. Transfer learning provides the\npossibility to solve this problem, but there are too many features in natural\nimages that are not related to the target domain. As a source domain, redundant\nfeatures that are not conducive to the task will be extracted. Migration\nbetween ultrasound images can avoid this problem, but there are few types of\npublic datasets, and it is difficult to find sufficiently similar source\ndomains. Compared with natural images, ultrasound images have less information,\nand there are fewer transferable features between different ultrasound images,\nwhich may cause negative transfer. To this end, a multi-source adversarial\ntransfer learning network for ultrasound image segmentation is proposed.\nSpecifically, to address the lack of annotations, the idea of adversarial\ntransfer learning is used to adaptively extract common features between a\ncertain pair of source and target domains, which provides the possibility to\nutilize unlabeled ultrasound data. To alleviate the lack of knowledge in a\nsingle source domain, multi-source transfer learning is adopted to fuse\nknowledge from multiple source domains. In order to ensure the effectiveness of\nthe fusion and maximize the use of precious data, a multi-source domain\nindependent strategy is also proposed to improve the estimation of the target\ndomain data distribution, which further increases the learning ability of the\nmulti-source adversarial migration learning network in multiple domains.",
                "Are Large Kernels Better Teachers than Transformers for ConvNets?\nThis paper reveals a new appeal of the recently emerged large-kernel\nConvolutional Neural Networks (ConvNets): as the teacher in Knowledge\nDistillation (KD) for small-kernel ConvNets. While Transformers have led\nstate-of-the-art (SOTA) performance in various fields with ever-larger models\nand labeled data, small-kernel ConvNets are considered more suitable for\nresource-limited applications due to the efficient convolution operation and\ncompact weight sharing. KD is widely used to boost the performance of\nsmall-kernel ConvNets. However, previous research shows that it is not quite\neffective to distill knowledge (e.g., global information) from Transformers to\nsmall-kernel ConvNets, presumably due to their disparate architectures. We\nhereby carry out a first-of-its-kind study unveiling that modern large-kernel\nConvNets, a compelling competitor to Vision Transformers, are remarkably more\neffective teachers for small-kernel ConvNets, due to more similar\narchitectures. Our findings are backed up by extensive experiments on both\nlogit-level and feature-level KD ``out of the box\", with no dedicated\narchitectural nor training recipe modifications. Notably, we obtain the\n\\textbf{best-ever pure ConvNet} under 30M parameters with \\textbf{83.1\\%} top-1\naccuracy on ImageNet, outperforming current SOTA methods including ConvNeXt V2\nand Swin V2. We also find that beneficial characteristics of large-kernel\nConvNets, e.g., larger effective receptive fields, can be seamlessly\ntransferred to students through this large-to-small kernel distillation. Code\nis available at: \\url{https://github.com/VITA-Group/SLaK}.",
                "Fine-grained Text Style Transfer with Diffusion-Based Language Models\nDiffusion probabilistic models have shown great success in generating\nhigh-quality images controllably, and researchers have tried to utilize this\ncontrollability into text generation domain. Previous works on diffusion-based\nlanguage models have shown that they can be trained without external knowledge\n(such as pre-trained weights) and still achieve stable performance and\ncontrollability. In this paper, we trained a diffusion-based model on StylePTB\ndataset, the standard benchmark for fine-grained text style transfers. The\ntasks in StylePTB requires much more refined control over the output text\ncompared to tasks evaluated in previous works, and our model was able to\nachieve state-of-the-art performance on StylePTB on both individual and\ncompositional transfers. Moreover, our model, trained on limited data from\nStylePTB without external knowledge, outperforms previous works that utilized\npretrained weights, embeddings, and external grammar parsers, and this may\nindicate that diffusion-based language models have great potential under\nlow-resource settings.",
                "MSMix:An Interpolation-Based Text Data Augmentation Method Manifold Swap\n  Mixup\nTo solve the problem of poor performance of deep neural network models due to\ninsufficient data, a simple yet effective interpolation-based data augmentation\nmethod is proposed: MSMix (Manifold Swap Mixup). This method feeds two\ndifferent samples to the same deep neural network model, and then randomly\nselect a specific layer and partially replace hidden features at that layer of\none of the samples by the counterpart of the other. The mixed hidden features\nare fed to the model and go through the rest of the network. Two different\nselection strategies are also proposed to obtain richer hidden representation.\nExperiments are conducted on three Chinese intention recognition datasets, and\nthe results show that the MSMix method achieves better results than other\nmethods in both full-sample and small-sample configurations.",
                "Compositional diversity in visual concept learning\nHumans leverage compositionality to efficiently learn new concepts,\nunderstanding how familiar parts can combine together to form novel objects. In\ncontrast, popular computer vision models struggle to make the same types of\ninferences, requiring more data and generalizing less flexibly than people do.\nHere, we study these distinctively human abilities across a range of different\ntypes of visual composition, examining how people classify and generate ``alien\nfigures'' with rich relational structure. We also develop a Bayesian program\ninduction model which searches for the best programs for generating the\ncandidate visual figures, utilizing a large program space containing different\ncompositional mechanisms and abstractions. In few shot classification tasks, we\nfind that people and the program induction model can make a range of meaningful\ncompositional generalizations, with the model providing a strong account of the\nexperimental data as well as interpretable parameters that reveal human\nassumptions about the factors invariant to category membership (here, to\nrotation and changing part attachment). In few shot generation tasks, both\npeople and the models are able to construct compelling novel examples, with\npeople behaving in additional structured ways beyond the model capabilities,\ne.g. making choices that complete a set or reconfiguring existing parts in\nhighly novel ways. To capture these additional behavioral patterns, we develop\nan alternative model based on neuro-symbolic program induction: this model also\ncomposes new concepts from existing parts yet, distinctively, it utilizes\nneural network modules to successfully capture residual statistical structure.\nTogether, our behavioral and computational findings show how people and models\ncan produce a rich variety of compositional behavior when classifying and\ngenerating visual objects."
            ],
            "interesting paper": 1
        },
        {
            "papers": [
                "Transformers learn to implement preconditioned gradient descent for\n  in-context learning\nSeveral recent works demonstrate that transformers can implement algorithms\nlike gradient descent. By a careful construction of weights, these works show\nthat multiple layers of transformers are expressive enough to simulate\niterations of gradient descent. Going beyond the question of expressivity, we\nask: Can transformers learn to implement such algorithms by training over\nrandom problem instances? To our knowledge, we make the first theoretical\nprogress on this question via an analysis of the loss landscape for linear\ntransformers trained over random instances of linear regression. For a single\nattention layer, we prove the global minimum of the training objective\nimplements a single iteration of preconditioned gradient descent. Notably, the\npreconditioning matrix not only adapts to the input distribution but also to\nthe variance induced by data inadequacy. For a transformer with $L$ attention\nlayers, we prove certain critical points of the training objective implement\n$L$ iterations of preconditioned gradient descent. Our results call for future\ntheoretical studies on learning algorithms by training transformers.",
                "Addressing Negative Transfer in Diffusion Models\nDiffusion-based generative models have achieved remarkable success in various\ndomains. It trains a shared model on denoising tasks that encompass different\nnoise levels simultaneously, representing a form of multi-task learning (MTL).\nHowever, analyzing and improving diffusion models from an MTL perspective\nremains under-explored. In particular, MTL can sometimes lead to the well-known\nphenomenon of negative transfer, which results in the performance degradation\nof certain tasks due to conflicts between tasks. In this paper, we first aim to\nanalyze diffusion training from an MTL standpoint, presenting two key\nobservations: (O1) the task affinity between denoising tasks diminishes as the\ngap between noise levels widens, and (O2) negative transfer can arise even in\ndiffusion training. Building upon these observations, we aim to enhance\ndiffusion training by mitigating negative transfer. To achieve this, we propose\nleveraging existing MTL methods, but the presence of a huge number of denoising\ntasks makes this computationally expensive to calculate the necessary per-task\nloss or gradient. To address this challenge, we propose clustering the\ndenoising tasks into small task clusters and applying MTL methods to them.\nSpecifically, based on (O2), we employ interval clustering to enforce temporal\nproximity among denoising tasks within clusters. We show that interval\nclustering can be solved using dynamic programming, utilizing signal-to-noise\nratio, timestep, and task affinity for clustering objectives. Through this, our\napproach addresses the issue of negative transfer in diffusion models by\nallowing for efficient computation of MTL methods. We validate the efficacy of\nproposed clustering and its integration with MTL methods through various\nexperiments, demonstrating 1) improved generation quality and 2) faster\ntraining convergence of diffusion models.",
                "Diffused Redundancy in Pre-trained Representations\nRepresentations learned by pre-training a neural network on a large dataset\nare increasingly used successfully to perform a variety of downstream tasks. In\nthis work, we take a closer look at how features are encoded in such\npre-trained representations. We find that learned representations in a given\nlayer exhibit a degree of diffuse redundancy, ie, any randomly chosen subset of\nneurons in the layer that is larger than a threshold size shares a large degree\nof similarity with the full layer and is able to perform similarly as the whole\nlayer on a variety of downstream tasks. For example, a linear probe trained on\n$20\\%$ of randomly picked neurons from the penultimate layer of a ResNet50\npre-trained on ImageNet1k achieves an accuracy within $5\\%$ of a linear probe\ntrained on the full layer of neurons for downstream CIFAR10 classification. We\nconduct experiments on different neural architectures (including CNNs and\nTransformers) pre-trained on both ImageNet1k and ImageNet21k and evaluate a\nvariety of downstream tasks taken from the VTAB benchmark. We find that the\nloss and dataset used during pre-training largely govern the degree of diffuse\nredundancy and the \"critical mass\" of neurons needed often depends on the\ndownstream task, suggesting that there is a task-inherent\nredundancy-performance Pareto frontier. Our findings shed light on the nature\nof representations learned by pre-trained deep neural networks and suggest that\nentire layers might not be necessary to perform many downstream tasks. We\ninvestigate the potential for exploiting this redundancy to achieve efficient\ngeneralization for downstream tasks and also draw caution to certain possible\nunintended consequences. Our code is available at\n\\url{https://github.com/nvedant07/diffused-redundancy}.",
                "Efficient Diffusion Policies for Offline Reinforcement Learning\nOffline reinforcement learning (RL) aims to learn optimal policies from\noffline datasets, where the parameterization of policies is crucial but often\noverlooked. Recently, Diffsuion-QL significantly boosts the performance of\noffline RL by representing a policy with a diffusion model, whose success\nrelies on a parametrized Markov Chain with hundreds of steps for sampling.\nHowever, Diffusion-QL suffers from two critical limitations. 1) It is\ncomputationally inefficient to forward and backward through the whole Markov\nchain during training. 2) It is incompatible with maximum likelihood-based RL\nalgorithms (e.g., policy gradient methods) as the likelihood of diffusion\nmodels is intractable. Therefore, we propose efficient diffusion policy (EDP)\nto overcome these two challenges. EDP approximately constructs actions from\ncorrupted ones at training to avoid running the sampling chain. We conduct\nextensive experiments on the D4RL benchmark. The results show that EDP can\nreduce the diffusion policy training time from 5 days to 5 hours on\ngym-locomotion tasks. Moreover, we show that EDP is compatible with various\noffline RL algorithms (TD3, CRR, and IQL) and achieves new state-of-the-art on\nD4RL by large margins over previous methods. Our code is available at\nhttps://github.com/sail-sg/edp.",
                "The Effects of Input Type and Pronunciation Dictionary Usage in Transfer\n  Learning for Low-Resource Text-to-Speech\nWe compare phone labels and articulatory features as input for cross-lingual\ntransfer learning in text-to-speech (TTS) for low-resource languages (LRLs).\nExperiments with FastSpeech 2 and the LRL West Frisian show that using\narticulatory features outperformed using phone labels in both intelligibility\nand naturalness. For LRLs without pronunciation dictionaries, we propose two\nnovel approaches: a) using a massively multilingual model to convert\ngrapheme-to-phone (G2P) in both training and synthesizing, and b) using a\nuniversal phone recognizer to create a makeshift dictionary. Results show that\nthe G2P approach performs largely on par with using a ground-truth dictionary\nand the phone recognition approach, while performing generally worse, remains a\nviable option for LRLs less suitable for the G2P approach. Within each\napproach, using articulatory features as input outperforms using phone labels.",
                "Out-of-distribution forgetting: vulnerability of continual learning to\n  intra-class distribution shift\nContinual learning (CL) is an important technique to allow artificial neural\nnetworks to work in open environments. CL enables a system to learn new tasks\nwithout severe interference to its performance on old tasks, i.e., overcome the\nproblems of catastrophic forgetting. In joint learning, it is well known that\nthe out-of-distribution (OOD) problem caused by intentional attacks or\nenvironmental perturbations will severely impair the ability of networks to\ngeneralize. In this work, we reported a special form of catastrophic forgetting\nraised by the OOD problem in continual learning settings, and we named it\nout-of-distribution forgetting (OODF). In continual image classification tasks,\nwe found that for a given category, introducing an intra-class distribution\nshift significantly impaired the recognition accuracy of CL methods for that\ncategory during subsequent learning. Interestingly, this phenomenon is special\nfor CL as the same level of distribution shift had only negligible effects in\nthe joint learning scenario. We verified that CL methods without dedicating\nsubnetworks for individual tasks are all vulnerable to OODF. Moreover, OODF\ndoes not depend on any specific way of shifting the distribution, suggesting it\nis a risk for CL in a wide range of circumstances. Taken together, our work\nidentified an under-attended risk during CL, highlighting the importance of\ndeveloping approaches that can overcome OODF. Code available:\n\\url{https://github.com/Hiroid/OODF}"
            ],
            "interesting paper": 2
        },
        {
            "papers": [
                "Domain Generalization for Domain-Linked Classes\nDomain generalization (DG) focuses on transferring domain-invariant knowledge\nfrom multiple source domains (available at train time) to an, a priori, unseen\ntarget domain(s). This requires a class to be expressed in multiple domains for\nthe learning algorithm to break the spurious correlations between domain and\nclass. However, in the real-world, classes may often be domain-linked, i.e.\nexpressed only in a specific domain, which leads to extremely poor\ngeneralization performance for these classes. In this work, we aim to learn\ngeneralizable representations for these domain-linked classes by transferring\ndomain-invariant knowledge from classes expressed in multiple source domains\n(domain-shared classes). To this end, we introduce this task to the community\nand propose a Fair and cONtrastive feature-space regularization algorithm for\nDomain-linked DG, FOND. Rigorous and reproducible experiments with baselines\nacross popular DG tasks demonstrate our method and its variants' ability to\naccomplish state-of-the-art DG results for domain-linked classes. We also\nprovide practical insights on data conditions that increase domain-linked class\ngeneralizability to tackle real-world data scarcity.",
                "Learning Disentangled Prompts for Compositional Image Synthesis\nWe study domain-adaptive image synthesis, the problem of teaching pretrained\nimage generative models a new style or concept from as few as one image to\nsynthesize novel images, to better understand the compositional image\nsynthesis. We present a framework that leverages a pretrained class-conditional\ngeneration model and visual prompt tuning. Specifically, we propose a novel\nsource class distilled visual prompt that learns disentangled prompts of\nsemantic (e.g., class) and domain (e.g., style) from a few images. Learned\ndomain prompt is then used to synthesize images of any classes in the style of\ntarget domain. We conduct studies on various target domains with the number of\nimages ranging from one to a few to many, and show qualitative results which\nshow the compositional generalization of our method. Moreover, we show that our\nmethod can help improve zero-shot domain adaptation classification accuracy.",
                "Improved Cross-Lingual Transfer Learning For Automatic Speech\n  Translation\nResearch in multilingual speech-to-text translation is topical. Having a\nsingle model that supports multiple translation tasks is desirable. The goal of\nthis work it to improve cross-lingual transfer learning in multilingual\nspeech-to-text translation via semantic knowledge distillation. We show that by\ninitializing the encoder of the encoder-decoder sequence-to-sequence\ntranslation model with SAMU-XLS-R, a multilingual speech transformer encoder\ntrained using multi-modal (speech-text) semantic knowledge distillation, we\nachieve significantly better cross-lingual task knowledge transfer than the\nbaseline XLS-R, a multilingual speech transformer encoder trained via\nself-supervised learning. We demonstrate the effectiveness of our approach on\ntwo popular datasets, namely, CoVoST-2 and Europarl. On the 21 translation\ntasks of the CoVoST-2 benchmark, we achieve an average improvement of 12.8 BLEU\npoints over the baselines. In the zero-shot translation scenario, we achieve an\naverage gain of 18.8 and 11.9 average BLEU points on unseen medium and\nlow-resource languages. We make similar observations on Europarl speech\ntranslation benchmark.",
                "Rotating Features for Object Discovery\nThe binding problem in human cognition, concerning how the brain represents\nand connects objects within a fixed network of neural connections, remains a\nsubject of intense debate. Most machine learning efforts addressing this issue\nin an unsupervised setting have focused on slot-based methods, which may be\nlimiting due to their discrete nature and difficulty to express uncertainty.\nRecently, the Complex AutoEncoder was proposed as an alternative that learns\ncontinuous and distributed object-centric representations. However, it is only\napplicable to simple toy data. In this paper, we present Rotating Features, a\ngeneralization of complex-valued features to higher dimensions, and a new\nevaluation procedure for extracting objects from distributed representations.\nAdditionally, we show the applicability of our approach to pre-trained\nfeatures. Together, these advancements enable us to scale distributed\nobject-centric representations from simple toy to real-world data. We believe\nthis work advances a new paradigm for addressing the binding problem in machine\nlearning and has the potential to inspire further innovation in the field.",
                "Scaling Up Semi-supervised Learning with Unconstrained Unlabelled Data\nWe propose UnMixMatch, a semi-supervised learning framework which can learn\neffective representations from unconstrained unlabelled data in order to scale\nup performance. Most existing semi-supervised methods rely on the assumption\nthat labelled and unlabelled samples are drawn from the same distribution,\nwhich limits the potential for improvement through the use of free-living\nunlabeled data. Consequently, the generalizability and scalability of\nsemi-supervised learning are often hindered by this assumption. Our method aims\nto overcome these constraints and effectively utilize unconstrained unlabelled\ndata in semi-supervised learning. UnMixMatch consists of three main components:\na supervised learner with hard augmentations that provides strong\nregularization, a contrastive consistency regularizer to learn underlying\nrepresentations from the unlabelled data, and a self-supervised loss to enhance\nthe representations that are learnt from the unlabelled data. We perform\nextensive experiments on 4 commonly used datasets and demonstrate superior\nperformance over existing semi-supervised methods with a performance boost of\n4.79%. Extensive ablation and sensitivity studies show the effectiveness and\nimpact of each of the proposed components of our method.",
                "Normalization Enhances Generalization in Visual Reinforcement Learning\nRecent advances in visual reinforcement learning (RL) have led to impressive\nsuccess in handling complex tasks. However, these methods have demonstrated\nlimited generalization capability to visual disturbances, which poses a\nsignificant challenge for their real-world application and adaptability. Though\nnormalization techniques have demonstrated huge success in supervised and\nunsupervised learning, their applications in visual RL are still scarce. In\nthis paper, we explore the potential benefits of integrating normalization into\nvisual RL methods with respect to generalization performance. We find that,\nperhaps surprisingly, incorporating suitable normalization techniques is\nsufficient to enhance the generalization capabilities, without any additional\nspecial design. We utilize the combination of two normalization techniques,\nCrossNorm and SelfNorm, for generalizable visual RL. Extensive experiments are\nconducted on DMControl Generalization Benchmark and CARLA to validate the\neffectiveness of our method. We show that our method significantly improves\ngeneralization capability while only marginally affecting sample efficiency. In\nparticular, when integrated with DrQ-v2, our method enhances the test\nperformance of DrQ-v2 on CARLA across various scenarios, from 14% of the\ntraining performance to 97%."
            ],
            "interesting paper": 1
        }
    ],
    "Quinn Garcia": [
        {
            "papers": [
                "Deep Learning and Ethics\nThis article appears as chapter 21 of Prince (2023, Understanding Deep\nLearning); a complete draft of the textbook is available here:\nhttp://udlbook.com. This chapter considers potential harms arising from the\ndesign and use of AI systems. These include algorithmic bias, lack of\nexplainability, data privacy violations, militarization, fraud, and\nenvironmental concerns. The aim is not to provide advice on being more ethical.\nInstead, the goal is to express ideas and start conversations in key areas that\nhave received attention in philosophy, political science, and the broader\nsocial sciences.",
                "Science in the Era of ChatGPT, Large Language Models and Generative AI:\n  Challenges for Research Ethics and How to Respond\nLarge language models of artificial intelligence (AI), such as ChatGPT, find\nremarkable but controversial applicability in science and research. This paper\nreviews epistemological challenges, ethical and integrity risks in science\nconduct in the advent of generative AI. This is with the aim to lay new timely\nfoundations for a high-quality research ethics review. The role of AI language\nmodels as a research instrument and subject is scrutinized along with ethical\nimplications for scientists, participants and reviewers. New emerging practices\nfor research ethics review are discussed, concluding with ten recommendations\nthat shape a response for a more responsible research conduct in the era of AI.",
                "Model evaluation for extreme risks\nCurrent approaches to building general-purpose AI systems tend to produce\nsystems with both beneficial and harmful capabilities. Further progress in AI\ndevelopment could lead to capabilities that pose extreme risks, such as\noffensive cyber capabilities or strong manipulation skills. We explain why\nmodel evaluation is critical for addressing extreme risks. Developers must be\nable to identify dangerous capabilities (through \"dangerous capability\nevaluations\") and the propensity of models to apply their capabilities for harm\n(through \"alignment evaluations\"). These evaluations will become critical for\nkeeping policymakers and other stakeholders informed, and for making\nresponsible decisions about model training, deployment, and security.",
                "Towards a Capability Assessment Model for the Comprehension and Adoption\n  of AI in Organisations\nThe comprehension and adoption of Artificial Intelligence (AI) are beset with\npractical and ethical problems. This article presents a 5-level AI Capability\nAssessment Model (AI-CAM) and a related AI Capabilities Matrix (AI-CM) to\nassist practitioners in AI comprehension and adoption. These practical tools\nwere developed with business executives, technologists, and other\norganisational stakeholders in mind. They are founded on a comprehensive\nconception of AI compared to those in other AI adoption models and are also\nopen-source artefacts. Thus, the AI-CAM and AI-CM present an accessible\nresource to help inform organisational decision-makers on the capability\nrequirements for (1) AI-based data analytics use cases based on machine\nlearning technologies; (2) Knowledge representation to engineer and represent\ndata, information and knowledge using semantic technologies; and (3) AI-based\nsolutions that seek to emulate human reasoning and decision-making. The AI-CAM\ncovers the core capability dimensions (business, data, technology,\norganisation, AI skills, risks, and ethical considerations) required at the\nfive capability maturity levels to achieve optimal use of AI in organisations.",
                "An Experimental Investigation into the Evaluation of Explainability\n  Methods\nEXplainable Artificial Intelligence (XAI) aims to help users to grasp the\nreasoning behind the predictions of an Artificial Intelligence (AI) system.\nMany XAI approaches have emerged in recent years. Consequently, a subfield\nrelated to the evaluation of XAI methods has gained considerable attention,\nwith the aim to determine which methods provide the best explanation using\nvarious approaches and criteria. However, the literature lacks a comparison of\nthe evaluation metrics themselves, that one can use to evaluate XAI methods.\nThis work aims to fill this gap by comparing 14 different metrics when applied\nto nine state-of-the-art XAI methods and three dummy methods (e.g., random\nsaliency maps) used as references. Experimental results show which of these\nmetrics produces highly correlated results, indicating potential redundancy. We\nalso demonstrate the significant impact of varying the baseline hyperparameter\non the evaluation metric values. Finally, we use dummy methods to assess the\nreliability of metrics in terms of ranking, pointing out their limitations.",
                "Asking Before Acting: Gather Information in Embodied Decision Making\n  with Language Models\nWith strong capabilities of reasoning and a broad understanding of the world,\nLarge Language Models (LLMs) have demonstrated immense potential in building\nversatile embodied decision-making agents capable of executing a wide array of\ntasks. Nevertheless, when deployed in unfamiliar environments, we show that LLM\nagents encounter challenges in efficiently gathering essential information,\nleading to suboptimal performance. Conversely, human individuals often seek\nadditional information from their peers prior to taking action, harnessing\nexternal knowledge to avoid unnecessary trial and error. Drawing inspiration\nfrom this behavior, we propose \\textit{Asking Before Acting} (ABA), a method\nthat empowers the agent to proactively inquire with external sources for\npertinent information using natural language during their interactions within\nthe environment. In this way, the agent is able to enhance its efficiency and\nperformance by circumventing potentially laborious steps and combating the\ndifficulties associated with exploration in unfamiliar environments and\nvagueness of the instructions. We conduct extensive experiments involving a\nspectrum of environments including text-based household everyday tasks, robot\narm manipulation tasks, and real world open domain image based embodied tasks.\nThe experiments involve various models from Vicuna to GPT-4. The results\ndemonstrate that, even with modest prompts modifications, ABA exhibits\nsubstantial advantages on both performance and efficiency over baseline LLM\nagents. Further finetuning ABA with reformulated metadata (ABA-FT) faciliates\nlearning the rationale for asking and allows for additional enhancements\nespecially in tasks that baselines struggle to solve."
            ],
            "interesting paper": 2
        },
        {
            "papers": [
                "Enhancing Human Capabilities through Symbiotic Artificial Intelligence\n  with Shared Sensory Experiences\nThe merging of human intelligence and artificial intelligence has long been a\nsubject of interest in both science fiction and academia. In this paper, we\nintroduce a novel concept in Human-AI interaction called Symbiotic Artificial\nIntelligence with Shared Sensory Experiences (SAISSE), which aims to establish\na mutually beneficial relationship between AI systems and human users through\nshared sensory experiences. By integrating multiple sensory input channels and\nprocessing human experiences, SAISSE fosters a strong human-AI bond, enabling\nAI systems to learn from and adapt to individual users, providing personalized\nsupport, assistance, and enhancement. Furthermore, we discuss the incorporation\nof memory storage units for long-term growth and development of both the AI\nsystem and its human user. As we address user privacy and ethical guidelines\nfor responsible AI-human symbiosis, we also explore potential biases and\ninequalities in AI-human symbiosis and propose strategies to mitigate these\nchallenges. Our research aims to provide a comprehensive understanding of the\nSAISSE concept and its potential to effectively support and enhance individual\nhuman users through symbiotic AI systems. This position article aims at\ndiscussing poteintial AI-human interaction related topics within the scientific\ncommunity, rather than providing experimental or theoretical results.",
                "Alert of the Second Decision-maker: An Introduction to Human-AI Conflict\nThe collaboration between humans and artificial intelligence (AI) is a\nsignificant feature in this digital age. However, humans and AI may have\nobservation, interpretation, and action conflicts when working synchronously.\nThis phenomenon is often masked by faults and, unfortunately, overlooked. This\npaper systematically introduces the human-AI conflict concept, causes,\nmeasurement methods, and risk assessment. The results highlight that there is a\npotential second decision-maker besides the human, which is the AI; the\nhuman-AI conflict is a unique and emerging risk in digitalized process systems;\nand this is an interdisciplinary field that needs to be distinguished from\ntraditional fault and failure analysis; the conflict risk is significant and\ncannot be ignored.",
                "Applying Interdisciplinary Frameworks to Understand Algorithmic\n  Decision-Making\nWe argue that explanations for \"algorithmic decision-making\" (ADM) systems\ncan profit by adopting practices that are already used in the learning\nsciences. We shortly introduce the importance of explaining ADM systems, give a\nbrief overview of approaches drawing from other disciplines to improve\nexplanations, and present the results of our qualitative task-based study\nincorporating the \"six facets of understanding\" framework. We close with\nquestions guiding the discussion of how future studies can leverage an\ninterdisciplinary approach.",
                "Attention Paper: How Generative AI Reshapes Digital Shadow Industry?\nThe rapid development of digital economy has led to the emergence of various\nblack and shadow internet industries, which pose potential risks that can be\nidentified and managed through digital risk management (DRM) that uses\ndifferent techniques such as machine learning and deep learning. The evolution\nof DRM architecture has been driven by changes in data forms. However, the\ndevelopment of AI-generated content (AIGC) technology, such as ChatGPT and\nStable Diffusion, has given black and shadow industries powerful tools to\npersonalize data and generate realistic images and conversations for fraudulent\nactivities. This poses a challenge for DRM systems to control risks from the\nsource of data generation and to respond quickly to the fast-changing risk\nenvironment. This paper aims to provide a technical analysis of the challenges\nand opportunities of AIGC from upstream, midstream, and downstream paths of\nblack/shadow industries and suggest future directions for improving existing\nrisk control systems. The paper will explore the new black and shadow\ntechniques triggered by generative AI technology and provide insights for\nbuilding the next-generation DRM system.",
                "Quantifying the Intrinsic Usefulness of Attributional Explanations for\n  Graph Neural Networks with Artificial Simulatability Studies\nDespite the increasing relevance of explainable AI, assessing the quality of\nexplanations remains a challenging issue. Due to the high costs associated with\nhuman-subject experiments, various proxy metrics are often used to\napproximately quantify explanation quality. Generally, one possible\ninterpretation of the quality of an explanation is its inherent value for\nteaching a related concept to a student. In this work, we extend artificial\nsimulatability studies to the domain of graph neural networks. Instead of\ncostly human trials, we use explanation-supervisable graph neural networks to\nperform simulatability studies to quantify the inherent usefulness of\nattributional graph explanations. We perform an extensive ablation study to\ninvestigate the conditions under which the proposed analyses are most\nmeaningful. We additionally validate our methods applicability on real-world\ngraph classification and regression datasets. We find that relevant\nexplanations can significantly boost the sample efficiency of graph neural\nnetworks and analyze the robustness towards noise and bias in the explanations.\nWe believe that the notion of usefulness obtained from our proposed\nsimulatability analysis provides a dimension of explanation quality that is\nlargely orthogonal to the common practice of faithfulness and has great\npotential to expand the toolbox of explanation quality assessments,\nspecifically for graph explanations.",
                "A Survey on ChatGPT: AI-Generated Contents, Challenges, and Solutions\nWith the widespread use of large artificial intelligence (AI) models such as\nChatGPT, AI-generated content (AIGC) has garnered increasing attention and is\nleading a paradigm shift in content creation and knowledge representation. AIGC\nuses generative large AI algorithms to assist or replace humans in creating\nmassive, high-quality, and human-like content at a faster pace and lower cost,\nbased on user-provided prompts. Despite the recent significant progress in\nAIGC, security, privacy, ethical, and legal challenges still need to be\naddressed. This paper presents an in-depth survey of working principles,\nsecurity and privacy threats, state-of-the-art solutions, and future challenges\nof the AIGC paradigm. Specifically, we first explore the enabling technologies,\ngeneral architecture of AIGC, and discuss its working modes and key\ncharacteristics. Then, we investigate the taxonomy of security and privacy\nthreats to AIGC and highlight the ethical and societal implications of GPT and\nAIGC technologies. Furthermore, we review the state-of-the-art AIGC\nwatermarking approaches for regulatable AIGC paradigms regarding the AIGC model\nand its produced content. Finally, we identify future challenges and open\nresearch directions related to AIGC."
            ],
            "interesting paper": 2
        },
        {
            "papers": [
                "Frontier AI developers need an internal audit function\nThis article argues that frontier artificial intelligence (AI) developers\nneed an internal audit function. First, it describes the role of internal audit\nin corporate governance: internal audit evaluates the adequacy and\neffectiveness of a company's risk management, control, and governance\nprocesses. It is organizationally independent from senior management and\nreports directly to the board of directors, typically its audit committee. In\nthe IIA's Three Lines Model, internal audit serves as the third line and is\nresponsible for providing assurance to the board, while the Combined Assurance\nFramework highlights the need to coordinate the activities of internal and\nexternal assurance providers. Next, the article provides an overview of key\ngovernance challenges in frontier AI development: dangerous capabilities can\narise unpredictably and undetected; it is difficult to prevent a deployed model\nfrom causing harm; frontier models can proliferate rapidly; it is inherently\ndifficult to assess frontier AI risks; and frontier AI developers do not seem\nto follow best practices in risk governance. Finally, the article discusses how\nan internal audit function could address some of these challenges: internal\naudit could identify ineffective risk management practices; it could ensure\nthat the board of directors has a more accurate understanding of the current\nlevel of risk and the adequacy of the developer's risk management practices;\nand it could serve as a contact point for whistleblowers. In light of rapid\nprogress in AI research and development, frontier AI developers need to\nstrengthen their risk governance. Instead of reinventing the wheel, they should\nfollow existing best practices. While this might not be sufficient, they should\nnot skip this obvious first step.",
                "Moral Machine or Tyranny of the Majority?\nWith Artificial Intelligence systems increasingly applied in consequential\ndomains, researchers have begun to ask how these systems ought to act in\nethically charged situations where even humans lack consensus. In the Moral\nMachine project, researchers crowdsourced answers to \"Trolley Problems\"\nconcerning autonomous vehicles. Subsequently, Noothigattu et al. (2018)\nproposed inferring linear functions that approximate each individual's\npreferences and aggregating these linear models by averaging parameters across\nthe population. In this paper, we examine this averaging mechanism, focusing on\nfairness concerns in the presence of strategic effects. We investigate a simple\nsetting where the population consists of two groups, with the minority\nconstituting an {\\alpha} < 0.5 share of the population. To simplify the\nanalysis, we consider the extreme case in which within-group preferences are\nhomogeneous. Focusing on the fraction of contested cases where the minority\ngroup prevails, we make the following observations: (a) even when all parties\nreport their preferences truthfully, the fraction of disputes where the\nminority prevails is less than proportionate in {\\alpha}; (b) the degree of\nsub-proportionality grows more severe as the level of disagreement between the\ngroups increases; (c) when parties report preferences strategically, pure\nstrategy equilibria do not always exist; and (d) whenever a pure strategy\nequilibrium exists, the majority group prevails 100% of the time. These\nfindings raise concerns about stability and fairness of preference vector\naveraging as a mechanism for aggregating diverging voices. Finally, we discuss\nalternatives, including randomized dictatorship and median-based mechanisms.",
                "How Do UX Practitioners Communicate AI as a Design Material? Artifacts,\n  Conceptions, and Propositions\nUX practitioners (UXPs) face novel challenges when working with and\ncommunicating artificial intelligence (AI) as a design material. We explore how\nUXPs communicate AI concepts when given hands-on experience training and\nexperimenting with AI models. To do so, we conducted a task-based design study\nwith 27 UXPs in which they prototyped and created a design presentation for a\nAI-enabled interface while having access to a simple AI model training tool.\nThrough analyzing UXPs' design presentations and post-activity interviews, we\nfound that although UXPs struggled to clearly communicate some AI concepts,\ntinkering with AI broadened common ground when communicating with technical\nstakeholders. UXPs also identified key risks and benefits of AI in their\ndesigns, and proposed concrete next steps for both UX and AI work. We conclude\nwith a sensitizing concept and recommendations for design and AI tools to\nenhance multi-stakeholder communication and collaboration when crafting\nhuman-centered AI experiences.",
                "Towards Cognitive Bots: Architectural Research Challenges\nSoftware bots operating in multiple virtual digital platforms must understand\nthe platforms' affordances and behave like human users. Platform affordances or\nfeatures differ from one application platform to another or through a life\ncycle, requiring such bots to be adaptable. Moreover, bots in such platforms\ncould cooperate with humans or other software agents for work or to learn\nspecific behavior patterns. However, present-day bots, particularly chatbots,\nother than language processing and prediction, are far from reaching a human\nuser's behavior level within complex business information systems. They lack\nthe cognitive capabilities to sense and act in such virtual environments,\nrendering their development a challenge to artificial general intelligence\nresearch. In this study, we problematize and investigate assumptions in\nconceptualizing software bot architecture by directing attention to significant\narchitectural research challenges in developing cognitive bots endowed with\ncomplex behavior for operation on information systems. As an outlook, we\npropose alternate architectural assumptions to consider in future bot design\nand bot development frameworks.",
                "Mindstorms in Natural Language-Based Societies of Mind\nBoth Minsky's \"society of mind\" and Schmidhuber's \"learning to think\" inspire\ndiverse societies of large multimodal neural networks (NNs) that solve problems\nby interviewing each other in a \"mindstorm.\" Recent implementations of NN-based\nsocieties of minds consist of large language models (LLMs) and other NN-based\nexperts communicating through a natural language interface. In doing so, they\novercome the limitations of single LLMs, improving multimodal zero-shot\nreasoning. In these natural language-based societies of mind (NLSOMs), new\nagents -- all communicating through the same universal symbolic language -- are\neasily added in a modular fashion. To demonstrate the power of NLSOMs, we\nassemble and experiment with several of them (having up to 129 members),\nleveraging mindstorms in them to solve some practical AI tasks: visual question\nanswering, image captioning, text-to-image synthesis, 3D generation, egocentric\nretrieval, embodied AI, and general language-based task solving. We view this\nas a starting point towards much larger NLSOMs with billions of agents-some of\nwhich may be humans. And with this emergence of great societies of\nheterogeneous minds, many new research questions have suddenly become paramount\nto the future of artificial intelligence. What should be the social structure\nof an NLSOM? What would be the (dis)advantages of having a monarchical rather\nthan a democratic structure? How can principles of NN economies be used to\nmaximize the total reward of a reinforcement learning NLSOM? In this work, we\nidentify, discuss, and try to answer some of these questions.",
                "Training Socially Aligned Language Models on Simulated Social\n  Interactions\nSocial alignment in AI systems aims to ensure that these models behave\naccording to established societal values. However, unlike humans, who derive\nconsensus on value judgments through social interaction, current language\nmodels (LMs) are trained to rigidly replicate their training corpus in\nisolation, leading to subpar generalization in unfamiliar scenarios and\nvulnerability to adversarial attacks. This work presents a novel training\nparadigm that permits LMs to learn from simulated social interactions. In\ncomparison to existing methodologies, our approach is considerably more\nscalable and efficient, demonstrating superior performance in alignment\nbenchmarks and human evaluations. This paradigm shift in the training of LMs\nbrings us a step closer to developing AI systems that can robustly and\naccurately reflect societal norms and values."
            ],
            "interesting paper": 2
        },
        {
            "papers": [
                "Optimization's Neglected Normative Commitments\nOptimization is offered as an objective approach to resolving complex,\nreal-world decisions involving uncertainty and conflicting interests. It drives\nbusiness strategies as well as public policies and, increasingly, lies at the\nheart of sophisticated machine learning systems. A paradigm used to approach\npotentially high-stakes decisions, optimization relies on abstracting the real\nworld to a set of decision(s), objective(s) and constraint(s). Drawing from the\nmodeling process and a range of actual cases, this paper describes the\nnormative choices and assumptions that are necessarily part of using\noptimization. It then identifies six emergent problems that may be neglected:\n1) Misspecified values can yield optimizations that omit certain imperatives\naltogether or incorporate them incorrectly as a constraint or as part of the\nobjective, 2) Problematic decision boundaries can lead to faulty modularity\nassumptions and feedback loops, 3) Failing to account for multiple agents'\ndivergent goals and decisions can lead to policies that serve only certain\nnarrow interests, 4) Mislabeling and mismeasurement can introduce bias and\nimprecision, 5) Faulty use of relaxation and approximation methods,\nunaccompanied by formal characterizations and guarantees, can severely impede\napplicability, and 6) Treating optimization as a justification for action,\nwithout specifying the necessary contextual information, can lead to ethically\ndubious or faulty decisions. Suggestions are given to further understand and\ncurb the harms that can arise when optimization is used wrongfully.",
                "AI Coach Assist: An Automated Approach for Call Recommendation in\n  Contact Centers for Agent Coaching\nIn recent years, the utilization of Artificial Intelligence (AI) in the\ncontact center industry is on the rise. One area where AI can have a\nsignificant impact is in the coaching of contact center agents. By analyzing\ncall transcripts using Natural Language Processing (NLP) techniques, it would\nbe possible to quickly determine which calls are most relevant for coaching\npurposes. In this paper, we present AI Coach Assist, which leverages the\npre-trained transformer-based language models to determine whether a given call\nis coachable or not based on the quality assurance (QA) questions asked by the\ncontact center managers or supervisors. The system was trained and evaluated on\na large dataset collected from real-world contact centers and provides an\neffective way to recommend calls to the contact center managers that are more\nlikely to contain coachable moments. Our experimental findings demonstrate the\npotential of AI Coach Assist to improve the coaching process, resulting in\nenhancing the performance of contact center agents.",
                "Incentivizing honest performative predictions with proper scoring rules\nProper scoring rules incentivize experts to accurately report beliefs,\nassuming predictions cannot influence outcomes. We relax this assumption and\ninvestigate incentives when predictions are performative, i.e., when they can\ninfluence the outcome of the prediction, such as when making public predictions\nabout the stock market. We say a prediction is a fixed point if it accurately\nreflects the expert's beliefs after that prediction has been made. We show that\nin this setting, reports maximizing expected score generally do not reflect an\nexpert's beliefs, and we give bounds on the inaccuracy of such reports. We show\nthat, for binary predictions, if the influence of the expert's prediction on\noutcomes is bounded, it is possible to define scoring rules under which optimal\nreports are arbitrarily close to fixed points. However, this is impossible for\npredictions over more than two outcomes. We also perform numerical simulations\nin a toy setting, showing that our bounds are tight in some situations and that\nprediction error is often substantial (greater than 5-10%). Lastly, we discuss\nalternative notions of optimality, including performative stability, and show\nthat they incentivize reporting fixed points.",
                "On the Value of Myopic Behavior in Policy Reuse\nLeveraging learned strategies in unfamiliar scenarios is fundamental to human\nintelligence. In reinforcement learning, rationally reusing the policies\nacquired from other tasks or human experts is critical for tackling problems\nthat are difficult to learn from scratch. In this work, we present a framework\ncalled Selective Myopic bEhavior Control~(SMEC), which results from the insight\nthat the short-term behaviors of prior policies are sharable across tasks. By\nevaluating the behaviors of prior policies via a hybrid value function\narchitecture, SMEC adaptively aggregates the sharable short-term behaviors of\nprior policies and the long-term behaviors of the task policy, leading to\ncoordinated decisions. Empirical results on a collection of manipulation and\nlocomotion tasks demonstrate that SMEC outperforms existing methods, and\nvalidate the ability of SMEC to leverage related prior policies.",
                "Backdooring Neural Code Search\nReusing off-the-shelf code snippets from online repositories is a common\npractice, which significantly enhances the productivity of software developers.\nTo find desired code snippets, developers resort to code search engines through\nnatural language queries. Neural code search models are hence behind many such\nengines. These models are based on deep learning and gain substantial attention\ndue to their impressive performance. However, the security aspect of these\nmodels is rarely studied. Particularly, an adversary can inject a backdoor in\nneural code search models, which return buggy or even vulnerable code with\nsecurity/privacy issues. This may impact the downstream software (e.g., stock\ntrading systems and autonomous driving) and cause financial loss and/or\nlife-threatening incidents. In this paper, we demonstrate such attacks are\nfeasible and can be quite stealthy. By simply modifying one variable/function\nname, the attacker can make buggy/vulnerable code rank in the top 11%. Our\nattack BADCODE features a special trigger generation and injection procedure,\nmaking the attack more effective and stealthy. The evaluation is conducted on\ntwo neural code search models and the results show our attack outperforms\nbaselines by 60%. Our user study demonstrates that our attack is more stealthy\nthan the baseline by two times based on the F1 score.",
                "Integrating Action Knowledge and LLMs for Task Planning and Situation\n  Handling in Open Worlds\nTask planning systems have been developed to help robots use human knowledge\n(about actions) to complete long-horizon tasks. Most of them have been\ndeveloped for \"closed worlds\" while assuming the robot is provided with\ncomplete world knowledge. However, the real world is generally open, and the\nrobots frequently encounter unforeseen situations that can potentially break\nthe planner's completeness. Could we leverage the recent advances on\npre-trained Large Language Models (LLMs) to enable classical planning systems\nto deal with novel situations?\n  This paper introduces a novel framework, called COWP, for open-world task\nplanning and situation handling. COWP dynamically augments the robot's action\nknowledge, including the preconditions and effects of actions, with\ntask-oriented commonsense knowledge. COWP embraces the openness from LLMs, and\nis grounded to specific domains via action knowledge. For systematic\nevaluations, we collected a dataset that includes 1,085 execution-time\nsituations. Each situation corresponds to a state instance wherein a robot is\npotentially unable to complete a task using a solution that normally works.\nExperimental results show that our approach outperforms competitive baselines\nfrom the literature in the success rate of service tasks. Additionally, we have\ndemonstrated COWP using a mobile manipulator. Supplementary materials are\navailable at: https://cowplanning.github.io/"
            ],
            "interesting paper": 1
        },
        {
            "papers": [
                "AI Audit: A Card Game to Reflect on Everyday AI Systems\nAn essential element of K-12 AI literacy is educating learners about the\nethical and societal implications of AI systems. Previous work in AI ethics\nliteracy have developed curriculum and classroom activities that engage\nlearners in reflecting on the ethical implications of AI systems and developing\nresponsible AI. There is little work in using game-based learning methods in AI\nliteracy. Games are known to be compelling media to teach children about\ncomplex STEM concepts. In this work, we developed a competitive card game for\nmiddle and high school students called \"AI Audit\" where they play as AI\nstart-up founders building novel AI-powered technology. Players can challenge\nother players with potential harms of their technology or defend their own\nbusinesses by features that mitigate these harms. The game mechanics reward\nsystems that are ethically developed or that take steps to mitigate potential\nharms. In this paper, we present the game design, teacher resources for\nclassroom deployment and early playtesting results. We discuss our reflections\nabout using games as teaching tools for AI literacy in K-12 classrooms.",
                "Voluminous yet Vacuous? Semantic Capital in an Age of Large Language\n  Models\nLarge Language Models (LLMs) have emerged as transformative forces in the\nrealm of natural language processing, wielding the power to generate human-like\ntext. However, despite their potential for content creation, they carry the\nrisk of eroding our Semantic Capital (SC) - the collective knowledge within our\ndigital ecosystem - thereby posing diverse social epistemic challenges. This\npaper explores the evolution, capabilities, and limitations of these models,\nwhile highlighting ethical concerns they raise. The study contribution is\ntwo-fold: first, it is acknowledged that, withstanding the challenges of\ntracking and controlling LLM impacts, it is necessary to reconsider our\ninteraction with these AI technologies and the narratives that form public\nperception of them. It is argued that before achieving this goal, it is\nessential to confront a potential deontological tipping point in an increasing\nAI-driven infosphere. This goes beyond just adhering to AI ethical norms or\nregulations and requires understanding the spectrum of social epistemic risks\nLLMs might bring to our collective SC. Secondly, building on Luciano Floridi's\ntaxonomy for SC risks, those are mapped within the functionality and\nconstraints of LLMs. By this outlook, we aim to protect and enrich our SC while\nfostering a collaborative environment between humans and AI that augments human\nintelligence rather than replacing it.",
                "Re-imagining health and well-being in low resource African settings\n  using an augmented AI system and a 3D digital twin\nThis paper discusses and explores the potential and relevance of recent\ndevelopments in artificial intelligence (AI) and digital twins for health and\nwell-being in low-resource African countries. We use the case of public health\nemergency response to disease outbreaks and epidemic control. There is\npotential to take advantage of the increasing availability of data and\ndigitization to develop advanced AI methods for analysis and prediction. Using\nan AI systems perspective, we review emerging trends in AI systems and digital\ntwins and propose an initial augmented AI system architecture to illustrate how\nan AI system can work with a 3D digital twin to address public health goals. We\nhighlight scientific knowledge discovery, continual learning, pragmatic\ninteroperability, and interactive explanation and decision-making as essential\nresearch challenges for AI systems and digital twins.",
                "The Digital Divide in Process Safety: Quantitative Risk Analysis of\n  Human-AI Collaboration\nDigital technologies have dramatically accelerated the digital transformation\nin process industries, boosted new industrial applications, upgraded the\nproduction system, and enhanced operational efficiency. In contrast, the\nchallenges and gaps between human and artificial intelligence (AI) have become\nmore and more prominent, whereas the digital divide in process safety is\naggregating. The study attempts to address the following questions: (i)What is\nAI in the process safety context? (ii)What is the difference between AI and\nhumans in process safety? (iii)How do AI and humans collaborate in process\nsafety? (iv)What are the challenges and gaps in human-AI collaboration? (v)How\nto quantify the risk of human-AI collaboration in process safety? Qualitative\nrisk analysis based on brainstorming and literature review, and quantitative\nrisk analysis based on layer of protection analysis (LOPA) and Bayesian network\n(BN), were applied to explore and model. The importance of human reliability\nshould be stressed in the digital age, not usually to increase the reliability\nof AI, and human-centered AI design in process safety needs to be propagated.",
                "The Social Value of Dark Energy\nAstrophysics is a social enterprise exemplified here by the Dark Energy\nSurvey (DES) which completed its fieldwork in 2019 after 16 years of\npreparation and observation, while data analysis continues. Society funds\nastrophysics on a grand scale. For human capital and for governance the\ndiscipline draws on a self-governing \"republic of science\", while the funds\nwere provided by philanthropists in the past, and by governments today. The\nbenefits accrue initially to scientists themselves, in the form of a rewarding\nvocation. For the social benefit it is tempting to apply formal cost benefit\nanalysis, but that approach ignores the option value of science and imposes\nquestionable assumptions from welfare economics. Astrophysics generates some\nuseful spinoffs, offers attractive careers, appeals to the popular imagination,\nspeaks to metaphysical cravings and constitutes a good in itself. The rise of\nAI also suggests a role in exploring future habitats for intelligence and\ncognition.",
                "Taming AI Bots: Controllability of Neural States in Large Language\n  Models\nWe tackle the question of whether an agent can, by suitable choice of\nprompts, control an AI bot to any state. To that end, we first introduce a\nformal definition of ``meaning'' that is amenable to analysis. Then, we\ncharacterize ``meaningful data'' on which large language models (LLMs) are\nostensibly trained, and ``well-trained LLMs'' through conditions that are\nlargely met by today's LLMs. While a well-trained LLM constructs an embedding\nspace of meanings that is Euclidean, meanings themselves do not form a vector\n(linear) subspace, but rather a quotient space within. We then characterize the\nsubset of meanings that can be reached by the state of the LLMs for some input\nprompt, and show that a well-trained bot can reach any meaning albeit with\nsmall probability. We then introduce a stronger notion of controllability as\n{\\em almost certain reachability}, and show that, when restricted to the space\nof meanings, an AI bot is controllable. We do so after introducing a functional\ncharacterization of attentive AI bots, and finally derive necessary and\nsufficient conditions for controllability. The fact that AI bots are\ncontrollable means that an adversary could steer them towards any state.\nHowever, the sampling process can be designed to counteract adverse actions and\navoid reaching undesirable regions of state space before their boundary is\ncrossed."
            ],
            "interesting paper": 2
        },
        {
            "papers": [
                "RE-centric Recommendations for the Development of Trustworthy(er)\n  Autonomous Systems\nComplying with the EU AI Act (AIA) guidelines while developing and\nimplementing AI systems will soon be mandatory within the EU. However,\npractitioners lack actionable instructions to operationalise ethics during AI\nsystems development. A literature review of different ethical guidelines\nrevealed inconsistencies in the principles addressed and the terminology used\nto describe them. Furthermore, requirements engineering (RE), which is\nidentified to foster trustworthiness in the AI development process from the\nearly stages was observed to be absent in a lot of frameworks that support the\ndevelopment of ethical and trustworthy AI. This incongruous phrasing combined\nwith a lack of concrete development practices makes trustworthy AI development\nharder. To address this concern, we formulated a comparison table for the\nterminology used and the coverage of the ethical AI principles in major ethical\nAI guidelines. We then examined the applicability of ethical AI development\nframeworks for performing effective RE during the development of trustworthy AI\nsystems. A tertiary review and meta-analysis of literature discussing ethical\nAI frameworks revealed their limitations when developing trustworthy AI. Based\non our findings, we propose recommendations to address such limitations during\nthe development of trustworthy AI.",
                "Doing the right thing for the right reason: Evaluating artificial moral\n  cognition by probing cost insensitivity\nIs it possible to evaluate the moral cognition of complex artificial agents?\nIn this work, we take a look at one aspect of morality: `doing the right thing\nfor the right reasons.' We propose a behavior-based analysis of artificial\nmoral cognition which could also be applied to humans to facilitate\nlike-for-like comparison. Morally-motivated behavior should persist despite\nmounting cost; by measuring an agent's sensitivity to this cost, we gain deeper\ninsight into underlying motivations. We apply this evaluation to a particular\nset of deep reinforcement learning agents, trained by memory-based\nmeta-reinforcement learning. Our results indicate that agents trained with a\nreward function that includes other-regarding preferences perform helping\nbehavior in a way that is less sensitive to increasing cost than agents trained\nwith more self-interested preferences.",
                "Generalized Disparate Impact for Configurable Fairness Solutions in ML\nWe make two contributions in the field of AI fairness over continuous\nprotected attributes. First, we show that the Hirschfeld-Gebelein-Renyi (HGR)\nindicator (the only one currently available for such a case) is valuable but\nsubject to a few crucial limitations regarding semantics, interpretability, and\nrobustness. Second, we introduce a family of indicators that are: 1)\ncomplementary to HGR in terms of semantics; 2) fully interpretable and\ntransparent; 3) robust over finite samples; 4) configurable to suit specific\napplications. Our approach also allows us to define fine-grained constraints to\npermit certain types of dependence and forbid others selectively. By expanding\nthe available options for continuous protected attributes, our approach\nrepresents a significant contribution to the area of fair artificial\nintelligence.",
                "Towards a Unifying Model of Rationality in Multiagent Systems\nMultiagent systems deployed in the real world need to cooperate with other\nagents (including humans) nearly as effectively as these agents cooperate with\none another. To design such AI, and provide guarantees of its effectiveness, we\nneed to clearly specify what types of agents our AI must be able to cooperate\nwith. In this work we propose a generic model of socially intelligent agents,\nwhich are individually rational learners that are also able to cooperate with\none another (in the sense that their joint behavior is Pareto efficient). We\ndefine rationality in terms of the regret incurred by each agent over its\nlifetime, and show how we can construct socially intelligent agents for\ndifferent forms of regret. We then discuss the implications of this model for\nthe development of \"robust\" MAS that can cooperate with a wide variety of\nsocially intelligent agents.",
                "Perceived Trustworthiness of Natural Language Generators\nNatural Language Generation tools, such as chatbots that can generate\nhuman-like conversational text, are becoming more common both for personal and\nprofessional use. However, there are concerns about their trustworthiness and\nethical implications. The paper addresses the problem of understanding how\ndifferent users (e.g., linguists, engineers) perceive and adopt these tools and\ntheir perception of machine-generated text quality. It also discusses the\nperceived advantages and limitations of Natural Language Generation tools, as\nwell as users' beliefs on governance strategies. The main findings of this\nstudy include the impact of users' field and level of expertise on the\nperceived trust and adoption of Natural Language Generation tools, the users'\nassessment of the accuracy, fluency, and potential biases of machine-generated\ntext in comparison to human-written text, and an analysis of the advantages and\nethical risks associated with these tools as identified by the participants.\nMoreover, this paper discusses the potential implications of these findings for\nenhancing the AI development process. The paper sheds light on how different\nuser characteristics shape their beliefs on the quality and overall\ntrustworthiness of machine-generated text. Furthermore, it examines the\nbenefits and risks of these tools from the perspectives of different users.",
                "Game of Tones: Faculty detection of GPT-4 generated content in\n  university assessments\nThis study explores the robustness of university assessments against the use\nof Open AI's Generative Pre-Trained Transformer 4 (GPT-4) generated content and\nevaluates the ability of academic staff to detect its use when supported by the\nTurnitin Artificial Intelligence (AI) detection tool. The research involved\ntwenty-two GPT-4 generated submissions being created and included in the\nassessment process to be marked by fifteen different faculty members. The study\nreveals that although the detection tool identified 91% of the experimental\nsubmissions as containing some AI-generated content, the total detected content\nwas only 54.8%. This suggests that the use of adversarial techniques regarding\nprompt engineering is an effective method in evading AI detection tools and\nhighlights that improvements to AI detection software are needed. Using the\nTurnitin AI detect tool, faculty reported 54.5% of the experimental submissions\nto the academic misconduct process, suggesting the need for increased awareness\nand training into these tools. Genuine submissions received a mean score of\n54.4, whereas AI-generated content scored 52.3, indicating the comparable\nperformance of GPT-4 in real-life situations. Recommendations include adjusting\nassessment strategies to make them more resistant to the use of AI tools, using\nAI-inclusive assessment where possible, and providing comprehensive training\nprograms for faculty and students. This research contributes to understanding\nthe relationship between AI-generated content and academic assessment, urging\nfurther investigation to preserve academic integrity."
            ],
            "interesting paper": 1
        },
        {
            "papers": [
                "Intent-aligned AI systems deplete human agency: the need for agency\n  foundations research in AI safety\nThe rapid advancement of artificial intelligence (AI) systems suggests that\nartificial general intelligence (AGI) systems may soon arrive. Many researchers\nare concerned that AIs and AGIs will harm humans via intentional misuse\n(AI-misuse) or through accidents (AI-accidents). In respect of AI-accidents,\nthere is an increasing effort focused on developing algorithms and paradigms\nthat ensure AI systems are aligned to what humans intend, e.g. AI systems that\nyield actions or recommendations that humans might judge as consistent with\ntheir intentions and goals. Here we argue that alignment to human intent is\ninsufficient for safe AI systems and that preservation of long-term agency of\nhumans may be a more robust standard, and one that needs to be separated\nexplicitly and a priori during optimization. We argue that AI systems can\nreshape human intention and discuss the lack of biological and psychological\nmechanisms that protect humans from loss of agency. We provide the first formal\ndefinition of agency-preserving AI-human interactions which focuses on\nforward-looking agency evaluations and argue that AI systems - not humans -\nmust be increasingly tasked with making these evaluations. We show how agency\nloss can occur in simple environments containing embedded agents that use\ntemporal-difference learning to make action recommendations. Finally, we\npropose a new area of research called \"agency foundations\" and pose four\ninitial topics designed to improve our understanding of agency in AI-human\ninteractions: benevolent game theory, algorithmic foundations of human rights,\nmechanistic interpretability of agency representation in neural-networks and\nreinforcement learning from internal states.",
                "Disentangling and Operationalizing AI Fairness at LinkedIn\nOperationalizing AI fairness at LinkedIn's scale is challenging not only\nbecause there are multiple mutually incompatible definitions of fairness but\nalso because determining what is fair depends on the specifics and context of\nthe product where AI is deployed. Moreover, AI practitioners need clarity on\nwhat fairness expectations need to be addressed at the AI level. In this paper,\nwe present the evolving AI fairness framework used at LinkedIn to address these\nthree challenges. The framework disentangles AI fairness by separating out\nequal treatment and equitable product expectations. Rather than imposing a\ntrade-off between these two commonly opposing interpretations of fairness, the\nframework provides clear guidelines for operationalizing equal AI treatment\ncomplemented with a product equity strategy. This paper focuses on the equal AI\ntreatment component of LinkedIn's AI fairness framework, shares the principles\nthat support it, and illustrates their application through a case study. We\nhope this paper will encourage other big tech companies to join us in sharing\ntheir approach to operationalizing AI fairness at scale, so that together we\ncan keep advancing this constantly evolving field.",
                "Towards Explainable and Language-Agnostic LLMs: Symbolic Reverse\n  Engineering of Language at Scale\nLarge language models (LLMs) have achieved a milestone that undenia-bly\nchanged many held beliefs in artificial intelligence (AI). However, there\nremains many limitations of these LLMs when it comes to true language\nunderstanding, limitations that are a byproduct of the under-lying architecture\nof deep neural networks. Moreover, and due to their subsymbolic nature,\nwhatever knowledge these models acquire about how language works will always be\nburied in billions of microfeatures (weights), none of which is meaningful on\nits own, making such models hopelessly unexplainable. To address these\nlimitations, we suggest com-bining the strength of symbolic representations\nwith what we believe to be the key to the success of LLMs, namely a successful\nbottom-up re-verse engineering of language at scale. As such we argue for a\nbottom-up reverse engineering of language in a symbolic setting. Hints on what\nthis project amounts to have been suggested by several authors, and we discuss\nin some detail here how this project could be accomplished.",
                "Evaluating GPT's Programming Capability through CodeWars' Katas\nIn the burgeoning field of artificial intelligence (AI), understanding the\ncapabilities and limitations of programming-oriented models is crucial. This\npaper presents a novel evaluation of the programming proficiency of Generative\nPretrained Transformer (GPT) models, specifically GPT-3.5 and GPT-4, against\ncoding problems of varying difficulty levels drawn from Codewars. The\nexperiments reveal a distinct boundary at the 3kyu level, beyond which these\nGPT models struggle to provide solutions. These findings led to the proposal of\na measure for coding problem complexity that incorporates both problem\ndifficulty and the time required for solution. The research emphasizes the need\nfor validation and creative thinking capabilities in AI models to better\nemulate human problem-solving techniques. Future work aims to refine this\nproposed complexity measure, enhance AI models with these suggested\ncapabilities, and develop an objective measure for programming problem\ndifficulty. The results of this research offer invaluable insights for\nimproving AI programming capabilities and advancing the frontier of AI\nproblem-solving abilities.",
                "Building Extractive Question Answering System to Support Human-AI Health\n  Coaching Model for Sleep Domain\nNon-communicable diseases (NCDs) are a leading cause of global deaths,\nnecessitating a focus on primary prevention and lifestyle behavior change.\nHealth coaching, coupled with Question Answering (QA) systems, has the\npotential to transform preventive healthcare. This paper presents a\nhuman-Artificial Intelligence (AI) health coaching model incorporating a\ndomain-specific extractive QA system. A sleep-focused dataset, SleepQA, was\nmanually assembled and used to fine-tune domain-specific BERT models. The QA\nsystem was evaluated using automatic and human methods. A data-centric\nframework enhanced the system's performance by improving passage retrieval and\nquestion reformulation. Although the system did not outperform the baseline in\nautomatic evaluation, it excelled in the human evaluation of real-world\nquestions. Integration into a Human-AI health coaching model was tested in a\npilot Randomized Controlled Trial (RCT).",
                "Seeing Seeds Beyond Weeds: Green Teaming Generative AI for Beneficial\n  Uses\nLarge generative AI models (GMs) like GPT and DALL-E are trained to generate\ncontent for general, wide-ranging purposes. GM content filters are generalized\nto filter out content which has a risk of harm in many cases, e.g., hate\nspeech. However, prohibited content is not always harmful -- there are\ninstances where generating prohibited content can be beneficial. So, when GMs\nfilter out content, they preclude beneficial use cases along with harmful ones.\nWhich use cases are precluded reflects the values embedded in GM content\nfiltering. Recent work on red teaming proposes methods to bypass GM content\nfilters to generate harmful content. We coin the term green teaming to describe\nmethods of bypassing GM content filters to design for beneficial use cases. We\nshowcase green teaming by: 1) Using ChatGPT as a virtual patient to simulate a\nperson experiencing suicidal ideation, for suicide support training; 2) Using\nCodex to intentionally generate buggy solutions to train students on debugging;\nand 3) Examining an Instagram page using Midjourney to generate images of\nanti-LGBTQ+ politicians in drag. Finally, we discuss how our use cases\ndemonstrate green teaming as both a practical design method and a mode of\ncritique, which problematizes and subverts current understandings of harms and\nvalues in generative AI."
            ],
            "interesting paper": 1
        },
        {
            "papers": [
                "Responsible Design Patterns for Machine Learning Pipelines\nIntegrating ethical practices into the AI development process for artificial\nintelligence (AI) is essential to ensure safe, fair, and responsible operation.\nAI ethics involves applying ethical principles to the entire life cycle of AI\nsystems. This is essential to mitigate potential risks and harms associated\nwith AI, such as algorithm biases. To achieve this goal, responsible design\npatterns (RDPs) are critical for Machine Learning (ML) pipelines to guarantee\nethical and fair outcomes. In this paper, we propose a comprehensive framework\nincorporating RDPs into ML pipelines to mitigate risks and ensure the ethical\ndevelopment of AI systems. Our framework comprises new responsible AI design\npatterns for ML pipelines identified through a survey of AI ethics and data\nmanagement experts and validated through real-world scenarios with expert\nfeedback. The framework guides AI developers, data scientists, and\npolicy-makers to implement ethical practices in AI development and deploy\nresponsible AI systems in production.",
                "Citizen Perspectives on Necessary Safeguards to the Use of AI by Law\n  Enforcement Agencies\nIn the light of modern technological advances, Artificial Intelligence (AI)\nis relied upon to enhance performance, increase efficiency, and maximize gains.\nFor Law Enforcement Agencies (LEAs), it can prove valuable in optimizing\nevidence analysis and establishing proactive prevention measures. Nevertheless,\ncitizens raise legitimate concerns around privacy invasions, biases,\ninequalities, and inaccurate decisions. This study explores the views of 111\ncitizens towards AI use by police through interviews, and integrates societal\nconcerns along with propositions of safeguards from negative effects of AI use\nby LEAs in the context of cybercrime and terrorism.",
                "AI Imagery and the Overton Window\nAI-based text-to-image generation has undergone a significant leap in the\nproduction of visually comprehensive and aesthetic imagery over the past year,\nto the point where differentiating between a man-made piece of art and an\nAI-generated image is becoming more difficult. Generative Models such as Stable\nDiffusion, Midjourney and others are expected to affect several major\nindustries in technological and ethical aspects. Striking the balance between\nraising human standard of life and work vs exploiting one group of people to\nenrich another is a complex and crucial part of the discussion. Due to the\nrapid growth of this technology, the way in which its models operate, and gray\narea legalities, visual and artistic domains - including the video game\nindustry, are at risk of being taken over from creators by AI infrastructure\nowners. This paper is a literature review examining the concerns facing both AI\ndevelopers and users today, including identity theft, data laundering and more.\nIt discusses legalization challenges and ethical concerns, and concludes with\nhow AI generative models can be tremendously useful in streamlining the process\nof visual creativity in both static and interactive media given proper\nregulation.\n  Keywords: AI text-to-image generation, Midjourney, Stable Diffusion, AI\nEthics, Game Design, Digital Art, Data Laundering",
                "From Human-Centered to Social-Centered Artificial Intelligence:\n  Assessing ChatGPT's Impact through Disruptive Events\nLarge language models (LLMs) and dialogue agents have existed for years, but\nthe release of recent GPT models has been a watershed moment for artificial\nintelligence (AI) research and society at large. Immediately recognized for its\ngenerative capabilities and versatility, ChatGPT's impressive proficiency\nacross technical and creative domains led to its widespread adoption. While\nsociety grapples with the emerging cultural impacts of ChatGPT, critiques of\nChatGPT's impact within the machine learning community have coalesced around\nits performance or other conventional Responsible AI evaluations relating to\nbias, toxicity, and 'hallucination.' We argue that these latter critiques draw\nheavily on a particular conceptualization of the 'human-centered' framework,\nwhich tends to cast atomized individuals as the key recipients of both the\nbenefits and detriments of technology. In this article, we direct attention to\nanother dimension of LLMs and dialogue agents' impact: their effect on social\ngroups, institutions, and accompanying norms and practices. By illustrating\nChatGPT's social impact through three disruptive events, we challenge\nindividualistic approaches in AI development and contribute to ongoing debates\naround the ethical and responsible implementation of AI systems. We hope this\neffort will call attention to more comprehensive and longitudinal evaluation\ntools and compel technologists to go beyond human-centered thinking and ground\ntheir efforts through social-centered AI.",
                "Human Control: Definitions and Algorithms\nHow can humans stay in control of advanced artificial intelligence systems?\nOne proposal is corrigibility, which requires the agent to follow the\ninstructions of a human overseer, without inappropriately influencing them. In\nthis paper, we formally define a variant of corrigibility called shutdown\ninstructability, and show that it implies appropriate shutdown behavior,\nretention of human autonomy, and avoidance of user harm. We also analyse the\nrelated concepts of non-obstruction and shutdown alignment, three previously\nproposed algorithms for human control, and one new algorithm.",
                "EMOTE: An Explainable architecture for Modelling the Other Through\n  Empathy\nWe can usually assume others have goals analogous to our own. This assumption\ncan also, at times, be applied to multi-agent games - e.g. Agent 1's attraction\nto green pellets is analogous to Agent 2's attraction to red pellets. This\n\"analogy\" assumption is tied closely to the cognitive process known as empathy.\nInspired by empathy, we design a simple and explainable architecture to model\nanother agent's action-value function. This involves learning an \"Imagination\nNetwork\" to transform the other agent's observed state in order to produce a\nhuman-interpretable \"empathetic state\" which, when presented to the learning\nagent, produces behaviours that mimic the other agent. Our approach is\napplicable to multi-agent scenarios consisting of a single learning agent and\nother (independent) agents acting according to fixed policies. This\narchitecture is particularly beneficial for (but not limited to) algorithms\nusing a composite value or reward function. We show our method produces better\nperformance in multi-agent games, where it robustly estimates the other's model\nin different environment configurations. Additionally, we show that the\nempathetic states are human interpretable, and thus verifiable."
            ],
            "interesting paper": 1
        },
        {
            "papers": [
                "The ethical ambiguity of AI data enrichment: Measuring gaps in research\n  ethics norms and practices\nThe technical progression of artificial intelligence (AI) research has been\nbuilt on breakthroughs in fields such as computer science, statistics, and\nmathematics. However, in the past decade AI researchers have increasingly\nlooked to the social sciences, turning to human interactions to solve the\nchallenges of model development. Paying crowdsourcing workers to generate or\ncurate data, or data enrichment, has become indispensable for many areas of AI\nresearch, from natural language processing to reinforcement learning from human\nfeedback (RLHF). Other fields that routinely interact with crowdsourcing\nworkers, such as Psychology, have developed common governance requirements and\nnorms to ensure research is undertaken ethically. This study explores how, and\nto what extent, comparable research ethics requirements and norms have\ndeveloped for AI research and data enrichment. We focus on the approach taken\nby two leading conferences: ICLR and NeurIPS, and journal publisher Springer.\nIn a longitudinal study of accepted papers, and via a comparison with\nPsychology and CHI papers, this work finds that leading AI venues have begun to\nestablish protocols for human data collection, but these are are inconsistently\nfollowed by authors. Whilst Psychology papers engaging with crowdsourcing\nworkers frequently disclose ethics reviews, payment data, demographic data and\nother information, similar disclosures are far less common in leading AI venues\ndespite similar guidance. The work concludes with hypotheses to explain these\ngaps in research ethics practices and considerations for its implications.",
                "AI Liability Insurance With an Example in AI-Powered E-diagnosis System\nArtificial Intelligence (AI) has received an increasing amount of attention\nin multiple areas. The uncertainties and risks in AI-powered systems have\ncreated reluctance in their wild adoption. As an economic solution to\ncompensate for potential damages, AI liability insurance is a promising market\nto enhance the integration of AI into daily life. In this work, we use an\nAI-powered E-diagnosis system as an example to study AI liability insurance. We\nprovide a quantitative risk assessment model with evidence-based numerical\nanalysis. We discuss the insurability criteria for AI technologies and suggest\nnecessary adjustments to accommodate the features of AI products. We show that\nAI liability insurance can act as a regulatory mechanism to incentivize\ncompliant behaviors and serve as a certificate of high-quality AI systems.\nFurthermore, we suggest premium adjustment to reflect the dynamic evolution of\nthe inherent uncertainty in AI. Moral hazard problems are discussed and\nsuggestions for AI liability insurance are provided.",
                "AI and the creative realm: A short review of current and future\n  applications\nThis study explores the concept of creativity and artificial intelligence\n(AI) and their recent integration. While AI has traditionally been perceived as\nincapable of generating new ideas or creating art, the development of more\nsophisticated AI models and the proliferation of human-computer interaction\ntools have opened up new possibilities for AI in artistic creation. This study\ninvestigates the various applications of AI in a creative context,\ndifferentiating between the type of art, language, and algorithms used. It also\nconsiders the philosophical implications of AI and creativity, questioning\nwhether consciousness can be researched in machines and AI's potential\ninterests and decision-making capabilities. Overall, we aim to stimulate a\nreflection on AI's use and ethical implications in creative contexts.",
                "Navigating Fairness in Radiology AI: Concepts, Consequences,and Crucial\n  Considerations\nArtificial Intelligence (AI) has significantly revolutionized radiology,\npromising improved patient outcomes and streamlined processes. However, it's\ncritical to ensure the fairness of AI models to prevent stealthy bias and\ndisparities from leading to unequal outcomes. This review discusses the concept\nof fairness in AI, focusing on bias auditing using the Aequitas toolkit, and\nits real-world implications in radiology, particularly in disease screening\nscenarios. Aequitas, an open-source bias audit toolkit, scrutinizes AI models'\ndecisions, identifying hidden biases that may result in disparities across\ndifferent demographic groups and imaging equipment brands. This toolkit\noperates on statistical theories, analyzing a large dataset to reveal a model's\nfairness. It excels in its versatility to handle various variables\nsimultaneously, especially in a field as diverse as radiology. The review\nexplicates essential fairness metrics: Equal and Proportional Parity, False\nPositive Rate Parity, False Discovery Rate Parity, False Negative Rate Parity,\nand False Omission Rate Parity. Each metric serves unique purposes and offers\ndifferent insights. We present hypothetical scenarios to demonstrate their\nrelevance in disease screening settings, and how disparities can lead to\nsignificant real-world impacts.",
                "Interpretable and Explainable Logical Policies via Neurally Guided\n  Symbolic Abstraction\nThe limited priors required by neural networks make them the dominating\nchoice to encode and learn policies using reinforcement learning (RL). However,\nthey are also black-boxes, making it hard to understand the agent's behaviour,\nespecially when working on the image level. Therefore, neuro-symbolic RL aims\nat creating policies that are interpretable in the first place. Unfortunately,\ninterpretability is not explainability. To achieve both, we introduce Neurally\ngUided Differentiable loGic policiEs (NUDGE). NUDGE exploits trained neural\nnetwork-based agents to guide the search of candidate-weighted logic rules,\nthen uses differentiable logic to train the logic agents. Our experimental\nevaluation demonstrates that NUDGE agents can induce interpretable and\nexplainable policies while outperforming purely neural ones and showing good\nflexibility to environments of different initial states and problem sizes.",
                "ExTRUST: Reducing Exploit Stockpiles with a Privacy-Preserving Depletion\n  System for Inter-State Relationships\nCyberspace is a fragile construct threatened by malicious cyber operations of\ndifferent actors, with vulnerabilities in IT hardware and software forming the\nbasis for such activities, thus also posing a threat to global IT security.\nAdvancements in the field of artificial intelligence accelerate this\ndevelopment, either with artificial intelligence enabled cyber weapons,\nautomated cyber defense measures, or artificial intelligence-based threat and\nvulnerability detection. Especially state actors, with their long-term\nstrategic security interests, often stockpile such knowledge of vulnerabilities\nand exploits to enable their military or intelligence service cyberspace\noperations. While treaties and regulations to limit these developments and to\nenhance global IT security by disclosing vulnerabilities are currently being\ndiscussed on the international level, these efforts are hindered by state\nconcerns about the disclosure of unique knowledge and about giving up tactical\nadvantages. This leads to a situation where multiple states are likely to\nstockpile at least some identical exploits, with technical measures to enable a\ndepletion process for these stockpiles that preserve state secrecy interests\nand consider the special constraints of interacting states as well as the\nrequirements within such environments being non-existent. This paper proposes\nsuch a privacy-preserving approach that allows multiple state parties to\nprivately compare their stock of vulnerabilities and exploits to check for\nitems that occur in multiple stockpiles without revealing them so that their\ndisclosure can be considered. We call our system ExTRUST and show that it is\nscalable and can withstand several attack scenarios. Beyond the\nintergovernmental setting, ExTRUST can also be used for other zero-trust use\ncases, such as bug-bounty programs."
            ],
            "interesting paper": 1
        }
    ],
    "Charlie Davis": [
        {
            "papers": [
                "Prompt Evolution for Generative AI: A Classifier-Guided Approach\nSynthesis of digital artifacts conditioned on user prompts has become an\nimportant paradigm facilitating an explosion of use cases with generative AI.\nHowever, such models often fail to connect the generated outputs and desired\ntarget concepts/preferences implied by the prompts. Current research addressing\nthis limitation has largely focused on enhancing the prompts before output\ngeneration or improving the model's performance up front. In contrast, this\npaper conceptualizes prompt evolution, imparting evolutionary selection\npressure and variation during the generative process to produce multiple\noutputs that satisfy the target concepts/preferences better. We propose a\nmulti-objective instantiation of this broader idea that uses a multi-label\nimage classifier-guided approach. The predicted labels from the classifiers\nserve as multiple objectives to optimize, with the aim of producing diversified\nimages that meet user preferences. A novelty of our evolutionary algorithm is\nthat the pre-trained generative model gives us implicit mutation operations,\nleveraging the model's stochastic generative capability to automate the\ncreation of Pareto-optimized images more faithful to user preferences.",
                "Large Language Models for User Interest Journeys\nLarge language models (LLMs) have shown impressive capabilities in natural\nlanguage understanding and generation. Their potential for deeper user\nunderstanding and improved personalized user experience on recommendation\nplatforms is, however, largely untapped. This paper aims to address this gap.\nRecommender systems today capture users' interests through encoding their\nhistorical activities on the platforms. The generated user representations are\nhard to examine or interpret. On the other hand, if we were to ask people about\ninterests they pursue in their life, they might talk about their hobbies, like\nI just started learning the ukulele, or their relaxation routines, e.g., I like\nto watch Saturday Night Live, or I want to plant a vertical garden. We argue,\nand demonstrate through extensive experiments, that LLMs as foundation models\ncan reason through user activities, and describe their interests in nuanced and\ninteresting ways, similar to how a human would.\n  We define interest journeys as the persistent and overarching user interests,\nin other words, the non-transient ones. These are the interests that we believe\nwill benefit most from the nuanced and personalized descriptions. We introduce\na framework in which we first perform personalized extraction of interest\njourneys, and then summarize the extracted journeys via LLMs, using techniques\nlike few-shot prompting, prompt-tuning and fine-tuning. Together, our results\nin prompting LLMs to name extracted user journeys in a large-scale industrial\nplatform demonstrate great potential of these models in providing deeper, more\ninterpretable, and controllable user understanding. We believe LLM powered user\nunderstanding can be a stepping stone to entirely new user experiences on\nrecommendation platforms that are journey-aware, assistive, and enabling\nfrictionless conversation down the line.",
                "HuatuoGPT, towards Taming Language Model to Be a Doctor\nIn this paper, we present HuatuoGPT, a large language model (LLM) for medical\nconsultation. The core recipe of HuatuoGPT is to leverage both\n\\textit{distilled data from ChatGPT} and \\textit{real-world data from doctors}\nin the supervised fine-tuned stage. The responses of ChatGPT are usually\ndetailed, well-presented and informative while it cannot perform like a doctor\nin many aspects, e.g. for integrative diagnosis. We argue that real-world data\nfrom doctors would be complementary to distilled data in the sense the former\ncould tame a distilled language model to perform like doctors. To better\nleverage the strengths of both data, we train a reward model to align the\nlanguage model with the merits that both data bring, following an RLAIF\n(reinforced learning from AI feedback) fashion. To evaluate and benchmark the\nmodels, we propose a comprehensive evaluation scheme (including automatic and\nmanual metrics). Experimental results demonstrate that HuatuoGPT achieves\nstate-of-the-art results in performing medical consultation among open-source\nLLMs in GPT-4 evaluation, human evaluation, and medical benchmark datasets. It\nis worth noting that by using additional real-world data and RLAIF, the\ndistilled language model (i.e., HuatuoGPT) outperforms its teacher model\nChatGPT in most cases. Our code, data, and models are publicly available at\n\\url{https://github.com/FreedomIntelligence/HuatuoGPT}. The online demo is\navailable at \\url{https://www.HuatuoGPT.cn/}.",
                "An Experimental Investigation into the Evaluation of Explainability\n  Methods\nEXplainable Artificial Intelligence (XAI) aims to help users to grasp the\nreasoning behind the predictions of an Artificial Intelligence (AI) system.\nMany XAI approaches have emerged in recent years. Consequently, a subfield\nrelated to the evaluation of XAI methods has gained considerable attention,\nwith the aim to determine which methods provide the best explanation using\nvarious approaches and criteria. However, the literature lacks a comparison of\nthe evaluation metrics themselves, that one can use to evaluate XAI methods.\nThis work aims to fill this gap by comparing 14 different metrics when applied\nto nine state-of-the-art XAI methods and three dummy methods (e.g., random\nsaliency maps) used as references. Experimental results show which of these\nmetrics produces highly correlated results, indicating potential redundancy. We\nalso demonstrate the significant impact of varying the baseline hyperparameter\non the evaluation metric values. Finally, we use dummy methods to assess the\nreliability of metrics in terms of ranking, pointing out their limitations.",
                "Topic-Guided Self-Introduction Generation for Social Media Users\nMillions of users are active on social media. To allow users to better\nshowcase themselves and network with others, we explore the auto-generation of\nsocial media self-introduction, a short sentence outlining a user's personal\ninterests. While most prior work profiles users with tags (e.g., ages), we\ninvestigate sentence-level self-introductions to provide a more natural and\nengaging way for users to know each other. Here we exploit a user's tweeting\nhistory to generate their self-introduction. The task is non-trivial because\nthe history content may be lengthy, noisy, and exhibit various personal\ninterests. To address this challenge, we propose a novel unified topic-guided\nencoder-decoder (UTGED) framework; it models latent topics to reflect salient\nuser interest, whose topic mixture then guides encoding a user's history and\ntopic words control decoding their self-introduction. For experiments, we\ncollect a large-scale Twitter dataset, and extensive results show the\nsuperiority of our UTGED to the advanced encoder-decoder models without topic\nmodeling.",
                "Visually-Situated Natural Language Understanding with Contrastive\n  Reading Model and Frozen Large Language Models\nRecent advances in Large Language Models (LLMs) have stimulated a surge of\nresearch aimed at extending their applications to the visual domain. While\nthese models exhibit promise in generating abstract image captions and\nfacilitating natural conversations, their performance on text-rich images still\nrequires improvement. In this paper, we introduce Contrastive Reading Model\n(Cream), a novel neural architecture designed to enhance the language-image\nunderstanding capability of LLMs by capturing intricate details that are often\noverlooked in existing methods. Cream combines vision and auxiliary encoders,\nfortified by a contrastive feature alignment technique, to achieve a more\neffective comprehension of language information in visually situated contexts\nwithin the images. Our approach bridges the gap between vision and language\nunderstanding, paving the way for the development of more sophisticated\nDocument Intelligence Assistants. Through rigorous evaluations across diverse\nvisually-situated language understanding tasks that demand reasoning\ncapabilities, we demonstrate the compelling performance of Cream, positioning\nit as a prominent model in the field of visual document understanding. We\nprovide our codebase and newly-generated datasets at\nhttps://github.com/naver-ai/cream ."
            ],
            "interesting paper": 2
        },
        {
            "papers": [
                "Multimodal Recommendation Dialog with Subjective Preference: A New\n  Challenge and Benchmark\nExisting multimodal task-oriented dialog data fails to demonstrate the\ndiverse expressions of user subjective preferences and recommendation acts in\nthe real-life shopping scenario. This paper introduces a new dataset SURE\n(Multimodal Recommendation Dialog with SUbjective PREference), which contains\n12K shopping dialogs in complex store scenes. The data is built in two phases\nwith human annotations to ensure quality and diversity. SURE is well-annotated\nwith subjective preferences and recommendation acts proposed by sales experts.\nA comprehensive analysis is given to reveal the distinguishing features of\nSURE. Three benchmark tasks are then proposed on the data to evaluate the\ncapability of multimodal recommendation agents. Based on the SURE, we propose a\nbaseline model, powered by a state-of-the-art multimodal model, for these\ntasks.",
                "Enhancing Human Capabilities through Symbiotic Artificial Intelligence\n  with Shared Sensory Experiences\nThe merging of human intelligence and artificial intelligence has long been a\nsubject of interest in both science fiction and academia. In this paper, we\nintroduce a novel concept in Human-AI interaction called Symbiotic Artificial\nIntelligence with Shared Sensory Experiences (SAISSE), which aims to establish\na mutually beneficial relationship between AI systems and human users through\nshared sensory experiences. By integrating multiple sensory input channels and\nprocessing human experiences, SAISSE fosters a strong human-AI bond, enabling\nAI systems to learn from and adapt to individual users, providing personalized\nsupport, assistance, and enhancement. Furthermore, we discuss the incorporation\nof memory storage units for long-term growth and development of both the AI\nsystem and its human user. As we address user privacy and ethical guidelines\nfor responsible AI-human symbiosis, we also explore potential biases and\ninequalities in AI-human symbiosis and propose strategies to mitigate these\nchallenges. Our research aims to provide a comprehensive understanding of the\nSAISSE concept and its potential to effectively support and enhance individual\nhuman users through symbiotic AI systems. This position article aims at\ndiscussing poteintial AI-human interaction related topics within the scientific\ncommunity, rather than providing experimental or theoretical results.",
                "Explainability Techniques for Chemical Language Models\nExplainability techniques are crucial in gaining insights into the reasons\nbehind the predictions of deep learning models, which have not yet been applied\nto chemical language models. We propose an explainable AI technique that\nattributes the importance of individual atoms towards the predictions made by\nthese models. Our method backpropagates the relevance information towards the\nchemical input string and visualizes the importance of individual atoms. We\nfocus on self-attention Transformers operating on molecular string\nrepresentations and leverage a pretrained encoder for finetuning. We showcase\nthe method by predicting and visualizing solubility in water and organic\nsolvents. We achieve competitive model performance while obtaining\ninterpretable predictions, which we use to inspect the pretrained model.",
                "Mitigating Exploitation Bias in Learning to Rank with an\n  Uncertainty-aware Empirical Bayes Approach\nRanking is at the core of many artificial intelligence (AI) applications,\nincluding search engines, recommender systems, etc. Modern ranking systems are\noften constructed with learning-to-rank (LTR) models built from user behavior\nsignals. While previous studies have demonstrated the effectiveness of using\nuser behavior signals (e.g., clicks) as both features and labels of LTR\nalgorithms, we argue that existing LTR algorithms that indiscriminately treat\nbehavior and non-behavior signals in input features could lead to suboptimal\nperformance in practice. Particularly because user behavior signals often have\nstrong correlations with the ranking objective and can only be collected on\nitems that have already been shown to users, directly using behavior signals in\nLTR could create an exploitation bias that hurts the system performance in the\nlong run.\n  To address the exploitation bias, we propose EBRank, an empirical Bayes-based\nuncertainty-aware ranking algorithm. Specifically, to overcome exploitation\nbias brought by behavior features in ranking models, EBRank uses a sole\nnon-behavior feature based prior model to get a prior estimation of relevance.\nIn the dynamic training and serving of ranking systems, EBRank uses the\nobserved user behaviors to update posterior relevance estimation instead of\nconcatenating behaviors as features in ranking models. Besides, EBRank\nadditionally applies an uncertainty-aware exploration strategy to explore\nactively, collect user behaviors for empirical Bayesian modeling and improve\nranking performance. Experiments on three public datasets show that EBRank is\neffective, practical and significantly outperforms state-of-the-art ranking\nalgorithms.",
                "Multiview Identifiers Enhanced Generative Retrieval\nInstead of simply matching a query to pre-existing passages, generative\nretrieval generates identifier strings of passages as the retrieval target. At\na cost, the identifier must be distinctive enough to represent a passage.\nCurrent approaches use either a numeric ID or a text piece (such as a title or\nsubstrings) as the identifier. However, these identifiers cannot cover a\npassage's content well. As such, we are motivated to propose a new type of\nidentifier, synthetic identifiers, that are generated based on the content of a\npassage and could integrate contextualized information that text pieces lack.\nFurthermore, we simultaneously consider multiview identifiers, including\nsynthetic identifiers, titles, and substrings. These views of identifiers\ncomplement each other and facilitate the holistic ranking of passages from\nmultiple perspectives. We conduct a series of experiments on three public\ndatasets, and the results indicate that our proposed approach performs the best\nin generative retrieval, demonstrating its effectiveness and robustness.",
                "GenQ: Automated Question Generation to Support Caregivers While Reading\n  Stories with Children\nWhen caregivers ask open--ended questions to motivate dialogue with children,\nit facilitates the child's reading comprehension skills.Although there is scope\nfor use of technological tools, referred here as \"intelligent tutoring\nsystems\", to scaffold this process, it is currently unclear whether existing\nintelligent systems that generate human--language like questions is beneficial.\nAdditionally, training data used in the development of these automated question\ngeneration systems is typically sourced without attention to demographics, but\npeople with different cultural backgrounds may ask different questions. As a\npart of a broader project to design an intelligent reading support app for\nLatinx children, we crowdsourced questions from Latinx caregivers and\nnoncaregivers as well as caregivers and noncaregivers from other demographics.\nWe examine variations in question--asking within this dataset mediated by\nindividual, cultural, and contextual factors. We then design a system that\nautomatically extracts templates from this data to generate open--ended\nquestions that are representative of those asked by Latinx caregivers."
            ],
            "interesting paper": 1
        },
        {
            "papers": [
                "How Do UX Practitioners Communicate AI as a Design Material? Artifacts,\n  Conceptions, and Propositions\nUX practitioners (UXPs) face novel challenges when working with and\ncommunicating artificial intelligence (AI) as a design material. We explore how\nUXPs communicate AI concepts when given hands-on experience training and\nexperimenting with AI models. To do so, we conducted a task-based design study\nwith 27 UXPs in which they prototyped and created a design presentation for a\nAI-enabled interface while having access to a simple AI model training tool.\nThrough analyzing UXPs' design presentations and post-activity interviews, we\nfound that although UXPs struggled to clearly communicate some AI concepts,\ntinkering with AI broadened common ground when communicating with technical\nstakeholders. UXPs also identified key risks and benefits of AI in their\ndesigns, and proposed concrete next steps for both UX and AI work. We conclude\nwith a sensitizing concept and recommendations for design and AI tools to\nenhance multi-stakeholder communication and collaboration when crafting\nhuman-centered AI experiences.",
                "Towards Explainable Conversational Recommender Systems\nExplanations in conventional recommender systems have demonstrated benefits\nin helping the user understand the rationality of the recommendations and\nimproving the system's efficiency, transparency, and trustworthiness. In the\nconversational environment, multiple contextualized explanations need to be\ngenerated, which poses further challenges for explanations. To better measure\nexplainability in conversational recommender systems (CRS), we propose ten\nevaluation perspectives based on concepts from conventional recommender systems\ntogether with the characteristics of CRS. We assess five existing CRS benchmark\ndatasets using these metrics and observe the necessity of improving the\nexplanation quality of CRS. To achieve this, we conduct manual and automatic\napproaches to extend these dialogues and construct a new CRS dataset, namely\nExplainable Recommendation Dialogues (E-ReDial). It includes 756 dialogues with\nover 2,000 high-quality rewritten explanations. We compare two baseline\napproaches to perform explanation generation based on E-ReDial. Experimental\nresults suggest that models trained on E-ReDial can significantly improve\nexplainability while introducing knowledge into the models can further improve\nthe performance. GPT-3 in the in-context learning setting can generate more\nrealistic and diverse movie descriptions. In contrast, T5 training on E-ReDial\ncan better generate clear reasons for recommendations based on user\npreferences. E-ReDial is available at https://github.com/Superbooming/E-ReDial.",
                "Towards Cognitive Bots: Architectural Research Challenges\nSoftware bots operating in multiple virtual digital platforms must understand\nthe platforms' affordances and behave like human users. Platform affordances or\nfeatures differ from one application platform to another or through a life\ncycle, requiring such bots to be adaptable. Moreover, bots in such platforms\ncould cooperate with humans or other software agents for work or to learn\nspecific behavior patterns. However, present-day bots, particularly chatbots,\nother than language processing and prediction, are far from reaching a human\nuser's behavior level within complex business information systems. They lack\nthe cognitive capabilities to sense and act in such virtual environments,\nrendering their development a challenge to artificial general intelligence\nresearch. In this study, we problematize and investigate assumptions in\nconceptualizing software bot architecture by directing attention to significant\narchitectural research challenges in developing cognitive bots endowed with\ncomplex behavior for operation on information systems. As an outlook, we\npropose alternate architectural assumptions to consider in future bot design\nand bot development frameworks.",
                "BiomedGPT: A Generalist Vision-Language Foundation Model for Diverse\n  Biomedical Tasks\nTraditional biomedical artificial intelligence (AI) models, designed for\nspecific tasks or modalities, often exhibit limited flexibility in real-world\ndeployment and struggle to utilize holistic information. Generalist AI holds\nthe potential to address these limitations due to its versatility in\ninterpreting different data types and generating tailored outputs for diverse\nneeds. However, existing biomedical generalist AI solutions are typically\nheavyweight and closed source to researchers, practitioners, and patients.\nHere, we propose BiomedGPT, the first open-source and lightweight\nvision-language foundation model, designed as a generalist capable of\nperforming various biomedical tasks. BiomedGPT achieved state-of-the-art\nresults in 16 out of 25 experiments while maintaining a computing-friendly\nmodel scale. We also conducted human evaluations to assess the capabilities of\nBiomedGPT in radiology visual question answering, report generation, and\nsummarization. BiomedGPT exhibits robust prediction ability with a low error\nrate of 3.8% in question answering, satisfactory performance with an error rate\nof 8.3% in writing complex radiology reports, and competitive summarization\nability with a nearly equivalent preference score to human experts. Our method\ndemonstrates that effective training with diverse data can lead to more\npractical biomedical AI for improving diagnosis and workflow efficiency.",
                "Training Socially Aligned Language Models on Simulated Social\n  Interactions\nSocial alignment in AI systems aims to ensure that these models behave\naccording to established societal values. However, unlike humans, who derive\nconsensus on value judgments through social interaction, current language\nmodels (LMs) are trained to rigidly replicate their training corpus in\nisolation, leading to subpar generalization in unfamiliar scenarios and\nvulnerability to adversarial attacks. This work presents a novel training\nparadigm that permits LMs to learn from simulated social interactions. In\ncomparison to existing methodologies, our approach is considerably more\nscalable and efficient, demonstrating superior performance in alignment\nbenchmarks and human evaluations. This paradigm shift in the training of LMs\nbrings us a step closer to developing AI systems that can robustly and\naccurately reflect societal norms and values.",
                "Justification vs. Transparency: Why and How Visual Explanations in a\n  Scientific Literature Recommender System\nSignificant attention has been paid to enhancing recommender systems (RS)\nwith explanation facilities to help users make informed decisions and increase\ntrust in and satisfaction with the RS. Justification and transparency represent\ntwo crucial goals in explainable recommendation. Different from transparency,\nwhich faithfully exposes the reasoning behind the recommendation mechanism,\njustification conveys a conceptual model that may differ from that of the\nunderlying algorithm. An explanation is an answer to a question. In explainable\nrecommendation, a user would want to ask questions (referred to as\nintelligibility types) to understand results given by the RS. In this paper, we\nidentify relationships between Why and How explanation intelligibility types\nand the explanation goals of justification and transparency. We followed the\nHuman-Centered Design (HCD) approach and leveraged the What-Why-How\nvisualization framework to systematically design and implement Why and How\nvisual explanations in the transparent Recommendation and Interest Modeling\nApplication (RIMA). Furthermore, we conducted a qualitative user study (N=12)\nto investigate the potential effects of providing Why and How explanations\ntogether in an explainable RS on the users' perceptions regarding transparency,\ntrust, and satisfaction. Our study showed qualitative evidence confirming that\nthe choice of the explanation intelligibility types depends on the explanation\ngoal and user type."
            ],
            "interesting paper": 2
        },
        {
            "papers": [
                "AI Coach Assist: An Automated Approach for Call Recommendation in\n  Contact Centers for Agent Coaching\nIn recent years, the utilization of Artificial Intelligence (AI) in the\ncontact center industry is on the rise. One area where AI can have a\nsignificant impact is in the coaching of contact center agents. By analyzing\ncall transcripts using Natural Language Processing (NLP) techniques, it would\nbe possible to quickly determine which calls are most relevant for coaching\npurposes. In this paper, we present AI Coach Assist, which leverages the\npre-trained transformer-based language models to determine whether a given call\nis coachable or not based on the quality assurance (QA) questions asked by the\ncontact center managers or supervisors. The system was trained and evaluated on\na large dataset collected from real-world contact centers and provides an\neffective way to recommend calls to the contact center managers that are more\nlikely to contain coachable moments. Our experimental findings demonstrate the\npotential of AI Coach Assist to improve the coaching process, resulting in\nenhancing the performance of contact center agents.",
                "Data Minimization at Inference Time\nIn domains with high stakes such as law, recruitment, and healthcare,\nlearning models frequently rely on sensitive user data for inference,\nnecessitating the complete set of features. This not only poses significant\nprivacy risks for individuals but also demands substantial human effort from\norganizations to verify information accuracy. This paper asks whether it is\nnecessary to use \\emph{all} input features for accurate predictions at\ninference time. The paper demonstrates that, in a personalized setting,\nindividuals may only need to disclose a small subset of their features without\ncompromising decision-making accuracy. The paper also provides an efficient\nsequential algorithm to determine the appropriate attributes for each\nindividual to provide. Evaluations across various learning tasks show that\nindividuals can potentially report as little as 10\\% of their information while\nmaintaining the same accuracy level as a model that employs the full set of\nuser information.",
                "Counterfactual Formulation of Patient-Specific Root Causes of Disease\nRoot causes of disease intuitively correspond to root vertices that increase\nthe likelihood of a diagnosis. This description of a root cause nevertheless\nlacks the rigorous mathematical formulation needed for the development of\ncomputer algorithms designed to automatically detect root causes from data.\nPrior work defined patient-specific root causes of disease using an\ninterventionalist account that only climbs to the second rung of Pearl's Ladder\nof Causation. In this theoretical piece, we climb to the third rung by\nproposing a counterfactual definition matching clinical intuition based on\nfixed factual data alone. We then show how to assign a root causal contribution\nscore to each variable using Shapley values from explainable artificial\nintelligence. The proposed counterfactual formulation of patient-specific root\ncauses of disease accounts for noisy labels, adapts to disease prevalence and\nadmits fast computation without the need for counterfactual simulation.",
                "Synthesizing a Progression of Subtasks for Block-Based Visual\n  Programming Tasks\nBlock-based visual programming environments play an increasingly important\nrole in introducing computing concepts to K-12 students. In recent years, they\nhave also gained popularity in neuro-symbolic AI, serving as a benchmark to\nevaluate general problem-solving and logical reasoning skills. The open-ended\nand conceptual nature of these visual programming tasks make them challenging,\nboth for state-of-the-art AI agents as well as for novice programmers. A\nnatural approach to providing assistance for problem-solving is breaking down a\ncomplex task into a progression of simpler subtasks; however, this is not\ntrivial given that the solution codes are typically nested and have non-linear\nexecution behavior. In this paper, we formalize the problem of synthesizing\nsuch a progression for a given reference block-based visual programming task.\nWe propose a novel synthesis algorithm that generates a progression of subtasks\nthat are high-quality, well-spaced in terms of their complexity, and solving\nthis progression leads to solving the reference task. We show the utility of\nour synthesis algorithm in improving the efficacy of AI agents (in this case,\nneural program synthesizers) for solving tasks in the Karel programming\nenvironment. Then, we conduct a user study to demonstrate that our synthesized\nprogression of subtasks can assist a novice programmer in solving tasks in the\nHour of Code: Maze Challenge by Code-dot-org.",
                "Modeling Dynamic Environments with Scene Graph Memory\nEmbodied AI agents that search for objects in large environments such as\nhouseholds often need to make efficient decisions by predicting object\nlocations based on partial information. We pose this as a new type of link\nprediction problem: link prediction on partially observable dynamic graphs. Our\ngraph is a representation of a scene in which rooms and objects are nodes, and\ntheir relationships are encoded in the edges; only parts of the changing graph\nare known to the agent at each timestep. This partial observability poses a\nchallenge to existing link prediction approaches, which we address. We propose\na novel state representation -- Scene Graph Memory (SGM) -- with captures the\nagent's accumulated set of observations, as well as a neural net architecture\ncalled a Node Edge Predictor (NEP) that extracts information from the SGM to\nsearch efficiently. We evaluate our method in the Dynamic House Simulator, a\nnew benchmark that creates diverse dynamic graphs following the semantic\npatterns typically seen at homes, and show that NEP can be trained to predict\nthe locations of objects in a variety of environments with diverse object\nmovement dynamics, outperforming baselines both in terms of new scene\nadaptability and overall accuracy. The codebase and more can be found at\nhttps://www.scenegraphmemory.com.",
                "The Curse of Recursion: Training on Generated Data Makes Models Forget\nStable Diffusion revolutionised image creation from descriptive text. GPT-2,\nGPT-3(.5) and GPT-4 demonstrated astonishing performance across a variety of\nlanguage tasks. ChatGPT introduced such language models to the general public.\nIt is now clear that large language models (LLMs) are here to stay, and will\nbring about drastic change in the whole ecosystem of online text and images. In\nthis paper we consider what the future might hold. What will happen to GPT-{n}\nonce LLMs contribute much of the language found online? We find that use of\nmodel-generated content in training causes irreversible defects in the\nresulting models, where tails of the original content distribution disappear.\nWe refer to this effect as Model Collapse and show that it can occur in\nVariational Autoencoders, Gaussian Mixture Models and LLMs. We build\ntheoretical intuition behind the phenomenon and portray its ubiquity amongst\nall learned generative models. We demonstrate that it has to be taken seriously\nif we are to sustain the benefits of training from large-scale data scraped\nfrom the web. Indeed, the value of data collected about genuine human\ninteractions with systems will be increasingly valuable in the presence of\ncontent generated by LLMs in data crawled from the Internet."
            ],
            "interesting paper": 2
        },
        {
            "papers": [
                "Ask an Expert: Leveraging Language Models to Improve Strategic Reasoning\n  in Goal-Oriented Dialogue Models\nExisting dialogue models may encounter scenarios which are not\nwell-represented in the training data, and as a result generate responses that\nare unnatural, inappropriate, or unhelpful. We propose the \"Ask an Expert\"\nframework in which the model is trained with access to an \"expert\" which it can\nconsult at each turn. Advice is solicited via a structured dialogue with the\nexpert, and the model is optimized to selectively utilize (or ignore) it given\nthe context and dialogue history. In this work the expert takes the form of an\nLLM. We evaluate this framework in a mental health support domain, where the\nstructure of the expert conversation is outlined by pre-specified prompts which\nreflect a reasoning strategy taught to practitioners in the field. Blenderbot\nmodels utilizing \"Ask an Expert\" show quality improvements across all expert\nsizes, including those with fewer parameters than the dialogue model itself.\nOur best model provides a $\\sim 10\\%$ improvement over baselines, approaching\nhuman-level scores on \"engingingness\" and \"helpfulness\" metrics.",
                "Taming AI Bots: Controllability of Neural States in Large Language\n  Models\nWe tackle the question of whether an agent can, by suitable choice of\nprompts, control an AI bot to any state. To that end, we first introduce a\nformal definition of ``meaning'' that is amenable to analysis. Then, we\ncharacterize ``meaningful data'' on which large language models (LLMs) are\nostensibly trained, and ``well-trained LLMs'' through conditions that are\nlargely met by today's LLMs. While a well-trained LLM constructs an embedding\nspace of meanings that is Euclidean, meanings themselves do not form a vector\n(linear) subspace, but rather a quotient space within. We then characterize the\nsubset of meanings that can be reached by the state of the LLMs for some input\nprompt, and show that a well-trained bot can reach any meaning albeit with\nsmall probability. We then introduce a stronger notion of controllability as\n{\\em almost certain reachability}, and show that, when restricted to the space\nof meanings, an AI bot is controllable. We do so after introducing a functional\ncharacterization of attentive AI bots, and finally derive necessary and\nsufficient conditions for controllability. The fact that AI bots are\ncontrollable means that an adversary could steer them towards any state.\nHowever, the sampling process can be designed to counteract adverse actions and\navoid reaching undesirable regions of state space before their boundary is\ncrossed.",
                "Re-imagining health and well-being in low resource African settings\n  using an augmented AI system and a 3D digital twin\nThis paper discusses and explores the potential and relevance of recent\ndevelopments in artificial intelligence (AI) and digital twins for health and\nwell-being in low-resource African countries. We use the case of public health\nemergency response to disease outbreaks and epidemic control. There is\npotential to take advantage of the increasing availability of data and\ndigitization to develop advanced AI methods for analysis and prediction. Using\nan AI systems perspective, we review emerging trends in AI systems and digital\ntwins and propose an initial augmented AI system architecture to illustrate how\nan AI system can work with a 3D digital twin to address public health goals. We\nhighlight scientific knowledge discovery, continual learning, pragmatic\ninteroperability, and interactive explanation and decision-making as essential\nresearch challenges for AI systems and digital twins.",
                "Sequential Condition Evolved Interaction Knowledge Graph for Traditional\n  Chinese Medicine Recommendation\nTraditional Chinese Medicine (TCM) has a rich history of utilizing natural\nherbs to treat a diversity of illnesses. In practice, TCM diagnosis and\ntreatment are highly personalized and organically holistic, requiring\ncomprehensive consideration of the patient's state and symptoms over time.\nHowever, existing TCM recommendation approaches overlook the changes in patient\nstatus and only explore potential patterns between symptoms and prescriptions.\nIn this paper, we propose a novel Sequential Condition Evolved Interaction\nKnowledge Graph (SCEIKG), a framework that treats the model as a sequential\nprescription-making problem by considering the dynamics of the patient's\ncondition across multiple visits. In addition, we incorporate an interaction\nknowledge graph to enhance the accuracy of recommendations by considering the\ninteractions between different herbs and the patient's condition. Experimental\nresults on a real-world dataset demonstrate that our approach outperforms\nexisting TCM recommendation methods, achieving state-of-the-art performance.",
                "ContrastNER: Contrastive-based Prompt Tuning for Few-shot NER\nPrompt-based language models have produced encouraging results in numerous\napplications, including Named Entity Recognition (NER) tasks. NER aims to\nidentify entities in a sentence and provide their types. However, the strong\nperformance of most available NER approaches is heavily dependent on the design\nof discrete prompts and a verbalizer to map the model-predicted outputs to\nentity categories, which are complicated undertakings. To address these\nchallenges, we present ContrastNER, a prompt-based NER framework that employs\nboth discrete and continuous tokens in prompts and uses a contrastive learning\napproach to learn the continuous prompts and forecast entity types. The\nexperimental results demonstrate that ContrastNER obtains competitive\nperformance to the state-of-the-art NER methods in high-resource settings and\noutperforms the state-of-the-art models in low-resource circumstances without\nrequiring extensive manual prompt engineering and verbalizer design.",
                "Key-Value Transformer\nTransformers have emerged as the prevailing standard solution for various AI\ntasks, including computer vision and natural language processing. The widely\nadopted Query, Key, and Value formulation (QKV) has played a significant role\nin this. Nevertheless, no research has examined the essentiality of these three\ncomponents for transformer performance. Therefore, we conducted an evaluation\nof the key-value formulation (KV), which generates symmetric attention maps,\nalong with an asymmetric version that incorporates a 2D positional encoding\ninto the attention matrix. Remarkably, this transformer requires fewer\nparameters and computation than the original one. Through experiments\nencompassing three task types -- synthetics (such as reversing or sorting a\nlist), vision (mnist or cifar classification), and NLP (character generation\nand translation) -- we discovered that the KV transformer occasionally\noutperforms the QKV transformer. However, it also exhibits instances of\nunderperformance compared to QKV, making it challenging to draw a definitive\nconclusion. Nonetheless, we consider the reported results to be encouraging and\nanticipate that they may pave the way for more efficient transformers in the\nfuture."
            ],
            "interesting paper": 1
        },
        {
            "papers": [
                "Partially Personalized Federated Learning: Breaking the Curse of Data\n  Heterogeneity\nWe present a partially personalized formulation of Federated Learning (FL)\nthat strikes a balance between the flexibility of personalization and\ncooperativeness of global training. In our framework, we split the variables\ninto global parameters, which are shared across all clients, and individual\nlocal parameters, which are kept private. We prove that under the right split\nof parameters, it is possible to find global parameters that allow each client\nto fit their data perfectly, and refer to the obtained problem as\noverpersonalized. For instance, the shared global parameters can be used to\nlearn good data representations, whereas the personalized layers are fine-tuned\nfor a specific client. Moreover, we present a simple algorithm for the\npartially personalized formulation that offers significant benefits to all\nclients. In particular, it breaks the curse of data heterogeneity in several\nsettings, such as training with local steps, asynchronous training, and\nByzantine-robust training.",
                "An Annotated Dataset for Explainable Interpersonal Risk Factors of\n  Mental Disturbance in Social Media Posts\nWith a surge in identifying suicidal risk and its severity in social media\nposts, we argue that a more consequential and explainable research is required\nfor optimal impact on clinical psychology practice and personalized mental\nhealthcare. The success of computational intelligence techniques for inferring\nmental illness from social media resources, points to natural language\nprocessing as a lens for determining Interpersonal Risk Factors (IRF) in human\nwritings. Motivated with limited availability of datasets for social NLP\nresearch community, we construct and release a new annotated dataset with\nhuman-labelled explanations and classification of IRF affecting mental\ndisturbance on social media: (i) Thwarted Belongingness (TBe), and (ii)\nPerceived Burdensomeness (PBu). We establish baseline models on our dataset\nfacilitating future research directions to develop real-time personalized AI\nmodels by detecting patterns of TBe and PBu in emotional spectrum of user's\nhistorical social media profile.",
                "Contextual Bandits with Budgeted Information Reveal\nContextual bandit algorithms are commonly used in digital health to recommend\npersonalized treatments. However, to ensure the effectiveness of the\ntreatments, patients are often requested to take actions that have no immediate\nbenefit to them, which we refer to as pro-treatment actions. In practice,\nclinicians have a limited budget to encourage patients to take these actions\nand collect additional information. We introduce a novel optimization and\nlearning algorithm to address this problem. This algorithm effectively combines\nthe strengths of two algorithmic approaches in a seamless manner, including 1)\nan online primal-dual algorithm for deciding the optimal timing to reach out to\npatients, and 2) a contextual bandit learning algorithm to deliver personalized\ntreatment to the patient. We prove that this algorithm admits a sub-linear\nregret bound. We illustrate the usefulness of this algorithm on both synthetic\nand real-world data.",
                "Perceived Trustworthiness of Natural Language Generators\nNatural Language Generation tools, such as chatbots that can generate\nhuman-like conversational text, are becoming more common both for personal and\nprofessional use. However, there are concerns about their trustworthiness and\nethical implications. The paper addresses the problem of understanding how\ndifferent users (e.g., linguists, engineers) perceive and adopt these tools and\ntheir perception of machine-generated text quality. It also discusses the\nperceived advantages and limitations of Natural Language Generation tools, as\nwell as users' beliefs on governance strategies. The main findings of this\nstudy include the impact of users' field and level of expertise on the\nperceived trust and adoption of Natural Language Generation tools, the users'\nassessment of the accuracy, fluency, and potential biases of machine-generated\ntext in comparison to human-written text, and an analysis of the advantages and\nethical risks associated with these tools as identified by the participants.\nMoreover, this paper discusses the potential implications of these findings for\nenhancing the AI development process. The paper sheds light on how different\nuser characteristics shape their beliefs on the quality and overall\ntrustworthiness of machine-generated text. Furthermore, it examines the\nbenefits and risks of these tools from the perspectives of different users.",
                "Code Prompting: a Neural Symbolic Method for Complex Reasoning in Large\n  Language Models\nLarge language models (LLMs) have scaled up to unlock a wide range of complex\nreasoning tasks with the aid of various prompting methods. However, current\nprompting methods generate natural language intermediate steps to help\nreasoning, which can cause imperfect task reduction and confusion. To mitigate\nsuch limitations, we explore code prompting, a neural symbolic prompting method\nwith both zero-shot and few-shot versions which triggers code as intermediate\nsteps. We conduct experiments on 7 widely-used benchmarks involving symbolic\nreasoning and arithmetic reasoning. Code prompting generally outperforms\nchain-of-thought (CoT) prompting. To further understand the performance and\nlimitations of code prompting, we perform extensive ablation studies and error\nanalyses, and identify several exclusive advantages of using symbolic\npromptings compared to natural language. We also consider the ensemble of code\nprompting and CoT prompting to combine the strengths of both. Finally, we show\nthrough experiments how code annotations and their locations affect code\nprompting.",
                "Shapley Based Residual Decomposition for Instance Analysis\nIn this paper, we introduce the idea of decomposing the residuals of\nregression with respect to the data instances instead of features. This allows\nus to determine the effects of each individual instance on the model and each\nother, and in doing so makes for a model-agnostic method of identifying\ninstances of interest. In doing so, we can also determine the appropriateness\nof the model and data in the wider context of a given study. The paper focuses\non the possible applications that such a framework brings to the relatively\nunexplored field of instance analysis in the context of Explainable AI tasks."
            ],
            "interesting paper": 1
        },
        {
            "papers": [
                "Less Likely Brainstorming: Using Language Models to Generate Alternative\n  Hypotheses\nA human decision-maker benefits the most from an AI assistant that corrects\nfor their biases. For problems such as generating interpretation of a radiology\nreport given findings, a system predicting only highly likely outcomes may be\nless useful, where such outcomes are already obvious to the user. To alleviate\nbiases in human decision-making, it is worth considering a broad differential\ndiagnosis, going beyond the most likely options. We introduce a new task, \"less\nlikely brainstorming,\" that asks a model to generate outputs that humans think\nare relevant but less likely to happen. We explore the task in two settings: a\nbrain MRI interpretation generation setting and an everyday commonsense\nreasoning setting. We found that a baseline approach of training with less\nlikely hypotheses as targets generates outputs that humans evaluate as either\nlikely or irrelevant nearly half of the time; standard MLE training is not\neffective. To tackle this problem, we propose a controlled text generation\nmethod that uses a novel contrastive learning strategy to encourage models to\ndifferentiate between generating likely and less likely outputs according to\nhumans. We compare our method with several state-of-the-art controlled text\ngeneration models via automatic and human evaluations and show that our models'\ncapability of generating less likely outputs is improved.",
                "Conceptual Design Generation Using Large Language Models\nConcept generation is a creative step in the conceptual design phase, where\ndesigners often turn to brainstorming, mindmapping, or crowdsourcing design\nideas to complement their own knowledge of the domain. Recent advances in\nnatural language processing (NLP) and machine learning (ML) have led to the\nrise of Large Language Models (LLMs) capable of generating seemingly creative\noutputs from textual prompts. The success of these models has led to their\nintegration and application across a variety of domains, including art,\nentertainment, and other creative work. In this paper, we leverage LLMs to\ngenerate solutions for a set of 12 design problems and compare them to a\nbaseline of crowdsourced solutions. We evaluate the differences between\ngenerated and crowdsourced design solutions through multiple perspectives,\nincluding human expert evaluations and computational metrics. Expert\nevaluations indicate that the LLM-generated solutions have higher average\nfeasibility and usefulness while the crowdsourced solutions have more novelty.\nWe experiment with prompt engineering and find that leveraging few-shot\nlearning can lead to the generation of solutions that are more similar to the\ncrowdsourced solutions. These findings provide insight into the quality of\ndesign solutions generated with LLMs and begins to evaluate prompt engineering\ntechniques that could be leveraged by practitioners to generate higher-quality\ndesign solutions synergistically with LLMs.",
                "Probabilistic computation and uncertainty quantification with emerging\n  covariance\nBuilding robust, interpretable, and secure AI system requires quantifying and\nrepresenting uncertainty under a probabilistic perspective to mimic human\ncognitive abilities. However, probabilistic computation presents significant\nchallenges for most conventional artificial neural network, as they are\nessentially implemented in a deterministic manner. In this paper, we develop an\nefficient probabilistic computation framework by truncating the probabilistic\nrepresentation of neural activation up to its mean and covariance and construct\na moment neural network that encapsulates the nonlinear coupling between the\nmean and covariance of the underlying stochastic network. We reveal that when\nonly the mean but not the covariance is supervised during gradient-based\nlearning, the unsupervised covariance spontaneously emerges from its nonlinear\ncoupling with the mean and faithfully captures the uncertainty associated with\nmodel predictions. Our findings highlight the inherent simplicity of\nprobabilistic computation by seamlessly incorporating uncertainty into model\nprediction, paving the way for integrating it into large-scale AI systems.",
                "Intent-aligned AI systems deplete human agency: the need for agency\n  foundations research in AI safety\nThe rapid advancement of artificial intelligence (AI) systems suggests that\nartificial general intelligence (AGI) systems may soon arrive. Many researchers\nare concerned that AIs and AGIs will harm humans via intentional misuse\n(AI-misuse) or through accidents (AI-accidents). In respect of AI-accidents,\nthere is an increasing effort focused on developing algorithms and paradigms\nthat ensure AI systems are aligned to what humans intend, e.g. AI systems that\nyield actions or recommendations that humans might judge as consistent with\ntheir intentions and goals. Here we argue that alignment to human intent is\ninsufficient for safe AI systems and that preservation of long-term agency of\nhumans may be a more robust standard, and one that needs to be separated\nexplicitly and a priori during optimization. We argue that AI systems can\nreshape human intention and discuss the lack of biological and psychological\nmechanisms that protect humans from loss of agency. We provide the first formal\ndefinition of agency-preserving AI-human interactions which focuses on\nforward-looking agency evaluations and argue that AI systems - not humans -\nmust be increasingly tasked with making these evaluations. We show how agency\nloss can occur in simple environments containing embedded agents that use\ntemporal-difference learning to make action recommendations. Finally, we\npropose a new area of research called \"agency foundations\" and pose four\ninitial topics designed to improve our understanding of agency in AI-human\ninteractions: benevolent game theory, algorithmic foundations of human rights,\nmechanistic interpretability of agency representation in neural-networks and\nreinforcement learning from internal states.",
                "Unsupervised Melody-to-Lyric Generation\nAutomatic melody-to-lyric generation is a task in which song lyrics are\ngenerated to go with a given melody. It is of significant practical interest\nand more challenging than unconstrained lyric generation as the music imposes\nadditional constraints onto the lyrics. The training data is limited as most\nsongs are copyrighted, resulting in models that underfit the complicated\ncross-modal relationship between melody and lyrics. In this work, we propose a\nmethod for generating high-quality lyrics without training on any aligned\nmelody-lyric data. Specifically, we design a hierarchical lyric generation\nframework that first generates a song outline and second the complete lyrics.\nThe framework enables disentanglement of training (based purely on text) from\ninference (melody-guided text generation) to circumvent the shortage of\nparallel data.\n  We leverage the segmentation and rhythm alignment between melody and lyrics\nto compile the given melody into decoding constraints as guidance during\ninference. The two-step hierarchical design also enables content control via\nthe lyric outline, a much-desired feature for democratizing collaborative song\ncreation. Experimental results show that our model can generate high-quality\nlyrics that are more on-topic, singable, intelligible, and coherent than strong\nbaselines, for example SongMASS, a SOTA model trained on a parallel dataset,\nwith a 24% relative overall quality improvement based on human ratings.",
                "Towards Explainable and Language-Agnostic LLMs: Symbolic Reverse\n  Engineering of Language at Scale\nLarge language models (LLMs) have achieved a milestone that undenia-bly\nchanged many held beliefs in artificial intelligence (AI). However, there\nremains many limitations of these LLMs when it comes to true language\nunderstanding, limitations that are a byproduct of the under-lying architecture\nof deep neural networks. Moreover, and due to their subsymbolic nature,\nwhatever knowledge these models acquire about how language works will always be\nburied in billions of microfeatures (weights), none of which is meaningful on\nits own, making such models hopelessly unexplainable. To address these\nlimitations, we suggest com-bining the strength of symbolic representations\nwith what we believe to be the key to the success of LLMs, namely a successful\nbottom-up re-verse engineering of language at scale. As such we argue for a\nbottom-up reverse engineering of language in a symbolic setting. Hints on what\nthis project amounts to have been suggested by several authors, and we discuss\nin some detail here how this project could be accomplished."
            ],
            "interesting paper": 2
        },
        {
            "papers": [
                "Decision-Oriented Dialogue for Human-AI Collaboration\nWe describe a class of tasks called decision-oriented dialogues, in which AI\nassistants such as large language models (LMs) must collaborate with one or\nmore humans via natural language to help them make complex decisions. We\nformalize three domains in which users face everyday decisions: (1) choosing an\nassignment of reviewers to conference papers, (2) planning a multi-step\nitinerary in a city, and (3) negotiating travel plans for a group of friends.\nIn each of these settings, AI assistants and users have disparate abilities\nthat they must combine to arrive at the best decision: assistants can access\nand process large amounts of information, while users have preferences and\nconstraints external to the system. For each task, we build a dialogue\nenvironment where agents receive a reward based on the quality of the final\ndecision they reach. We evaluate LMs in self-play and in collaboration with\nhumans and find that they fall short compared to human assistants, achieving\nmuch lower rewards despite engaging in longer dialogues. We highlight a number\nof challenges models face in decision-oriented dialogues, ranging from\ngoal-directed behavior to reasoning and optimization, and release our\nenvironments as a testbed for future work.",
                "Speaking the Language of Your Listener: Audience-Aware Adaptation via\n  Plug-and-Play Theory of Mind\nDialogue participants may have varying levels of knowledge about the topic\nunder discussion. In such cases, it is essential for speakers to adapt their\nutterances by taking their audience into account. Yet, it is an open question\nhow such adaptation can be modelled in computational agents. In this paper, we\nmodel a visually grounded referential game between a knowledgeable speaker and\na listener with more limited visual and linguistic experience. Inspired by\npsycholinguistic theories, we endow our speaker with the ability to adapt its\nreferring expressions via a simulation module that monitors the effectiveness\nof planned utterances from the listener's perspective. We propose an adaptation\nmechanism building on plug-and-play approaches to controlled language\ngeneration, where utterance generation is steered on the fly by the simulator\nwithout finetuning the speaker's underlying language model. Our results and\nanalyses show that our approach is effective: the speaker's utterances become\ncloser to the listener's domain of expertise, which leads to higher\ncommunicative success.",
                "TransAct: Transformer-based Realtime User Action Model for\n  Recommendation at Pinterest\nSequential models that encode user activity for next action prediction have\nbecome a popular design choice for building web-scale personalized\nrecommendation systems. Traditional methods of sequential recommendation either\nutilize end-to-end learning on realtime user actions, or learn user\nrepresentations separately in an offline batch-generated manner. This paper (1)\npresents Pinterest's ranking architecture for Homefeed, our personalized\nrecommendation product and the largest engagement surface; (2) proposes\nTransAct, a sequential model that extracts users' short-term preferences from\ntheir realtime activities; (3) describes our hybrid approach to ranking, which\ncombines end-to-end sequential modeling via TransAct with batch-generated user\nembeddings. The hybrid approach allows us to combine the advantages of\nresponsiveness from learning directly on realtime user activity with the\ncost-effectiveness of batch user representations learned over a longer time\nperiod. We describe the results of ablation studies, the challenges we faced\nduring productionization, and the outcome of an online A/B experiment, which\nvalidates the effectiveness of our hybrid ranking model. We further demonstrate\nthe effectiveness of TransAct on other surfaces such as contextual\nrecommendations and search. Our model has been deployed to production in\nHomefeed, Related Pins, Notifications, and Search at Pinterest.",
                "AI Imagery and the Overton Window\nAI-based text-to-image generation has undergone a significant leap in the\nproduction of visually comprehensive and aesthetic imagery over the past year,\nto the point where differentiating between a man-made piece of art and an\nAI-generated image is becoming more difficult. Generative Models such as Stable\nDiffusion, Midjourney and others are expected to affect several major\nindustries in technological and ethical aspects. Striking the balance between\nraising human standard of life and work vs exploiting one group of people to\nenrich another is a complex and crucial part of the discussion. Due to the\nrapid growth of this technology, the way in which its models operate, and gray\narea legalities, visual and artistic domains - including the video game\nindustry, are at risk of being taken over from creators by AI infrastructure\nowners. This paper is a literature review examining the concerns facing both AI\ndevelopers and users today, including identity theft, data laundering and more.\nIt discusses legalization challenges and ethical concerns, and concludes with\nhow AI generative models can be tremendously useful in streamlining the process\nof visual creativity in both static and interactive media given proper\nregulation.\n  Keywords: AI text-to-image generation, Midjourney, Stable Diffusion, AI\nEthics, Game Design, Digital Art, Data Laundering",
                "Human Control: Definitions and Algorithms\nHow can humans stay in control of advanced artificial intelligence systems?\nOne proposal is corrigibility, which requires the agent to follow the\ninstructions of a human overseer, without inappropriately influencing them. In\nthis paper, we formally define a variant of corrigibility called shutdown\ninstructability, and show that it implies appropriate shutdown behavior,\nretention of human autonomy, and avoidance of user harm. We also analyse the\nrelated concepts of non-obstruction and shutdown alignment, three previously\nproposed algorithms for human control, and one new algorithm.",
                "Enrichment of the NLST and NSCLC-Radiomics computed tomography\n  collections with AI-derived annotations\nPublic imaging datasets are critical for the development and evaluation of\nautomated tools in cancer imaging. Unfortunately, many do not include\nannotations or image-derived features, complicating their downstream analysis.\nArtificial intelligence-based annotation tools have been shown to achieve\nacceptable performance and thus can be used to automatically annotate large\ndatasets. As part of the effort to enrich public data available within NCI\nImaging Data Commons (IDC), here we introduce AI-generated annotations for two\ncollections of computed tomography images of the chest, NSCLC-Radiomics, and\nthe National Lung Screening Trial. Using publicly available AI algorithms we\nderived volumetric annotations of thoracic organs at risk, their corresponding\nradiomics features, and slice-level annotations of anatomical landmarks and\nregions. The resulting annotations are publicly available within IDC, where the\nDICOM format is used to harmonize the data and achieve FAIR principles. The\nannotations are accompanied by cloud-enabled notebooks demonstrating their use.\nThis study reinforces the need for large, publicly accessible curated datasets\nand demonstrates how AI can be used to aid in cancer imaging."
            ],
            "interesting paper": 3
        },
        {
            "papers": [
                "AI Liability Insurance With an Example in AI-Powered E-diagnosis System\nArtificial Intelligence (AI) has received an increasing amount of attention\nin multiple areas. The uncertainties and risks in AI-powered systems have\ncreated reluctance in their wild adoption. As an economic solution to\ncompensate for potential damages, AI liability insurance is a promising market\nto enhance the integration of AI into daily life. In this work, we use an\nAI-powered E-diagnosis system as an example to study AI liability insurance. We\nprovide a quantitative risk assessment model with evidence-based numerical\nanalysis. We discuss the insurability criteria for AI technologies and suggest\nnecessary adjustments to accommodate the features of AI products. We show that\nAI liability insurance can act as a regulatory mechanism to incentivize\ncompliant behaviors and serve as a certificate of high-quality AI systems.\nFurthermore, we suggest premium adjustment to reflect the dynamic evolution of\nthe inherent uncertainty in AI. Moral hazard problems are discussed and\nsuggestions for AI liability insurance are provided.",
                "Self Contrastive Learning for Session-based Recommendation\nSession-based recommendation, which aims to predict the next item of users'\ninterest as per an existing sequence interaction of items, has attracted\ngrowing applications of Contrastive Learning (CL) with improved user and item\nrepresentations. However, these contrastive objectives: (1) serve a similar\nrole as the cross-entropy loss while ignoring the item representation space\noptimisation; and (2) commonly require complicated modelling, including complex\npositive/negative sample constructions and extra data augmentation. In this\nwork, we introduce Self-Contrastive Learning (SCL), which simplifies the\napplication of CL and enhances the performance of state-of-the-art CL-based\nrecommendation techniques. Specifically, SCL is formulated as an objective\nfunction that directly promotes a uniform distribution among item\nrepresentations and efficiently replaces all the existing contrastive objective\ncomponents of state-of-the-art models. Unlike previous works, SCL eliminates\nthe need for any positive/negative sample construction or data augmentation,\nleading to enhanced interpretability of the item representation space and\nfacilitating its extensibility to existing recommender systems. Through\nexperiments on three benchmark datasets, we demonstrate that SCL consistently\nimproves the performance of state-of-the-art models with statistical\nsignificance. Notably, our experiments show that SCL improves the performance\nof two best-performing models by 8.2% and 9.5% in P@10 (Precision) and 9.9% and\n11.2% in MRR@10 (Mean Reciprocal Rank) on average across different benchmarks.\nAdditionally, our analysis elucidates the improvement in terms of alignment and\nuniformity of representations, as well as the effectiveness of SCL with a low\ncomputational cost.",
                "Cook-Gen: Robust Generative Modeling of Cooking Actions from Recipes\nAs people become more aware of their food choices, food computation models\nhave become increasingly popular in assisting people in maintaining healthy\neating habits. For example, food recommendation systems analyze recipe\ninstructions to assess nutritional contents and provide recipe recommendations.\nThe recent and remarkable successes of generative AI methods, such as\nauto-regressive large language models, can lead to robust methods for a more\ncomprehensive understanding of recipes for healthy food recommendations beyond\nsurface-level nutrition content assessments. In this study, we explore the use\nof generative AI methods to extend current food computation models, primarily\ninvolving the analysis of nutrition and ingredients, to also incorporate\ncooking actions (e.g., add salt, fry the meat, boil the vegetables, etc.).\nCooking actions are notoriously hard to model using statistical learning\nmethods due to irregular data patterns - significantly varying natural language\ndescriptions for the same action (e.g., marinate the meat vs. marinate the meat\nand leave overnight) and infrequently occurring patterns (e.g., add salt occurs\nfar more frequently than marinating the meat). The prototypical approach to\nhandling irregular data patterns is to increase the volume of data that the\nmodel ingests by orders of magnitude. Unfortunately, in the cooking domain,\nthese problems are further compounded with larger data volumes presenting a\nunique challenge that is not easily handled by simply scaling up. In this work,\nwe propose novel aggregation-based generative AI methods, Cook-Gen, that\nreliably generate cooking actions from recipes, despite difficulties with\nirregular data patterns, while also outperforming Large Language Models and\nother strong baselines.",
                "ViCo: Plug-and-play Visual Condition for Personalized Text-to-image\n  Generation\nPersonalized text-to-image generation using diffusion models has recently\nemerged and garnered significant interest. This task learns a novel concept\n(e.g., a unique toy), illustrated in a handful of images, into a generative\nmodel that captures fine visual details and generates photorealistic images\nbased on textual embeddings. In this paper, we present ViCo, a novel\nlightweight plug-and-play method that seamlessly integrates visual condition\ninto personalized text-to-image generation. ViCo stands out for its unique\nfeature of not requiring any fine-tuning of the original diffusion model\nparameters, thereby facilitating more flexible and scalable model deployment.\nThis key advantage distinguishes ViCo from most existing models that\nnecessitate partial or full diffusion fine-tuning. ViCo incorporates an image\nattention module that conditions the diffusion process on patch-wise visual\nsemantics, and an attention-based object mask that comes at no extra cost from\nthe attention module. Despite only requiring light parameter training (~6%\ncompared to the diffusion U-Net), ViCo delivers performance that is on par\nwith, or even surpasses, all state-of-the-art models, both qualitatively and\nquantitatively. This underscores the efficacy of ViCo, making it a highly\npromising solution for personalized text-to-image generation without the need\nfor diffusion model fine-tuning. Code: https://github.com/haoosz/ViCo",
                "KL-Divergence Guided Temperature Sampling\nTemperature sampling is a conventional approach to diversify large language\nmodel predictions. As temperature increases, the prediction becomes diverse but\nalso vulnerable to hallucinations -- generating tokens that are sensible but\nnot factual. One common approach to mitigate hallucinations is to provide\nsource/grounding documents and the model is trained to produce predictions that\nbind to and are attributable to the provided source. It appears that there is a\ntrade-off between diversity and attribution. To mitigate any such trade-off, we\npropose to relax the constraint of having a fixed temperature over decoding\nsteps, and a mechanism to guide the dynamic temperature according to its\nrelevance to the source through KL-divergence. Our experiments justifies the\ntrade-off, and shows that our sampling algorithm outperforms the conventional\ntop-k and top-p algorithms in conversational question-answering and\nsummarization tasks.",
                "Modeling and Analyzing Scorer Preferences in Short-Answer Math Questions\nAutomated scoring of student responses to open-ended questions, including\nshort-answer questions, has great potential to scale to a large number of\nresponses. Recent approaches for automated scoring rely on supervised learning,\ni.e., training classifiers or fine-tuning language models on a small number of\nresponses with human-provided score labels. However, since scoring is a\nsubjective process, these human scores are noisy and can be highly variable,\ndepending on the scorer. In this paper, we investigate a collection of models\nthat account for the individual preferences and tendencies of each human scorer\nin the automated scoring task. We apply these models to a short-answer math\nresponse dataset where each response is scored (often differently) by multiple\ndifferent human scorers. We conduct quantitative experiments to show that our\nscorer models lead to improved automated scoring accuracy. We also conduct\nquantitative experiments and case studies to analyze the individual preferences\nand tendencies of scorers. We found that scorers can be grouped into several\nobvious clusters, with each cluster having distinct features, and analyzed them\nin detail."
            ],
            "interesting paper": 2
        }
    ],
    "Charlie Garcia": [
        {
            "papers": [
                "A Mini Review on the utilization of Reinforcement Learning with OPC UA\nReinforcement Learning (RL) is a powerful machine learning paradigm that has\nbeen applied in various fields such as robotics, natural language processing\nand game playing achieving state-of-the-art results. Targeted to solve\nsequential decision making problems, it is by design able to learn from\nexperience and therefore adapt to changing dynamic environments. These\ncapabilities make it a prime candidate for controlling and optimizing complex\nprocesses in industry. The key to fully exploiting this potential is the\nseamless integration of RL into existing industrial systems. The industrial\ncommunication standard Open Platform Communications UnifiedArchitecture (OPC\nUA) could bridge this gap. However, since RL and OPC UA are from different\nfields,there is a need for researchers to bridge the gap between the two\ntechnologies. This work serves to bridge this gap by providing a brief\ntechnical overview of both technologies and carrying out a semi-exhaustive\nliterature review to gain insights on how RL and OPC UA are applied in\ncombination. With this survey, three main research topics have been identified,\nfollowing the intersection of RL with OPC UA. The results of the literature\nreview show that RL is a promising technology for the control and optimization\nof industrial processes, but does not yet have the necessary standardized\ninterfaces to be deployed in real-world scenarios with reasonably low effort.",
                "From Interactive to Co-Constructive Task Learning\nHumans have developed the capability to teach relevant aspects of new or\nadapted tasks to a social peer with very few task demonstrations by making use\nof scaffolding strategies that leverage prior knowledge and importantly prior\njoint experience to yield a joint understanding and a joint execution of the\nrequired steps to solve the task. This process has been discovered and analyzed\nin parent-infant interaction and constitutes a ``co-construction'' as it allows\nboth, the teacher and the learner, to jointly contribute to the task. We\npropose to focus research in robot interactive learning on this co-construction\nprocess to enable robots to learn from non-expert users in everyday situations.\nIn the following, we will review current proposals for interactive task\nlearning and discuss their main contributions with respect to the entailing\ninteraction. We then discuss our notion of co-construction and summarize\nresearch insights from adult-child and human-robot interactions to elucidate\nits nature in more detail. From this overview we finally derive research\ndesiderata that entail the dimensions architecture, representation, interaction\nand explainability.",
                "Black-Box vs. Gray-Box: A Case Study on Learning Table Tennis Ball\n  Trajectory Prediction with Spin and Impacts\nIn this paper, we present a method for table tennis ball trajectory filtering\nand prediction. Our gray-box approach builds on a physical model. At the same\ntime, we use data to learn parameters of the dynamics model, of an extended\nKalman filter, and of a neural model that infers the ball's initial condition.\nWe demonstrate superior prediction performance of our approach over two\nblack-box approaches, which are not supplied with physical prior knowledge. We\ndemonstrate that initializing the spin from parameters of the ball launcher\nusing a neural network drastically improves long-time prediction performance\nover estimating the spin purely from measured ball positions. An accurate\nprediction of the ball trajectory is crucial for successful returns. We\ntherefore evaluate the return performance with a pneumatic artificial muscular\nrobot and achieve a return rate of 29/30 (97.7%).",
                "Asking Before Acting: Gather Information in Embodied Decision Making\n  with Language Models\nWith strong capabilities of reasoning and a broad understanding of the world,\nLarge Language Models (LLMs) have demonstrated immense potential in building\nversatile embodied decision-making agents capable of executing a wide array of\ntasks. Nevertheless, when deployed in unfamiliar environments, we show that LLM\nagents encounter challenges in efficiently gathering essential information,\nleading to suboptimal performance. Conversely, human individuals often seek\nadditional information from their peers prior to taking action, harnessing\nexternal knowledge to avoid unnecessary trial and error. Drawing inspiration\nfrom this behavior, we propose \\textit{Asking Before Acting} (ABA), a method\nthat empowers the agent to proactively inquire with external sources for\npertinent information using natural language during their interactions within\nthe environment. In this way, the agent is able to enhance its efficiency and\nperformance by circumventing potentially laborious steps and combating the\ndifficulties associated with exploration in unfamiliar environments and\nvagueness of the instructions. We conduct extensive experiments involving a\nspectrum of environments including text-based household everyday tasks, robot\narm manipulation tasks, and real world open domain image based embodied tasks.\nThe experiments involve various models from Vicuna to GPT-4. The results\ndemonstrate that, even with modest prompts modifications, ABA exhibits\nsubstantial advantages on both performance and efficiency over baseline LLM\nagents. Further finetuning ABA with reformulated metadata (ABA-FT) faciliates\nlearning the rationale for asking and allows for additional enhancements\nespecially in tasks that baselines struggle to solve.",
                "Decomposing the Enigma: Subgoal-based Demonstration Learning for Formal\n  Theorem Proving\nLarge language models~(LLMs) present an intriguing avenue of exploration in\nthe domain of formal theorem proving. Nonetheless, the full utilization of\nthese models, particularly in terms of demonstration formatting and\norganization, remains an underexplored area. In an endeavor to enhance the\nefficacy of LLMs, we introduce a subgoal-based demonstration learning\nframework, consisting of two primary elements: Firstly, drawing upon the\ninsights of subgoal learning from the domains of reinforcement learning and\nrobotics, we propose the construction of distinct subgoals for each\ndemonstration example and refine these subgoals in accordance with the\npertinent theories of subgoal learning. Secondly, we build upon recent advances\nin diffusion models to predict the optimal organization, simultaneously\naddressing two intricate issues that persist within the domain of demonstration\norganization: subset selection and order determination. Through the integration\nof subgoal-based learning methodologies, we have successfully increased the\nprevailing proof accuracy from 38.9\\% to 44.3\\% on the miniF2F benchmark.\nFurthermore, the adoption of diffusion models for demonstration organization\ncan lead to an additional enhancement in accuracy to 45.5\\%, or a $5\\times$\nimprovement in sampling efficiency compared with the long-standing\nstate-of-the-art method. Our code is available at\n\\url{https://github.com/HKUNLP/subgoal-theorem-prover}.",
                "Lucy-SKG: Learning to Play Rocket League Efficiently Using Deep\n  Reinforcement Learning\nA successful tactic that is followed by the scientific community for\nadvancing AI is to treat games as problems, which has been proven to lead to\nvarious breakthroughs. We adapt this strategy in order to study Rocket League,\na widely popular but rather under-explored 3D multiplayer video game with a\ndistinct physics engine and complex dynamics that pose a significant challenge\nin developing efficient and high-performance game-playing agents. In this\npaper, we present Lucy-SKG, a Reinforcement Learning-based model that learned\nhow to play Rocket League in a sample-efficient manner, outperforming by a\nnotable margin the two highest-ranking bots in this game, namely Necto (2022\nbot champion) and its successor Nexto, thus becoming a state-of-the-art agent.\nOur contributions include: a) the development of a reward analysis and\nvisualization library, b) novel parameterizable reward shape functions that\ncapture the utility of complex reward types via our proposed Kinesthetic Reward\nCombination (KRC) technique, and c) design of auxiliary neural architectures\nfor training on reward prediction and state representation tasks in an\non-policy fashion for enhanced efficiency in learning speed and performance. By\nperforming thorough ablation studies for each component of Lucy-SKG, we showed\ntheir independent effectiveness in overall performance. In doing so, we\ndemonstrate the prospects and challenges of using sample-efficient\nReinforcement Learning techniques for controlling complex dynamical systems\nunder competitive team-based multiplayer conditions."
            ],
            "interesting paper": 3
        },
        {
            "papers": [
                "Individuality in Swarm Robots with the Case Study of Kilobots: Noise,\n  Bug, or Feature?\nInter-individual differences are studied in natural systems, such as fish,\nbees, and humans, as they contribute to the complexity of both individual and\ncollective behaviors. However, individuality in artificial systems, such as\nrobotic swarms, is undervalued or even overlooked. Agent-specific deviations\nfrom the norm in swarm robotics are usually understood as mere noise that can\nbe minimized, for example, by calibration. We observe that robots have\nconsistent deviations and argue that awareness and knowledge of these can be\nexploited to serve a task. We measure heterogeneity in robot swarms caused by\nindividual differences in how robots act, sense, and oscillate. Our use case is\nKilobots and we provide example behaviors where the performance of robots\nvaries depending on individual differences. We show a non-intuitive example of\nphototaxis with Kilobots where the non-calibrated Kilobots show better\nperformance than the calibrated supposedly ``ideal\" one. We measure the\ninter-individual variations for heterogeneity in sensing and oscillation, too.\nWe briefly discuss how these variations can enhance the complexity of\ncollective behaviors. We suggest that by recognizing and exploring this new\nperspective on individuality, and hence diversity, in robotic swarms, we can\ngain a deeper understanding of these systems and potentially unlock new\npossibilities for their design and implementation of applications.",
                "On Computing Universal Plans for Partially Observable Multi-Agent Path\n  Finding\nMulti-agent routing problems have drawn significant attention nowadays due to\ntheir broad industrial applications in, e.g., warehouse robots, logistics\nautomation, and traffic control. Conventionally, they are modelled as classical\nplanning problems. In this paper, we argue that it is beneficial to formulate\nthem as universal planning problems. We therefore propose universal plans, also\nknown as policies, as the solution concepts, and implement a system called\nASP-MAUPF (Answer Set Programming for Multi-Agent Universal Plan Finding) for\ncomputing them. Given an arbitrary two-dimensional map and a profile of goals\nfor the agents, the system finds a feasible universal plan for each agent that\nensures no collision with others. We use the system to conduct some\nexperiments, and make some observations on the types of goal profiles and\nenvironments that will have feasible policies, and how they may depend on\nagents' sensors. We also demonstrate how users can customize action preferences\nto compute more efficient policies, even (near-)optimal ones.",
                "Physics-Regulated Deep Reinforcement Learning: Invariant Embeddings\nThis paper proposes the Phy-DRL: a physics-regulated deep reinforcement\nlearning (DRL) framework for safety-critical autonomous systems. The Phy-DRL\nhas three distinguished invariant-embedding designs: i) residual action policy\n(i.e., integrating data-driven-DRL action policy and physics-model-based action\npolicy), ii) automatically constructed safety-embedded reward, and iii)\nphysics-model-guided neural network (NN) editing, including link editing and\nactivation editing. Theoretically, the Phy-DRL exhibits 1) a mathematically\nprovable safety guarantee and 2) strict compliance of critic and actor networks\nwith physics knowledge about the action-value function and action policy.\nFinally, we evaluate the Phy-DRL on a cart-pole system and a quadruped robot.\nThe experiments validate our theoretical results and demonstrate that Phy-DRL\nfeatures guaranteed safety compared to purely data-driven DRL and solely\nmodel-based design while offering remarkably fewer learning parameters and fast\ntraining towards safety guarantee.",
                "Understanding the Capabilities of Large Language Models for Automated\n  Planning\nAutomated planning is concerned with developing efficient algorithms to\ngenerate plans or sequences of actions to achieve a specific goal in a given\nenvironment. Emerging Large Language Models (LLMs) can answer questions, write\nhigh-quality programming code, and predict protein folding, showcasing their\nversatility in solving various tasks beyond language-based problems. In this\npaper, we aim to explore how LLMs can also be used for automated planning. To\ndo so, we seek to answer four key questions. Firstly, we want to understand the\nextent to which LLMs can be used for plan generation. Secondly, we aim to\nidentify which pre-training data is most effective in facilitating plan\ngeneration. Thirdly, we investigate whether fine-tuning or prompting is a more\neffective approach for plan generation. Finally, we explore whether LLMs are\ncapable of plan generalization. By answering these questions, the study seeks\nto shed light on the capabilities of LLMs in solving complex planning problems\nand provide insights into the most effective approaches for using LLMs in this\ncontext.",
                "Learning Safety Constraints from Demonstrations with Unknown Rewards\nWe propose Convex Constraint Learning for Reinforcement Learning (CoCoRL), a\nnovel approach for inferring shared constraints in a Constrained Markov\nDecision Process (CMDP) from a set of safe demonstrations with possibly\ndifferent reward functions. While previous work is limited to demonstrations\nwith known rewards or fully known environment dynamics, CoCoRL can learn\nconstraints from demonstrations with different unknown rewards without\nknowledge of the environment dynamics. CoCoRL constructs a convex safe set\nbased on demonstrations, which provably guarantees safety even for potentially\nsub-optimal (but safe) demonstrations. For near-optimal demonstrations, CoCoRL\nconverges to the true safe set with no policy regret. We evaluate CoCoRL in\ngridworld environments and a driving simulation with multiple constraints.\nCoCoRL learns constraints that lead to safe driving behavior. Importantly, we\ncan safely transfer the learned constraints to different tasks and\nenvironments. In contrast, alternative methods based on Inverse Reinforcement\nLearning (IRL) often exhibit poor performance and learn unsafe policies.",
                "Sim-Suction: Learning a Suction Grasp Policy for Cluttered Environments\n  Using a Synthetic Benchmark\nThis paper presents Sim-Suction, a robust object-aware suction grasp policy\nfor mobile manipulation platforms with dynamic camera viewpoints, designed to\npick up unknown objects from cluttered environments. Suction grasp policies\ntypically employ data-driven approaches, necessitating large-scale,\naccurately-annotated suction grasp datasets. However, the generation of suction\ngrasp datasets in cluttered environments remains underexplored, leaving\nuncertainties about the relationship between the object of interest and its\nsurroundings. To address this, we propose a benchmark synthetic dataset,\nSim-Suction-Dataset, comprising 500 cluttered environments with 3.2 million\nannotated suction grasp poses. The efficient Sim-Suction-Dataset generation\nprocess provides novel insights by combining analytical models with dynamic\nphysical simulations to create fast and accurate suction grasp pose\nannotations. We introduce Sim-Suction-Pointnet to generate robust 6D suction\ngrasp poses by learning point-wise affordances from the Sim-Suction-Dataset,\nleveraging the synergy of zero-shot text-to-segmentation. Real-world\nexperiments for picking up all objects demonstrate that Sim-Suction-Pointnet\nachieves success rates of 96.76%, 94.23%, and 92.39% on cluttered level 1\nobjects (prismatic shape), cluttered level 2 objects (more complex geometry),\nand cluttered mixed objects, respectively. The Sim-Suction policies outperform\nstate-of-the-art benchmarks tested by approximately 21% in cluttered mixed\nscenes."
            ],
            "interesting paper": 1
        },
        {
            "papers": [
                "Formal Modelling for Multi-Robot Systems Under Uncertainty\nPurpose of Review: To effectively synthesise and analyse multi-robot\nbehaviour, we require formal task-level models which accurately capture\nmulti-robot execution. In this paper, we review modelling formalisms for\nmulti-robot systems under uncertainty, and discuss how they can be used for\nplanning, reinforcement learning, model checking, and simulation.\n  Recent Findings: Recent work has investigated models which more accurately\ncapture multi-robot execution by considering different forms of uncertainty,\nsuch as temporal uncertainty and partial observability, and modelling the\neffects of robot interactions on action execution. Other strands of work have\npresented approaches for reducing the size of multi-robot models to admit more\nefficient solution methods. This can be achieved by decoupling the robots under\nindependence assumptions, or reasoning over higher level macro actions.\n  Summary: Existing multi-robot models demonstrate a trade off between\naccurately capturing robot dependencies and uncertainty, and being small enough\nto tractably solve real world problems. Therefore, future research should\nexploit realistic assumptions over multi-robot behaviour to develop smaller\nmodels which retain accurate representations of uncertainty and robot\ninteractions; and exploit the structure of multi-robot problems, such as\nfactored state spaces, to develop scalable solution methods.",
                "A Categorical Representation Language and Computational System for\n  Knowledge-Based Planning\nClassical planning representation languages based on first-order logic have\npreliminarily been used to model and solve robotic task planning problems.\nWider adoption of these representation languages, however, is hindered by the\nlimitations present when managing implicit world changes with concise action\nmodels. To address this problem, we propose an alternative approach to\nrepresenting and managing updates to world states during planning. Based on the\ncategory-theoretic concepts of $\\mathsf{C}$-sets and double-pushout rewriting\n(DPO), our proposed representation can effectively handle structured knowledge\nabout world states that support domain abstractions at all levels. It\nformalizes the semantics of predicates according to a user-provided ontology\nand preserves the semantics when transitioning between world states. This\nmethod provides a formal semantics for using knowledge graphs and relational\ndatabases to model world states and updates in planning. In this paper, we\nconceptually compare our category-theoretic representation with the classical\nplanning representation. We show that our proposed representation has\nadvantages over the classical representation in terms of handling implicit\npreconditions and effects, and provides a more structured framework in which to\nmodel and solve planning problems.",
                "Adaptive PD Control using Deep Reinforcement Learning for Local-Remote\n  Teleoperation with Stochastic Time Delays\nLocal-remote systems allow robots to execute complex tasks in hazardous\nenvironments such as space and nuclear power stations. However, establishing\naccurate positional mapping between local and remote devices can be difficult\ndue to time delays that can compromise system performance and stability.\nEnhancing the synchronicity and stability of local-remote systems is vital for\nenabling robots to interact with environments at greater distances and under\nhighly challenging network conditions, including time delays. We introduce an\nadaptive control method employing reinforcement learning to tackle the\ntime-delayed control problem. By adjusting controller parameters in real-time,\nthis adaptive controller compensates for stochastic delays and improves\nsynchronicity between local and remote robotic manipulators. To improve the\nadaptive PD controller's performance, we devise a model-based reinforcement\nlearning approach that effectively incorporates multi-step delays into the\nlearning framework. Utilizing this proposed technique, the local-remote\nsystem's performance is stabilized for stochastic communication time-delays of\nup to 290ms. Our results demonstrate that the suggested model-based\nreinforcement learning method surpasses the Soft-Actor Critic and augmented\nstate Soft-Actor Critic techniques. Access the code at:\nhttps://github.com/CAV-Research-Lab/Predictive-Model-Delay-Correction",
                "NavGPT: Explicit Reasoning in Vision-and-Language Navigation with Large\n  Language Models\nTrained with an unprecedented scale of data, large language models (LLMs)\nlike ChatGPT and GPT-4 exhibit the emergence of significant reasoning abilities\nfrom model scaling. Such a trend underscored the potential of training LLMs\nwith unlimited language data, advancing the development of a universal embodied\nagent. In this work, we introduce the NavGPT, a purely LLM-based\ninstruction-following navigation agent, to reveal the reasoning capability of\nGPT models in complex embodied scenes by performing zero-shot sequential action\nprediction for vision-and-language navigation (VLN). At each step, NavGPT takes\nthe textual descriptions of visual observations, navigation history, and future\nexplorable directions as inputs to reason the agent's current status, and makes\nthe decision to approach the target. Through comprehensive experiments, we\ndemonstrate NavGPT can explicitly perform high-level planning for navigation,\nincluding decomposing instruction into sub-goal, integrating commonsense\nknowledge relevant to navigation task resolution, identifying landmarks from\nobserved scenes, tracking navigation progress, and adapting to exceptions with\nplan adjustment. Furthermore, we show that LLMs is capable of generating\nhigh-quality navigational instructions from observations and actions along a\npath, as well as drawing accurate top-down metric trajectory given the agent's\nnavigation history. Despite the performance of using NavGPT to zero-shot R2R\ntasks still falling short of trained models, we suggest adapting multi-modality\ninputs for LLMs to use as visual navigation agents and applying the explicit\nreasoning of LLMs to benefit learning-based models.",
                "Communication-Efficient Reinforcement Learning in Swarm Robotic Networks\n  for Maze Exploration\nSmooth coordination within a swarm robotic system is essential for the\neffective execution of collective robot missions. Having efficient\ncommunication is key to the successful coordination of swarm robots. This paper\nproposes a new communication-efficient decentralized cooperative reinforcement\nlearning algorithm for coordinating swarm robots. It is made efficient by\nhierarchically building on the use of local information exchanges. We consider\na case study application of maze solving through cooperation among a group of\nrobots, where the time and costs are minimized while avoiding inter-robot\ncollisions and path overlaps during exploration. With a solid theoretical\nbasis, we extensively analyze the algorithm with realistic CORE network\nsimulations and evaluate it against state-of-the-art solutions in terms of maze\ncoverage percentage and efficiency under communication-degraded environments.\nThe results demonstrate significantly higher coverage accuracy and efficiency\nwhile reducing costs and overlaps even in high packet loss and low\ncommunication range scenarios.",
                "Modelling, Analysis and Control of OmniMorph: an Omnidirectional\n  Morphing Multi-rotor UAV\nThis paper introduces for the first time the design, modelling, and control\nof a novel morphing multi-rotor Unmanned Aerial Vehicle (UAV) that we call the\nOmniMorph. The morphing ability allows the selection of the configuration that\noptimizes energy consumption while ensuring the needed maneuverability for the\nrequired task. The most energy-efficient uni-directional thrust (UDT)\nconfiguration can be used, e.g., during standard point-to-point displacements.\nFully-actuated (FA) and omnidirectional (OD) configurations can be instead used\nfor full pose tracking, such as, e.g., constant attitude horizontal motions and\nfull rotations on the spot, and for full wrench 6D interaction control and 6D\ndisturbance rejection. Morphing is obtained using a single servomotor, allowing\npossible minimization of weight, costs, and maintenance complexity. The\nactuation properties are studied, and an optimal controller that compromises\nbetween performance and control effort is proposed and validated in realistic\nsimulations. Preliminary tests on the prototype are presented to assess the\npropellers' mutual aerodynamic interference."
            ],
            "interesting paper": 3
        },
        {
            "papers": [
                "Integrating Action Knowledge and LLMs for Task Planning and Situation\n  Handling in Open Worlds\nTask planning systems have been developed to help robots use human knowledge\n(about actions) to complete long-horizon tasks. Most of them have been\ndeveloped for \"closed worlds\" while assuming the robot is provided with\ncomplete world knowledge. However, the real world is generally open, and the\nrobots frequently encounter unforeseen situations that can potentially break\nthe planner's completeness. Could we leverage the recent advances on\npre-trained Large Language Models (LLMs) to enable classical planning systems\nto deal with novel situations?\n  This paper introduces a novel framework, called COWP, for open-world task\nplanning and situation handling. COWP dynamically augments the robot's action\nknowledge, including the preconditions and effects of actions, with\ntask-oriented commonsense knowledge. COWP embraces the openness from LLMs, and\nis grounded to specific domains via action knowledge. For systematic\nevaluations, we collected a dataset that includes 1,085 execution-time\nsituations. Each situation corresponds to a state instance wherein a robot is\npotentially unable to complete a task using a solution that normally works.\nExperimental results show that our approach outperforms competitive baselines\nfrom the literature in the success rate of service tasks. Additionally, we have\ndemonstrated COWP using a mobile manipulator. Supplementary materials are\navailable at: https://cowplanning.github.io/",
                "On the Value of Myopic Behavior in Policy Reuse\nLeveraging learned strategies in unfamiliar scenarios is fundamental to human\nintelligence. In reinforcement learning, rationally reusing the policies\nacquired from other tasks or human experts is critical for tackling problems\nthat are difficult to learn from scratch. In this work, we present a framework\ncalled Selective Myopic bEhavior Control~(SMEC), which results from the insight\nthat the short-term behaviors of prior policies are sharable across tasks. By\nevaluating the behaviors of prior policies via a hybrid value function\narchitecture, SMEC adaptively aggregates the sharable short-term behaviors of\nprior policies and the long-term behaviors of the task policy, leading to\ncoordinated decisions. Empirical results on a collection of manipulation and\nlocomotion tasks demonstrate that SMEC outperforms existing methods, and\nvalidate the ability of SMEC to leverage related prior policies.",
                "Evolving Connectivity for Recurrent Spiking Neural Networks\nRecurrent spiking neural networks (RSNNs) hold great potential for advancing\nartificial general intelligence, as they draw inspiration from the biological\nnervous system and show promise in modeling complex dynamics. However, the\nwidely-used surrogate gradient-based training methods for RSNNs are inherently\ninaccurate and unfriendly to neuromorphic hardware. To address these\nlimitations, we propose the evolving connectivity (EC) framework, an\ninference-only method for training RSNNs. The EC framework reformulates\nweight-tuning as a search into parameterized connection probability\ndistributions, and employs Natural Evolution Strategies (NES) for optimizing\nthese distributions. Our EC framework circumvents the need for gradients and\nfeatures hardware-friendly characteristics, including sparse boolean\nconnections and high scalability. We evaluate EC on a series of standard\nrobotic locomotion tasks, where it achieves comparable performance with deep\nneural networks and outperforms gradient-trained RSNNs, even solving the\ncomplex 17-DoF humanoid task. Additionally, the EC framework demonstrates a two\nto three fold speedup in efficiency compared to directly evolving parameters.\nBy providing a performant and hardware-friendly alternative, the EC framework\nlays the groundwork for further energy-efficient applications of RSNNs and\nadvances the development of neuromorphic devices.",
                "Assumption Generation for the Verification of Learning-Enabled\n  Autonomous Systems\nProviding safety guarantees for autonomous systems is difficult as these\nsystems operate in complex environments that require the use of\nlearning-enabled components, such as deep neural networks (DNNs) for visual\nperception. DNNs are hard to analyze due to their size (they can have thousands\nor millions of parameters), lack of formal specifications (DNNs are typically\nlearnt from labeled data, in the absence of any formal requirements), and\nsensitivity to small changes in the environment. We present an assume-guarantee\nstyle compositional approach for the formal verification of system-level safety\nproperties of such autonomous systems. Our insight is that we can analyze the\nsystem in the absence of the DNN perception components by automatically\nsynthesizing assumptions on the DNN behaviour that guarantee the satisfaction\nof the required safety properties. The synthesized assumptions are the weakest\nin the sense that they characterize the output sequences of all the possible\nDNNs that, plugged into the autonomous system, guarantee the required safety\nproperties. The assumptions can be leveraged as run-time monitors over a\ndeployed DNN to guarantee the safety of the overall system; they can also be\nmined to extract local specifications for use during training and testing of\nDNNs. We illustrate our approach on a case study taken from the autonomous\nairplanes domain that uses a complex DNN for perception.",
                "Emergent Modularity in Pre-trained Transformers\nThis work examines the presence of modularity in pre-trained Transformers, a\nfeature commonly found in human brains and thought to be vital for general\nintelligence. In analogy to human brains, we consider two main characteristics\nof modularity: (1) functional specialization of neurons: we evaluate whether\neach neuron is mainly specialized in a certain function, and find that the\nanswer is yes. (2) function-based neuron grouping: we explore finding a\nstructure that groups neurons into modules by function, and each module works\nfor its corresponding function. Given the enormous amount of possible\nstructures, we focus on Mixture-of-Experts as a promising candidate, which\npartitions neurons into experts and usually activates different experts for\ndifferent inputs. Experimental results show that there are functional experts,\nwhere clustered are the neurons specialized in a certain function. Moreover,\nperturbing the activations of functional experts significantly affects the\ncorresponding function. Finally, we study how modularity emerges during\npre-training, and find that the modular structure is stabilized at the early\nstage, which is faster than neuron stabilization. It suggests that Transformers\nfirst construct the modular structure and then learn fine-grained neuron\nfunctions. Our code and data are available at\nhttps://github.com/THUNLP/modularity-analysis.",
                "Modeling Dynamic Environments with Scene Graph Memory\nEmbodied AI agents that search for objects in large environments such as\nhouseholds often need to make efficient decisions by predicting object\nlocations based on partial information. We pose this as a new type of link\nprediction problem: link prediction on partially observable dynamic graphs. Our\ngraph is a representation of a scene in which rooms and objects are nodes, and\ntheir relationships are encoded in the edges; only parts of the changing graph\nare known to the agent at each timestep. This partial observability poses a\nchallenge to existing link prediction approaches, which we address. We propose\na novel state representation -- Scene Graph Memory (SGM) -- with captures the\nagent's accumulated set of observations, as well as a neural net architecture\ncalled a Node Edge Predictor (NEP) that extracts information from the SGM to\nsearch efficiently. We evaluate our method in the Dynamic House Simulator, a\nnew benchmark that creates diverse dynamic graphs following the semantic\npatterns typically seen at homes, and show that NEP can be trained to predict\nthe locations of objects in a variety of environments with diverse object\nmovement dynamics, outperforming baselines both in terms of new scene\nadaptability and overall accuracy. The codebase and more can be found at\nhttps://www.scenegraphmemory.com."
            ],
            "interesting paper": 1
        },
        {
            "papers": [
                "RL + Model-based Control: Using On-demand Optimal Control to Learn\n  Versatile Legged Locomotion\nThis paper presents a control framework that combines model-based optimal\ncontrol and reinforcement learning (RL) to achieve versatile and robust legged\nlocomotion. Our approach enhances the RL training process by incorporating\non-demand reference motions generated through finite-horizon optimal control,\ncovering a broad range of velocities and gaits. These reference motions serve\nas targets for the RL policy to imitate, leading to the development of robust\ncontrol policies that can be learned with reliability. Furthermore, by\nutilizing realistic simulation data that captures whole-body dynamics, RL\neffectively overcomes the inherent limitations in reference motions imposed by\nmodeling simplifications. We validate the robustness and controllability of the\nRL training process within our framework through a series of experiments. In\nthese experiments, our method showcases its capability to generalize reference\nmotions and effectively handle more complex locomotion tasks that may pose\nchallenges for the simplified model, thanks to RL's flexibility. Additionally,\nour framework effortlessly supports the training of control policies for robots\nwith diverse dimensions, eliminating the necessity for robot-specific\nadjustments in the reward function and hyperparameters.",
                "Visual Affordance Prediction for Guiding Robot Exploration\nMotivated by the intuitive understanding humans have about the space of\npossible interactions, and the ease with which they can generalize this\nunderstanding to previously unseen scenes, we develop an approach for learning\nvisual affordances for guiding robot exploration. Given an input image of a\nscene, we infer a distribution over plausible future states that can be\nachieved via interactions with it. We use a Transformer-based model to learn a\nconditional distribution in the latent embedding space of a VQ-VAE and show\nthat these models can be trained using large-scale and diverse passive data,\nand that the learned models exhibit compositional generalization to diverse\nobjects beyond the training distribution. We show how the trained affordance\nmodel can be used for guiding exploration by acting as a goal-sampling\ndistribution, during visual goal-conditioned policy learning in robotic\nmanipulation.",
                "Towards Autonomous and Safe Last-mile Deliveries with AI-augmented\n  Self-driving Delivery Robots\nIn addition to its crucial impact on customer satisfaction, last-mile\ndelivery (LMD) is notorious for being the most time-consuming and costly stage\nof the shipping process. Pressing environmental concerns combined with the\nrecent surge of e-commerce sales have sparked renewed interest in automation\nand electrification of last-mile logistics. To address the hurdles faced by\nexisting robotic couriers, this paper introduces a customer-centric and\nsafety-conscious LMD system for small urban communities based on AI-assisted\nautonomous delivery robots. The presented framework enables end-to-end\nautomation and optimization of the logistic process while catering for\nreal-world imposed operational uncertainties, clients' preferred time\nschedules, and safety of pedestrians. To this end, the integrated optimization\ncomponent is modeled as a robust variant of the Cumulative Capacitated Vehicle\nRouting Problem with Time Windows, where routes are constructed under uncertain\ntravel times with an objective to minimize the total latency of deliveries\n(i.e., the overall waiting time of customers, which can negatively affect their\nsatisfaction). We demonstrate the proposed LMD system's utility through\nreal-world trials in a university campus with a single robotic courier.\nImplementation aspects as well as the findings and practical insights gained\nfrom the deployment are discussed in detail. Lastly, we round up the\ncontributions with numerical simulations to investigate the scalability of the\ndeveloped mathematical formulation with respect to the number of robotic\nvehicles and customers.",
                "Universal Mechanical Polycomputation in Granular Matter\nUnconventional computing devices are increasingly of interest as they can\noperate in environments hostile to silicon-based electronics, or compute in\nways that traditional electronics cannot. Mechanical computers, wherein\ninformation processing is a material property emerging from the interaction of\ncomponents with the environment, are one such class of devices. This\ninformation processing can be manifested in various physical substrates, one of\nwhich is granular matter. In a granular assembly, vibration can be treated as\nthe information-bearing mode. This can be exploited to realize \"polycomputing\":\nmaterials can be evolved such that a single grain within them can report the\nresult of multiple logical operations simultaneously at different frequencies,\nwithout recourse to quantum effects. Here, we demonstrate the evolution of a\nmaterial in which one grain acts simultaneously as two different NAND gates at\ntwo different frequencies. NAND gates are of interest as any logical operations\ncan be built from them. Moreover, they are nonlinear thus demonstrating a step\ntoward general-purpose, computationally dense mechanical computers.\nPolycomputation was found to be distributed across each evolved material,\nsuggesting the material's robustness. With recent advances in material\nsciences, hardware realization of these materials may eventually provide\ndevices that challenge the computational density of traditional computers.",
                "Taming AI Bots: Controllability of Neural States in Large Language\n  Models\nWe tackle the question of whether an agent can, by suitable choice of\nprompts, control an AI bot to any state. To that end, we first introduce a\nformal definition of ``meaning'' that is amenable to analysis. Then, we\ncharacterize ``meaningful data'' on which large language models (LLMs) are\nostensibly trained, and ``well-trained LLMs'' through conditions that are\nlargely met by today's LLMs. While a well-trained LLM constructs an embedding\nspace of meanings that is Euclidean, meanings themselves do not form a vector\n(linear) subspace, but rather a quotient space within. We then characterize the\nsubset of meanings that can be reached by the state of the LLMs for some input\nprompt, and show that a well-trained bot can reach any meaning albeit with\nsmall probability. We then introduce a stronger notion of controllability as\n{\\em almost certain reachability}, and show that, when restricted to the space\nof meanings, an AI bot is controllable. We do so after introducing a functional\ncharacterization of attentive AI bots, and finally derive necessary and\nsufficient conditions for controllability. The fact that AI bots are\ncontrollable means that an adversary could steer them towards any state.\nHowever, the sampling process can be designed to counteract adverse actions and\navoid reaching undesirable regions of state space before their boundary is\ncrossed.",
                "VCVW-3D: A Virtual Construction Vehicles and Workers Dataset with 3D\n  Annotations\nCurrently, object detection applications in construction are almost based on\npure 2D data (both image and annotation are 2D-based), resulting in the\ndeveloped artificial intelligence (AI) applications only applicable to some\nscenarios that only require 2D information. However, most advanced applications\nusually require AI agents to perceive 3D spatial information, which limits the\nfurther development of the current computer vision (CV) in construction. The\nlack of 3D annotated datasets for construction object detection worsens the\nsituation. Therefore, this study creates and releases a virtual dataset with 3D\nannotations named VCVW-3D, which covers 15 construction scenes and involves ten\ncategories of construction vehicles and workers. The VCVW-3D dataset is\ncharacterized by multi-scene, multi-category, multi-randomness,\nmulti-viewpoint, multi-annotation, and binocular vision. Several typical 2D and\nmonocular 3D object detection models are then trained and evaluated on the\nVCVW-3D dataset to provide a benchmark for subsequent research. The VCVW-3D is\nexpected to bring considerable economic benefits and practical significance by\nreducing the costs of data construction, prototype development, and exploration\nof space-awareness applications, thus promoting the development of CV in\nconstruction, especially those of 3D applications."
            ],
            "interesting paper": 1
        },
        {
            "papers": [
                "Development of a ROS-based Architecture for Intelligent Autonomous on\n  Demand Last Mile Delivery\nThis paper presents the development of the JKU-ITS Last Mile Delivery Robot.\nThe proposed approach utilizes a combination of one 3D LIDAR, RGB-D camera, IMU\nand GPS sensor on top of a mobile robot slope mower. An embedded computer,\nrunning ROS1, is utilized to process the sensor data streams to enable 2D and\n3D Simultaneous Localization and Mapping, 2D localization and object detection\nusing a convolutional neural network.",
                "AlphaBlock: Embodied Finetuning for Vision-Language Reasoning in Robot\n  Manipulation\nWe propose a novel framework for learning high-level cognitive capabilities\nin robot manipulation tasks, such as making a smiley face using building\nblocks. These tasks often involve complex multi-step reasoning, presenting\nsignificant challenges due to the limited paired data connecting human\ninstructions (e.g., making a smiley face) and robot actions (e.g., end-effector\nmovement). Existing approaches relieve this challenge by adopting an open-loop\nparadigm decomposing high-level instructions into simple sub-task plans, and\nexecuting them step-by-step using low-level control models. However, these\napproaches are short of instant observations in multi-step reasoning, leading\nto sub-optimal results. To address this issue, we propose to automatically\ncollect a cognitive robot dataset by Large Language Models (LLMs). The\nresulting dataset AlphaBlock consists of 35 comprehensive high-level tasks of\nmulti-step text plans and paired observation sequences. To enable efficient\ndata acquisition, we employ elaborated multi-round prompt designs that\neffectively reduce the burden of extensive human involvement. We further\npropose a closed-loop multi-modal embodied planning model that autoregressively\ngenerates plans by taking image observations as input. To facilitate effective\nlearning, we leverage MiniGPT-4 with a frozen visual encoder and LLM, and\nfinetune additional vision adapter and Q-former to enable fine-grained spatial\nperception for manipulation tasks. We conduct experiments to verify the\nsuperiority over existing open and closed-loop methods, and achieve a\nsignificant increase in success rate by 21.4% and 14.5% over ChatGPT and GPT-4\nbased robot tasks. Real-world demos are shown in\nhttps://www.youtube.com/watch?v=ayAzID1_qQk .",
                "Subequivariant Graph Reinforcement Learning in 3D Environments\nLearning a shared policy that guides the locomotion of different agents is of\ncore interest in Reinforcement Learning (RL), which leads to the study of\nmorphology-agnostic RL. However, existing benchmarks are highly restrictive in\nthe choice of starting point and target point, constraining the movement of the\nagents within 2D space. In this work, we propose a novel setup for\nmorphology-agnostic RL, dubbed Subequivariant Graph RL in 3D environments\n(3D-SGRL). Specifically, we first introduce a new set of more practical yet\nchallenging benchmarks in 3D space that allows the agent to have full\nDegree-of-Freedoms to explore in arbitrary directions starting from arbitrary\nconfigurations. Moreover, to optimize the policy over the enlarged state-action\nspace, we propose to inject geometric symmetry, i.e., subequivariance, into the\nmodeling of the policy and Q-function such that the policy can generalize to\nall directions, improving exploration efficiency. This goal is achieved by a\nnovel SubEquivariant Transformer (SET) that permits expressive message\nexchange. Finally, we evaluate the proposed method on the proposed benchmarks,\nwhere our method consistently and significantly outperforms existing approaches\non single-task, multi-task, and zero-shot generalization scenarios. Extensive\nablations are also conducted to verify our design. Code and videos are\navailable on our project page: https://alpc91.github.io/SGRL/.",
                "Controllable Path of Destruction\nPath of Destruction (PoD) is a self-supervised method for learning iterative\ngenerators. The core idea is to produce a training set by destroying a set of\nartifacts, and for each destructive step create a training instance based on\nthe corresponding repair action. A generator trained on this dataset can then\ngenerate new artifacts by repairing from arbitrary states. The PoD method is\nvery data-efficient in terms of original training examples and well-suited to\nfunctional artifacts composed of categorical data, such as game levels and\ndiscrete 3D structures. In this paper, we extend the Path of Destruction method\nto allow designer control over aspects of the generated artifacts.\nControllability is introduced by adding conditional inputs to the state-action\npairs that make up the repair trajectories. We test the controllable PoD method\nin a 2D dungeon setting, as well as in the domain of small 3D Lego cars.",
                "Towards a Unifying Model of Rationality in Multiagent Systems\nMultiagent systems deployed in the real world need to cooperate with other\nagents (including humans) nearly as effectively as these agents cooperate with\none another. To design such AI, and provide guarantees of its effectiveness, we\nneed to clearly specify what types of agents our AI must be able to cooperate\nwith. In this work we propose a generic model of socially intelligent agents,\nwhich are individually rational learners that are also able to cooperate with\none another (in the sense that their joint behavior is Pareto efficient). We\ndefine rationality in terms of the regret incurred by each agent over its\nlifetime, and show how we can construct socially intelligent agents for\ndifferent forms of regret. We then discuss the implications of this model for\nthe development of \"robust\" MAS that can cooperate with a wide variety of\nsocially intelligent agents.",
                "Experience Filter: Using Past Experiences on Unseen Tasks or\n  Environments\nOne of the bottlenecks of training autonomous vehicle (AV) agents is the\nvariability of training environments. Since learning optimal policies for\nunseen environments is often very costly and requires substantial data\ncollection, it becomes computationally intractable to train the agent on every\npossible environment or task the AV may encounter. This paper introduces a\nzero-shot filtering approach to interpolate learned policies of past\nexperiences to generalize to unseen ones. We use an experience kernel to\ncorrelate environments. These correlations are then exploited to produce\npolicies for new tasks or environments from learned policies. We demonstrate\nour methods on an autonomous vehicle driving through T-intersections with\ndifferent characteristics, where its behavior is modeled as a partially\nobservable Markov decision process (POMDP). We first construct compact\nrepresentations of learned policies for POMDPs with unknown transition\nfunctions given a dataset of sequential actions and observations. Then, we\nfilter parameterized policies of previously visited environments to generate\npolicies to new, unseen environments. We demonstrate our approaches on both an\nactual AV and a high-fidelity simulator. Results indicate that our experience\nfilter offers a fast, low-effort, and near-optimal solution to create policies\nfor tasks or environments never seen before. Furthermore, the generated new\npolicies outperform the policy learned using the entire data collected from\npast environments, suggesting that the correlation among different environments\ncan be exploited and irrelevant ones can be filtered out."
            ],
            "interesting paper": 2
        },
        {
            "papers": [
                "Symmetry-Aware Robot Design with Structured Subgroups\nRobot design aims at learning to create robots that can be easily controlled\nand perform tasks efficiently. Previous works on robot design have proven its\nability to generate robots for various tasks. However, these works searched the\nrobots directly from the vast design space and ignored common structures,\nresulting in abnormal robots and poor performance. To tackle this problem, we\npropose a Symmetry-Aware Robot Design (SARD) framework that exploits the\nstructure of the design space by incorporating symmetry searching into the\nrobot design process. Specifically, we represent symmetries with the subgroups\nof the dihedral group and search for the optimal symmetry in structured\nsubgroups. Then robots are designed under the searched symmetry. In this way,\nSARD can design efficient symmetric robots while covering the original design\nspace, which is theoretically analyzed. We further empirically evaluate SARD on\nvarious tasks, and the results show its superior efficiency and\ngeneralizability.",
                "Language-Conditioned Imitation Learning with Base Skill Priors under\n  Unstructured Data\nThe growing interest in language-conditioned robot manipulation aims to\ndevelop robots capable of understanding and executing complex tasks, with the\nobjective of enabling robots to interpret language commands and manipulate\nobjects accordingly. While language-conditioned approaches demonstrate\nimpressive capabilities for addressing tasks in familiar environments, they\nencounter limitations in adapting to unfamiliar environment settings. In this\nstudy, we propose a general-purpose, language-conditioned approach that\ncombines base skill priors and imitation learning under unstructured data to\nenhance the algorithm's generalization in adapting to unfamiliar environments.\nWe assess our model's performance in both simulated and real-world environments\nusing a zero-shot setting. In the simulated environment, the proposed approach\nsurpasses previously reported scores for CALVIN benchmark, especially in the\nchallenging Zero-Shot Multi-Environment setting. The average completed task\nlength, indicating the average number of tasks the agent can continuously\ncomplete, improves more than 2.5 times compared to the state-of-the-art method\nHULC. In addition, we conduct a zero-shot evaluation of our policy in a\nreal-world setting, following training exclusively in simulated environments\nwithout additional specific adaptations. In this evaluation, we set up ten\ntasks and achieved an average 30% improvement in our approach compared to the\ncurrent state-of-the-art approach, demonstrating a high generalization\ncapability in both simulated environments and the real world. For further\ndetails, including access to our code and videos, please refer to\nhttps://hk-zh.github.io/spil/",
                "SPADA: A Toolbox of Designing Soft Pneumatic Actuators for Shape\n  Matching based on Surrogate Modeling\nSoft pneumatic actuators (SPAs) produce motions for soft robots with simple\npressure input, however they require to be appropriately designed to fit the\ntarget application. Available design methods employ kinematic models and\noptimization to estimate the actuator response and the optimal design\nparameters, to achieve a target actuator's shape. Within SPAs, Bellow-SPAs\nexcel in rapid prototyping and large deformation, yet their kinematic models\noften lack accuracy due to the geometry complexity and the material\nnonlinearity. Furthermore, existing shape-matching algorithms are not providing\nan end-to-end solution from the desired shape to the actuator. In addition,\ndespite the availability of computational design pipelines, an accessible and\nuser-friendly toolbox for direct application remains elusive. This paper\naddresses these challenges, offering an end-to-end shape-matching design\nframework for bellow-SPAs to streamline the design process, and the open-source\ntoolbox SPADA (Soft Pneumatic Actuator Design frAmework) implementing the\nframework with a GUI for easy access. It provides a kinematic model grounded on\na modular design to improve accuracy, Finite Element Method (FEM) simulations,\nand piecewise constant curvature (PCC) approximation. An Artificial Neural\nNetwork-trained surrogate model, based on FEM simulation data, is trained for\nfast computation in optimization. A shape-matching algorithm, merging 3D PCC\nsegmentation and a surrogate model-based genetic algorithm, identifies optimal\nactuator design parameters for desired shapes. The toolbox, implementing the\nproposed design framework, has proven its end-to-end capability in designing\nactuators to precisely match 2D shapes with root-mean-square errors of 4.16,\n2.70, and 2.51mm, and demonstrating its potential by designing a 3D deformable\nactuator.",
                "GAN-MPC: Training Model Predictive Controllers with Parameterized Cost\n  Functions using Demonstrations from Non-identical Experts\nModel predictive control (MPC) is a popular approach for trajectory\noptimization in practical robotics applications. MPC policies can optimize\ntrajectory parameters under kinodynamic and safety constraints and provide\nguarantees on safety, optimality, generalizability, interpretability, and\nexplainability. However, some behaviors are complex and it is difficult to\nhand-craft an MPC objective function. A special class of MPC policies called\nLearnable-MPC addresses this difficulty using imitation learning from expert\ndemonstrations. However, they require the demonstrator and the imitator agents\nto be identical which is hard to satisfy in many real world applications of\nrobotics. In this paper, we address the practical problem of training\nLearnable-MPC policies when the demonstrator and the imitator do not share the\nsame dynamics and their state spaces may have a partial overlap. We propose a\nnovel approach that uses a generative adversarial network (GAN) to minimize the\nJensen-Shannon divergence between the state-trajectory distributions of the\ndemonstrator and the imitator. We evaluate our approach on a variety of\nsimulated robotics tasks of DeepMind Control suite and demonstrate the efficacy\nof our approach at learning the demonstrator's behavior without having to copy\ntheir actions.",
                "Data and Knowledge for Overtaking Scenarios in Autonomous Driving\nAutonomous driving has become one of the most popular research topics within\nArtificial Intelligence. An autonomous vehicle is understood as a system that\ncombines perception, decision-making, planning, and control. All of those tasks\nrequire that the vehicle collects surrounding data in order to make a good\ndecision and action. In particular, the overtaking maneuver is one of the most\ncritical actions of driving. The process involves lane changes, acceleration\nand deceleration actions, and estimation of the speed and distance of the\nvehicle in front or in the lane in which it is moving. Despite the amount of\nwork available in the literature, just a few handle overtaking maneuvers and,\nbecause overtaking can be risky, no real-world dataset is available. This work\ncontributes in this area by presenting a new synthetic dataset whose focus is\nthe overtaking maneuver. We start by performing a thorough review of the state\nof the art in autonomous driving and then explore the main datasets found in\nthe literature (public and private, synthetic and real), highlighting their\nlimitations, and suggesting a new set of features whose focus is the overtaking\nmaneuver.",
                "SO(2)-Equivariant Downwash Models for Close Proximity Flight\nMultirotors flying in close proximity induce aerodynamic wake effects on each\nother through propeller downwash. Conventional methods have fallen short of\nproviding adequate 3D force-based models that can be incorporated into robust\ncontrol paradigms for deploying dense formations. Thus, learning a model for\nthese downwash patterns presents an attractive solution. In this paper, we\npresent a novel learning-based approach for modelling the downwash forces that\nexploits the latent geometries (i.e. symmetries) present in the problem. We\ndemonstrate that when trained with only 5 minutes of real-world flight data,\nour geometry-aware model outperforms state-of-the-art baseline models trained\nwith more than 15 minutes of data. In dense real-world flights with two\nvehicles, deploying our model online improves 3D trajectory tracking by nearly\n36% on average (and vertical tracking by 56%)."
            ],
            "interesting paper": 1
        },
        {
            "papers": [
                "Bio-Inspired 4D-Printed Mechanisms with Programmable Morphology\nTraditional robotic mechanisms contain a series of rigid links connected by\nrotational joints that provide powered motion, all of which is controlled by a\ncentral processor. By contrast, analogous mechanisms found in nature, such as\noctopus tentacles, contain sensors, actuators, and even neurons distributed\nthroughout the appendage, thereby allowing for motion with superior complexity,\nfluidity, and reaction time. Smart materials provide a means with which we can\nmimic these features artificially. These specialized materials undergo shape\nchange in response to changes in their environment. Previous studies have\ndeveloped material-based actuators that could produce targeted shape changes.\nHere we extend this capability by introducing a novel computational and\nexperimental method for design and synthesis of material-based morphing\nmechanisms capable of achieving complex pre-programmed motion. By combining\nactive and passive materials, the algorithm can encode the desired movement\ninto the material distribution of the mechanism. We demonstrate this new\ncapability by de novo design of a 3D printed self-tying knot. This method\nadvances a new paradigm in mechanism design that could enable a new generation\nof material-driven machines that are lightweight, adaptable, robust to damage,\nand easily manufacturable by 3D printing.",
                "Neural LerPlane Representations for Fast 4D Reconstruction of Deformable\n  Tissues\nReconstructing deformable tissues from endoscopic stereo videos in robotic\nsurgery is crucial for various clinical applications. However, existing methods\nrelying only on implicit representations are computationally expensive and\nrequire dozens of hours, which limits further practical applications. To\naddress this challenge, we introduce LerPlane, a novel method for fast and\naccurate reconstruction of surgical scenes under a single-viewpoint setting.\nLerPlane treats surgical procedures as 4D volumes and factorizes them into\nexplicit 2D planes of static and dynamic fields, leading to a compact memory\nfootprint and significantly accelerated optimization. The efficient\nfactorization is accomplished by fusing features obtained through linear\ninterpolation of each plane and enables using lightweight neural networks to\nmodel surgical scenes. Besides, LerPlane shares static fields, significantly\nreducing the workload of dynamic tissue modeling. We also propose a novel\nsample scheme to boost optimization and improve performance in regions with\ntool occlusion and large motions. Experiments on DaVinci robotic surgery videos\ndemonstrate that LerPlane accelerates optimization by over 100$\\times$ while\nmaintaining high quality across various non-rigid deformations, showing\nsignificant promise for future intraoperative surgery applications.",
                "Human Control: Definitions and Algorithms\nHow can humans stay in control of advanced artificial intelligence systems?\nOne proposal is corrigibility, which requires the agent to follow the\ninstructions of a human overseer, without inappropriately influencing them. In\nthis paper, we formally define a variant of corrigibility called shutdown\ninstructability, and show that it implies appropriate shutdown behavior,\nretention of human autonomy, and avoidance of user harm. We also analyse the\nrelated concepts of non-obstruction and shutdown alignment, three previously\nproposed algorithms for human control, and one new algorithm.",
                "Low Voltage Electrohydraulic Actuators for Untethered Robotics\nRigid robots can be precise in repetitive tasks, but struggle in unstructured\nenvironments. Nature's versatility in such environments inspires researchers to\ndevelop biomimetic robots that incorporate compliant and contracting artificial\nmuscles. Among the recently proposed artificial muscle technologies,\nelectrohydraulic actuators are promising since they offer performance\ncomparable to that of mammalian muscles in terms of speed and power density.\nHowever, they require high driving voltages and have safety concerns due to\nexposed electrodes. These high voltages lead to either bulky or inefficient\ndriving electronics that make untethered, high-degree-of-freedom bio-inspired\nrobots difficult to realize. Here, we present hydraulically amplified low\nvoltage electrostatic (HALVE) actuators that match mammalian skeletal muscles\nin average power density (50.5 W kg-1) and peak strain rate (971 % s-1) at a\ndriving voltage of just 1100 V. This driving voltage is approx. 5-7 times lower\ncompared to other electrohydraulic actuators using paraelectric dielectrics.\nFurthermore, HALVE actuators are safe to touch, waterproof, and self-clearing,\nwhich makes them easy to implement in wearables and robotics. We characterize,\nmodel, and physically validate key performance metrics of the actuator and\ncompare its performance to state-of-the-art electrohydraulic designs. Finally,\nwe demonstrate the utility of our actuators on two muscle-based\nelectrohydraulic robots: an untethered soft robotic swimmer and a robotic\ngripper. We foresee that HALVE actuators can become a key building block for\nfuture highly-biomimetic untethered robots and wearables with many independent\nartificial muscles such as biomimetic hands, faces, or exoskeletons.",
                "Environmental Memory Boosts Group Formation of Clueless Individuals\nThe formation of groups of interacting individuals improves performance and\nfitness in many decentralised systems, from micro-organisms to social insects,\nfrom robotic swarms to artificial intelligence algorithms. Often, group\nformation and high-level coordination in these systems emerge from individuals\nwith limited information-processing capabilities implementing low-level rules\nof communication to signal to each other. Here, we show that, even in a\ncommunity of clueless individuals incapable of processing information and\ncommunicating, a dynamic environment can coordinate group formation by\ntransiently storing memory of the earlier passage of individuals. Our results\nidentify a new mechanism of indirect coordination via shared memory that is\nprimarily promoted and reinforced by dynamic environmental factors, thus\novershadowing the need for any form of explicit signalling between individuals.\nWe expect this pathway to group formation to be relevant for understanding and\ncontrolling self-organisation and collective decision making in both living and\nartificial active matter in real-life environments.",
                "Latent Exploration for Reinforcement Learning\nIn Reinforcement Learning, agents learn policies by exploring and interacting\nwith the environment. Due to the curse of dimensionality, learning policies\nthat map high-dimensional sensory input to motor output is particularly\nchallenging. During training, state of the art methods (SAC, PPO, etc.) explore\nthe environment by perturbing the actuation with independent Gaussian noise.\nWhile this unstructured exploration has proven successful in numerous tasks, it\ncan be suboptimal for overactuated systems. When multiple actuators, such as\nmotors or muscles, drive behavior, uncorrelated perturbations risk diminishing\neach other's effect, or modifying the behavior in a task-irrelevant way. While\nsolutions to introduce time correlation across action perturbations exist,\nintroducing correlation across actuators has been largely ignored. Here, we\npropose LATent TIme-Correlated Exploration (Lattice), a method to inject\ntemporally-correlated noise into the latent state of the policy network, which\ncan be seamlessly integrated with on- and off-policy algorithms. We demonstrate\nthat the noisy actions generated by perturbing the network's activations can be\nmodeled as a multivariate Gaussian distribution with a full covariance matrix.\nIn the PyBullet locomotion tasks, Lattice-SAC achieves state of the art\nresults, and reaches 18% higher reward than unstructured exploration in the\nHumanoid environment. In the musculoskeletal control environments of MyoSuite,\nLattice-PPO achieves higher reward in most reaching and object manipulation\ntasks, while also finding more energy-efficient policies with reductions of\n20-60%. Overall, we demonstrate the effectiveness of structured action noise in\ntime and actuator space for complex motor control tasks. The code is available\nat: https://github.com/amathislab/lattice."
            ],
            "interesting paper": 1
        },
        {
            "papers": [
                "Train Offline, Test Online: A Real Robot Learning Benchmark\nThree challenges limit the progress of robot learning research: robots are\nexpensive (few labs can participate), everyone uses different robots (findings\ndo not generalize across labs), and we lack internet-scale robotics data. We\ntake on these challenges via a new benchmark: Train Offline, Test Online\n(TOTO). TOTO provides remote users with access to shared robotic hardware for\nevaluating methods on common tasks and an open-source dataset of these tasks\nfor offline training. Its manipulation task suite requires challenging\ngeneralization to unseen objects, positions, and lighting. We present initial\nresults on TOTO comparing five pretrained visual representations and four\noffline policy learning baselines, remotely contributed by five institutions.\nThe real promise of TOTO, however, lies in the future: we release the benchmark\nfor additional submissions from any user, enabling easy, direct comparison to\nseveral methods without the need to obtain hardware or collect data.",
                "LIV: Language-Image Representations and Rewards for Robotic Control\nWe present Language-Image Value learning (LIV), a unified objective for\nvision-language representation and reward learning from action-free videos with\ntext annotations. Exploiting a novel connection between dual reinforcement\nlearning and mutual information contrastive learning, the LIV objective trains\na multi-modal representation that implicitly encodes a universal value function\nfor tasks specified as language or image goals. We use LIV to pre-train the\nfirst control-centric vision-language representation from large human video\ndatasets such as EpicKitchen. Given only a language or image goal, the\npre-trained LIV model can assign dense rewards to each frame in videos of\nunseen robots or humans attempting that task in unseen environments. Further,\nwhen some target domain-specific data is available, the same objective can be\nused to fine-tune and improve LIV and even other pre-trained representations\nfor robotic control and reward specification in that domain. In our experiments\non several simulated and real-world robot environments, LIV models consistently\noutperform the best prior input state representations for imitation learning,\nas well as reward specification methods for policy synthesis. Our results\nvalidate the advantages of joint vision-language representation and reward\nlearning within the unified, compact LIV framework.",
                "Multi-Robot Path Planning Combining Heuristics and Multi-Agent\n  Reinforcement Learning\nMulti-robot path finding in dynamic environments is a highly challenging\nclassic problem. In the movement process, robots need to avoid collisions with\nother moving robots while minimizing their travel distance. Previous methods\nfor this problem either continuously replan paths using heuristic search\nmethods to avoid conflicts or choose appropriate collision avoidance strategies\nbased on learning approaches. The former may result in long travel distances\ndue to frequent replanning, while the latter may have low learning efficiency\ndue to low sample exploration and utilization, and causing high training costs\nfor the model. To address these issues, we propose a path planning method,\nMAPPOHR, which combines heuristic search, empirical rules, and multi-agent\nreinforcement learning. The method consists of two layers: a real-time planner\nbased on the multi-agent reinforcement learning algorithm, MAPPO, which embeds\nempirical rules in the action output layer and reward functions, and a\nheuristic search planner used to create a global guiding path. During movement,\nthe heuristic search planner replans new paths based on the instructions of the\nreal-time planner. We tested our method in 10 different conflict scenarios. The\nexperiments show that the planning performance of MAPPOHR is better than that\nof existing learning and heuristic methods. Due to the utilization of empirical\nknowledge and heuristic search, the learning efficiency of MAPPOHR is higher\nthan that of existing learning methods.",
                "Egocentric Planning for Scalable Embodied Task Achievement\nEmbodied agents face significant challenges when tasked with performing\nactions in diverse environments, particularly in generalizing across object\ntypes and executing suitable actions to accomplish tasks. Furthermore, agents\nshould exhibit robustness, minimizing the execution of illegal actions. In this\nwork, we present Egocentric Planning, an innovative approach that combines\nsymbolic planning and Object-oriented POMDPs to solve tasks in complex\nenvironments, harnessing existing models for visual perception and natural\nlanguage processing. We evaluated our approach in ALFRED, a simulated\nenvironment designed for domestic tasks, and demonstrated its high scalability,\nachieving an impressive 36.07% unseen success rate in the ALFRED benchmark and\nwinning the ALFRED challenge at CVPR Embodied AI workshop. Our method requires\nreliable perception and the specification or learning of a symbolic description\nof the preconditions and effects of the agent's actions, as well as what object\ntypes reveal information about others. It is capable of naturally scaling to\nsolve new tasks beyond ALFRED, as long as they can be solved using the\navailable skills. This work offers a solid baseline for studying end-to-end and\nhybrid methods that aim to generalize to new tasks, including recent approaches\nrelying on LLMs, but often struggle to scale to long sequences of actions or\nproduce robust plans for novel tasks.",
                "An Architecture for Deploying Reinforcement Learning in Industrial\n  Environments\nIndustry 4.0 is driven by demands like shorter time-to-market, mass\ncustomization of products, and batch size one production. Reinforcement\nLearning (RL), a machine learning paradigm shown to possess a great potential\nin improving and surpassing human level performance in numerous complex tasks,\nallows coping with the mentioned demands. In this paper, we present an OPC UA\nbased Operational Technology (OT)-aware RL architecture, which extends the\nstandard RL setting, combining it with the setting of digital twins. Moreover,\nwe define an OPC UA information model allowing for a generalized plug-and-play\nlike approach for exchanging the RL agent used. In conclusion, we demonstrate\nand evaluate the architecture, by creating a proof of concept. By means of\nsolving a toy example, we show that this architecture can be used to determine\nthe optimal policy using a real control system.",
                "Investigating Navigation Strategies in the Morris Water Maze through\n  Deep Reinforcement Learning\nNavigation is a complex skill with a long history of research in animals and\nhumans. In this work, we simulate the Morris Water Maze in 2D to train deep\nreinforcement learning agents. We perform automatic classification of\nnavigation strategies, analyze the distribution of strategies used by\nartificial agents, and compare them with experimental data to show similar\nlearning dynamics as those seen in humans and rodents. We develop\nenvironment-specific auxiliary tasks and examine factors affecting their\nusefulness. We suggest that the most beneficial tasks are potentially more\nbiologically feasible for real agents to use. Lastly, we explore the\ndevelopment of internal representations in the activations of artificial agent\nneural networks. These representations resemble place cells and head-direction\ncells found in mouse brains, and their presence has correlation to the\nnavigation strategies that artificial agents employ."
            ],
            "interesting paper": 2
        }
    ],
    "Riley Martinez": [
        {
            "papers": [
                "Deep Learning and Ethics\nThis article appears as chapter 21 of Prince (2023, Understanding Deep\nLearning); a complete draft of the textbook is available here:\nhttp://udlbook.com. This chapter considers potential harms arising from the\ndesign and use of AI systems. These include algorithmic bias, lack of\nexplainability, data privacy violations, militarization, fraud, and\nenvironmental concerns. The aim is not to provide advice on being more ethical.\nInstead, the goal is to express ideas and start conversations in key areas that\nhave received attention in philosophy, political science, and the broader\nsocial sciences.",
                "Science in the Era of ChatGPT, Large Language Models and Generative AI:\n  Challenges for Research Ethics and How to Respond\nLarge language models of artificial intelligence (AI), such as ChatGPT, find\nremarkable but controversial applicability in science and research. This paper\nreviews epistemological challenges, ethical and integrity risks in science\nconduct in the advent of generative AI. This is with the aim to lay new timely\nfoundations for a high-quality research ethics review. The role of AI language\nmodels as a research instrument and subject is scrutinized along with ethical\nimplications for scientists, participants and reviewers. New emerging practices\nfor research ethics review are discussed, concluding with ten recommendations\nthat shape a response for a more responsible research conduct in the era of AI.",
                "Lucy-SKG: Learning to Play Rocket League Efficiently Using Deep\n  Reinforcement Learning\nA successful tactic that is followed by the scientific community for\nadvancing AI is to treat games as problems, which has been proven to lead to\nvarious breakthroughs. We adapt this strategy in order to study Rocket League,\na widely popular but rather under-explored 3D multiplayer video game with a\ndistinct physics engine and complex dynamics that pose a significant challenge\nin developing efficient and high-performance game-playing agents. In this\npaper, we present Lucy-SKG, a Reinforcement Learning-based model that learned\nhow to play Rocket League in a sample-efficient manner, outperforming by a\nnotable margin the two highest-ranking bots in this game, namely Necto (2022\nbot champion) and its successor Nexto, thus becoming a state-of-the-art agent.\nOur contributions include: a) the development of a reward analysis and\nvisualization library, b) novel parameterizable reward shape functions that\ncapture the utility of complex reward types via our proposed Kinesthetic Reward\nCombination (KRC) technique, and c) design of auxiliary neural architectures\nfor training on reward prediction and state representation tasks in an\non-policy fashion for enhanced efficiency in learning speed and performance. By\nperforming thorough ablation studies for each component of Lucy-SKG, we showed\ntheir independent effectiveness in overall performance. In doing so, we\ndemonstrate the prospects and challenges of using sample-efficient\nReinforcement Learning techniques for controlling complex dynamical systems\nunder competitive team-based multiplayer conditions.",
                "Model evaluation for extreme risks\nCurrent approaches to building general-purpose AI systems tend to produce\nsystems with both beneficial and harmful capabilities. Further progress in AI\ndevelopment could lead to capabilities that pose extreme risks, such as\noffensive cyber capabilities or strong manipulation skills. We explain why\nmodel evaluation is critical for addressing extreme risks. Developers must be\nable to identify dangerous capabilities (through \"dangerous capability\nevaluations\") and the propensity of models to apply their capabilities for harm\n(through \"alignment evaluations\"). These evaluations will become critical for\nkeeping policymakers and other stakeholders informed, and for making\nresponsible decisions about model training, deployment, and security.",
                "A Systematic Review of Machine Learning Enabled Phishing\nDevelopments in artificial intelligence (AI) are likely to affect social\nengineering and change cyber defense operations. The broad and sweeping nature\nof AI impact means that many aspects of social engineering could be automated,\npotentially giving adversaries an advantage. In this review, we assess the ways\nphishing and spear-phishing might be affected by machine learning techniques.\nBy performing a systematic review of demonstrated ML-enabled phishing\ncampaigns, we take a broad survey the space for current developments. We\ndevelop a detailed approach for evaluation by creating a risk framework for\nanalyzing and contextualizing these developments. The object of this review is\nto answer the research questions: (1) Are there high-risk ML-enabled phishing\nuse cases? (2) Is there a meaningful difference between traditional targeted\nphishing campaigns and ML-enabled phishing campaigns? Practitioners may use\nthis review to inform standards, future research directions, and cyber defense\nstrategies.",
                "Asking Before Acting: Gather Information in Embodied Decision Making\n  with Language Models\nWith strong capabilities of reasoning and a broad understanding of the world,\nLarge Language Models (LLMs) have demonstrated immense potential in building\nversatile embodied decision-making agents capable of executing a wide array of\ntasks. Nevertheless, when deployed in unfamiliar environments, we show that LLM\nagents encounter challenges in efficiently gathering essential information,\nleading to suboptimal performance. Conversely, human individuals often seek\nadditional information from their peers prior to taking action, harnessing\nexternal knowledge to avoid unnecessary trial and error. Drawing inspiration\nfrom this behavior, we propose \\textit{Asking Before Acting} (ABA), a method\nthat empowers the agent to proactively inquire with external sources for\npertinent information using natural language during their interactions within\nthe environment. In this way, the agent is able to enhance its efficiency and\nperformance by circumventing potentially laborious steps and combating the\ndifficulties associated with exploration in unfamiliar environments and\nvagueness of the instructions. We conduct extensive experiments involving a\nspectrum of environments including text-based household everyday tasks, robot\narm manipulation tasks, and real world open domain image based embodied tasks.\nThe experiments involve various models from Vicuna to GPT-4. The results\ndemonstrate that, even with modest prompts modifications, ABA exhibits\nsubstantial advantages on both performance and efficiency over baseline LLM\nagents. Further finetuning ABA with reformulated metadata (ABA-FT) faciliates\nlearning the rationale for asking and allows for additional enhancements\nespecially in tasks that baselines struggle to solve."
            ],
            "interesting paper": 2
        },
        {
            "papers": [
                "Enhancing Human Capabilities through Symbiotic Artificial Intelligence\n  with Shared Sensory Experiences\nThe merging of human intelligence and artificial intelligence has long been a\nsubject of interest in both science fiction and academia. In this paper, we\nintroduce a novel concept in Human-AI interaction called Symbiotic Artificial\nIntelligence with Shared Sensory Experiences (SAISSE), which aims to establish\na mutually beneficial relationship between AI systems and human users through\nshared sensory experiences. By integrating multiple sensory input channels and\nprocessing human experiences, SAISSE fosters a strong human-AI bond, enabling\nAI systems to learn from and adapt to individual users, providing personalized\nsupport, assistance, and enhancement. Furthermore, we discuss the incorporation\nof memory storage units for long-term growth and development of both the AI\nsystem and its human user. As we address user privacy and ethical guidelines\nfor responsible AI-human symbiosis, we also explore potential biases and\ninequalities in AI-human symbiosis and propose strategies to mitigate these\nchallenges. Our research aims to provide a comprehensive understanding of the\nSAISSE concept and its potential to effectively support and enhance individual\nhuman users through symbiotic AI systems. This position article aims at\ndiscussing poteintial AI-human interaction related topics within the scientific\ncommunity, rather than providing experimental or theoretical results.",
                "Alert of the Second Decision-maker: An Introduction to Human-AI Conflict\nThe collaboration between humans and artificial intelligence (AI) is a\nsignificant feature in this digital age. However, humans and AI may have\nobservation, interpretation, and action conflicts when working synchronously.\nThis phenomenon is often masked by faults and, unfortunately, overlooked. This\npaper systematically introduces the human-AI conflict concept, causes,\nmeasurement methods, and risk assessment. The results highlight that there is a\npotential second decision-maker besides the human, which is the AI; the\nhuman-AI conflict is a unique and emerging risk in digitalized process systems;\nand this is an interdisciplinary field that needs to be distinguished from\ntraditional fault and failure analysis; the conflict risk is significant and\ncannot be ignored.",
                "Attention Paper: How Generative AI Reshapes Digital Shadow Industry?\nThe rapid development of digital economy has led to the emergence of various\nblack and shadow internet industries, which pose potential risks that can be\nidentified and managed through digital risk management (DRM) that uses\ndifferent techniques such as machine learning and deep learning. The evolution\nof DRM architecture has been driven by changes in data forms. However, the\ndevelopment of AI-generated content (AIGC) technology, such as ChatGPT and\nStable Diffusion, has given black and shadow industries powerful tools to\npersonalize data and generate realistic images and conversations for fraudulent\nactivities. This poses a challenge for DRM systems to control risks from the\nsource of data generation and to respond quickly to the fast-changing risk\nenvironment. This paper aims to provide a technical analysis of the challenges\nand opportunities of AIGC from upstream, midstream, and downstream paths of\nblack/shadow industries and suggest future directions for improving existing\nrisk control systems. The paper will explore the new black and shadow\ntechniques triggered by generative AI technology and provide insights for\nbuilding the next-generation DRM system.",
                "A Hierarchical Approach to Population Training for Human-AI\n  Collaboration\nA major challenge for deep reinforcement learning (DRL) agents is to\ncollaborate with novel partners that were not encountered by them during the\ntraining phase. This is specifically worsened by an increased variance in\naction responses when the DRL agents collaborate with human partners due to the\nlack of consistency in human behaviors. Recent work have shown that training a\nsingle agent as the best response to a diverse population of training partners\nsignificantly increases an agent's robustness to novel partners. We further\nenhance the population-based training approach by introducing a Hierarchical\nReinforcement Learning (HRL) based method for Human-AI Collaboration. Our agent\nis able to learn multiple best-response policies as its low-level policy while\nat the same time, it learns a high-level policy that acts as a manager which\nallows the agent to dynamically switch between the low-level best-response\npolicies based on its current partner. We demonstrate that our method is able\nto dynamically adapt to novel partners of different play styles and skill\nlevels in the 2-player collaborative Overcooked game environment. We also\nconducted a human study in the same environment to test the effectiveness of\nour method when partnering with real human subjects.",
                "Quantifying the Intrinsic Usefulness of Attributional Explanations for\n  Graph Neural Networks with Artificial Simulatability Studies\nDespite the increasing relevance of explainable AI, assessing the quality of\nexplanations remains a challenging issue. Due to the high costs associated with\nhuman-subject experiments, various proxy metrics are often used to\napproximately quantify explanation quality. Generally, one possible\ninterpretation of the quality of an explanation is its inherent value for\nteaching a related concept to a student. In this work, we extend artificial\nsimulatability studies to the domain of graph neural networks. Instead of\ncostly human trials, we use explanation-supervisable graph neural networks to\nperform simulatability studies to quantify the inherent usefulness of\nattributional graph explanations. We perform an extensive ablation study to\ninvestigate the conditions under which the proposed analyses are most\nmeaningful. We additionally validate our methods applicability on real-world\ngraph classification and regression datasets. We find that relevant\nexplanations can significantly boost the sample efficiency of graph neural\nnetworks and analyze the robustness towards noise and bias in the explanations.\nWe believe that the notion of usefulness obtained from our proposed\nsimulatability analysis provides a dimension of explanation quality that is\nlargely orthogonal to the common practice of faithfulness and has great\npotential to expand the toolbox of explanation quality assessments,\nspecifically for graph explanations.",
                "Mapping ChatGPT in Mainstream Media to Unravel Jobs and Diversity\n  Challenges: Early Quantitative Insights through Sentiment Analysis and Word\n  Frequency Analysis\nThe exponential growth in user acquisition and popularity of OpenAIs ChatGPT,\nan artificial intelligence(AI) powered chatbot, was accompanied by widespread\nmainstream media coverage. This article presents a quantitative data analysis\nof the early trends and sentiments revealed by conducting text mining and NLP\nmethods onto a corpus of 10,902 mainstream news headlines related to the\nsubject of ChatGPT and artificial intelligence, from the launch of ChatGPT in\nNovember 2022 to March 2023. The findings revealed in sentiment analysis,\nChatGPT and artificial intelligence, were perceived more positively than\nnegatively in the mainstream media. In regards to word frequency results, over\nsixty-five percent of the top frequency words were focused on Big Tech issues\nand actors while topics such as jobs, diversity, ethics, copyright, gender and\nwomen were poorly represented or completely absent and only accounted for six\npercent of the total corpus. This article is a critical analysis into the power\nstructures and collusions between Big Tech and Big Media in their hegemonic\nexclusion of diversity and job challenges from mainstream media."
            ],
            "interesting paper": 1
        },
        {
            "papers": [
                "Training Socially Aligned Language Models on Simulated Social\n  Interactions\nSocial alignment in AI systems aims to ensure that these models behave\naccording to established societal values. However, unlike humans, who derive\nconsensus on value judgments through social interaction, current language\nmodels (LMs) are trained to rigidly replicate their training corpus in\nisolation, leading to subpar generalization in unfamiliar scenarios and\nvulnerability to adversarial attacks. This work presents a novel training\nparadigm that permits LMs to learn from simulated social interactions. In\ncomparison to existing methodologies, our approach is considerably more\nscalable and efficient, demonstrating superior performance in alignment\nbenchmarks and human evaluations. This paradigm shift in the training of LMs\nbrings us a step closer to developing AI systems that can robustly and\naccurately reflect societal norms and values.",
                "Mindstorms in Natural Language-Based Societies of Mind\nBoth Minsky's \"society of mind\" and Schmidhuber's \"learning to think\" inspire\ndiverse societies of large multimodal neural networks (NNs) that solve problems\nby interviewing each other in a \"mindstorm.\" Recent implementations of NN-based\nsocieties of minds consist of large language models (LLMs) and other NN-based\nexperts communicating through a natural language interface. In doing so, they\novercome the limitations of single LLMs, improving multimodal zero-shot\nreasoning. In these natural language-based societies of mind (NLSOMs), new\nagents -- all communicating through the same universal symbolic language -- are\neasily added in a modular fashion. To demonstrate the power of NLSOMs, we\nassemble and experiment with several of them (having up to 129 members),\nleveraging mindstorms in them to solve some practical AI tasks: visual question\nanswering, image captioning, text-to-image synthesis, 3D generation, egocentric\nretrieval, embodied AI, and general language-based task solving. We view this\nas a starting point towards much larger NLSOMs with billions of agents-some of\nwhich may be humans. And with this emergence of great societies of\nheterogeneous minds, many new research questions have suddenly become paramount\nto the future of artificial intelligence. What should be the social structure\nof an NLSOM? What would be the (dis)advantages of having a monarchical rather\nthan a democratic structure? How can principles of NN economies be used to\nmaximize the total reward of a reinforcement learning NLSOM? In this work, we\nidentify, discuss, and try to answer some of these questions.",
                "Attention Schema in Neural Agents\nAttention has become a common ingredient in deep learning architectures. It\nadds a dynamical selection of information on top of the static selection of\ninformation supported by weights. In the same way, we can imagine a\nhigher-order informational filter built on top of attention: an Attention\nSchema (AS), namely, a descriptive and predictive model of attention. In\ncognitive neuroscience, Attention Schema Theory (AST) supports this idea of\ndistinguishing attention from AS. A strong prediction of this theory is that an\nagent can use its own AS to also infer the states of other agents' attention\nand consequently enhance coordination with other agents. As such, multi-agent\nreinforcement learning would be an ideal setting to experimentally test the\nvalidity of AST. We explore different ways in which attention and AS interact\nwith each other. Our preliminary results indicate that agents that implement\nthe AS as a recurrent internal control achieve the best performance. In\ngeneral, these exploratory experiments suggest that equipping artificial agents\nwith a model of attention can enhance their social intelligence.",
                "A Framework for Incentivized Collaborative Learning\nCollaborations among various entities, such as companies, research labs, AI\nagents, and edge devices, have become increasingly crucial for achieving\nmachine learning tasks that cannot be accomplished by a single entity alone.\nThis is likely due to factors such as security constraints, privacy concerns,\nand limitations in computation resources. As a result, collaborative learning\n(CL) research has been gaining momentum. However, a significant challenge in\npractical applications of CL is how to effectively incentivize multiple\nentities to collaborate before any collaboration occurs. In this study, we\npropose ICL, a general framework for incentivized collaborative learning, and\nprovide insights into the critical issue of when and why incentives can improve\ncollaboration performance. Furthermore, we show the broad applicability of ICL\nto specific cases in federated learning, assisted learning, and multi-armed\nbandit with both theory and experimental results.",
                "Moral Machine or Tyranny of the Majority?\nWith Artificial Intelligence systems increasingly applied in consequential\ndomains, researchers have begun to ask how these systems ought to act in\nethically charged situations where even humans lack consensus. In the Moral\nMachine project, researchers crowdsourced answers to \"Trolley Problems\"\nconcerning autonomous vehicles. Subsequently, Noothigattu et al. (2018)\nproposed inferring linear functions that approximate each individual's\npreferences and aggregating these linear models by averaging parameters across\nthe population. In this paper, we examine this averaging mechanism, focusing on\nfairness concerns in the presence of strategic effects. We investigate a simple\nsetting where the population consists of two groups, with the minority\nconstituting an {\\alpha} < 0.5 share of the population. To simplify the\nanalysis, we consider the extreme case in which within-group preferences are\nhomogeneous. Focusing on the fraction of contested cases where the minority\ngroup prevails, we make the following observations: (a) even when all parties\nreport their preferences truthfully, the fraction of disputes where the\nminority prevails is less than proportionate in {\\alpha}; (b) the degree of\nsub-proportionality grows more severe as the level of disagreement between the\ngroups increases; (c) when parties report preferences strategically, pure\nstrategy equilibria do not always exist; and (d) whenever a pure strategy\nequilibrium exists, the majority group prevails 100% of the time. These\nfindings raise concerns about stability and fairness of preference vector\naveraging as a mechanism for aggregating diverging voices. Finally, we discuss\nalternatives, including randomized dictatorship and median-based mechanisms.",
                "Towards Cognitive Bots: Architectural Research Challenges\nSoftware bots operating in multiple virtual digital platforms must understand\nthe platforms' affordances and behave like human users. Platform affordances or\nfeatures differ from one application platform to another or through a life\ncycle, requiring such bots to be adaptable. Moreover, bots in such platforms\ncould cooperate with humans or other software agents for work or to learn\nspecific behavior patterns. However, present-day bots, particularly chatbots,\nother than language processing and prediction, are far from reaching a human\nuser's behavior level within complex business information systems. They lack\nthe cognitive capabilities to sense and act in such virtual environments,\nrendering their development a challenge to artificial general intelligence\nresearch. In this study, we problematize and investigate assumptions in\nconceptualizing software bot architecture by directing attention to significant\narchitectural research challenges in developing cognitive bots endowed with\ncomplex behavior for operation on information systems. As an outlook, we\npropose alternate architectural assumptions to consider in future bot design\nand bot development frameworks."
            ],
            "interesting paper": 1
        },
        {
            "papers": [
                "AI Coach Assist: An Automated Approach for Call Recommendation in\n  Contact Centers for Agent Coaching\nIn recent years, the utilization of Artificial Intelligence (AI) in the\ncontact center industry is on the rise. One area where AI can have a\nsignificant impact is in the coaching of contact center agents. By analyzing\ncall transcripts using Natural Language Processing (NLP) techniques, it would\nbe possible to quickly determine which calls are most relevant for coaching\npurposes. In this paper, we present AI Coach Assist, which leverages the\npre-trained transformer-based language models to determine whether a given call\nis coachable or not based on the quality assurance (QA) questions asked by the\ncontact center managers or supervisors. The system was trained and evaluated on\na large dataset collected from real-world contact centers and provides an\neffective way to recommend calls to the contact center managers that are more\nlikely to contain coachable moments. Our experimental findings demonstrate the\npotential of AI Coach Assist to improve the coaching process, resulting in\nenhancing the performance of contact center agents.",
                "Modeling Dynamic Environments with Scene Graph Memory\nEmbodied AI agents that search for objects in large environments such as\nhouseholds often need to make efficient decisions by predicting object\nlocations based on partial information. We pose this as a new type of link\nprediction problem: link prediction on partially observable dynamic graphs. Our\ngraph is a representation of a scene in which rooms and objects are nodes, and\ntheir relationships are encoded in the edges; only parts of the changing graph\nare known to the agent at each timestep. This partial observability poses a\nchallenge to existing link prediction approaches, which we address. We propose\na novel state representation -- Scene Graph Memory (SGM) -- with captures the\nagent's accumulated set of observations, as well as a neural net architecture\ncalled a Node Edge Predictor (NEP) that extracts information from the SGM to\nsearch efficiently. We evaluate our method in the Dynamic House Simulator, a\nnew benchmark that creates diverse dynamic graphs following the semantic\npatterns typically seen at homes, and show that NEP can be trained to predict\nthe locations of objects in a variety of environments with diverse object\nmovement dynamics, outperforming baselines both in terms of new scene\nadaptability and overall accuracy. The codebase and more can be found at\nhttps://www.scenegraphmemory.com.",
                "Integrating Action Knowledge and LLMs for Task Planning and Situation\n  Handling in Open Worlds\nTask planning systems have been developed to help robots use human knowledge\n(about actions) to complete long-horizon tasks. Most of them have been\ndeveloped for \"closed worlds\" while assuming the robot is provided with\ncomplete world knowledge. However, the real world is generally open, and the\nrobots frequently encounter unforeseen situations that can potentially break\nthe planner's completeness. Could we leverage the recent advances on\npre-trained Large Language Models (LLMs) to enable classical planning systems\nto deal with novel situations?\n  This paper introduces a novel framework, called COWP, for open-world task\nplanning and situation handling. COWP dynamically augments the robot's action\nknowledge, including the preconditions and effects of actions, with\ntask-oriented commonsense knowledge. COWP embraces the openness from LLMs, and\nis grounded to specific domains via action knowledge. For systematic\nevaluations, we collected a dataset that includes 1,085 execution-time\nsituations. Each situation corresponds to a state instance wherein a robot is\npotentially unable to complete a task using a solution that normally works.\nExperimental results show that our approach outperforms competitive baselines\nfrom the literature in the success rate of service tasks. Additionally, we have\ndemonstrated COWP using a mobile manipulator. Supplementary materials are\navailable at: https://cowplanning.github.io/",
                "Synthesizing a Progression of Subtasks for Block-Based Visual\n  Programming Tasks\nBlock-based visual programming environments play an increasingly important\nrole in introducing computing concepts to K-12 students. In recent years, they\nhave also gained popularity in neuro-symbolic AI, serving as a benchmark to\nevaluate general problem-solving and logical reasoning skills. The open-ended\nand conceptual nature of these visual programming tasks make them challenging,\nboth for state-of-the-art AI agents as well as for novice programmers. A\nnatural approach to providing assistance for problem-solving is breaking down a\ncomplex task into a progression of simpler subtasks; however, this is not\ntrivial given that the solution codes are typically nested and have non-linear\nexecution behavior. In this paper, we formalize the problem of synthesizing\nsuch a progression for a given reference block-based visual programming task.\nWe propose a novel synthesis algorithm that generates a progression of subtasks\nthat are high-quality, well-spaced in terms of their complexity, and solving\nthis progression leads to solving the reference task. We show the utility of\nour synthesis algorithm in improving the efficacy of AI agents (in this case,\nneural program synthesizers) for solving tasks in the Karel programming\nenvironment. Then, we conduct a user study to demonstrate that our synthesized\nprogression of subtasks can assist a novice programmer in solving tasks in the\nHour of Code: Maze Challenge by Code-dot-org.",
                "Observation Denoising in CYRUS Soccer Simulation 2D Team For RoboCup\n  2023\nThe RoboCup competitions hold various leagues, and the Soccer Simulation 2D\nLeague is a major one among them. Soccer Simulation 2D (SS2D) match involves\ntwo teams, including 11 players and a coach, competing against each other. The\nplayers can only communicate with the Soccer Simulation Server during the game.\nThis paper presents the latest research of the CYRUS soccer simulation 2D team,\nthe champion of RoboCup 2021. We will explain our denoising idea powered by\nlong short-term memory networks (LSTM) and deep neural networks (DNN). The\nCYRUS team uses the CYRUS2D base code that was developed based on the Helios\nand Gliders bases.",
                "On the Value of Myopic Behavior in Policy Reuse\nLeveraging learned strategies in unfamiliar scenarios is fundamental to human\nintelligence. In reinforcement learning, rationally reusing the policies\nacquired from other tasks or human experts is critical for tackling problems\nthat are difficult to learn from scratch. In this work, we present a framework\ncalled Selective Myopic bEhavior Control~(SMEC), which results from the insight\nthat the short-term behaviors of prior policies are sharable across tasks. By\nevaluating the behaviors of prior policies via a hybrid value function\narchitecture, SMEC adaptively aggregates the sharable short-term behaviors of\nprior policies and the long-term behaviors of the task policy, leading to\ncoordinated decisions. Empirical results on a collection of manipulation and\nlocomotion tasks demonstrate that SMEC outperforms existing methods, and\nvalidate the ability of SMEC to leverage related prior policies."
            ],
            "interesting paper": 3
        },
        {
            "papers": [
                "Re-imagining health and well-being in low resource African settings\n  using an augmented AI system and a 3D digital twin\nThis paper discusses and explores the potential and relevance of recent\ndevelopments in artificial intelligence (AI) and digital twins for health and\nwell-being in low-resource African countries. We use the case of public health\nemergency response to disease outbreaks and epidemic control. There is\npotential to take advantage of the increasing availability of data and\ndigitization to develop advanced AI methods for analysis and prediction. Using\nan AI systems perspective, we review emerging trends in AI systems and digital\ntwins and propose an initial augmented AI system architecture to illustrate how\nan AI system can work with a 3D digital twin to address public health goals. We\nhighlight scientific knowledge discovery, continual learning, pragmatic\ninteroperability, and interactive explanation and decision-making as essential\nresearch challenges for AI systems and digital twins.",
                "AI Audit: A Card Game to Reflect on Everyday AI Systems\nAn essential element of K-12 AI literacy is educating learners about the\nethical and societal implications of AI systems. Previous work in AI ethics\nliteracy have developed curriculum and classroom activities that engage\nlearners in reflecting on the ethical implications of AI systems and developing\nresponsible AI. There is little work in using game-based learning methods in AI\nliteracy. Games are known to be compelling media to teach children about\ncomplex STEM concepts. In this work, we developed a competitive card game for\nmiddle and high school students called \"AI Audit\" where they play as AI\nstart-up founders building novel AI-powered technology. Players can challenge\nother players with potential harms of their technology or defend their own\nbusinesses by features that mitigate these harms. The game mechanics reward\nsystems that are ethically developed or that take steps to mitigate potential\nharms. In this paper, we present the game design, teacher resources for\nclassroom deployment and early playtesting results. We discuss our reflections\nabout using games as teaching tools for AI literacy in K-12 classrooms.",
                "The Social Value of Dark Energy\nAstrophysics is a social enterprise exemplified here by the Dark Energy\nSurvey (DES) which completed its fieldwork in 2019 after 16 years of\npreparation and observation, while data analysis continues. Society funds\nastrophysics on a grand scale. For human capital and for governance the\ndiscipline draws on a self-governing \"republic of science\", while the funds\nwere provided by philanthropists in the past, and by governments today. The\nbenefits accrue initially to scientists themselves, in the form of a rewarding\nvocation. For the social benefit it is tempting to apply formal cost benefit\nanalysis, but that approach ignores the option value of science and imposes\nquestionable assumptions from welfare economics. Astrophysics generates some\nuseful spinoffs, offers attractive careers, appeals to the popular imagination,\nspeaks to metaphysical cravings and constitutes a good in itself. The rise of\nAI also suggests a role in exploring future habitats for intelligence and\ncognition.",
                "Voluminous yet Vacuous? Semantic Capital in an Age of Large Language\n  Models\nLarge Language Models (LLMs) have emerged as transformative forces in the\nrealm of natural language processing, wielding the power to generate human-like\ntext. However, despite their potential for content creation, they carry the\nrisk of eroding our Semantic Capital (SC) - the collective knowledge within our\ndigital ecosystem - thereby posing diverse social epistemic challenges. This\npaper explores the evolution, capabilities, and limitations of these models,\nwhile highlighting ethical concerns they raise. The study contribution is\ntwo-fold: first, it is acknowledged that, withstanding the challenges of\ntracking and controlling LLM impacts, it is necessary to reconsider our\ninteraction with these AI technologies and the narratives that form public\nperception of them. It is argued that before achieving this goal, it is\nessential to confront a potential deontological tipping point in an increasing\nAI-driven infosphere. This goes beyond just adhering to AI ethical norms or\nregulations and requires understanding the spectrum of social epistemic risks\nLLMs might bring to our collective SC. Secondly, building on Luciano Floridi's\ntaxonomy for SC risks, those are mapped within the functionality and\nconstraints of LLMs. By this outlook, we aim to protect and enrich our SC while\nfostering a collaborative environment between humans and AI that augments human\nintelligence rather than replacing it.",
                "Taming AI Bots: Controllability of Neural States in Large Language\n  Models\nWe tackle the question of whether an agent can, by suitable choice of\nprompts, control an AI bot to any state. To that end, we first introduce a\nformal definition of ``meaning'' that is amenable to analysis. Then, we\ncharacterize ``meaningful data'' on which large language models (LLMs) are\nostensibly trained, and ``well-trained LLMs'' through conditions that are\nlargely met by today's LLMs. While a well-trained LLM constructs an embedding\nspace of meanings that is Euclidean, meanings themselves do not form a vector\n(linear) subspace, but rather a quotient space within. We then characterize the\nsubset of meanings that can be reached by the state of the LLMs for some input\nprompt, and show that a well-trained bot can reach any meaning albeit with\nsmall probability. We then introduce a stronger notion of controllability as\n{\\em almost certain reachability}, and show that, when restricted to the space\nof meanings, an AI bot is controllable. We do so after introducing a functional\ncharacterization of attentive AI bots, and finally derive necessary and\nsufficient conditions for controllability. The fact that AI bots are\ncontrollable means that an adversary could steer them towards any state.\nHowever, the sampling process can be designed to counteract adverse actions and\navoid reaching undesirable regions of state space before their boundary is\ncrossed.",
                "The Digital Divide in Process Safety: Quantitative Risk Analysis of\n  Human-AI Collaboration\nDigital technologies have dramatically accelerated the digital transformation\nin process industries, boosted new industrial applications, upgraded the\nproduction system, and enhanced operational efficiency. In contrast, the\nchallenges and gaps between human and artificial intelligence (AI) have become\nmore and more prominent, whereas the digital divide in process safety is\naggregating. The study attempts to address the following questions: (i)What is\nAI in the process safety context? (ii)What is the difference between AI and\nhumans in process safety? (iii)How do AI and humans collaborate in process\nsafety? (iv)What are the challenges and gaps in human-AI collaboration? (v)How\nto quantify the risk of human-AI collaboration in process safety? Qualitative\nrisk analysis based on brainstorming and literature review, and quantitative\nrisk analysis based on layer of protection analysis (LOPA) and Bayesian network\n(BN), were applied to explore and model. The importance of human reliability\nshould be stressed in the digital age, not usually to increase the reliability\nof AI, and human-centered AI design in process safety needs to be propagated."
            ],
            "interesting paper": 2
        },
        {
            "papers": [
                "Towards a Unifying Model of Rationality in Multiagent Systems\nMultiagent systems deployed in the real world need to cooperate with other\nagents (including humans) nearly as effectively as these agents cooperate with\none another. To design such AI, and provide guarantees of its effectiveness, we\nneed to clearly specify what types of agents our AI must be able to cooperate\nwith. In this work we propose a generic model of socially intelligent agents,\nwhich are individually rational learners that are also able to cooperate with\none another (in the sense that their joint behavior is Pareto efficient). We\ndefine rationality in terms of the regret incurred by each agent over its\nlifetime, and show how we can construct socially intelligent agents for\ndifferent forms of regret. We then discuss the implications of this model for\nthe development of \"robust\" MAS that can cooperate with a wide variety of\nsocially intelligent agents.",
                "RE-centric Recommendations for the Development of Trustworthy(er)\n  Autonomous Systems\nComplying with the EU AI Act (AIA) guidelines while developing and\nimplementing AI systems will soon be mandatory within the EU. However,\npractitioners lack actionable instructions to operationalise ethics during AI\nsystems development. A literature review of different ethical guidelines\nrevealed inconsistencies in the principles addressed and the terminology used\nto describe them. Furthermore, requirements engineering (RE), which is\nidentified to foster trustworthiness in the AI development process from the\nearly stages was observed to be absent in a lot of frameworks that support the\ndevelopment of ethical and trustworthy AI. This incongruous phrasing combined\nwith a lack of concrete development practices makes trustworthy AI development\nharder. To address this concern, we formulated a comparison table for the\nterminology used and the coverage of the ethical AI principles in major ethical\nAI guidelines. We then examined the applicability of ethical AI development\nframeworks for performing effective RE during the development of trustworthy AI\nsystems. A tertiary review and meta-analysis of literature discussing ethical\nAI frameworks revealed their limitations when developing trustworthy AI. Based\non our findings, we propose recommendations to address such limitations during\nthe development of trustworthy AI.",
                "Doing the right thing for the right reason: Evaluating artificial moral\n  cognition by probing cost insensitivity\nIs it possible to evaluate the moral cognition of complex artificial agents?\nIn this work, we take a look at one aspect of morality: `doing the right thing\nfor the right reasons.' We propose a behavior-based analysis of artificial\nmoral cognition which could also be applied to humans to facilitate\nlike-for-like comparison. Morally-motivated behavior should persist despite\nmounting cost; by measuring an agent's sensitivity to this cost, we gain deeper\ninsight into underlying motivations. We apply this evaluation to a particular\nset of deep reinforcement learning agents, trained by memory-based\nmeta-reinforcement learning. Our results indicate that agents trained with a\nreward function that includes other-regarding preferences perform helping\nbehavior in a way that is less sensitive to increasing cost than agents trained\nwith more self-interested preferences.",
                "An Annotated Dataset for Explainable Interpersonal Risk Factors of\n  Mental Disturbance in Social Media Posts\nWith a surge in identifying suicidal risk and its severity in social media\nposts, we argue that a more consequential and explainable research is required\nfor optimal impact on clinical psychology practice and personalized mental\nhealthcare. The success of computational intelligence techniques for inferring\nmental illness from social media resources, points to natural language\nprocessing as a lens for determining Interpersonal Risk Factors (IRF) in human\nwritings. Motivated with limited availability of datasets for social NLP\nresearch community, we construct and release a new annotated dataset with\nhuman-labelled explanations and classification of IRF affecting mental\ndisturbance on social media: (i) Thwarted Belongingness (TBe), and (ii)\nPerceived Burdensomeness (PBu). We establish baseline models on our dataset\nfacilitating future research directions to develop real-time personalized AI\nmodels by detecting patterns of TBe and PBu in emotional spectrum of user's\nhistorical social media profile.",
                "Generalized Disparate Impact for Configurable Fairness Solutions in ML\nWe make two contributions in the field of AI fairness over continuous\nprotected attributes. First, we show that the Hirschfeld-Gebelein-Renyi (HGR)\nindicator (the only one currently available for such a case) is valuable but\nsubject to a few crucial limitations regarding semantics, interpretability, and\nrobustness. Second, we introduce a family of indicators that are: 1)\ncomplementary to HGR in terms of semantics; 2) fully interpretable and\ntransparent; 3) robust over finite samples; 4) configurable to suit specific\napplications. Our approach also allows us to define fine-grained constraints to\npermit certain types of dependence and forbid others selectively. By expanding\nthe available options for continuous protected attributes, our approach\nrepresents a significant contribution to the area of fair artificial\nintelligence.",
                "Perceived Trustworthiness of Natural Language Generators\nNatural Language Generation tools, such as chatbots that can generate\nhuman-like conversational text, are becoming more common both for personal and\nprofessional use. However, there are concerns about their trustworthiness and\nethical implications. The paper addresses the problem of understanding how\ndifferent users (e.g., linguists, engineers) perceive and adopt these tools and\ntheir perception of machine-generated text quality. It also discusses the\nperceived advantages and limitations of Natural Language Generation tools, as\nwell as users' beliefs on governance strategies. The main findings of this\nstudy include the impact of users' field and level of expertise on the\nperceived trust and adoption of Natural Language Generation tools, the users'\nassessment of the accuracy, fluency, and potential biases of machine-generated\ntext in comparison to human-written text, and an analysis of the advantages and\nethical risks associated with these tools as identified by the participants.\nMoreover, this paper discusses the potential implications of these findings for\nenhancing the AI development process. The paper sheds light on how different\nuser characteristics shape their beliefs on the quality and overall\ntrustworthiness of machine-generated text. Furthermore, it examines the\nbenefits and risks of these tools from the perspectives of different users."
            ],
            "interesting paper": 4
        },
        {
            "papers": [
                "Intent-aligned AI systems deplete human agency: the need for agency\n  foundations research in AI safety\nThe rapid advancement of artificial intelligence (AI) systems suggests that\nartificial general intelligence (AGI) systems may soon arrive. Many researchers\nare concerned that AIs and AGIs will harm humans via intentional misuse\n(AI-misuse) or through accidents (AI-accidents). In respect of AI-accidents,\nthere is an increasing effort focused on developing algorithms and paradigms\nthat ensure AI systems are aligned to what humans intend, e.g. AI systems that\nyield actions or recommendations that humans might judge as consistent with\ntheir intentions and goals. Here we argue that alignment to human intent is\ninsufficient for safe AI systems and that preservation of long-term agency of\nhumans may be a more robust standard, and one that needs to be separated\nexplicitly and a priori during optimization. We argue that AI systems can\nreshape human intention and discuss the lack of biological and psychological\nmechanisms that protect humans from loss of agency. We provide the first formal\ndefinition of agency-preserving AI-human interactions which focuses on\nforward-looking agency evaluations and argue that AI systems - not humans -\nmust be increasingly tasked with making these evaluations. We show how agency\nloss can occur in simple environments containing embedded agents that use\ntemporal-difference learning to make action recommendations. Finally, we\npropose a new area of research called \"agency foundations\" and pose four\ninitial topics designed to improve our understanding of agency in AI-human\ninteractions: benevolent game theory, algorithmic foundations of human rights,\nmechanistic interpretability of agency representation in neural-networks and\nreinforcement learning from internal states.",
                "Disentangling and Operationalizing AI Fairness at LinkedIn\nOperationalizing AI fairness at LinkedIn's scale is challenging not only\nbecause there are multiple mutually incompatible definitions of fairness but\nalso because determining what is fair depends on the specifics and context of\nthe product where AI is deployed. Moreover, AI practitioners need clarity on\nwhat fairness expectations need to be addressed at the AI level. In this paper,\nwe present the evolving AI fairness framework used at LinkedIn to address these\nthree challenges. The framework disentangles AI fairness by separating out\nequal treatment and equitable product expectations. Rather than imposing a\ntrade-off between these two commonly opposing interpretations of fairness, the\nframework provides clear guidelines for operationalizing equal AI treatment\ncomplemented with a product equity strategy. This paper focuses on the equal AI\ntreatment component of LinkedIn's AI fairness framework, shares the principles\nthat support it, and illustrates their application through a case study. We\nhope this paper will encourage other big tech companies to join us in sharing\ntheir approach to operationalizing AI fairness at scale, so that together we\ncan keep advancing this constantly evolving field.",
                "Seeing Seeds Beyond Weeds: Green Teaming Generative AI for Beneficial\n  Uses\nLarge generative AI models (GMs) like GPT and DALL-E are trained to generate\ncontent for general, wide-ranging purposes. GM content filters are generalized\nto filter out content which has a risk of harm in many cases, e.g., hate\nspeech. However, prohibited content is not always harmful -- there are\ninstances where generating prohibited content can be beneficial. So, when GMs\nfilter out content, they preclude beneficial use cases along with harmful ones.\nWhich use cases are precluded reflects the values embedded in GM content\nfiltering. Recent work on red teaming proposes methods to bypass GM content\nfilters to generate harmful content. We coin the term green teaming to describe\nmethods of bypassing GM content filters to design for beneficial use cases. We\nshowcase green teaming by: 1) Using ChatGPT as a virtual patient to simulate a\nperson experiencing suicidal ideation, for suicide support training; 2) Using\nCodex to intentionally generate buggy solutions to train students on debugging;\nand 3) Examining an Instagram page using Midjourney to generate images of\nanti-LGBTQ+ politicians in drag. Finally, we discuss how our use cases\ndemonstrate green teaming as both a practical design method and a mode of\ncritique, which problematizes and subverts current understandings of harms and\nvalues in generative AI.",
                "Building Extractive Question Answering System to Support Human-AI Health\n  Coaching Model for Sleep Domain\nNon-communicable diseases (NCDs) are a leading cause of global deaths,\nnecessitating a focus on primary prevention and lifestyle behavior change.\nHealth coaching, coupled with Question Answering (QA) systems, has the\npotential to transform preventive healthcare. This paper presents a\nhuman-Artificial Intelligence (AI) health coaching model incorporating a\ndomain-specific extractive QA system. A sleep-focused dataset, SleepQA, was\nmanually assembled and used to fine-tune domain-specific BERT models. The QA\nsystem was evaluated using automatic and human methods. A data-centric\nframework enhanced the system's performance by improving passage retrieval and\nquestion reformulation. Although the system did not outperform the baseline in\nautomatic evaluation, it excelled in the human evaluation of real-world\nquestions. Integration into a Human-AI health coaching model was tested in a\npilot Randomized Controlled Trial (RCT).",
                "Bigger, Better, Faster: Human-level Atari with human-level efficiency\nWe introduce a value-based RL agent, which we call BBF, that achieves\nsuper-human performance in the Atari 100K benchmark. BBF relies on scaling the\nneural networks used for value estimation, as well as a number of other design\nchoices that enable this scaling in a sample-efficient manner. We conduct\nextensive analyses of these design choices and provide insights for future\nwork. We end with a discussion about updating the goalposts for\nsample-efficient RL research on the ALE. We make our code and data publicly\navailable at\nhttps://github.com/google-research/google-research/tree/master/bigger_better_faster.",
                "NetHack is Hard to Hack\nNeural policy learning methods have achieved remarkable results in various\ncontrol problems, ranging from Atari games to simulated locomotion. However,\nthese methods struggle in long-horizon tasks, especially in open-ended\nenvironments with multi-modal observations, such as the popular dungeon-crawler\ngame, NetHack. Intriguingly, the NeurIPS 2021 NetHack Challenge revealed that\nsymbolic agents outperformed neural approaches by over four times in median\ngame score. In this paper, we delve into the reasons behind this performance\ngap and present an extensive study on neural policy learning for NetHack. To\nconduct this study, we analyze the winning symbolic agent, extending its\ncodebase to track internal strategy selection in order to generate one of the\nlargest available demonstration datasets. Utilizing this dataset, we examine\n(i) the advantages of an action hierarchy; (ii) enhancements in neural\narchitecture; and (iii) the integration of reinforcement learning with\nimitation learning. Our investigations produce a state-of-the-art neural agent\nthat surpasses previous fully neural policies by 127% in offline settings and\n25% in online settings on median game score. However, we also demonstrate that\nmere scaling is insufficient to bridge the performance gap with the best\nsymbolic models or even the top human players."
            ],
            "interesting paper": 3
        },
        {
            "papers": [
                "From Human-Centered to Social-Centered Artificial Intelligence:\n  Assessing ChatGPT's Impact through Disruptive Events\nLarge language models (LLMs) and dialogue agents have existed for years, but\nthe release of recent GPT models has been a watershed moment for artificial\nintelligence (AI) research and society at large. Immediately recognized for its\ngenerative capabilities and versatility, ChatGPT's impressive proficiency\nacross technical and creative domains led to its widespread adoption. While\nsociety grapples with the emerging cultural impacts of ChatGPT, critiques of\nChatGPT's impact within the machine learning community have coalesced around\nits performance or other conventional Responsible AI evaluations relating to\nbias, toxicity, and 'hallucination.' We argue that these latter critiques draw\nheavily on a particular conceptualization of the 'human-centered' framework,\nwhich tends to cast atomized individuals as the key recipients of both the\nbenefits and detriments of technology. In this article, we direct attention to\nanother dimension of LLMs and dialogue agents' impact: their effect on social\ngroups, institutions, and accompanying norms and practices. By illustrating\nChatGPT's social impact through three disruptive events, we challenge\nindividualistic approaches in AI development and contribute to ongoing debates\naround the ethical and responsible implementation of AI systems. We hope this\neffort will call attention to more comprehensive and longitudinal evaluation\ntools and compel technologists to go beyond human-centered thinking and ground\ntheir efforts through social-centered AI.",
                "Human or Not? A Gamified Approach to the Turing Test\nWe present \"Human or Not?\", an online game inspired by the Turing test, that\nmeasures the capability of AI chatbots to mimic humans in dialog, and of humans\nto tell bots from other humans. Over the course of a month, the game was played\nby over 1.5 million users who engaged in anonymous two-minute chat sessions\nwith either another human or an AI language model which was prompted to behave\nlike humans. The task of the players was to correctly guess whether they spoke\nto a person or to an AI. This largest scale Turing-style test conducted to date\nrevealed some interesting facts. For example, overall users guessed the\nidentity of their partners correctly in only 68% of the games. In the subset of\nthe games in which users faced an AI bot, users had even lower correct guess\nrates of 60% (that is, not much higher than chance). This white paper details\nthe development, deployment, and results of this unique experiment. While this\nexperiment calls for many extensions and refinements, these findings already\nbegin to shed light on the inevitable near future which will commingle humans\nand AI.",
                "EMOTE: An Explainable architecture for Modelling the Other Through\n  Empathy\nWe can usually assume others have goals analogous to our own. This assumption\ncan also, at times, be applied to multi-agent games - e.g. Agent 1's attraction\nto green pellets is analogous to Agent 2's attraction to red pellets. This\n\"analogy\" assumption is tied closely to the cognitive process known as empathy.\nInspired by empathy, we design a simple and explainable architecture to model\nanother agent's action-value function. This involves learning an \"Imagination\nNetwork\" to transform the other agent's observed state in order to produce a\nhuman-interpretable \"empathetic state\" which, when presented to the learning\nagent, produces behaviours that mimic the other agent. Our approach is\napplicable to multi-agent scenarios consisting of a single learning agent and\nother (independent) agents acting according to fixed policies. This\narchitecture is particularly beneficial for (but not limited to) algorithms\nusing a composite value or reward function. We show our method produces better\nperformance in multi-agent games, where it robustly estimates the other's model\nin different environment configurations. Additionally, we show that the\nempathetic states are human interpretable, and thus verifiable.",
                "Citizen Perspectives on Necessary Safeguards to the Use of AI by Law\n  Enforcement Agencies\nIn the light of modern technological advances, Artificial Intelligence (AI)\nis relied upon to enhance performance, increase efficiency, and maximize gains.\nFor Law Enforcement Agencies (LEAs), it can prove valuable in optimizing\nevidence analysis and establishing proactive prevention measures. Nevertheless,\ncitizens raise legitimate concerns around privacy invasions, biases,\ninequalities, and inaccurate decisions. This study explores the views of 111\ncitizens towards AI use by police through interviews, and integrates societal\nconcerns along with propositions of safeguards from negative effects of AI use\nby LEAs in the context of cybercrime and terrorism.",
                "Responsible Design Patterns for Machine Learning Pipelines\nIntegrating ethical practices into the AI development process for artificial\nintelligence (AI) is essential to ensure safe, fair, and responsible operation.\nAI ethics involves applying ethical principles to the entire life cycle of AI\nsystems. This is essential to mitigate potential risks and harms associated\nwith AI, such as algorithm biases. To achieve this goal, responsible design\npatterns (RDPs) are critical for Machine Learning (ML) pipelines to guarantee\nethical and fair outcomes. In this paper, we propose a comprehensive framework\nincorporating RDPs into ML pipelines to mitigate risks and ensure the ethical\ndevelopment of AI systems. Our framework comprises new responsible AI design\npatterns for ML pipelines identified through a survey of AI ethics and data\nmanagement experts and validated through real-world scenarios with expert\nfeedback. The framework guides AI developers, data scientists, and\npolicy-makers to implement ethical practices in AI development and deploy\nresponsible AI systems in production.",
                "Environmental Memory Boosts Group Formation of Clueless Individuals\nThe formation of groups of interacting individuals improves performance and\nfitness in many decentralised systems, from micro-organisms to social insects,\nfrom robotic swarms to artificial intelligence algorithms. Often, group\nformation and high-level coordination in these systems emerge from individuals\nwith limited information-processing capabilities implementing low-level rules\nof communication to signal to each other. Here, we show that, even in a\ncommunity of clueless individuals incapable of processing information and\ncommunicating, a dynamic environment can coordinate group formation by\ntransiently storing memory of the earlier passage of individuals. Our results\nidentify a new mechanism of indirect coordination via shared memory that is\nprimarily promoted and reinforced by dynamic environmental factors, thus\novershadowing the need for any form of explicit signalling between individuals.\nWe expect this pathway to group formation to be relevant for understanding and\ncontrolling self-organisation and collective decision making in both living and\nartificial active matter in real-life environments."
            ],
            "interesting paper": 1
        },
        {
            "papers": [
                "The ethical ambiguity of AI data enrichment: Measuring gaps in research\n  ethics norms and practices\nThe technical progression of artificial intelligence (AI) research has been\nbuilt on breakthroughs in fields such as computer science, statistics, and\nmathematics. However, in the past decade AI researchers have increasingly\nlooked to the social sciences, turning to human interactions to solve the\nchallenges of model development. Paying crowdsourcing workers to generate or\ncurate data, or data enrichment, has become indispensable for many areas of AI\nresearch, from natural language processing to reinforcement learning from human\nfeedback (RLHF). Other fields that routinely interact with crowdsourcing\nworkers, such as Psychology, have developed common governance requirements and\nnorms to ensure research is undertaken ethically. This study explores how, and\nto what extent, comparable research ethics requirements and norms have\ndeveloped for AI research and data enrichment. We focus on the approach taken\nby two leading conferences: ICLR and NeurIPS, and journal publisher Springer.\nIn a longitudinal study of accepted papers, and via a comparison with\nPsychology and CHI papers, this work finds that leading AI venues have begun to\nestablish protocols for human data collection, but these are are inconsistently\nfollowed by authors. Whilst Psychology papers engaging with crowdsourcing\nworkers frequently disclose ethics reviews, payment data, demographic data and\nother information, similar disclosures are far less common in leading AI venues\ndespite similar guidance. The work concludes with hypotheses to explain these\ngaps in research ethics practices and considerations for its implications.",
                "AI and the creative realm: A short review of current and future\n  applications\nThis study explores the concept of creativity and artificial intelligence\n(AI) and their recent integration. While AI has traditionally been perceived as\nincapable of generating new ideas or creating art, the development of more\nsophisticated AI models and the proliferation of human-computer interaction\ntools have opened up new possibilities for AI in artistic creation. This study\ninvestigates the various applications of AI in a creative context,\ndifferentiating between the type of art, language, and algorithms used. It also\nconsiders the philosophical implications of AI and creativity, questioning\nwhether consciousness can be researched in machines and AI's potential\ninterests and decision-making capabilities. Overall, we aim to stimulate a\nreflection on AI's use and ethical implications in creative contexts.",
                "AI Liability Insurance With an Example in AI-Powered E-diagnosis System\nArtificial Intelligence (AI) has received an increasing amount of attention\nin multiple areas. The uncertainties and risks in AI-powered systems have\ncreated reluctance in their wild adoption. As an economic solution to\ncompensate for potential damages, AI liability insurance is a promising market\nto enhance the integration of AI into daily life. In this work, we use an\nAI-powered E-diagnosis system as an example to study AI liability insurance. We\nprovide a quantitative risk assessment model with evidence-based numerical\nanalysis. We discuss the insurability criteria for AI technologies and suggest\nnecessary adjustments to accommodate the features of AI products. We show that\nAI liability insurance can act as a regulatory mechanism to incentivize\ncompliant behaviors and serve as a certificate of high-quality AI systems.\nFurthermore, we suggest premium adjustment to reflect the dynamic evolution of\nthe inherent uncertainty in AI. Moral hazard problems are discussed and\nsuggestions for AI liability insurance are provided.",
                "Minding Language Models' (Lack of) Theory of Mind: A Plug-and-Play\n  Multi-Character Belief Tracker\nTheory of Mind (ToM)$\\unicode{x2014}$the ability to reason about the mental\nstates of other people$\\unicode{x2014}$is a key element of our social\nintelligence. Yet, despite their ever more impressive performance, large-scale\nneural language models still lack basic theory of mind capabilities\nout-of-the-box. We posit that simply scaling up models will not imbue them with\ntheory of mind due to the inherently symbolic and implicit nature of the\nphenomenon, and instead investigate an alternative: can we design a\ndecoding-time algorithm that enhances theory of mind of off-the-shelf neural\nlanguage models without explicit supervision? We present SymbolicToM, a\nplug-and-play approach to reason about the belief states of multiple characters\nin reading comprehension tasks via explicit symbolic representation. More\nconcretely, our approach tracks each entity's beliefs, their estimation of\nother entities' beliefs, and higher-order levels of reasoning, all through\ngraphical representations, allowing for more precise and interpretable\nreasoning than previous approaches. Empirical results on the well-known ToMi\nbenchmark (Le et al., 2019) demonstrate that SymbolicToM dramatically enhances\noff-the-shelf neural networks' theory of mind in a zero-shot setting while\nshowing robust out-of-distribution performance compared to supervised\nbaselines. Our work also reveals spurious patterns in existing theory of mind\nbenchmarks, emphasizing the importance of out-of-distribution evaluation and\nmethods that do not overfit a particular dataset.",
                "Egocentric Planning for Scalable Embodied Task Achievement\nEmbodied agents face significant challenges when tasked with performing\nactions in diverse environments, particularly in generalizing across object\ntypes and executing suitable actions to accomplish tasks. Furthermore, agents\nshould exhibit robustness, minimizing the execution of illegal actions. In this\nwork, we present Egocentric Planning, an innovative approach that combines\nsymbolic planning and Object-oriented POMDPs to solve tasks in complex\nenvironments, harnessing existing models for visual perception and natural\nlanguage processing. We evaluated our approach in ALFRED, a simulated\nenvironment designed for domestic tasks, and demonstrated its high scalability,\nachieving an impressive 36.07% unseen success rate in the ALFRED benchmark and\nwinning the ALFRED challenge at CVPR Embodied AI workshop. Our method requires\nreliable perception and the specification or learning of a symbolic description\nof the preconditions and effects of the agent's actions, as well as what object\ntypes reveal information about others. It is capable of naturally scaling to\nsolve new tasks beyond ALFRED, as long as they can be solved using the\navailable skills. This work offers a solid baseline for studying end-to-end and\nhybrid methods that aim to generalize to new tasks, including recent approaches\nrelying on LLMs, but often struggle to scale to long sequences of actions or\nproduce robust plans for novel tasks.",
                "LIV: Language-Image Representations and Rewards for Robotic Control\nWe present Language-Image Value learning (LIV), a unified objective for\nvision-language representation and reward learning from action-free videos with\ntext annotations. Exploiting a novel connection between dual reinforcement\nlearning and mutual information contrastive learning, the LIV objective trains\na multi-modal representation that implicitly encodes a universal value function\nfor tasks specified as language or image goals. We use LIV to pre-train the\nfirst control-centric vision-language representation from large human video\ndatasets such as EpicKitchen. Given only a language or image goal, the\npre-trained LIV model can assign dense rewards to each frame in videos of\nunseen robots or humans attempting that task in unseen environments. Further,\nwhen some target domain-specific data is available, the same objective can be\nused to fine-tune and improve LIV and even other pre-trained representations\nfor robotic control and reward specification in that domain. In our experiments\non several simulated and real-world robot environments, LIV models consistently\noutperform the best prior input state representations for imitation learning,\nas well as reward specification methods for policy synthesis. Our results\nvalidate the advantages of joint vision-language representation and reward\nlearning within the unified, compact LIV framework."
            ],
            "interesting paper": 4
        }
    ],
    "Casey Hernandez": [
        {
            "papers": [
                "A Mini Review on the utilization of Reinforcement Learning with OPC UA\nReinforcement Learning (RL) is a powerful machine learning paradigm that has\nbeen applied in various fields such as robotics, natural language processing\nand game playing achieving state-of-the-art results. Targeted to solve\nsequential decision making problems, it is by design able to learn from\nexperience and therefore adapt to changing dynamic environments. These\ncapabilities make it a prime candidate for controlling and optimizing complex\nprocesses in industry. The key to fully exploiting this potential is the\nseamless integration of RL into existing industrial systems. The industrial\ncommunication standard Open Platform Communications UnifiedArchitecture (OPC\nUA) could bridge this gap. However, since RL and OPC UA are from different\nfields,there is a need for researchers to bridge the gap between the two\ntechnologies. This work serves to bridge this gap by providing a brief\ntechnical overview of both technologies and carrying out a semi-exhaustive\nliterature review to gain insights on how RL and OPC UA are applied in\ncombination. With this survey, three main research topics have been identified,\nfollowing the intersection of RL with OPC UA. The results of the literature\nreview show that RL is a promising technology for the control and optimization\nof industrial processes, but does not yet have the necessary standardized\ninterfaces to be deployed in real-world scenarios with reasonably low effort.",
                "Local control for the collective dynamics of self-propelled particles\nUtilizing a paradigmatic model for the motion of interacting self-propelled\nparticles, we demonstrate that local accelerations at the level of individual\nparticles can drive transitions between different collective dynamics, leading\nto a control process. We find that the ability to trigger such transitions is\nhierarchically distributed among the particles and can form distinctive spatial\npatterns within the collective. Chaotic dynamics occur during the transitions,\nwhich can be attributed to fractal basin boundaries mediating the control\nprocess. The particle hierarchies described in this study offer decentralized\ncapabilities for controlling artificial swarms.",
                "Selection for short-term empowerment accelerates the evolution of\n  homeostatic neural cellular automata\nEmpowerment -- a domain independent, information-theoretic metric -- has\npreviously been shown to assist in the evolutionary search for neural cellular\nautomata (NCA) capable of homeostasis when employed as a fitness function. In\nour previous study, we successfully extended empowerment, defined as maximum\ntime-lagged mutual information between agents' actions and future sensations,\nto a distributed sensorimotor system embodied as an NCA. However, the\ntime-delay between actions and their corresponding sensations was arbitrarily\nchosen. Here, we expand upon previous work by exploring how the time scale at\nwhich empowerment operates impacts its efficacy as an auxiliary objective to\naccelerate the discovery of homeostatic NCAs. We show that shorter time delays\nresult in marked improvements over empowerment with longer delays, when\ncompared to evolutionary selection only for homeostasis. Moreover, we evaluate\nstability and adaptability of evolved NCAs, both hallmarks of living systems\nthat are of interest to replicate in artificial ones. We find that short-term\nempowered NCA are more stable and are capable of generalizing better to unseen\nhomeostatic challenges. Taken together, these findings motivate the use of\nempowerment during the evolution of other artifacts, and suggest how it should\nbe incorporated to accelerate evolution of desired behaviors for them. Source\ncode for the experiments in this paper can be found at:\nhttps://github.com/caitlingrasso/empowered-nca-II.",
                "Applications of Intelligent Systems in Green Technology\nIntelligent Systems (ISs) are technologically advanced machines, which\nperceive and respond to the environment around them. They are usually of\nvarious forms ranging from software to hardware. ISs are generally the fusion\nof Artificial Intelligence (AI), robotics and Internet of things (IoT). In\norder to strengthen ISs, one of the key technologies is green technology (GT).\nIt refers to the continuously advancing methods and materials, which cover\ntechniques for producing energy to non-toxic cleaning products. It may also be\nbroadened to saving energy, and reducing toxic and waste materials in the\nenvironment. The motto of GT can be achieved by using the ISs. In this paper,\nwe present various applications of ISs in GT. Moreover, we discuss various\npossible solutions using ISs in order to overcome the on-going real-life\nproblems.",
                "Cooperative Control of Multi-Channel Linear Systems with Self-Organizing\n  Private Agents\nCooperative behavior design for multi-agent systems with collective tasks is\na critical issue in promoting swarm intelligence. This paper investigates\ncooperative control for a multi-channel system, where each channel is managed\nby an agent expected to self-organize a controller to stabilize the system\ncollaboratively by communicating with neighbors in a network. Integrating a\nstate decomposition technique and a fusion approach, a fully distributed\nprivacy-preserving mechanism is proposed to shield agents' private information\nfrom neighbors' eavesdropping. Moreover, the cost of introducing the\nprivacy-preserving mechanism and the benefit of adding more channels to the\nsystem are quantitatively analyzed. Finally, comparative simulation examples\nare provided to demonstrate the effectiveness of the theoretical results.",
                "On the Planning Abilities of Large Language Models : A Critical\n  Investigation\nIntrigued by the claims of emergent reasoning capabilities in LLMs trained on\ngeneral web corpora, in this paper, we set out to investigate their planning\ncapabilities. We aim to evaluate (1) the effectiveness of LLMs in generating\nplans autonomously in commonsense planning tasks and (2) the potential of LLMs\nin LLM-Modulo settings where they act as a source of heuristic guidance for\nexternal planners and verifiers. We conduct a systematic study by generating a\nsuite of instances on domains similar to the ones employed in the International\nPlanning Competition and evaluate LLMs in two distinct modes: autonomous and\nheuristic. Our findings reveal that LLMs' ability to generate executable plans\nautonomously is rather limited, with the best model (GPT-4) having an average\nsuccess rate of ~12% across the domains. However, the results in the LLM-Modulo\nsetting show more promise. In the LLM-Modulo setting, we demonstrate that\nLLM-generated plans can improve the search process for underlying sound\nplanners and additionally show that external verifiers can help provide\nfeedback on the generated plans and back-prompt the LLM for better plan\ngeneration."
            ],
            "interesting paper": 2
        },
        {
            "papers": [
                "On Computing Universal Plans for Partially Observable Multi-Agent Path\n  Finding\nMulti-agent routing problems have drawn significant attention nowadays due to\ntheir broad industrial applications in, e.g., warehouse robots, logistics\nautomation, and traffic control. Conventionally, they are modelled as classical\nplanning problems. In this paper, we argue that it is beneficial to formulate\nthem as universal planning problems. We therefore propose universal plans, also\nknown as policies, as the solution concepts, and implement a system called\nASP-MAUPF (Answer Set Programming for Multi-Agent Universal Plan Finding) for\ncomputing them. Given an arbitrary two-dimensional map and a profile of goals\nfor the agents, the system finds a feasible universal plan for each agent that\nensures no collision with others. We use the system to conduct some\nexperiments, and make some observations on the types of goal profiles and\nenvironments that will have feasible policies, and how they may depend on\nagents' sensors. We also demonstrate how users can customize action preferences\nto compute more efficient policies, even (near-)optimal ones.",
                "Individuality in Swarm Robots with the Case Study of Kilobots: Noise,\n  Bug, or Feature?\nInter-individual differences are studied in natural systems, such as fish,\nbees, and humans, as they contribute to the complexity of both individual and\ncollective behaviors. However, individuality in artificial systems, such as\nrobotic swarms, is undervalued or even overlooked. Agent-specific deviations\nfrom the norm in swarm robotics are usually understood as mere noise that can\nbe minimized, for example, by calibration. We observe that robots have\nconsistent deviations and argue that awareness and knowledge of these can be\nexploited to serve a task. We measure heterogeneity in robot swarms caused by\nindividual differences in how robots act, sense, and oscillate. Our use case is\nKilobots and we provide example behaviors where the performance of robots\nvaries depending on individual differences. We show a non-intuitive example of\nphototaxis with Kilobots where the non-calibrated Kilobots show better\nperformance than the calibrated supposedly ``ideal\" one. We measure the\ninter-individual variations for heterogeneity in sensing and oscillation, too.\nWe briefly discuss how these variations can enhance the complexity of\ncollective behaviors. We suggest that by recognizing and exploring this new\nperspective on individuality, and hence diversity, in robotic swarms, we can\ngain a deeper understanding of these systems and potentially unlock new\npossibilities for their design and implementation of applications.",
                "Learning Safety Constraints from Demonstrations with Unknown Rewards\nWe propose Convex Constraint Learning for Reinforcement Learning (CoCoRL), a\nnovel approach for inferring shared constraints in a Constrained Markov\nDecision Process (CMDP) from a set of safe demonstrations with possibly\ndifferent reward functions. While previous work is limited to demonstrations\nwith known rewards or fully known environment dynamics, CoCoRL can learn\nconstraints from demonstrations with different unknown rewards without\nknowledge of the environment dynamics. CoCoRL constructs a convex safe set\nbased on demonstrations, which provably guarantees safety even for potentially\nsub-optimal (but safe) demonstrations. For near-optimal demonstrations, CoCoRL\nconverges to the true safe set with no policy regret. We evaluate CoCoRL in\ngridworld environments and a driving simulation with multiple constraints.\nCoCoRL learns constraints that lead to safe driving behavior. Importantly, we\ncan safely transfer the learned constraints to different tasks and\nenvironments. In contrast, alternative methods based on Inverse Reinforcement\nLearning (IRL) often exhibit poor performance and learn unsafe policies.",
                "Markov Decision Processes under External Temporal Processes\nMost reinforcement learning algorithms treat the context under which they\noperate as a stationary, isolated, and undisturbed environment. However, in\nreal world applications, environments constantly change due to a variety of\nexternal events. To address this problem, we study Markov Decision Processes\n(MDP) under the influence of an external temporal process. First, we formalize\nthis notion and derive conditions under which the problem becomes tractable\nwith suitable solutions. We propose a policy iteration algorithm to solve this\nproblem and theoretically analyze its performance. Our analysis addresses the\nnon-stationarity present in the MDP as a result of non-Markovian events,\nnecessitating the formulation of policies that are contingent upon both the\ncurrent state and a history of prior events. Additionally, we derive insights\nregarding the sample complexity of the algorithm and incorporate factors that\ndefine the exogenous temporal process into the established bounds. Finally, we\nperform experiments to demonstrate our findings within a traditional control\nenvironment.",
                "Exploring the Adaptive Behaviors of Particle Lenia: A\n  Perturbation-Response Analysis for Computational Agency\nA firm cognitive subject or ``individual'' is presupposed for the emergence\nof mind. However, with the development of recent information technology, the\n``individual'' has become more dispersed in society and the cognitive subject\nhas become increasingly unstable and adaptive, necessitating an update in our\nunderstanding of the ``individual''. Autopoiesis serves as a model of the\ncognitive subject, which is unstable and requires effort to maintain itself to\nadapt to the environment. In this study, we evaluated adaptivity for a highly\nextensible multi-particle system model Particle Lenia through the response\nperturbation. As a result, we found that Particle Lenia has a particle\nconfiguration that is both temporally unstable and has multiple stable states.\nThis result suggests that Particle Lenia can express adaptive characteristics\nand is expected to be used as a computational model toward building an\nautopoietic cognitive agent.",
                "Physics-Regulated Deep Reinforcement Learning: Invariant Embeddings\nThis paper proposes the Phy-DRL: a physics-regulated deep reinforcement\nlearning (DRL) framework for safety-critical autonomous systems. The Phy-DRL\nhas three distinguished invariant-embedding designs: i) residual action policy\n(i.e., integrating data-driven-DRL action policy and physics-model-based action\npolicy), ii) automatically constructed safety-embedded reward, and iii)\nphysics-model-guided neural network (NN) editing, including link editing and\nactivation editing. Theoretically, the Phy-DRL exhibits 1) a mathematically\nprovable safety guarantee and 2) strict compliance of critic and actor networks\nwith physics knowledge about the action-value function and action policy.\nFinally, we evaluate the Phy-DRL on a cart-pole system and a quadruped robot.\nThe experiments validate our theoretical results and demonstrate that Phy-DRL\nfeatures guaranteed safety compared to purely data-driven DRL and solely\nmodel-based design while offering remarkably fewer learning parameters and fast\ntraining towards safety guarantee."
            ],
            "interesting paper": 2
        },
        {
            "papers": [
                "Formal Modelling for Multi-Robot Systems Under Uncertainty\nPurpose of Review: To effectively synthesise and analyse multi-robot\nbehaviour, we require formal task-level models which accurately capture\nmulti-robot execution. In this paper, we review modelling formalisms for\nmulti-robot systems under uncertainty, and discuss how they can be used for\nplanning, reinforcement learning, model checking, and simulation.\n  Recent Findings: Recent work has investigated models which more accurately\ncapture multi-robot execution by considering different forms of uncertainty,\nsuch as temporal uncertainty and partial observability, and modelling the\neffects of robot interactions on action execution. Other strands of work have\npresented approaches for reducing the size of multi-robot models to admit more\nefficient solution methods. This can be achieved by decoupling the robots under\nindependence assumptions, or reasoning over higher level macro actions.\n  Summary: Existing multi-robot models demonstrate a trade off between\naccurately capturing robot dependencies and uncertainty, and being small enough\nto tractably solve real world problems. Therefore, future research should\nexploit realistic assumptions over multi-robot behaviour to develop smaller\nmodels which retain accurate representations of uncertainty and robot\ninteractions; and exploit the structure of multi-robot problems, such as\nfactored state spaces, to develop scalable solution methods.",
                "Moral Machine or Tyranny of the Majority?\nWith Artificial Intelligence systems increasingly applied in consequential\ndomains, researchers have begun to ask how these systems ought to act in\nethically charged situations where even humans lack consensus. In the Moral\nMachine project, researchers crowdsourced answers to \"Trolley Problems\"\nconcerning autonomous vehicles. Subsequently, Noothigattu et al. (2018)\nproposed inferring linear functions that approximate each individual's\npreferences and aggregating these linear models by averaging parameters across\nthe population. In this paper, we examine this averaging mechanism, focusing on\nfairness concerns in the presence of strategic effects. We investigate a simple\nsetting where the population consists of two groups, with the minority\nconstituting an {\\alpha} < 0.5 share of the population. To simplify the\nanalysis, we consider the extreme case in which within-group preferences are\nhomogeneous. Focusing on the fraction of contested cases where the minority\ngroup prevails, we make the following observations: (a) even when all parties\nreport their preferences truthfully, the fraction of disputes where the\nminority prevails is less than proportionate in {\\alpha}; (b) the degree of\nsub-proportionality grows more severe as the level of disagreement between the\ngroups increases; (c) when parties report preferences strategically, pure\nstrategy equilibria do not always exist; and (d) whenever a pure strategy\nequilibrium exists, the majority group prevails 100% of the time. These\nfindings raise concerns about stability and fairness of preference vector\naveraging as a mechanism for aggregating diverging voices. Finally, we discuss\nalternatives, including randomized dictatorship and median-based mechanisms.",
                "Attention Schema in Neural Agents\nAttention has become a common ingredient in deep learning architectures. It\nadds a dynamical selection of information on top of the static selection of\ninformation supported by weights. In the same way, we can imagine a\nhigher-order informational filter built on top of attention: an Attention\nSchema (AS), namely, a descriptive and predictive model of attention. In\ncognitive neuroscience, Attention Schema Theory (AST) supports this idea of\ndistinguishing attention from AS. A strong prediction of this theory is that an\nagent can use its own AS to also infer the states of other agents' attention\nand consequently enhance coordination with other agents. As such, multi-agent\nreinforcement learning would be an ideal setting to experimentally test the\nvalidity of AST. We explore different ways in which attention and AS interact\nwith each other. Our preliminary results indicate that agents that implement\nthe AS as a recurrent internal control achieve the best performance. In\ngeneral, these exploratory experiments suggest that equipping artificial agents\nwith a model of attention can enhance their social intelligence.",
                "A Categorical Representation Language and Computational System for\n  Knowledge-Based Planning\nClassical planning representation languages based on first-order logic have\npreliminarily been used to model and solve robotic task planning problems.\nWider adoption of these representation languages, however, is hindered by the\nlimitations present when managing implicit world changes with concise action\nmodels. To address this problem, we propose an alternative approach to\nrepresenting and managing updates to world states during planning. Based on the\ncategory-theoretic concepts of $\\mathsf{C}$-sets and double-pushout rewriting\n(DPO), our proposed representation can effectively handle structured knowledge\nabout world states that support domain abstractions at all levels. It\nformalizes the semantics of predicates according to a user-provided ontology\nand preserves the semantics when transitioning between world states. This\nmethod provides a formal semantics for using knowledge graphs and relational\ndatabases to model world states and updates in planning. In this paper, we\nconceptually compare our category-theoretic representation with the classical\nplanning representation. We show that our proposed representation has\nadvantages over the classical representation in terms of handling implicit\npreconditions and effects, and provides a more structured framework in which to\nmodel and solve planning problems.",
                "Robust Lane Detection through Self Pre-training with Masked Sequential\n  Autoencoders and Fine-tuning with Customized PolyLoss\nLane detection is crucial for vehicle localization which makes it the\nfoundation for automated driving and many intelligent and advanced driving\nassistant systems. Available vision-based lane detection methods do not make\nfull use of the valuable features and aggregate contextual information,\nespecially the interrelationships between lane lines and other regions of the\nimages in continuous frames. To fill this research gap and upgrade lane\ndetection performance, this paper proposes a pipeline consisting of self\npre-training with masked sequential autoencoders and fine-tuning with\ncustomized PolyLoss for the end-to-end neural network models using\nmulti-continuous image frames. The masked sequential autoencoders are adopted\nto pre-train the neural network models with reconstructing the missing pixels\nfrom a random masked image as the objective. Then, in the fine-tuning\nsegmentation phase where lane detection segmentation is performed, the\ncontinuous image frames are served as the inputs, and the pre-trained model\nweights are transferred and further updated using the backpropagation mechanism\nwith customized PolyLoss calculating the weighted errors between the output\nlane detection results and the labeled ground truth. Extensive experiment\nresults demonstrate that, with the proposed pipeline, the lane detection model\nperformance on both normal and challenging scenes can be advanced beyond the\nstate-of-the-art, delivering the best testing accuracy (98.38%), precision\n(0.937), and F1-measure (0.924) on the normal scene testing set, together with\nthe best overall accuracy (98.36%) and precision (0.844) in the challenging\nscene test set, while the training time can be substantially shortened.",
                "Acting as Inverse Inverse Planning\nGreat storytellers know how to take us on a journey. They direct characters\nto act -- not necessarily in the most rational way -- but rather in a way that\nleads to interesting situations, and ultimately creates an impactful experience\nfor audience members looking on.\n  If audience experience is what matters most, then can we help artists and\nanimators *directly* craft such experiences, independent of the concrete\ncharacter actions needed to evoke those experiences? In this paper, we offer a\nnovel computational framework for such tools. Our key idea is to optimize\nanimations with respect to *simulated* audience members' experiences. To\nsimulate the audience, we borrow an established principle from cognitive\nscience: that human social intuition can be modeled as \"inverse planning,\" the\ntask of inferring an agent's (hidden) goals from its (observed) actions.\nBuilding on this model, we treat storytelling as \"*inverse* inverse planning,\"\nthe task of choosing actions to manipulate an inverse planner's inferences. Our\nframework is grounded in literary theory, naturally capturing many storytelling\nelements from first principles. We give a series of examples to demonstrate\nthis, with supporting evidence from human subject studies."
            ],
            "interesting paper": 3
        },
        {
            "papers": [
                "Assumption Generation for the Verification of Learning-Enabled\n  Autonomous Systems\nProviding safety guarantees for autonomous systems is difficult as these\nsystems operate in complex environments that require the use of\nlearning-enabled components, such as deep neural networks (DNNs) for visual\nperception. DNNs are hard to analyze due to their size (they can have thousands\nor millions of parameters), lack of formal specifications (DNNs are typically\nlearnt from labeled data, in the absence of any formal requirements), and\nsensitivity to small changes in the environment. We present an assume-guarantee\nstyle compositional approach for the formal verification of system-level safety\nproperties of such autonomous systems. Our insight is that we can analyze the\nsystem in the absence of the DNN perception components by automatically\nsynthesizing assumptions on the DNN behaviour that guarantee the satisfaction\nof the required safety properties. The synthesized assumptions are the weakest\nin the sense that they characterize the output sequences of all the possible\nDNNs that, plugged into the autonomous system, guarantee the required safety\nproperties. The assumptions can be leveraged as run-time monitors over a\ndeployed DNN to guarantee the safety of the overall system; they can also be\nmined to extract local specifications for use during training and testing of\nDNNs. We illustrate our approach on a case study taken from the autonomous\nairplanes domain that uses a complex DNN for perception.",
                "Integrating Action Knowledge and LLMs for Task Planning and Situation\n  Handling in Open Worlds\nTask planning systems have been developed to help robots use human knowledge\n(about actions) to complete long-horizon tasks. Most of them have been\ndeveloped for \"closed worlds\" while assuming the robot is provided with\ncomplete world knowledge. However, the real world is generally open, and the\nrobots frequently encounter unforeseen situations that can potentially break\nthe planner's completeness. Could we leverage the recent advances on\npre-trained Large Language Models (LLMs) to enable classical planning systems\nto deal with novel situations?\n  This paper introduces a novel framework, called COWP, for open-world task\nplanning and situation handling. COWP dynamically augments the robot's action\nknowledge, including the preconditions and effects of actions, with\ntask-oriented commonsense knowledge. COWP embraces the openness from LLMs, and\nis grounded to specific domains via action knowledge. For systematic\nevaluations, we collected a dataset that includes 1,085 execution-time\nsituations. Each situation corresponds to a state instance wherein a robot is\npotentially unable to complete a task using a solution that normally works.\nExperimental results show that our approach outperforms competitive baselines\nfrom the literature in the success rate of service tasks. Additionally, we have\ndemonstrated COWP using a mobile manipulator. Supplementary materials are\navailable at: https://cowplanning.github.io/",
                "Observation Denoising in CYRUS Soccer Simulation 2D Team For RoboCup\n  2023\nThe RoboCup competitions hold various leagues, and the Soccer Simulation 2D\nLeague is a major one among them. Soccer Simulation 2D (SS2D) match involves\ntwo teams, including 11 players and a coach, competing against each other. The\nplayers can only communicate with the Soccer Simulation Server during the game.\nThis paper presents the latest research of the CYRUS soccer simulation 2D team,\nthe champion of RoboCup 2021. We will explain our denoising idea powered by\nlong short-term memory networks (LSTM) and deep neural networks (DNN). The\nCYRUS team uses the CYRUS2D base code that was developed based on the Helios\nand Gliders bases.",
                "On the Value of Myopic Behavior in Policy Reuse\nLeveraging learned strategies in unfamiliar scenarios is fundamental to human\nintelligence. In reinforcement learning, rationally reusing the policies\nacquired from other tasks or human experts is critical for tackling problems\nthat are difficult to learn from scratch. In this work, we present a framework\ncalled Selective Myopic bEhavior Control~(SMEC), which results from the insight\nthat the short-term behaviors of prior policies are sharable across tasks. By\nevaluating the behaviors of prior policies via a hybrid value function\narchitecture, SMEC adaptively aggregates the sharable short-term behaviors of\nprior policies and the long-term behaviors of the task policy, leading to\ncoordinated decisions. Empirical results on a collection of manipulation and\nlocomotion tasks demonstrate that SMEC outperforms existing methods, and\nvalidate the ability of SMEC to leverage related prior policies.",
                "Probing reaction channels via reinforcement learning\nWe propose a reinforcement learning based method to identify important\nconfigurations that connect reactant and product states along chemical reaction\npaths. By shooting multiple trajectories from these configurations, we can\ngenerate an ensemble of configurations that concentrate on the transition path\nensemble. This configuration ensemble can be effectively employed in a neural\nnetwork-based partial differential equation solver to obtain an approximation\nsolution of a restricted Backward Kolmogorov equation, even when the dimension\nof the problem is very high. The resulting solution, known as the committor\nfunction, encodes mechanistic information for the reaction and can in turn be\nused to evaluate reaction rates.",
                "Modeling Dynamic Environments with Scene Graph Memory\nEmbodied AI agents that search for objects in large environments such as\nhouseholds often need to make efficient decisions by predicting object\nlocations based on partial information. We pose this as a new type of link\nprediction problem: link prediction on partially observable dynamic graphs. Our\ngraph is a representation of a scene in which rooms and objects are nodes, and\ntheir relationships are encoded in the edges; only parts of the changing graph\nare known to the agent at each timestep. This partial observability poses a\nchallenge to existing link prediction approaches, which we address. We propose\na novel state representation -- Scene Graph Memory (SGM) -- with captures the\nagent's accumulated set of observations, as well as a neural net architecture\ncalled a Node Edge Predictor (NEP) that extracts information from the SGM to\nsearch efficiently. We evaluate our method in the Dynamic House Simulator, a\nnew benchmark that creates diverse dynamic graphs following the semantic\npatterns typically seen at homes, and show that NEP can be trained to predict\nthe locations of objects in a variety of environments with diverse object\nmovement dynamics, outperforming baselines both in terms of new scene\nadaptability and overall accuracy. The codebase and more can be found at\nhttps://www.scenegraphmemory.com."
            ],
            "interesting paper": 2
        },
        {
            "papers": [
                "Safety of autonomous vehicles: A survey on Model-based vs. AI-based\n  approaches\nThe growing advancements in Autonomous Vehicles (AVs) have emphasized the\ncritical need to prioritize the absolute safety of AV maneuvers, especially in\ndynamic and unpredictable environments or situations. This objective becomes\neven more challenging due to the uniqueness of every traffic\nsituation/condition. To cope with all these very constrained and complex\nconfigurations, AVs must have appropriate control architectures with reliable\nand real-time Risk Assessment and Management Strategies (RAMS). These targeted\nRAMS must lead to reduce drastically the navigation risks. However, the lack of\nsafety guarantees proves, which is one of the key challenges to be addressed,\nlimit drastically the ambition to introduce more broadly AVs on our roads and\nrestrict the use of AVs to very limited use cases. Therefore, the focus and the\nambition of this paper is to survey research on autonomous vehicles while\nfocusing on the important topic of safety guarantee of AVs. For this purpose,\nit is proposed to review research on relevant methods and concepts defining an\noverall control architecture for AVs, with an emphasis on the safety assessment\nand decision-making systems composing these architectures. Moreover, it is\nintended through this reviewing process to highlight researches that use either\nmodel-based methods or AI-based approaches. This is performed while emphasizing\nthe strengths and weaknesses of each methodology and investigating the research\nthat proposes a comprehensive multi-modal design that combines model-based and\nAI approaches. This paper ends with discussions on the methods used to\nguarantee the safety of AVs namely: safety verification techniques and the\nstandardization/generalization of safety frameworks.",
                "Towards Autonomous and Safe Last-mile Deliveries with AI-augmented\n  Self-driving Delivery Robots\nIn addition to its crucial impact on customer satisfaction, last-mile\ndelivery (LMD) is notorious for being the most time-consuming and costly stage\nof the shipping process. Pressing environmental concerns combined with the\nrecent surge of e-commerce sales have sparked renewed interest in automation\nand electrification of last-mile logistics. To address the hurdles faced by\nexisting robotic couriers, this paper introduces a customer-centric and\nsafety-conscious LMD system for small urban communities based on AI-assisted\nautonomous delivery robots. The presented framework enables end-to-end\nautomation and optimization of the logistic process while catering for\nreal-world imposed operational uncertainties, clients' preferred time\nschedules, and safety of pedestrians. To this end, the integrated optimization\ncomponent is modeled as a robust variant of the Cumulative Capacitated Vehicle\nRouting Problem with Time Windows, where routes are constructed under uncertain\ntravel times with an objective to minimize the total latency of deliveries\n(i.e., the overall waiting time of customers, which can negatively affect their\nsatisfaction). We demonstrate the proposed LMD system's utility through\nreal-world trials in a university campus with a single robotic courier.\nImplementation aspects as well as the findings and practical insights gained\nfrom the deployment are discussed in detail. Lastly, we round up the\ncontributions with numerical simulations to investigate the scalability of the\ndeveloped mathematical formulation with respect to the number of robotic\nvehicles and customers.",
                "The Computational Complexity of Single-Player Imperfect-Recall Games\nWe study single-player extensive-form games with imperfect recall, such as\nthe Sleeping Beauty problem or the Absentminded Driver game. For such games,\ntwo natural equilibrium concepts have been proposed as alternative solution\nconcepts to ex-ante optimality. One equilibrium concept uses generalized double\nhalving (GDH) as a belief system and evidential decision theory (EDT), and\nanother one uses generalized thirding (GT) as a belief system and causal\ndecision theory (CDT). Our findings relate those three solution concepts of a\ngame to solution concepts of a polynomial maximization problem: global optima,\noptimal points with respect to subsets of variables and Karush-Kuhn-Tucker\n(KKT) points. Based on these correspondences, we are able to settle various\ncomplexity-theoretic questions on the computation of such strategies. For\nex-ante optimality and (EDT,GDH)-equilibria, we obtain NP-hardness and\ninapproximability, and for (CDT,GT)-equilibria we obtain CLS-completeness\nresults.",
                "Taming AI Bots: Controllability of Neural States in Large Language\n  Models\nWe tackle the question of whether an agent can, by suitable choice of\nprompts, control an AI bot to any state. To that end, we first introduce a\nformal definition of ``meaning'' that is amenable to analysis. Then, we\ncharacterize ``meaningful data'' on which large language models (LLMs) are\nostensibly trained, and ``well-trained LLMs'' through conditions that are\nlargely met by today's LLMs. While a well-trained LLM constructs an embedding\nspace of meanings that is Euclidean, meanings themselves do not form a vector\n(linear) subspace, but rather a quotient space within. We then characterize the\nsubset of meanings that can be reached by the state of the LLMs for some input\nprompt, and show that a well-trained bot can reach any meaning albeit with\nsmall probability. We then introduce a stronger notion of controllability as\n{\\em almost certain reachability}, and show that, when restricted to the space\nof meanings, an AI bot is controllable. We do so after introducing a functional\ncharacterization of attentive AI bots, and finally derive necessary and\nsufficient conditions for controllability. The fact that AI bots are\ncontrollable means that an adversary could steer them towards any state.\nHowever, the sampling process can be designed to counteract adverse actions and\navoid reaching undesirable regions of state space before their boundary is\ncrossed.",
                "An Efficient Safety-oriented Car-following Model for Connected Automated\n  Vehicles Considering Discrete Signals\nWith the rapid development of Connected and Automated Vehicle (CAV)\ntechnology, limited self-driving vehicles have been commercially available in\ncertain leading intelligent transportation system countries. When formulating\nthe car-following model for CAVs, safety is usually the basic constraint.\nSafety-oriented car-following models seek to specify a safe following distance\nthat can guarantee safety if the preceding vehicle were to brake hard suddenly.\nThe discrete signals of CAVs bring a series of phenomena, including discrete\ndecision-making, phase difference, and discretely distributed communication\ndelay. The influences of these phenomena on the car-following safety of CAVs\nare rarely considered in the literature. This paper proposes an efficient\nsafety-oriented car-following model for CAVs considering the impact of discrete\nsignals. The safety constraints during both normal driving and a sudden hard\nbrake are incorporated into one integrated model to eliminate possible\ncollisions during the whole driving process. The mechanical delay information\nof the preceding vehicle is used to improve car-following efficiency. Four\nmodules are designed to enhance driving comfort and string stability in case of\nheavy packet losses. Simulations of a platoon with diversified vehicle types\ndemonstrate the safety, efficiency, and string stability of the proposed model.\nTests with different packet loss rates imply that the model could guarantee\nsafety and driving comfort in even poor communication environments.",
                "Towards a Technology-Driven Adaptive Decision Support System for\n  Integrated Pavement and Maintenance strategies (TDADSS-IPM): focus on risk\n  assessment framework for climate change adaptation\nDecision Support Systems for pavement and maintenance strategies have\ntraditionally been designed as silos led to local optimum systems. Moreover,\nsince big data usage didn't exist as result of Industry 4.0 as of today, DSSs\nwere not initially designed adaptive to the sources of uncertainties led to\nrigid decisions. Motivated by the vulnerability of the road assets to the\nclimate phenomena, this paper takes a visionary step towards introducing a\nTechnology-Driven Adaptive Decision Support System for Integrated Pavement and\nMaintenance activities called TDADSS-IPM. As part of such DSS, a bottom-up risk\nassessment model is met via Bayesian Belief Networks (BBN) to realize the\nactual condition of the Danish roads due to weather condition. Such model fills\nthe gaps in the knowledge domain and develops a platform that can be trained\nover time, and applied in real-time to the actual event."
            ],
            "interesting paper": 1
        },
        {
            "papers": [
                "Towards a Unifying Model of Rationality in Multiagent Systems\nMultiagent systems deployed in the real world need to cooperate with other\nagents (including humans) nearly as effectively as these agents cooperate with\none another. To design such AI, and provide guarantees of its effectiveness, we\nneed to clearly specify what types of agents our AI must be able to cooperate\nwith. In this work we propose a generic model of socially intelligent agents,\nwhich are individually rational learners that are also able to cooperate with\none another (in the sense that their joint behavior is Pareto efficient). We\ndefine rationality in terms of the regret incurred by each agent over its\nlifetime, and show how we can construct socially intelligent agents for\ndifferent forms of regret. We then discuss the implications of this model for\nthe development of \"robust\" MAS that can cooperate with a wide variety of\nsocially intelligent agents.",
                "Development of a ROS-based Architecture for Intelligent Autonomous on\n  Demand Last Mile Delivery\nThis paper presents the development of the JKU-ITS Last Mile Delivery Robot.\nThe proposed approach utilizes a combination of one 3D LIDAR, RGB-D camera, IMU\nand GPS sensor on top of a mobile robot slope mower. An embedded computer,\nrunning ROS1, is utilized to process the sensor data streams to enable 2D and\n3D Simultaneous Localization and Mapping, 2D localization and object detection\nusing a convolutional neural network.",
                "Controllable Path of Destruction\nPath of Destruction (PoD) is a self-supervised method for learning iterative\ngenerators. The core idea is to produce a training set by destroying a set of\nartifacts, and for each destructive step create a training instance based on\nthe corresponding repair action. A generator trained on this dataset can then\ngenerate new artifacts by repairing from arbitrary states. The PoD method is\nvery data-efficient in terms of original training examples and well-suited to\nfunctional artifacts composed of categorical data, such as game levels and\ndiscrete 3D structures. In this paper, we extend the Path of Destruction method\nto allow designer control over aspects of the generated artifacts.\nControllability is introduced by adding conditional inputs to the state-action\npairs that make up the repair trajectories. We test the controllable PoD method\nin a 2D dungeon setting, as well as in the domain of small 3D Lego cars.",
                "RLAD: Reinforcement Learning from Pixels for Autonomous Driving in Urban\n  Environments\nCurrent approaches of Reinforcement Learning (RL) applied in urban Autonomous\nDriving (AD) focus on decoupling the perception training from the driving\npolicy training. The main reason is to avoid training a convolution encoder\nalongside a policy network, which is known to have issues related to sample\nefficiency, degenerated feature representations, and catastrophic\nself-overfitting. However, this paradigm can lead to representations of the\nenvironment that are not aligned with the downstream task, which may result in\nsuboptimal performances. To address this limitation, this paper proposes RLAD,\nthe first Reinforcement Learning from Pixels (RLfP) method applied in the urban\nAD domain. We propose several techniques to enhance the performance of an RLfP\nalgorithm in this domain, including: i) an image encoder that leverages both\nimage augmentations and Adaptive Local Signal Mixing (A-LIX) layers; ii)\nWayConv1D, which is a waypoint encoder that harnesses the 2D geometrical\ninformation of the waypoints using 1D convolutions; and iii) an auxiliary loss\nto increase the significance of the traffic lights in the latent representation\nof the environment. Experimental results show that RLAD significantly\noutperforms all state-of-the-art RLfP methods on the NoCrash benchmark. We also\npresent an infraction analysis on the NoCrash-regular benchmark, which\nindicates that RLAD performs better than all other methods in terms of both\ncollision rate and red light infractions.",
                "Subequivariant Graph Reinforcement Learning in 3D Environments\nLearning a shared policy that guides the locomotion of different agents is of\ncore interest in Reinforcement Learning (RL), which leads to the study of\nmorphology-agnostic RL. However, existing benchmarks are highly restrictive in\nthe choice of starting point and target point, constraining the movement of the\nagents within 2D space. In this work, we propose a novel setup for\nmorphology-agnostic RL, dubbed Subequivariant Graph RL in 3D environments\n(3D-SGRL). Specifically, we first introduce a new set of more practical yet\nchallenging benchmarks in 3D space that allows the agent to have full\nDegree-of-Freedoms to explore in arbitrary directions starting from arbitrary\nconfigurations. Moreover, to optimize the policy over the enlarged state-action\nspace, we propose to inject geometric symmetry, i.e., subequivariance, into the\nmodeling of the policy and Q-function such that the policy can generalize to\nall directions, improving exploration efficiency. This goal is achieved by a\nnovel SubEquivariant Transformer (SET) that permits expressive message\nexchange. Finally, we evaluate the proposed method on the proposed benchmarks,\nwhere our method consistently and significantly outperforms existing approaches\non single-task, multi-task, and zero-shot generalization scenarios. Extensive\nablations are also conducted to verify our design. Code and videos are\navailable on our project page: https://alpc91.github.io/SGRL/.",
                "An Emergency Disposal Decision-making Method with Human--Machine\n  Collaboration\nRapid developments in artificial intelligence technology have led to unmanned\nsystems replacing human beings in many fields requiring high-precision\npredictions and decisions. In modern operational environments, all job plans\nare affected by emergency events such as equipment failures and resource\nshortages, making a quick resolution critical. The use of unmanned systems to\nassist decision-making can improve resolution efficiency, but their\ndecision-making is not interpretable and may make the wrong decisions. Current\nunmanned systems require human supervision and control. Based on this, we\npropose a collaborative human--machine method for resolving unplanned events\nusing two phases: task filtering and task scheduling. In the task filtering\nphase, we propose a human--machine collaborative decision-making algorithm for\ndynamic tasks. The GACRNN model is used to predict the state of the job nodes,\nlocate the key nodes, and generate a machine-predicted resolution task list. A\nhuman decision-maker supervises the list in real time and modifies and confirms\nthe machine-predicted list through the human--machine interface. In the task\nscheduling phase, we propose a scheduling algorithm that integrates human\nexperience constraints. The steps to resolve an event are inserted into the\nnormal job sequence to schedule the resolution. We propose several\nhuman--machine collaboration methods in each phase to generate steps to resolve\nan unplanned event while minimizing the impact on the original job plan."
            ],
            "interesting paper": 4
        },
        {
            "papers": [
                "Data and Knowledge for Overtaking Scenarios in Autonomous Driving\nAutonomous driving has become one of the most popular research topics within\nArtificial Intelligence. An autonomous vehicle is understood as a system that\ncombines perception, decision-making, planning, and control. All of those tasks\nrequire that the vehicle collects surrounding data in order to make a good\ndecision and action. In particular, the overtaking maneuver is one of the most\ncritical actions of driving. The process involves lane changes, acceleration\nand deceleration actions, and estimation of the speed and distance of the\nvehicle in front or in the lane in which it is moving. Despite the amount of\nwork available in the literature, just a few handle overtaking maneuvers and,\nbecause overtaking can be risky, no real-world dataset is available. This work\ncontributes in this area by presenting a new synthetic dataset whose focus is\nthe overtaking maneuver. We start by performing a thorough review of the state\nof the art in autonomous driving and then explore the main datasets found in\nthe literature (public and private, synthetic and real), highlighting their\nlimitations, and suggesting a new set of features whose focus is the overtaking\nmaneuver.",
                "Intent-aligned AI systems deplete human agency: the need for agency\n  foundations research in AI safety\nThe rapid advancement of artificial intelligence (AI) systems suggests that\nartificial general intelligence (AGI) systems may soon arrive. Many researchers\nare concerned that AIs and AGIs will harm humans via intentional misuse\n(AI-misuse) or through accidents (AI-accidents). In respect of AI-accidents,\nthere is an increasing effort focused on developing algorithms and paradigms\nthat ensure AI systems are aligned to what humans intend, e.g. AI systems that\nyield actions or recommendations that humans might judge as consistent with\ntheir intentions and goals. Here we argue that alignment to human intent is\ninsufficient for safe AI systems and that preservation of long-term agency of\nhumans may be a more robust standard, and one that needs to be separated\nexplicitly and a priori during optimization. We argue that AI systems can\nreshape human intention and discuss the lack of biological and psychological\nmechanisms that protect humans from loss of agency. We provide the first formal\ndefinition of agency-preserving AI-human interactions which focuses on\nforward-looking agency evaluations and argue that AI systems - not humans -\nmust be increasingly tasked with making these evaluations. We show how agency\nloss can occur in simple environments containing embedded agents that use\ntemporal-difference learning to make action recommendations. Finally, we\npropose a new area of research called \"agency foundations\" and pose four\ninitial topics designed to improve our understanding of agency in AI-human\ninteractions: benevolent game theory, algorithmic foundations of human rights,\nmechanistic interpretability of agency representation in neural-networks and\nreinforcement learning from internal states.",
                "Symmetry-Aware Robot Design with Structured Subgroups\nRobot design aims at learning to create robots that can be easily controlled\nand perform tasks efficiently. Previous works on robot design have proven its\nability to generate robots for various tasks. However, these works searched the\nrobots directly from the vast design space and ignored common structures,\nresulting in abnormal robots and poor performance. To tackle this problem, we\npropose a Symmetry-Aware Robot Design (SARD) framework that exploits the\nstructure of the design space by incorporating symmetry searching into the\nrobot design process. Specifically, we represent symmetries with the subgroups\nof the dihedral group and search for the optimal symmetry in structured\nsubgroups. Then robots are designed under the searched symmetry. In this way,\nSARD can design efficient symmetric robots while covering the original design\nspace, which is theoretically analyzed. We further empirically evaluate SARD on\nvarious tasks, and the results show its superior efficiency and\ngeneralizability.",
                "Bigger, Better, Faster: Human-level Atari with human-level efficiency\nWe introduce a value-based RL agent, which we call BBF, that achieves\nsuper-human performance in the Atari 100K benchmark. BBF relies on scaling the\nneural networks used for value estimation, as well as a number of other design\nchoices that enable this scaling in a sample-efficient manner. We conduct\nextensive analyses of these design choices and provide insights for future\nwork. We end with a discussion about updating the goalposts for\nsample-efficient RL research on the ALE. We make our code and data publicly\navailable at\nhttps://github.com/google-research/google-research/tree/master/bigger_better_faster.",
                "SO(2)-Equivariant Downwash Models for Close Proximity Flight\nMultirotors flying in close proximity induce aerodynamic wake effects on each\nother through propeller downwash. Conventional methods have fallen short of\nproviding adequate 3D force-based models that can be incorporated into robust\ncontrol paradigms for deploying dense formations. Thus, learning a model for\nthese downwash patterns presents an attractive solution. In this paper, we\npresent a novel learning-based approach for modelling the downwash forces that\nexploits the latent geometries (i.e. symmetries) present in the problem. We\ndemonstrate that when trained with only 5 minutes of real-world flight data,\nour geometry-aware model outperforms state-of-the-art baseline models trained\nwith more than 15 minutes of data. In dense real-world flights with two\nvehicles, deploying our model online improves 3D trajectory tracking by nearly\n36% on average (and vertical tracking by 56%).",
                "Active causal structure learning with advice\nWe introduce the problem of active causal structure learning with advice. In\nthe typical well-studied setting, the learning algorithm is given the essential\ngraph for the observational distribution and is asked to recover the underlying\ncausal directed acyclic graph (DAG) $G^*$ while minimizing the number of\ninterventions made. In our setting, we are additionally given side information\nabout $G^*$ as advice, e.g. a DAG $G$ purported to be $G^*$. We ask whether the\nlearning algorithm can benefit from the advice when it is close to being\ncorrect, while still having worst-case guarantees even when the advice is\narbitrarily bad. Our work is in the same space as the growing body of research\non algorithms with predictions. When the advice is a DAG $G$, we design an\nadaptive search algorithm to recover $G^*$ whose intervention cost is at most\n$O(\\max\\{1, \\log \\psi\\})$ times the cost for verifying $G^*$; here, $\\psi$ is a\ndistance measure between $G$ and $G^*$ that is upper bounded by the number of\nvariables $n$, and is exactly 0 when $G=G^*$. Our approximation factor matches\nthe state-of-the-art for the advice-less setting."
            ],
            "interesting paper": 2
        },
        {
            "papers": [
                "Human Control: Definitions and Algorithms\nHow can humans stay in control of advanced artificial intelligence systems?\nOne proposal is corrigibility, which requires the agent to follow the\ninstructions of a human overseer, without inappropriately influencing them. In\nthis paper, we formally define a variant of corrigibility called shutdown\ninstructability, and show that it implies appropriate shutdown behavior,\nretention of human autonomy, and avoidance of user harm. We also analyse the\nrelated concepts of non-obstruction and shutdown alignment, three previously\nproposed algorithms for human control, and one new algorithm.",
                "EMOTE: An Explainable architecture for Modelling the Other Through\n  Empathy\nWe can usually assume others have goals analogous to our own. This assumption\ncan also, at times, be applied to multi-agent games - e.g. Agent 1's attraction\nto green pellets is analogous to Agent 2's attraction to red pellets. This\n\"analogy\" assumption is tied closely to the cognitive process known as empathy.\nInspired by empathy, we design a simple and explainable architecture to model\nanother agent's action-value function. This involves learning an \"Imagination\nNetwork\" to transform the other agent's observed state in order to produce a\nhuman-interpretable \"empathetic state\" which, when presented to the learning\nagent, produces behaviours that mimic the other agent. Our approach is\napplicable to multi-agent scenarios consisting of a single learning agent and\nother (independent) agents acting according to fixed policies. This\narchitecture is particularly beneficial for (but not limited to) algorithms\nusing a composite value or reward function. We show our method produces better\nperformance in multi-agent games, where it robustly estimates the other's model\nin different environment configurations. Additionally, we show that the\nempathetic states are human interpretable, and thus verifiable.",
                "Efficient Learning of Urban Driving Policies Using Bird's-Eye-View State\n  Representations\nAutonomous driving involves complex decision-making in highly interactive\nenvironments, requiring thoughtful negotiation with other traffic participants.\nWhile reinforcement learning provides a way to learn such interaction behavior,\nefficient learning critically depends on scalable state representations.\nContrary to imitation learning methods, high-dimensional state representations\nstill constitute a major bottleneck for deep reinforcement learning methods in\nautonomous driving. In this paper, we study the challenges of constructing\nbird's-eye-view representations for autonomous driving and propose a recurrent\nlearning architecture for long-horizon driving. Our PPO-based approach, called\nRecurrDriveNet, is demonstrated on a simulated autonomous driving task in\nCARLA, where it outperforms traditional frame-stacking methods while only\nrequiring one million experiences for efficient training. RecurrDriveNet causes\nless than one infraction per driven kilometer by interacting safely with other\nroad users.",
                "Environmental Memory Boosts Group Formation of Clueless Individuals\nThe formation of groups of interacting individuals improves performance and\nfitness in many decentralised systems, from micro-organisms to social insects,\nfrom robotic swarms to artificial intelligence algorithms. Often, group\nformation and high-level coordination in these systems emerge from individuals\nwith limited information-processing capabilities implementing low-level rules\nof communication to signal to each other. Here, we show that, even in a\ncommunity of clueless individuals incapable of processing information and\ncommunicating, a dynamic environment can coordinate group formation by\ntransiently storing memory of the earlier passage of individuals. Our results\nidentify a new mechanism of indirect coordination via shared memory that is\nprimarily promoted and reinforced by dynamic environmental factors, thus\novershadowing the need for any form of explicit signalling between individuals.\nWe expect this pathway to group formation to be relevant for understanding and\ncontrolling self-organisation and collective decision making in both living and\nartificial active matter in real-life environments.",
                "Traffic Road Congestion System using by the internet of vehicles (IoV)\nTraffic problems have increased in modern life due to a huge number of\nvehicles, big cities, and ignoring the traffic rules. Vehicular ad hoc network\n(VANET) has improved the traffic system in previous some and plays a vital role\nin the best traffic control system in big cities. But due to some limitations,\nit is not enough to control some problems in specific conditions. Now a day\ninvention of new technologies of the Internet of Things (IoT) is used for\ncollaboratively and efficiently performing tasks. This technology was also\nintroduced in the transportation system which makes it an intelligent\ntransportation system (ITS), this is called the Internet of vehicles (IOV). We\nwill elaborate on traffic problems in the traditional system and elaborate on\nthe benefits, enhancements, and reasons to better IOV by Systematic Literature\nReview (SLR). This technique will be implemented by targeting needed papers\nthrough many search phrases. A systematic literature review is used for 121\narticles between 2014 and 2023. The IoV technologies and tools are required to\ncreate the IoV and resolve some traffic rules through SUMO (simulation of urban\nmobility) which is used for the design and simulation the road traffic. We have\ntried to contribute to the best model of the traffic control system. This paper\nwill analysis two vehicular congestion control models in term of select the\noptimized and efficient model and elaborate on the reasons for efficiency by\nsearching the solution SLR based questions. Due to some efficient features, we\nhave suggested the IOV based on vehicular clouds. These efficient features make\nthis model the best and most effective than the traditional model which is a\ngreat reason to enhance the network system.",
                "Causal Imitability Under Context-Specific Independence Relations\nDrawbacks of ignoring the causal mechanisms when performing imitation\nlearning have recently been acknowledged. Several approaches both to assess the\nfeasibility of imitation and to circumvent causal confounding and causal\nmisspecifications have been proposed in the literature. However, the potential\nbenefits of the incorporation of additional information about the underlying\ncausal structure are left unexplored. An example of such overlooked information\nis context-specific independence (CSI), i.e., independence that holds only in\ncertain contexts. We consider the problem of causal imitation learning when CSI\nrelations are known. We prove that the decision problem pertaining to the\nfeasibility of imitation in this setting is NP-hard. Further, we provide a\nnecessary graphical criterion for imitation learning under CSI and show that\nunder a structural assumption, this criterion is also sufficient. Finally, we\npropose a sound algorithmic approach for causal imitation learning which takes\nboth CSI relations and data into account."
            ],
            "interesting paper": 2
        },
        {
            "papers": [
                "On the Existence of Information Bottlenecks in Living and Non-Living\n  Systems\nIn many complex systems, we observe that `interesting behaviour' is often the\nconsequence of a system exploiting the existence of an Information Bottleneck\n(IB). These bottlenecks can occur at different scales, between individuals or\ncomponents of a system, and sometimes within individuals themselves.\nOftentimes, we regard these bottlenecks negatively; as merely the limitations\nof the individual's physiology and something that ought to be overcome when\ndesigning and implementing artificial systems. However, we suggest instead that\nIBs may serve a purpose beyond merely providing a minimally-viable channel for\ncoordination in collective systems. More specifically, we suggest that\ninteresting or novel behaviour occurs when the individuals in a system are\nconstrained or limited in their ability to share information and must discover\nnovel ways to exploit existing mechanisms, which are inherently bottlenecked,\nrather than circumventing or otherwise avoiding those mechanisms entirely.",
                "An Architecture for Deploying Reinforcement Learning in Industrial\n  Environments\nIndustry 4.0 is driven by demands like shorter time-to-market, mass\ncustomization of products, and batch size one production. Reinforcement\nLearning (RL), a machine learning paradigm shown to possess a great potential\nin improving and surpassing human level performance in numerous complex tasks,\nallows coping with the mentioned demands. In this paper, we present an OPC UA\nbased Operational Technology (OT)-aware RL architecture, which extends the\nstandard RL setting, combining it with the setting of digital twins. Moreover,\nwe define an OPC UA information model allowing for a generalized plug-and-play\nlike approach for exchanging the RL agent used. In conclusion, we demonstrate\nand evaluate the architecture, by creating a proof of concept. By means of\nsolving a toy example, we show that this architecture can be used to determine\nthe optimal policy using a real control system.",
                "AI Liability Insurance With an Example in AI-Powered E-diagnosis System\nArtificial Intelligence (AI) has received an increasing amount of attention\nin multiple areas. The uncertainties and risks in AI-powered systems have\ncreated reluctance in their wild adoption. As an economic solution to\ncompensate for potential damages, AI liability insurance is a promising market\nto enhance the integration of AI into daily life. In this work, we use an\nAI-powered E-diagnosis system as an example to study AI liability insurance. We\nprovide a quantitative risk assessment model with evidence-based numerical\nanalysis. We discuss the insurability criteria for AI technologies and suggest\nnecessary adjustments to accommodate the features of AI products. We show that\nAI liability insurance can act as a regulatory mechanism to incentivize\ncompliant behaviors and serve as a certificate of high-quality AI systems.\nFurthermore, we suggest premium adjustment to reflect the dynamic evolution of\nthe inherent uncertainty in AI. Moral hazard problems are discussed and\nsuggestions for AI liability insurance are provided.",
                "Identifiability and Generalizability in Constrained Inverse\n  Reinforcement Learning\nTwo main challenges in Reinforcement Learning (RL) are designing appropriate\nreward functions and ensuring the safety of the learned policy. To address\nthese challenges, we present a theoretical framework for Inverse Reinforcement\nLearning (IRL) in constrained Markov decision processes. From a convex-analytic\nperspective, we extend prior results on reward identifiability and\ngeneralizability to both the constrained setting and a more general class of\nregularizations. In particular, we show that identifiability up to potential\nshaping (Cao et al., 2021) is a consequence of entropy regularization and may\ngenerally no longer hold for other regularizations or in the presence of safety\nconstraints. We also show that to ensure generalizability to new transition\nlaws and constraints, the true reward must be identified up to a constant.\nAdditionally, we derive a finite sample guarantee for the suboptimality of the\nlearned rewards, and validate our results in a gridworld environment.",
                "Federated Learning Games for Reconfigurable Intelligent Surfaces via\n  Causal Representations\nIn this paper, we investigate the problem of robust Reconfigurable\nIntelligent Surface (RIS) phase-shifts configuration over heterogeneous\ncommunication environments. The problem is formulated as a distributed learning\nproblem over different environments in a Federated Learning (FL) setting.\nEquivalently, this corresponds to a game played between multiple RISs, as\nlearning agents, in heterogeneous environments. Using Invariant Risk\nMinimization (IRM) and its FL equivalent, dubbed FL Games, we solve the RIS\nconfiguration problem by learning invariant causal representations across\nmultiple environments and then predicting the phases. The solution corresponds\nto playing according to Best Response Dynamics (BRD) which yields the Nash\nEquilibrium of the FL game. The representation learner and the phase predictor\nare modeled by two neural networks, and their performance is validated via\nsimulations against other benchmarks from the literature. Our results show that\ncausality-based learning yields a predictor that is 15% more accurate in unseen\nOut-of-Distribution (OoD) environments.",
                "Investigating Navigation Strategies in the Morris Water Maze through\n  Deep Reinforcement Learning\nNavigation is a complex skill with a long history of research in animals and\nhumans. In this work, we simulate the Morris Water Maze in 2D to train deep\nreinforcement learning agents. We perform automatic classification of\nnavigation strategies, analyze the distribution of strategies used by\nartificial agents, and compare them with experimental data to show similar\nlearning dynamics as those seen in humans and rodents. We develop\nenvironment-specific auxiliary tasks and examine factors affecting their\nusefulness. We suggest that the most beneficial tasks are potentially more\nbiologically feasible for real agents to use. Lastly, we explore the\ndevelopment of internal representations in the activations of artificial agent\nneural networks. These representations resemble place cells and head-direction\ncells found in mouse brains, and their presence has correlation to the\nnavigation strategies that artificial agents employ."
            ],
            "interesting paper": 2
        }
    ],
    "Quinn Lopez": [
        {
            "papers": [
                "Deep Learning and Ethics\nThis article appears as chapter 21 of Prince (2023, Understanding Deep\nLearning); a complete draft of the textbook is available here:\nhttp://udlbook.com. This chapter considers potential harms arising from the\ndesign and use of AI systems. These include algorithmic bias, lack of\nexplainability, data privacy violations, militarization, fraud, and\nenvironmental concerns. The aim is not to provide advice on being more ethical.\nInstead, the goal is to express ideas and start conversations in key areas that\nhave received attention in philosophy, political science, and the broader\nsocial sciences.",
                "Science in the Era of ChatGPT, Large Language Models and Generative AI:\n  Challenges for Research Ethics and How to Respond\nLarge language models of artificial intelligence (AI), such as ChatGPT, find\nremarkable but controversial applicability in science and research. This paper\nreviews epistemological challenges, ethical and integrity risks in science\nconduct in the advent of generative AI. This is with the aim to lay new timely\nfoundations for a high-quality research ethics review. The role of AI language\nmodels as a research instrument and subject is scrutinized along with ethical\nimplications for scientists, participants and reviewers. New emerging practices\nfor research ethics review are discussed, concluding with ten recommendations\nthat shape a response for a more responsible research conduct in the era of AI.",
                "An Experimental Investigation into the Evaluation of Explainability\n  Methods\nEXplainable Artificial Intelligence (XAI) aims to help users to grasp the\nreasoning behind the predictions of an Artificial Intelligence (AI) system.\nMany XAI approaches have emerged in recent years. Consequently, a subfield\nrelated to the evaluation of XAI methods has gained considerable attention,\nwith the aim to determine which methods provide the best explanation using\nvarious approaches and criteria. However, the literature lacks a comparison of\nthe evaluation metrics themselves, that one can use to evaluate XAI methods.\nThis work aims to fill this gap by comparing 14 different metrics when applied\nto nine state-of-the-art XAI methods and three dummy methods (e.g., random\nsaliency maps) used as references. Experimental results show which of these\nmetrics produces highly correlated results, indicating potential redundancy. We\nalso demonstrate the significant impact of varying the baseline hyperparameter\non the evaluation metric values. Finally, we use dummy methods to assess the\nreliability of metrics in terms of ranking, pointing out their limitations.",
                "On the Planning Abilities of Large Language Models : A Critical\n  Investigation\nIntrigued by the claims of emergent reasoning capabilities in LLMs trained on\ngeneral web corpora, in this paper, we set out to investigate their planning\ncapabilities. We aim to evaluate (1) the effectiveness of LLMs in generating\nplans autonomously in commonsense planning tasks and (2) the potential of LLMs\nin LLM-Modulo settings where they act as a source of heuristic guidance for\nexternal planners and verifiers. We conduct a systematic study by generating a\nsuite of instances on domains similar to the ones employed in the International\nPlanning Competition and evaluate LLMs in two distinct modes: autonomous and\nheuristic. Our findings reveal that LLMs' ability to generate executable plans\nautonomously is rather limited, with the best model (GPT-4) having an average\nsuccess rate of ~12% across the domains. However, the results in the LLM-Modulo\nsetting show more promise. In the LLM-Modulo setting, we demonstrate that\nLLM-generated plans can improve the search process for underlying sound\nplanners and additionally show that external verifiers can help provide\nfeedback on the generated plans and back-prompt the LLM for better plan\ngeneration.",
                "Towards a Capability Assessment Model for the Comprehension and Adoption\n  of AI in Organisations\nThe comprehension and adoption of Artificial Intelligence (AI) are beset with\npractical and ethical problems. This article presents a 5-level AI Capability\nAssessment Model (AI-CAM) and a related AI Capabilities Matrix (AI-CM) to\nassist practitioners in AI comprehension and adoption. These practical tools\nwere developed with business executives, technologists, and other\norganisational stakeholders in mind. They are founded on a comprehensive\nconception of AI compared to those in other AI adoption models and are also\nopen-source artefacts. Thus, the AI-CAM and AI-CM present an accessible\nresource to help inform organisational decision-makers on the capability\nrequirements for (1) AI-based data analytics use cases based on machine\nlearning technologies; (2) Knowledge representation to engineer and represent\ndata, information and knowledge using semantic technologies; and (3) AI-based\nsolutions that seek to emulate human reasoning and decision-making. The AI-CAM\ncovers the core capability dimensions (business, data, technology,\norganisation, AI skills, risks, and ethical considerations) required at the\nfive capability maturity levels to achieve optimal use of AI in organisations.",
                "Asking Before Acting: Gather Information in Embodied Decision Making\n  with Language Models\nWith strong capabilities of reasoning and a broad understanding of the world,\nLarge Language Models (LLMs) have demonstrated immense potential in building\nversatile embodied decision-making agents capable of executing a wide array of\ntasks. Nevertheless, when deployed in unfamiliar environments, we show that LLM\nagents encounter challenges in efficiently gathering essential information,\nleading to suboptimal performance. Conversely, human individuals often seek\nadditional information from their peers prior to taking action, harnessing\nexternal knowledge to avoid unnecessary trial and error. Drawing inspiration\nfrom this behavior, we propose \\textit{Asking Before Acting} (ABA), a method\nthat empowers the agent to proactively inquire with external sources for\npertinent information using natural language during their interactions within\nthe environment. In this way, the agent is able to enhance its efficiency and\nperformance by circumventing potentially laborious steps and combating the\ndifficulties associated with exploration in unfamiliar environments and\nvagueness of the instructions. We conduct extensive experiments involving a\nspectrum of environments including text-based household everyday tasks, robot\narm manipulation tasks, and real world open domain image based embodied tasks.\nThe experiments involve various models from Vicuna to GPT-4. The results\ndemonstrate that, even with modest prompts modifications, ABA exhibits\nsubstantial advantages on both performance and efficiency over baseline LLM\nagents. Further finetuning ABA with reformulated metadata (ABA-FT) faciliates\nlearning the rationale for asking and allows for additional enhancements\nespecially in tasks that baselines struggle to solve."
            ],
            "interesting paper": 2
        },
        {
            "papers": [
                "Asking Before Acting: Gather Information in Embodied Decision Making\n  with Language Models\nWith strong capabilities of reasoning and a broad understanding of the world,\nLarge Language Models (LLMs) have demonstrated immense potential in building\nversatile embodied decision-making agents capable of executing a wide array of\ntasks. Nevertheless, when deployed in unfamiliar environments, we show that LLM\nagents encounter challenges in efficiently gathering essential information,\nleading to suboptimal performance. Conversely, human individuals often seek\nadditional information from their peers prior to taking action, harnessing\nexternal knowledge to avoid unnecessary trial and error. Drawing inspiration\nfrom this behavior, we propose \\textit{Asking Before Acting} (ABA), a method\nthat empowers the agent to proactively inquire with external sources for\npertinent information using natural language during their interactions within\nthe environment. In this way, the agent is able to enhance its efficiency and\nperformance by circumventing potentially laborious steps and combating the\ndifficulties associated with exploration in unfamiliar environments and\nvagueness of the instructions. We conduct extensive experiments involving a\nspectrum of environments including text-based household everyday tasks, robot\narm manipulation tasks, and real world open domain image based embodied tasks.\nThe experiments involve various models from Vicuna to GPT-4. The results\ndemonstrate that, even with modest prompts modifications, ABA exhibits\nsubstantial advantages on both performance and efficiency over baseline LLM\nagents. Further finetuning ABA with reformulated metadata (ABA-FT) faciliates\nlearning the rationale for asking and allows for additional enhancements\nespecially in tasks that baselines struggle to solve.",
                "From Interactive to Co-Constructive Task Learning\nHumans have developed the capability to teach relevant aspects of new or\nadapted tasks to a social peer with very few task demonstrations by making use\nof scaffolding strategies that leverage prior knowledge and importantly prior\njoint experience to yield a joint understanding and a joint execution of the\nrequired steps to solve the task. This process has been discovered and analyzed\nin parent-infant interaction and constitutes a ``co-construction'' as it allows\nboth, the teacher and the learner, to jointly contribute to the task. We\npropose to focus research in robot interactive learning on this co-construction\nprocess to enable robots to learn from non-expert users in everyday situations.\nIn the following, we will review current proposals for interactive task\nlearning and discuss their main contributions with respect to the entailing\ninteraction. We then discuss our notion of co-construction and summarize\nresearch insights from adult-child and human-robot interactions to elucidate\nits nature in more detail. From this overview we finally derive research\ndesiderata that entail the dimensions architecture, representation, interaction\nand explainability.",
                "Role-Play with Large Language Models\nAs dialogue agents become increasingly human-like in their performance, it is\nimperative that we develop effective ways to describe their behaviour in\nhigh-level terms without falling into the trap of anthropomorphism. In this\npaper, we foreground the concept of role-play. Casting dialogue agent behaviour\nin terms of role-play allows us to draw on familiar folk psychological terms,\nwithout ascribing human characteristics to language models they in fact lack.\nTwo important cases of dialogue agent behaviour are addressed this way, namely\n(apparent) deception and (apparent) self-awareness.",
                "An Experimental Investigation into the Evaluation of Explainability\n  Methods\nEXplainable Artificial Intelligence (XAI) aims to help users to grasp the\nreasoning behind the predictions of an Artificial Intelligence (AI) system.\nMany XAI approaches have emerged in recent years. Consequently, a subfield\nrelated to the evaluation of XAI methods has gained considerable attention,\nwith the aim to determine which methods provide the best explanation using\nvarious approaches and criteria. However, the literature lacks a comparison of\nthe evaluation metrics themselves, that one can use to evaluate XAI methods.\nThis work aims to fill this gap by comparing 14 different metrics when applied\nto nine state-of-the-art XAI methods and three dummy methods (e.g., random\nsaliency maps) used as references. Experimental results show which of these\nmetrics produces highly correlated results, indicating potential redundancy. We\nalso demonstrate the significant impact of varying the baseline hyperparameter\non the evaluation metric values. Finally, we use dummy methods to assess the\nreliability of metrics in terms of ranking, pointing out their limitations.",
                "HuatuoGPT, towards Taming Language Model to Be a Doctor\nIn this paper, we present HuatuoGPT, a large language model (LLM) for medical\nconsultation. The core recipe of HuatuoGPT is to leverage both\n\\textit{distilled data from ChatGPT} and \\textit{real-world data from doctors}\nin the supervised fine-tuned stage. The responses of ChatGPT are usually\ndetailed, well-presented and informative while it cannot perform like a doctor\nin many aspects, e.g. for integrative diagnosis. We argue that real-world data\nfrom doctors would be complementary to distilled data in the sense the former\ncould tame a distilled language model to perform like doctors. To better\nleverage the strengths of both data, we train a reward model to align the\nlanguage model with the merits that both data bring, following an RLAIF\n(reinforced learning from AI feedback) fashion. To evaluate and benchmark the\nmodels, we propose a comprehensive evaluation scheme (including automatic and\nmanual metrics). Experimental results demonstrate that HuatuoGPT achieves\nstate-of-the-art results in performing medical consultation among open-source\nLLMs in GPT-4 evaluation, human evaluation, and medical benchmark datasets. It\nis worth noting that by using additional real-world data and RLAIF, the\ndistilled language model (i.e., HuatuoGPT) outperforms its teacher model\nChatGPT in most cases. Our code, data, and models are publicly available at\n\\url{https://github.com/FreedomIntelligence/HuatuoGPT}. The online demo is\navailable at \\url{https://www.HuatuoGPT.cn/}.",
                "MPE4G: Multimodal Pretrained Encoder for Co-Speech Gesture Generation\nWhen virtual agents interact with humans, gestures are crucial to delivering\ntheir intentions with speech. Previous multimodal co-speech gesture generation\nmodels required encoded features of all modalities to generate gestures. If\nsome input modalities are removed or contain noise, the model may not generate\nthe gestures properly. To acquire robust and generalized encodings, we propose\na novel framework with a multimodal pre-trained encoder for co-speech gesture\ngeneration. In the proposed method, the multi-head-attention-based encoder is\ntrained with self-supervised learning to contain the information on each\nmodality. Moreover, we collect full-body gestures that consist of 3D joint\nrotations to improve visualization and apply gestures to the extensible body\nmodel. Through the series of experiments and human evaluation, the proposed\nmethod renders realistic co-speech gestures not only when all input modalities\nare given but also when the input modalities are missing or noisy."
            ],
            "interesting paper": 2
        },
        {
            "papers": [
                "Enhancing Human Capabilities through Symbiotic Artificial Intelligence\n  with Shared Sensory Experiences\nThe merging of human intelligence and artificial intelligence has long been a\nsubject of interest in both science fiction and academia. In this paper, we\nintroduce a novel concept in Human-AI interaction called Symbiotic Artificial\nIntelligence with Shared Sensory Experiences (SAISSE), which aims to establish\na mutually beneficial relationship between AI systems and human users through\nshared sensory experiences. By integrating multiple sensory input channels and\nprocessing human experiences, SAISSE fosters a strong human-AI bond, enabling\nAI systems to learn from and adapt to individual users, providing personalized\nsupport, assistance, and enhancement. Furthermore, we discuss the incorporation\nof memory storage units for long-term growth and development of both the AI\nsystem and its human user. As we address user privacy and ethical guidelines\nfor responsible AI-human symbiosis, we also explore potential biases and\ninequalities in AI-human symbiosis and propose strategies to mitigate these\nchallenges. Our research aims to provide a comprehensive understanding of the\nSAISSE concept and its potential to effectively support and enhance individual\nhuman users through symbiotic AI systems. This position article aims at\ndiscussing poteintial AI-human interaction related topics within the scientific\ncommunity, rather than providing experimental or theoretical results.",
                "Alert of the Second Decision-maker: An Introduction to Human-AI Conflict\nThe collaboration between humans and artificial intelligence (AI) is a\nsignificant feature in this digital age. However, humans and AI may have\nobservation, interpretation, and action conflicts when working synchronously.\nThis phenomenon is often masked by faults and, unfortunately, overlooked. This\npaper systematically introduces the human-AI conflict concept, causes,\nmeasurement methods, and risk assessment. The results highlight that there is a\npotential second decision-maker besides the human, which is the AI; the\nhuman-AI conflict is a unique and emerging risk in digitalized process systems;\nand this is an interdisciplinary field that needs to be distinguished from\ntraditional fault and failure analysis; the conflict risk is significant and\ncannot be ignored.",
                "Neural Task Synthesis for Visual Programming\nGenerative neural models hold great promise in enhancing programming\neducation by synthesizing new content. We seek to design neural models that can\nautomatically generate programming tasks for a given specification in the\ncontext of visual programming domains. Despite the recent successes of large\ngenerative models like GPT-4, our initial results show that these models are\nineffective in synthesizing visual programming tasks and struggle with logical\nand spatial reasoning. We propose a novel neuro-symbolic technique,\nNeurTaskSyn, that can synthesize programming tasks for a specification given in\nthe form of desired programming concepts exercised by its solution code and\nconstraints on the visual task. NeurTaskSyn has two components: the first\ncomponent is trained via imitation learning procedure to generate possible\nsolution codes, and the second component is trained via reinforcement learning\nprocedure to guide an underlying symbolic execution engine that generates\nvisual tasks for these codes. We demonstrate the effectiveness of NeurTaskSyn\nthrough an extensive empirical evaluation and a qualitative study on reference\ntasks taken from the Hour of Code: Classic Maze challenge by Code-dot-org and\nthe Intro to Programming with Karel course by CodeHS-dot-com.",
                "Quantifying the Intrinsic Usefulness of Attributional Explanations for\n  Graph Neural Networks with Artificial Simulatability Studies\nDespite the increasing relevance of explainable AI, assessing the quality of\nexplanations remains a challenging issue. Due to the high costs associated with\nhuman-subject experiments, various proxy metrics are often used to\napproximately quantify explanation quality. Generally, one possible\ninterpretation of the quality of an explanation is its inherent value for\nteaching a related concept to a student. In this work, we extend artificial\nsimulatability studies to the domain of graph neural networks. Instead of\ncostly human trials, we use explanation-supervisable graph neural networks to\nperform simulatability studies to quantify the inherent usefulness of\nattributional graph explanations. We perform an extensive ablation study to\ninvestigate the conditions under which the proposed analyses are most\nmeaningful. We additionally validate our methods applicability on real-world\ngraph classification and regression datasets. We find that relevant\nexplanations can significantly boost the sample efficiency of graph neural\nnetworks and analyze the robustness towards noise and bias in the explanations.\nWe believe that the notion of usefulness obtained from our proposed\nsimulatability analysis provides a dimension of explanation quality that is\nlargely orthogonal to the common practice of faithfulness and has great\npotential to expand the toolbox of explanation quality assessments,\nspecifically for graph explanations.",
                "Attention Paper: How Generative AI Reshapes Digital Shadow Industry?\nThe rapid development of digital economy has led to the emergence of various\nblack and shadow internet industries, which pose potential risks that can be\nidentified and managed through digital risk management (DRM) that uses\ndifferent techniques such as machine learning and deep learning. The evolution\nof DRM architecture has been driven by changes in data forms. However, the\ndevelopment of AI-generated content (AIGC) technology, such as ChatGPT and\nStable Diffusion, has given black and shadow industries powerful tools to\npersonalize data and generate realistic images and conversations for fraudulent\nactivities. This poses a challenge for DRM systems to control risks from the\nsource of data generation and to respond quickly to the fast-changing risk\nenvironment. This paper aims to provide a technical analysis of the challenges\nand opportunities of AIGC from upstream, midstream, and downstream paths of\nblack/shadow industries and suggest future directions for improving existing\nrisk control systems. The paper will explore the new black and shadow\ntechniques triggered by generative AI technology and provide insights for\nbuilding the next-generation DRM system.",
                "Transformative Effects of ChatGPT on Modern Education: Emerging Era of\n  AI Chatbots\nChatGPT, an AI-based chatbot, was released to provide coherent and useful\nreplies based on analysis of large volumes of data. In this article, leading\nscientists, researchers and engineers discuss the transformative effects of\nChatGPT on modern education. This research seeks to improve our knowledge of\nChatGPT capabilities and its use in the education sector, identifying potential\nconcerns and challenges. Our preliminary evaluation concludes that ChatGPT\nperformed differently in each subject area including finance, coding and maths.\nWhile ChatGPT has the ability to help educators by creating instructional\ncontent, offering suggestions and acting as an online educator to learners by\nanswering questions and promoting group work, there are clear drawbacks in its\nuse, such as the possibility of producing inaccurate or false data and\ncircumventing duplicate content (plagiarism) detectors where originality is\nessential. The often reported hallucinations within Generative AI in general,\nand also relevant for ChatGPT, can render its use of limited benefit where\naccuracy is essential. What ChatGPT lacks is a stochastic measure to help\nprovide sincere and sensitive communication with its users. Academic\nregulations and evaluation practices used in educational institutions need to\nbe updated, should ChatGPT be used as a tool in education. To address the\ntransformative effects of ChatGPT on the learning environment, educating\nteachers and students alike about its capabilities and limitations will be\ncrucial."
            ],
            "interesting paper": 1
        },
        {
            "papers": [
                "Alert of the Second Decision-maker: An Introduction to Human-AI Conflict\nThe collaboration between humans and artificial intelligence (AI) is a\nsignificant feature in this digital age. However, humans and AI may have\nobservation, interpretation, and action conflicts when working synchronously.\nThis phenomenon is often masked by faults and, unfortunately, overlooked. This\npaper systematically introduces the human-AI conflict concept, causes,\nmeasurement methods, and risk assessment. The results highlight that there is a\npotential second decision-maker besides the human, which is the AI; the\nhuman-AI conflict is a unique and emerging risk in digitalized process systems;\nand this is an interdisciplinary field that needs to be distinguished from\ntraditional fault and failure analysis; the conflict risk is significant and\ncannot be ignored.",
                "Enhancing Human Capabilities through Symbiotic Artificial Intelligence\n  with Shared Sensory Experiences\nThe merging of human intelligence and artificial intelligence has long been a\nsubject of interest in both science fiction and academia. In this paper, we\nintroduce a novel concept in Human-AI interaction called Symbiotic Artificial\nIntelligence with Shared Sensory Experiences (SAISSE), which aims to establish\na mutually beneficial relationship between AI systems and human users through\nshared sensory experiences. By integrating multiple sensory input channels and\nprocessing human experiences, SAISSE fosters a strong human-AI bond, enabling\nAI systems to learn from and adapt to individual users, providing personalized\nsupport, assistance, and enhancement. Furthermore, we discuss the incorporation\nof memory storage units for long-term growth and development of both the AI\nsystem and its human user. As we address user privacy and ethical guidelines\nfor responsible AI-human symbiosis, we also explore potential biases and\ninequalities in AI-human symbiosis and propose strategies to mitigate these\nchallenges. Our research aims to provide a comprehensive understanding of the\nSAISSE concept and its potential to effectively support and enhance individual\nhuman users through symbiotic AI systems. This position article aims at\ndiscussing poteintial AI-human interaction related topics within the scientific\ncommunity, rather than providing experimental or theoretical results.",
                "A Hierarchical Approach to Population Training for Human-AI\n  Collaboration\nA major challenge for deep reinforcement learning (DRL) agents is to\ncollaborate with novel partners that were not encountered by them during the\ntraining phase. This is specifically worsened by an increased variance in\naction responses when the DRL agents collaborate with human partners due to the\nlack of consistency in human behaviors. Recent work have shown that training a\nsingle agent as the best response to a diverse population of training partners\nsignificantly increases an agent's robustness to novel partners. We further\nenhance the population-based training approach by introducing a Hierarchical\nReinforcement Learning (HRL) based method for Human-AI Collaboration. Our agent\nis able to learn multiple best-response policies as its low-level policy while\nat the same time, it learns a high-level policy that acts as a manager which\nallows the agent to dynamically switch between the low-level best-response\npolicies based on its current partner. We demonstrate that our method is able\nto dynamically adapt to novel partners of different play styles and skill\nlevels in the 2-player collaborative Overcooked game environment. We also\nconducted a human study in the same environment to test the effectiveness of\nour method when partnering with real human subjects.",
                "Passive learning of active causal strategies in agents and language\n  models\nWhat can be learned about causality and experimentation from passive data?\nThis question is salient given recent successes of passively-trained language\nmodels in interactive domains such as tool use. Passive learning is inherently\nlimited. However, we show that purely passive learning can in fact allow an\nagent to learn generalizable strategies for determining and using causal\nstructures, as long as the agent can intervene at test time. We formally\nillustrate that learning a strategy of first experimenting, then seeking goals,\ncan allow generalization from passive learning in principle. We then show\nempirically that agents trained via imitation on expert data can indeed\ngeneralize at test time to infer and use causal links which are never present\nin the training data; these agents can also generalize experimentation\nstrategies to novel variable sets never observed in training. We then show that\nstrategies for causal intervention and exploitation can be generalized from\npassive data even in a more complex environment with high-dimensional\nobservations, with the support of natural language explanations. Explanations\ncan even allow passive learners to generalize out-of-distribution from\nperfectly-confounded training data. Finally, we show that language models,\ntrained only on passive next-word prediction, can generalize causal\nintervention strategies from a few-shot prompt containing examples of\nexperimentation, together with explanations and reasoning. These results\nhighlight the surprising power of passive learning of active causal strategies,\nand may help to understand the behaviors and capabilities of language models.",
                "Ghost in the Minecraft: Generally Capable Agents for Open-World\n  Environments via Large Language Models with Text-based Knowledge and Memory\nThe captivating realm of Minecraft has attracted substantial research\ninterest in recent years, serving as a rich platform for developing intelligent\nagents capable of functioning in open-world environments. However, the current\nresearch landscape predominantly focuses on specific objectives, such as the\npopular \"ObtainDiamond\" task, and has not yet shown effective generalization to\na broader spectrum of tasks. Furthermore, the current leading success rate for\nthe \"ObtainDiamond\" task stands at around 20%, highlighting the limitations of\nReinforcement Learning (RL) based controllers used in existing methods. To\ntackle these challenges, we introduce Ghost in the Minecraft (GITM), a novel\nframework integrates Large Language Models (LLMs) with text-based knowledge and\nmemory, aiming to create Generally Capable Agents (GCAs) in Minecraft. These\nagents, equipped with the logic and common sense capabilities of LLMs, can\nskillfully navigate complex, sparse-reward environments with text-based\ninteractions. We develop a set of structured actions and leverage LLMs to\ngenerate action plans for the agents to execute. The resulting LLM-based agent\nmarkedly surpasses previous methods, achieving a remarkable improvement of\n+47.5% in success rate on the \"ObtainDiamond\" task, demonstrating superior\nrobustness compared to traditional RL-based controllers. Notably, our agent is\nthe first to procure all items in the Minecraft Overworld technology tree,\ndemonstrating its extensive capabilities. GITM does not need any GPU for\ntraining, but a single CPU node with 32 CPU cores is enough. This research\nshows the potential of LLMs in developing capable agents for handling\nlong-horizon, complex tasks and adapting to uncertainties in open-world\nenvironments. See the project website at https://github.com/OpenGVLab/GITM.",
                "Combining Gamification and Intelligent Tutoring Systems in a Serious\n  Game for Engineering Education\nWe provide ongoing results from the development of a personalized learning\nsystem integrated into a serious game. Given limited instructor resources, the\nuse of computerized systems to help tutor students offers a way to provide\nhigher quality education and to improve educational efficacy. Personalized\nlearning systems like the one proposed in this paper offer an accessible\nsolution. Furthermore, by combining such a system with a serious game, students\nare further engaged in interacting with the system. The proposed learning\nsystem combines expert-driven structure and lesson planning with computational\nintelligence methods and gamification to provide students with a fun and\neducational experience. As the project is ongoing from past years, numerous\ndesign iterations have been made on the system based on feedback from students\nand classroom observations. Using computational intelligence, the system\nadaptively provides support to students based on data collected from both their\nin-game actions and by estimating their emotional state from webcam images. For\nour evaluation, we focus on student data gathered from in-classroom testing in\nrelevant courses, with both educational efficacy, results and student\nobservations. To demonstrate the effect of our proposed system, students in an\nearly electrical engineering course were instructed to interact with the system\nin place of a standard lab assignment. The system would then measure and help\nthem improve their background knowledge before allowing them to complete the\nlab assignment. As they played through the game, we observed their interactions\nwith the system to gather insights for future work. Additionally, we\ndemonstrate the system's educational efficacy through pre-post-test results\nfrom students who played the game with and without the personalized learning\nsystem."
            ],
            "interesting paper": 2
        },
        {
            "papers": [
                "How Do UX Practitioners Communicate AI as a Design Material? Artifacts,\n  Conceptions, and Propositions\nUX practitioners (UXPs) face novel challenges when working with and\ncommunicating artificial intelligence (AI) as a design material. We explore how\nUXPs communicate AI concepts when given hands-on experience training and\nexperimenting with AI models. To do so, we conducted a task-based design study\nwith 27 UXPs in which they prototyped and created a design presentation for a\nAI-enabled interface while having access to a simple AI model training tool.\nThrough analyzing UXPs' design presentations and post-activity interviews, we\nfound that although UXPs struggled to clearly communicate some AI concepts,\ntinkering with AI broadened common ground when communicating with technical\nstakeholders. UXPs also identified key risks and benefits of AI in their\ndesigns, and proposed concrete next steps for both UX and AI work. We conclude\nwith a sensitizing concept and recommendations for design and AI tools to\nenhance multi-stakeholder communication and collaboration when crafting\nhuman-centered AI experiences.",
                "Acting as Inverse Inverse Planning\nGreat storytellers know how to take us on a journey. They direct characters\nto act -- not necessarily in the most rational way -- but rather in a way that\nleads to interesting situations, and ultimately creates an impactful experience\nfor audience members looking on.\n  If audience experience is what matters most, then can we help artists and\nanimators *directly* craft such experiences, independent of the concrete\ncharacter actions needed to evoke those experiences? In this paper, we offer a\nnovel computational framework for such tools. Our key idea is to optimize\nanimations with respect to *simulated* audience members' experiences. To\nsimulate the audience, we borrow an established principle from cognitive\nscience: that human social intuition can be modeled as \"inverse planning,\" the\ntask of inferring an agent's (hidden) goals from its (observed) actions.\nBuilding on this model, we treat storytelling as \"*inverse* inverse planning,\"\nthe task of choosing actions to manipulate an inverse planner's inferences. Our\nframework is grounded in literary theory, naturally capturing many storytelling\nelements from first principles. We give a series of examples to demonstrate\nthis, with supporting evidence from human subject studies.",
                "Mindstorms in Natural Language-Based Societies of Mind\nBoth Minsky's \"society of mind\" and Schmidhuber's \"learning to think\" inspire\ndiverse societies of large multimodal neural networks (NNs) that solve problems\nby interviewing each other in a \"mindstorm.\" Recent implementations of NN-based\nsocieties of minds consist of large language models (LLMs) and other NN-based\nexperts communicating through a natural language interface. In doing so, they\novercome the limitations of single LLMs, improving multimodal zero-shot\nreasoning. In these natural language-based societies of mind (NLSOMs), new\nagents -- all communicating through the same universal symbolic language -- are\neasily added in a modular fashion. To demonstrate the power of NLSOMs, we\nassemble and experiment with several of them (having up to 129 members),\nleveraging mindstorms in them to solve some practical AI tasks: visual question\nanswering, image captioning, text-to-image synthesis, 3D generation, egocentric\nretrieval, embodied AI, and general language-based task solving. We view this\nas a starting point towards much larger NLSOMs with billions of agents-some of\nwhich may be humans. And with this emergence of great societies of\nheterogeneous minds, many new research questions have suddenly become paramount\nto the future of artificial intelligence. What should be the social structure\nof an NLSOM? What would be the (dis)advantages of having a monarchical rather\nthan a democratic structure? How can principles of NN economies be used to\nmaximize the total reward of a reinforcement learning NLSOM? In this work, we\nidentify, discuss, and try to answer some of these questions.",
                "Inferring the Future by Imagining the Past\nA single panel of a comic book can say a lot: it can depict not only where\nthe characters currently are, but also their motions, their motivations, their\nemotions, and what they might do next. More generally, humans routinely infer\ncomplex sequences of past and future events from a *static snapshot* of a\n*dynamic scene*, even in situations they have never seen before.\n  In this paper, we model how humans make such rapid and flexible inferences.\nBuilding on a long line of work in cognitive science, we offer a Monte Carlo\nalgorithm whose inferences correlate well with human intuitions in a wide\nvariety of domains, while only using a small, cognitively-plausible number of\nsamples. Our key technical insight is a surprising connection between our\ninference problem and Monte Carlo path tracing, which allows us to apply\ndecades of ideas from the computer graphics community to this\nseemingly-unrelated theory of mind task.",
                "Im-Promptu: In-Context Composition from Image Prompts\nLarge language models are few-shot learners that can solve diverse tasks from\na handful of demonstrations. This implicit understanding of tasks suggests that\nthe attention mechanisms over word tokens may play a role in analogical\nreasoning. In this work, we investigate whether analogical reasoning can enable\nin-context composition over composable elements of visual stimuli. First, we\nintroduce a suite of three benchmarks to test the generalization properties of\na visual in-context learner. We formalize the notion of an analogy-based\nin-context learner and use it to design a meta-learning framework called\nIm-Promptu. Whereas the requisite token granularity for language is well\nestablished, the appropriate compositional granularity for enabling in-context\ngeneralization in visual stimuli is usually unspecified. To this end, we use\nIm-Promptu to train multiple agents with different levels of compositionality,\nincluding vector representations, patch representations, and object slots. Our\nexperiments reveal tradeoffs between extrapolation abilities and the degree of\ncompositionality, with non-compositional representations extending learned\ncomposition rules to unseen domains but performing poorly on combinatorial\ntasks. Patch-based representations require patches to contain entire objects\nfor robust extrapolation. At the same time, object-centric tokenizers coupled\nwith a cross-attention module generate consistent and high-fidelity solutions,\nwith these inductive biases being particularly crucial for compositional\ngeneralization. Lastly, we demonstrate a use case of Im-Promptu as an intuitive\nprogramming interface for image generation.",
                "SwiftSage: A Generative Agent with Fast and Slow Thinking for Complex\n  Interactive Tasks\nWe introduce SwiftSage, a novel agent framework inspired by the dual-process\ntheory of human cognition, designed to excel in action planning for complex\ninteractive reasoning tasks. SwiftSage integrates the strengths of behavior\ncloning and prompting large language models (LLMs) to enhance task completion\nperformance. The framework comprises two primary modules: the Swift module,\nrepresenting fast and intuitive thinking, and the Sage module, emulating\ndeliberate thought processes. The Swift module is a small encoder-decoder LM\nfine-tuned on the oracle agent's action trajectories, while the Sage module\nemploys LLMs such as GPT-4 for subgoal planning and grounding. We develop a\nheuristic method to harmoniously integrate the two modules, resulting in a more\nefficient and robust problem-solving process. In 30 tasks from the ScienceWorld\nbenchmark, SwiftSage significantly outperforms other methods such as SayCan,\nReAct, and Reflexion, demonstrating its effectiveness in solving complex\ninteractive tasks."
            ],
            "interesting paper": 3
        },
        {
            "papers": [
                "Mindstorms in Natural Language-Based Societies of Mind\nBoth Minsky's \"society of mind\" and Schmidhuber's \"learning to think\" inspire\ndiverse societies of large multimodal neural networks (NNs) that solve problems\nby interviewing each other in a \"mindstorm.\" Recent implementations of NN-based\nsocieties of minds consist of large language models (LLMs) and other NN-based\nexperts communicating through a natural language interface. In doing so, they\novercome the limitations of single LLMs, improving multimodal zero-shot\nreasoning. In these natural language-based societies of mind (NLSOMs), new\nagents -- all communicating through the same universal symbolic language -- are\neasily added in a modular fashion. To demonstrate the power of NLSOMs, we\nassemble and experiment with several of them (having up to 129 members),\nleveraging mindstorms in them to solve some practical AI tasks: visual question\nanswering, image captioning, text-to-image synthesis, 3D generation, egocentric\nretrieval, embodied AI, and general language-based task solving. We view this\nas a starting point towards much larger NLSOMs with billions of agents-some of\nwhich may be humans. And with this emergence of great societies of\nheterogeneous minds, many new research questions have suddenly become paramount\nto the future of artificial intelligence. What should be the social structure\nof an NLSOM? What would be the (dis)advantages of having a monarchical rather\nthan a democratic structure? How can principles of NN economies be used to\nmaximize the total reward of a reinforcement learning NLSOM? In this work, we\nidentify, discuss, and try to answer some of these questions.",
                "How Do UX Practitioners Communicate AI as a Design Material? Artifacts,\n  Conceptions, and Propositions\nUX practitioners (UXPs) face novel challenges when working with and\ncommunicating artificial intelligence (AI) as a design material. We explore how\nUXPs communicate AI concepts when given hands-on experience training and\nexperimenting with AI models. To do so, we conducted a task-based design study\nwith 27 UXPs in which they prototyped and created a design presentation for a\nAI-enabled interface while having access to a simple AI model training tool.\nThrough analyzing UXPs' design presentations and post-activity interviews, we\nfound that although UXPs struggled to clearly communicate some AI concepts,\ntinkering with AI broadened common ground when communicating with technical\nstakeholders. UXPs also identified key risks and benefits of AI in their\ndesigns, and proposed concrete next steps for both UX and AI work. We conclude\nwith a sensitizing concept and recommendations for design and AI tools to\nenhance multi-stakeholder communication and collaboration when crafting\nhuman-centered AI experiences.",
                "SwiftSage: A Generative Agent with Fast and Slow Thinking for Complex\n  Interactive Tasks\nWe introduce SwiftSage, a novel agent framework inspired by the dual-process\ntheory of human cognition, designed to excel in action planning for complex\ninteractive reasoning tasks. SwiftSage integrates the strengths of behavior\ncloning and prompting large language models (LLMs) to enhance task completion\nperformance. The framework comprises two primary modules: the Swift module,\nrepresenting fast and intuitive thinking, and the Sage module, emulating\ndeliberate thought processes. The Swift module is a small encoder-decoder LM\nfine-tuned on the oracle agent's action trajectories, while the Sage module\nemploys LLMs such as GPT-4 for subgoal planning and grounding. We develop a\nheuristic method to harmoniously integrate the two modules, resulting in a more\nefficient and robust problem-solving process. In 30 tasks from the ScienceWorld\nbenchmark, SwiftSage significantly outperforms other methods such as SayCan,\nReAct, and Reflexion, demonstrating its effectiveness in solving complex\ninteractive tasks.",
                "Acting as Inverse Inverse Planning\nGreat storytellers know how to take us on a journey. They direct characters\nto act -- not necessarily in the most rational way -- but rather in a way that\nleads to interesting situations, and ultimately creates an impactful experience\nfor audience members looking on.\n  If audience experience is what matters most, then can we help artists and\nanimators *directly* craft such experiences, independent of the concrete\ncharacter actions needed to evoke those experiences? In this paper, we offer a\nnovel computational framework for such tools. Our key idea is to optimize\nanimations with respect to *simulated* audience members' experiences. To\nsimulate the audience, we borrow an established principle from cognitive\nscience: that human social intuition can be modeled as \"inverse planning,\" the\ntask of inferring an agent's (hidden) goals from its (observed) actions.\nBuilding on this model, we treat storytelling as \"*inverse* inverse planning,\"\nthe task of choosing actions to manipulate an inverse planner's inferences. Our\nframework is grounded in literary theory, naturally capturing many storytelling\nelements from first principles. We give a series of examples to demonstrate\nthis, with supporting evidence from human subject studies.",
                "Attention Schema in Neural Agents\nAttention has become a common ingredient in deep learning architectures. It\nadds a dynamical selection of information on top of the static selection of\ninformation supported by weights. In the same way, we can imagine a\nhigher-order informational filter built on top of attention: an Attention\nSchema (AS), namely, a descriptive and predictive model of attention. In\ncognitive neuroscience, Attention Schema Theory (AST) supports this idea of\ndistinguishing attention from AS. A strong prediction of this theory is that an\nagent can use its own AS to also infer the states of other agents' attention\nand consequently enhance coordination with other agents. As such, multi-agent\nreinforcement learning would be an ideal setting to experimentally test the\nvalidity of AST. We explore different ways in which attention and AS interact\nwith each other. Our preliminary results indicate that agents that implement\nthe AS as a recurrent internal control achieve the best performance. In\ngeneral, these exploratory experiments suggest that equipping artificial agents\nwith a model of attention can enhance their social intelligence.",
                "Towards Cognitive Bots: Architectural Research Challenges\nSoftware bots operating in multiple virtual digital platforms must understand\nthe platforms' affordances and behave like human users. Platform affordances or\nfeatures differ from one application platform to another or through a life\ncycle, requiring such bots to be adaptable. Moreover, bots in such platforms\ncould cooperate with humans or other software agents for work or to learn\nspecific behavior patterns. However, present-day bots, particularly chatbots,\nother than language processing and prediction, are far from reaching a human\nuser's behavior level within complex business information systems. They lack\nthe cognitive capabilities to sense and act in such virtual environments,\nrendering their development a challenge to artificial general intelligence\nresearch. In this study, we problematize and investigate assumptions in\nconceptualizing software bot architecture by directing attention to significant\narchitectural research challenges in developing cognitive bots endowed with\ncomplex behavior for operation on information systems. As an outlook, we\npropose alternate architectural assumptions to consider in future bot design\nand bot development frameworks."
            ],
            "interesting paper": 1
        },
        {
            "papers": [
                "Synthesizing a Progression of Subtasks for Block-Based Visual\n  Programming Tasks\nBlock-based visual programming environments play an increasingly important\nrole in introducing computing concepts to K-12 students. In recent years, they\nhave also gained popularity in neuro-symbolic AI, serving as a benchmark to\nevaluate general problem-solving and logical reasoning skills. The open-ended\nand conceptual nature of these visual programming tasks make them challenging,\nboth for state-of-the-art AI agents as well as for novice programmers. A\nnatural approach to providing assistance for problem-solving is breaking down a\ncomplex task into a progression of simpler subtasks; however, this is not\ntrivial given that the solution codes are typically nested and have non-linear\nexecution behavior. In this paper, we formalize the problem of synthesizing\nsuch a progression for a given reference block-based visual programming task.\nWe propose a novel synthesis algorithm that generates a progression of subtasks\nthat are high-quality, well-spaced in terms of their complexity, and solving\nthis progression leads to solving the reference task. We show the utility of\nour synthesis algorithm in improving the efficacy of AI agents (in this case,\nneural program synthesizers) for solving tasks in the Karel programming\nenvironment. Then, we conduct a user study to demonstrate that our synthesized\nprogression of subtasks can assist a novice programmer in solving tasks in the\nHour of Code: Maze Challenge by Code-dot-org.",
                "In-Context Analogical Reasoning with Pre-Trained Language Models\nAnalogical reasoning is a fundamental capacity of human cognition that allows\nus to reason abstractly about novel situations by relating them to past\nexperiences. While it is thought to be essential for robust reasoning in AI\nsystems, conventional approaches require significant training and/or\nhard-coding of domain knowledge to be applied to benchmark tasks. Inspired by\ncognitive science research that has found connections between human language\nand analogy-making, we explore the use of intuitive language-based abstractions\nto support analogy in AI systems. Specifically, we apply large pre-trained\nlanguage models (PLMs) to visual Raven's Progressive Matrices (RPM), a common\nrelational reasoning test. By simply encoding the perceptual features of the\nproblem into language form, we find that PLMs exhibit a striking capacity for\nzero-shot relational reasoning, exceeding human performance and nearing\nsupervised vision-based methods. We explore different encodings that vary the\nlevel of abstraction over task features, finding that higher-level abstractions\nfurther strengthen PLMs' analogical reasoning. Our detailed analysis reveals\ninsights on the role of model complexity, in-context learning, and prior\nknowledge in solving RPM tasks.",
                "A Match Made in Heaven: A Multi-task Framework for Hyperbole and\n  Metaphor Detection\nHyperbole and metaphor are common in day-to-day communication (e.g., \"I am in\ndeep trouble\": how does trouble have depth?), which makes their detection\nimportant, especially in a conversational AI setting. Existing approaches to\nautomatically detect metaphor and hyperbole have studied these language\nphenomena independently, but their relationship has hardly, if ever, been\nexplored computationally. In this paper, we propose a multi-task deep learning\nframework to detect hyperbole and metaphor simultaneously. We hypothesize that\nmetaphors help in hyperbole detection, and vice-versa. To test this hypothesis,\nwe annotate two hyperbole datasets- HYPO and HYPO-L- with metaphor labels.\nSimultaneously, we annotate two metaphor datasets- TroFi and LCC- with\nhyperbole labels. Experiments using these datasets give an improvement of the\nstate of the art of hyperbole detection by 12%. Additionally, our multi-task\nlearning (MTL) approach shows an improvement of up to 17% over single-task\nlearning (STL) for both hyperbole and metaphor detection, supporting our\nhypothesis. To the best of our knowledge, ours is the first demonstration of\ncomputational leveraging of linguistic intimacy between metaphor and hyperbole,\nleading to showing the superiority of MTL over STL for hyperbole and metaphor\ndetection.",
                "AI Coach Assist: An Automated Approach for Call Recommendation in\n  Contact Centers for Agent Coaching\nIn recent years, the utilization of Artificial Intelligence (AI) in the\ncontact center industry is on the rise. One area where AI can have a\nsignificant impact is in the coaching of contact center agents. By analyzing\ncall transcripts using Natural Language Processing (NLP) techniques, it would\nbe possible to quickly determine which calls are most relevant for coaching\npurposes. In this paper, we present AI Coach Assist, which leverages the\npre-trained transformer-based language models to determine whether a given call\nis coachable or not based on the quality assurance (QA) questions asked by the\ncontact center managers or supervisors. The system was trained and evaluated on\na large dataset collected from real-world contact centers and provides an\neffective way to recommend calls to the contact center managers that are more\nlikely to contain coachable moments. Our experimental findings demonstrate the\npotential of AI Coach Assist to improve the coaching process, resulting in\nenhancing the performance of contact center agents.",
                "Emergent Modularity in Pre-trained Transformers\nThis work examines the presence of modularity in pre-trained Transformers, a\nfeature commonly found in human brains and thought to be vital for general\nintelligence. In analogy to human brains, we consider two main characteristics\nof modularity: (1) functional specialization of neurons: we evaluate whether\neach neuron is mainly specialized in a certain function, and find that the\nanswer is yes. (2) function-based neuron grouping: we explore finding a\nstructure that groups neurons into modules by function, and each module works\nfor its corresponding function. Given the enormous amount of possible\nstructures, we focus on Mixture-of-Experts as a promising candidate, which\npartitions neurons into experts and usually activates different experts for\ndifferent inputs. Experimental results show that there are functional experts,\nwhere clustered are the neurons specialized in a certain function. Moreover,\nperturbing the activations of functional experts significantly affects the\ncorresponding function. Finally, we study how modularity emerges during\npre-training, and find that the modular structure is stabilized at the early\nstage, which is faster than neuron stabilization. It suggests that Transformers\nfirst construct the modular structure and then learn fine-grained neuron\nfunctions. Our code and data are available at\nhttps://github.com/THUNLP/modularity-analysis.",
                "Integrating Action Knowledge and LLMs for Task Planning and Situation\n  Handling in Open Worlds\nTask planning systems have been developed to help robots use human knowledge\n(about actions) to complete long-horizon tasks. Most of them have been\ndeveloped for \"closed worlds\" while assuming the robot is provided with\ncomplete world knowledge. However, the real world is generally open, and the\nrobots frequently encounter unforeseen situations that can potentially break\nthe planner's completeness. Could we leverage the recent advances on\npre-trained Large Language Models (LLMs) to enable classical planning systems\nto deal with novel situations?\n  This paper introduces a novel framework, called COWP, for open-world task\nplanning and situation handling. COWP dynamically augments the robot's action\nknowledge, including the preconditions and effects of actions, with\ntask-oriented commonsense knowledge. COWP embraces the openness from LLMs, and\nis grounded to specific domains via action knowledge. For systematic\nevaluations, we collected a dataset that includes 1,085 execution-time\nsituations. Each situation corresponds to a state instance wherein a robot is\npotentially unable to complete a task using a solution that normally works.\nExperimental results show that our approach outperforms competitive baselines\nfrom the literature in the success rate of service tasks. Additionally, we have\ndemonstrated COWP using a mobile manipulator. Supplementary materials are\navailable at: https://cowplanning.github.io/"
            ],
            "interesting paper": 2
        },
        {
            "papers": [
                "Integrating Action Knowledge and LLMs for Task Planning and Situation\n  Handling in Open Worlds\nTask planning systems have been developed to help robots use human knowledge\n(about actions) to complete long-horizon tasks. Most of them have been\ndeveloped for \"closed worlds\" while assuming the robot is provided with\ncomplete world knowledge. However, the real world is generally open, and the\nrobots frequently encounter unforeseen situations that can potentially break\nthe planner's completeness. Could we leverage the recent advances on\npre-trained Large Language Models (LLMs) to enable classical planning systems\nto deal with novel situations?\n  This paper introduces a novel framework, called COWP, for open-world task\nplanning and situation handling. COWP dynamically augments the robot's action\nknowledge, including the preconditions and effects of actions, with\ntask-oriented commonsense knowledge. COWP embraces the openness from LLMs, and\nis grounded to specific domains via action knowledge. For systematic\nevaluations, we collected a dataset that includes 1,085 execution-time\nsituations. Each situation corresponds to a state instance wherein a robot is\npotentially unable to complete a task using a solution that normally works.\nExperimental results show that our approach outperforms competitive baselines\nfrom the literature in the success rate of service tasks. Additionally, we have\ndemonstrated COWP using a mobile manipulator. Supplementary materials are\navailable at: https://cowplanning.github.io/",
                "AI Coach Assist: An Automated Approach for Call Recommendation in\n  Contact Centers for Agent Coaching\nIn recent years, the utilization of Artificial Intelligence (AI) in the\ncontact center industry is on the rise. One area where AI can have a\nsignificant impact is in the coaching of contact center agents. By analyzing\ncall transcripts using Natural Language Processing (NLP) techniques, it would\nbe possible to quickly determine which calls are most relevant for coaching\npurposes. In this paper, we present AI Coach Assist, which leverages the\npre-trained transformer-based language models to determine whether a given call\nis coachable or not based on the quality assurance (QA) questions asked by the\ncontact center managers or supervisors. The system was trained and evaluated on\na large dataset collected from real-world contact centers and provides an\neffective way to recommend calls to the contact center managers that are more\nlikely to contain coachable moments. Our experimental findings demonstrate the\npotential of AI Coach Assist to improve the coaching process, resulting in\nenhancing the performance of contact center agents.",
                "On the Value of Myopic Behavior in Policy Reuse\nLeveraging learned strategies in unfamiliar scenarios is fundamental to human\nintelligence. In reinforcement learning, rationally reusing the policies\nacquired from other tasks or human experts is critical for tackling problems\nthat are difficult to learn from scratch. In this work, we present a framework\ncalled Selective Myopic bEhavior Control~(SMEC), which results from the insight\nthat the short-term behaviors of prior policies are sharable across tasks. By\nevaluating the behaviors of prior policies via a hybrid value function\narchitecture, SMEC adaptively aggregates the sharable short-term behaviors of\nprior policies and the long-term behaviors of the task policy, leading to\ncoordinated decisions. Empirical results on a collection of manipulation and\nlocomotion tasks demonstrate that SMEC outperforms existing methods, and\nvalidate the ability of SMEC to leverage related prior policies.",
                "In-Context Analogical Reasoning with Pre-Trained Language Models\nAnalogical reasoning is a fundamental capacity of human cognition that allows\nus to reason abstractly about novel situations by relating them to past\nexperiences. While it is thought to be essential for robust reasoning in AI\nsystems, conventional approaches require significant training and/or\nhard-coding of domain knowledge to be applied to benchmark tasks. Inspired by\ncognitive science research that has found connections between human language\nand analogy-making, we explore the use of intuitive language-based abstractions\nto support analogy in AI systems. Specifically, we apply large pre-trained\nlanguage models (PLMs) to visual Raven's Progressive Matrices (RPM), a common\nrelational reasoning test. By simply encoding the perceptual features of the\nproblem into language form, we find that PLMs exhibit a striking capacity for\nzero-shot relational reasoning, exceeding human performance and nearing\nsupervised vision-based methods. We explore different encodings that vary the\nlevel of abstraction over task features, finding that higher-level abstractions\nfurther strengthen PLMs' analogical reasoning. Our detailed analysis reveals\ninsights on the role of model complexity, in-context learning, and prior\nknowledge in solving RPM tasks.",
                "Modeling Dynamic Environments with Scene Graph Memory\nEmbodied AI agents that search for objects in large environments such as\nhouseholds often need to make efficient decisions by predicting object\nlocations based on partial information. We pose this as a new type of link\nprediction problem: link prediction on partially observable dynamic graphs. Our\ngraph is a representation of a scene in which rooms and objects are nodes, and\ntheir relationships are encoded in the edges; only parts of the changing graph\nare known to the agent at each timestep. This partial observability poses a\nchallenge to existing link prediction approaches, which we address. We propose\na novel state representation -- Scene Graph Memory (SGM) -- with captures the\nagent's accumulated set of observations, as well as a neural net architecture\ncalled a Node Edge Predictor (NEP) that extracts information from the SGM to\nsearch efficiently. We evaluate our method in the Dynamic House Simulator, a\nnew benchmark that creates diverse dynamic graphs following the semantic\npatterns typically seen at homes, and show that NEP can be trained to predict\nthe locations of objects in a variety of environments with diverse object\nmovement dynamics, outperforming baselines both in terms of new scene\nadaptability and overall accuracy. The codebase and more can be found at\nhttps://www.scenegraphmemory.com.",
                "Probing reaction channels via reinforcement learning\nWe propose a reinforcement learning based method to identify important\nconfigurations that connect reactant and product states along chemical reaction\npaths. By shooting multiple trajectories from these configurations, we can\ngenerate an ensemble of configurations that concentrate on the transition path\nensemble. This configuration ensemble can be effectively employed in a neural\nnetwork-based partial differential equation solver to obtain an approximation\nsolution of a restricted Backward Kolmogorov equation, even when the dimension\nof the problem is very high. The resulting solution, known as the committor\nfunction, encodes mechanistic information for the reaction and can in turn be\nused to evaluate reaction rates."
            ],
            "interesting paper": 4
        },
        {
            "papers": [
                "AI Audit: A Card Game to Reflect on Everyday AI Systems\nAn essential element of K-12 AI literacy is educating learners about the\nethical and societal implications of AI systems. Previous work in AI ethics\nliteracy have developed curriculum and classroom activities that engage\nlearners in reflecting on the ethical implications of AI systems and developing\nresponsible AI. There is little work in using game-based learning methods in AI\nliteracy. Games are known to be compelling media to teach children about\ncomplex STEM concepts. In this work, we developed a competitive card game for\nmiddle and high school students called \"AI Audit\" where they play as AI\nstart-up founders building novel AI-powered technology. Players can challenge\nother players with potential harms of their technology or defend their own\nbusinesses by features that mitigate these harms. The game mechanics reward\nsystems that are ethically developed or that take steps to mitigate potential\nharms. In this paper, we present the game design, teacher resources for\nclassroom deployment and early playtesting results. We discuss our reflections\nabout using games as teaching tools for AI literacy in K-12 classrooms.",
                "Re-imagining health and well-being in low resource African settings\n  using an augmented AI system and a 3D digital twin\nThis paper discusses and explores the potential and relevance of recent\ndevelopments in artificial intelligence (AI) and digital twins for health and\nwell-being in low-resource African countries. We use the case of public health\nemergency response to disease outbreaks and epidemic control. There is\npotential to take advantage of the increasing availability of data and\ndigitization to develop advanced AI methods for analysis and prediction. Using\nan AI systems perspective, we review emerging trends in AI systems and digital\ntwins and propose an initial augmented AI system architecture to illustrate how\nan AI system can work with a 3D digital twin to address public health goals. We\nhighlight scientific knowledge discovery, continual learning, pragmatic\ninteroperability, and interactive explanation and decision-making as essential\nresearch challenges for AI systems and digital twins.",
                "The Digital Divide in Process Safety: Quantitative Risk Analysis of\n  Human-AI Collaboration\nDigital technologies have dramatically accelerated the digital transformation\nin process industries, boosted new industrial applications, upgraded the\nproduction system, and enhanced operational efficiency. In contrast, the\nchallenges and gaps between human and artificial intelligence (AI) have become\nmore and more prominent, whereas the digital divide in process safety is\naggregating. The study attempts to address the following questions: (i)What is\nAI in the process safety context? (ii)What is the difference between AI and\nhumans in process safety? (iii)How do AI and humans collaborate in process\nsafety? (iv)What are the challenges and gaps in human-AI collaboration? (v)How\nto quantify the risk of human-AI collaboration in process safety? Qualitative\nrisk analysis based on brainstorming and literature review, and quantitative\nrisk analysis based on layer of protection analysis (LOPA) and Bayesian network\n(BN), were applied to explore and model. The importance of human reliability\nshould be stressed in the digital age, not usually to increase the reliability\nof AI, and human-centered AI design in process safety needs to be propagated.",
                "Voluminous yet Vacuous? Semantic Capital in an Age of Large Language\n  Models\nLarge Language Models (LLMs) have emerged as transformative forces in the\nrealm of natural language processing, wielding the power to generate human-like\ntext. However, despite their potential for content creation, they carry the\nrisk of eroding our Semantic Capital (SC) - the collective knowledge within our\ndigital ecosystem - thereby posing diverse social epistemic challenges. This\npaper explores the evolution, capabilities, and limitations of these models,\nwhile highlighting ethical concerns they raise. The study contribution is\ntwo-fold: first, it is acknowledged that, withstanding the challenges of\ntracking and controlling LLM impacts, it is necessary to reconsider our\ninteraction with these AI technologies and the narratives that form public\nperception of them. It is argued that before achieving this goal, it is\nessential to confront a potential deontological tipping point in an increasing\nAI-driven infosphere. This goes beyond just adhering to AI ethical norms or\nregulations and requires understanding the spectrum of social epistemic risks\nLLMs might bring to our collective SC. Secondly, building on Luciano Floridi's\ntaxonomy for SC risks, those are mapped within the functionality and\nconstraints of LLMs. By this outlook, we aim to protect and enrich our SC while\nfostering a collaborative environment between humans and AI that augments human\nintelligence rather than replacing it.",
                "Taming AI Bots: Controllability of Neural States in Large Language\n  Models\nWe tackle the question of whether an agent can, by suitable choice of\nprompts, control an AI bot to any state. To that end, we first introduce a\nformal definition of ``meaning'' that is amenable to analysis. Then, we\ncharacterize ``meaningful data'' on which large language models (LLMs) are\nostensibly trained, and ``well-trained LLMs'' through conditions that are\nlargely met by today's LLMs. While a well-trained LLM constructs an embedding\nspace of meanings that is Euclidean, meanings themselves do not form a vector\n(linear) subspace, but rather a quotient space within. We then characterize the\nsubset of meanings that can be reached by the state of the LLMs for some input\nprompt, and show that a well-trained bot can reach any meaning albeit with\nsmall probability. We then introduce a stronger notion of controllability as\n{\\em almost certain reachability}, and show that, when restricted to the space\nof meanings, an AI bot is controllable. We do so after introducing a functional\ncharacterization of attentive AI bots, and finally derive necessary and\nsufficient conditions for controllability. The fact that AI bots are\ncontrollable means that an adversary could steer them towards any state.\nHowever, the sampling process can be designed to counteract adverse actions and\navoid reaching undesirable regions of state space before their boundary is\ncrossed.",
                "Employing Explainable Artificial Intelligence (XAI) Methodologies to\n  Analyze the Correlation between Input Variables and Tensile Strength in\n  Additively Manufactured Samples\nThis research paper explores the impact of various input parameters,\nincluding Infill percentage, Layer Height, Extrusion Temperature, and Print\nSpeed, on the resulting Tensile Strength in objects produced through additive\nmanufacturing. The main objective of this study is to enhance our understanding\nof the correlation between the input parameters and Tensile Strength, as well\nas to identify the key factors influencing the performance of the additive\nmanufacturing process. To achieve this objective, we introduced the utilization\nof Explainable Artificial Intelligence (XAI) techniques for the first time,\nwhich allowed us to analyze the data and gain valuable insights into the\nsystem's behavior. Specifically, we employed SHAP (SHapley Additive\nexPlanations), a widely adopted framework for interpreting machine learning\nmodel predictions, to provide explanations for the behavior of a machine\nlearning model trained on the data. Our findings reveal that the Infill\npercentage and Extrusion Temperature have the most significant influence on\nTensile Strength, while the impact of Layer Height and Print Speed is\nrelatively minor. Furthermore, we discovered that the relationship between the\ninput parameters and Tensile Strength is highly intricate and nonlinear, making\nit difficult to accurately describe using simple linear models."
            ],
            "interesting paper": 4
        },
        {
            "papers": [
                "The Digital Divide in Process Safety: Quantitative Risk Analysis of\n  Human-AI Collaboration\nDigital technologies have dramatically accelerated the digital transformation\nin process industries, boosted new industrial applications, upgraded the\nproduction system, and enhanced operational efficiency. In contrast, the\nchallenges and gaps between human and artificial intelligence (AI) have become\nmore and more prominent, whereas the digital divide in process safety is\naggregating. The study attempts to address the following questions: (i)What is\nAI in the process safety context? (ii)What is the difference between AI and\nhumans in process safety? (iii)How do AI and humans collaborate in process\nsafety? (iv)What are the challenges and gaps in human-AI collaboration? (v)How\nto quantify the risk of human-AI collaboration in process safety? Qualitative\nrisk analysis based on brainstorming and literature review, and quantitative\nrisk analysis based on layer of protection analysis (LOPA) and Bayesian network\n(BN), were applied to explore and model. The importance of human reliability\nshould be stressed in the digital age, not usually to increase the reliability\nof AI, and human-centered AI design in process safety needs to be propagated.",
                "IoT based Personal Voice Assistant\nToday, technological advancement is increasing day by day. Earlier, there was\nonly a computer system in which we could only perform a few tasks. But now,\nmachine learning, artificial intelligence, deep learning, and a few more\ntechnologies have made computer systems so advanced that we can perform any\ntype of task. In this era of advancement, if people are still struggling to\ninteract using various input devices, then it's not worth it. For this reason,\nwe developed a voice assistant using Python that allows the user to run any\ntype of command in Linux without interaction with the keyboard. The main task\nof the voice assistant is to minimize the use of input devices like the\nkeyboard and mouse. It will also reduce hardware space and cost.",
                "Visual Affordance Prediction for Guiding Robot Exploration\nMotivated by the intuitive understanding humans have about the space of\npossible interactions, and the ease with which they can generalize this\nunderstanding to previously unseen scenes, we develop an approach for learning\nvisual affordances for guiding robot exploration. Given an input image of a\nscene, we infer a distribution over plausible future states that can be\nachieved via interactions with it. We use a Transformer-based model to learn a\nconditional distribution in the latent embedding space of a VQ-VAE and show\nthat these models can be trained using large-scale and diverse passive data,\nand that the learned models exhibit compositional generalization to diverse\nobjects beyond the training distribution. We show how the trained affordance\nmodel can be used for guiding exploration by acting as a goal-sampling\ndistribution, during visual goal-conditioned policy learning in robotic\nmanipulation.",
                "Taming AI Bots: Controllability of Neural States in Large Language\n  Models\nWe tackle the question of whether an agent can, by suitable choice of\nprompts, control an AI bot to any state. To that end, we first introduce a\nformal definition of ``meaning'' that is amenable to analysis. Then, we\ncharacterize ``meaningful data'' on which large language models (LLMs) are\nostensibly trained, and ``well-trained LLMs'' through conditions that are\nlargely met by today's LLMs. While a well-trained LLM constructs an embedding\nspace of meanings that is Euclidean, meanings themselves do not form a vector\n(linear) subspace, but rather a quotient space within. We then characterize the\nsubset of meanings that can be reached by the state of the LLMs for some input\nprompt, and show that a well-trained bot can reach any meaning albeit with\nsmall probability. We then introduce a stronger notion of controllability as\n{\\em almost certain reachability}, and show that, when restricted to the space\nof meanings, an AI bot is controllable. We do so after introducing a functional\ncharacterization of attentive AI bots, and finally derive necessary and\nsufficient conditions for controllability. The fact that AI bots are\ncontrollable means that an adversary could steer them towards any state.\nHowever, the sampling process can be designed to counteract adverse actions and\navoid reaching undesirable regions of state space before their boundary is\ncrossed.",
                "Re-imagining health and well-being in low resource African settings\n  using an augmented AI system and a 3D digital twin\nThis paper discusses and explores the potential and relevance of recent\ndevelopments in artificial intelligence (AI) and digital twins for health and\nwell-being in low-resource African countries. We use the case of public health\nemergency response to disease outbreaks and epidemic control. There is\npotential to take advantage of the increasing availability of data and\ndigitization to develop advanced AI methods for analysis and prediction. Using\nan AI systems perspective, we review emerging trends in AI systems and digital\ntwins and propose an initial augmented AI system architecture to illustrate how\nan AI system can work with a 3D digital twin to address public health goals. We\nhighlight scientific knowledge discovery, continual learning, pragmatic\ninteroperability, and interactive explanation and decision-making as essential\nresearch challenges for AI systems and digital twins.",
                "Reinforcement Learning with Human Feedback: Learning Dynamic Choices via\n  Pessimism\nIn this paper, we study offline Reinforcement Learning with Human Feedback\n(RLHF) where we aim to learn the human's underlying reward and the MDP's\noptimal policy from a set of trajectories induced by human choices. RLHF is\nchallenging for multiple reasons: large state space but limited human feedback,\nthe bounded rationality of human decisions, and the off-policy distribution\nshift. In this paper, we focus on the Dynamic Discrete Choice (DDC) model for\nmodeling and understanding human choices. DCC, rooted in econometrics and\ndecision theory, is widely used to model a human decision-making process with\nforward-looking and bounded rationality. We propose a\n\\underline{D}ynamic-\\underline{C}hoice-\\underline{P}essimistic-\\underline{P}olicy-\\underline{O}ptimization\n(DCPPO) method. \\ The method involves a three-stage process: The first step is\nto estimate the human behavior policy and the state-action value function via\nmaximum likelihood estimation (MLE); the second step recovers the human reward\nfunction via minimizing Bellman mean squared error using the learned value\nfunctions; the third step is to plug in the learned reward and invoke\npessimistic value iteration for finding a near-optimal policy. With only\nsingle-policy coverage (i.e., optimal policy) of the dataset, we prove that the\nsuboptimality of DCPPO almost matches the classical pessimistic offline RL\nalgorithm in terms of suboptimality's dependency on distribution shift and\ndimension. To the best of our knowledge, this paper presents the first\ntheoretical guarantees for off-policy offline RLHF with dynamic discrete choice\nmodel."
            ],
            "interesting paper": 1
        },
        {
            "papers": [
                "Towards a Unifying Model of Rationality in Multiagent Systems\nMultiagent systems deployed in the real world need to cooperate with other\nagents (including humans) nearly as effectively as these agents cooperate with\none another. To design such AI, and provide guarantees of its effectiveness, we\nneed to clearly specify what types of agents our AI must be able to cooperate\nwith. In this work we propose a generic model of socially intelligent agents,\nwhich are individually rational learners that are also able to cooperate with\none another (in the sense that their joint behavior is Pareto efficient). We\ndefine rationality in terms of the regret incurred by each agent over its\nlifetime, and show how we can construct socially intelligent agents for\ndifferent forms of regret. We then discuss the implications of this model for\nthe development of \"robust\" MAS that can cooperate with a wide variety of\nsocially intelligent agents.",
                "RE-centric Recommendations for the Development of Trustworthy(er)\n  Autonomous Systems\nComplying with the EU AI Act (AIA) guidelines while developing and\nimplementing AI systems will soon be mandatory within the EU. However,\npractitioners lack actionable instructions to operationalise ethics during AI\nsystems development. A literature review of different ethical guidelines\nrevealed inconsistencies in the principles addressed and the terminology used\nto describe them. Furthermore, requirements engineering (RE), which is\nidentified to foster trustworthiness in the AI development process from the\nearly stages was observed to be absent in a lot of frameworks that support the\ndevelopment of ethical and trustworthy AI. This incongruous phrasing combined\nwith a lack of concrete development practices makes trustworthy AI development\nharder. To address this concern, we formulated a comparison table for the\nterminology used and the coverage of the ethical AI principles in major ethical\nAI guidelines. We then examined the applicability of ethical AI development\nframeworks for performing effective RE during the development of trustworthy AI\nsystems. A tertiary review and meta-analysis of literature discussing ethical\nAI frameworks revealed their limitations when developing trustworthy AI. Based\non our findings, we propose recommendations to address such limitations during\nthe development of trustworthy AI.",
                "Code Prompting: a Neural Symbolic Method for Complex Reasoning in Large\n  Language Models\nLarge language models (LLMs) have scaled up to unlock a wide range of complex\nreasoning tasks with the aid of various prompting methods. However, current\nprompting methods generate natural language intermediate steps to help\nreasoning, which can cause imperfect task reduction and confusion. To mitigate\nsuch limitations, we explore code prompting, a neural symbolic prompting method\nwith both zero-shot and few-shot versions which triggers code as intermediate\nsteps. We conduct experiments on 7 widely-used benchmarks involving symbolic\nreasoning and arithmetic reasoning. Code prompting generally outperforms\nchain-of-thought (CoT) prompting. To further understand the performance and\nlimitations of code prompting, we perform extensive ablation studies and error\nanalyses, and identify several exclusive advantages of using symbolic\npromptings compared to natural language. We also consider the ensemble of code\nprompting and CoT prompting to combine the strengths of both. Finally, we show\nthrough experiments how code annotations and their locations affect code\nprompting.",
                "DiffSketching: Sketch Control Image Synthesis with Diffusion Models\nCreative sketch is a universal way of visual expression, but translating\nimages from an abstract sketch is very challenging. Traditionally, creating a\ndeep learning model for sketch-to-image synthesis needs to overcome the\ndistorted input sketch without visual details, and requires to collect\nlarge-scale sketch-image datasets. We first study this task by using diffusion\nmodels. Our model matches sketches through the cross domain constraints, and\nuses a classifier to guide the image synthesis more accurately. Extensive\nexperiments confirmed that our method can not only be faithful to user's input\nsketches, but also maintain the diversity and imagination of synthetic image\nresults. Our model can beat GAN-based method in terms of generation quality and\nhuman evaluation, and does not rely on massive sketch-image datasets.\nAdditionally, we present applications of our method in image editing and\ninterpolation.",
                "Perceived Trustworthiness of Natural Language Generators\nNatural Language Generation tools, such as chatbots that can generate\nhuman-like conversational text, are becoming more common both for personal and\nprofessional use. However, there are concerns about their trustworthiness and\nethical implications. The paper addresses the problem of understanding how\ndifferent users (e.g., linguists, engineers) perceive and adopt these tools and\ntheir perception of machine-generated text quality. It also discusses the\nperceived advantages and limitations of Natural Language Generation tools, as\nwell as users' beliefs on governance strategies. The main findings of this\nstudy include the impact of users' field and level of expertise on the\nperceived trust and adoption of Natural Language Generation tools, the users'\nassessment of the accuracy, fluency, and potential biases of machine-generated\ntext in comparison to human-written text, and an analysis of the advantages and\nethical risks associated with these tools as identified by the participants.\nMoreover, this paper discusses the potential implications of these findings for\nenhancing the AI development process. The paper sheds light on how different\nuser characteristics shape their beliefs on the quality and overall\ntrustworthiness of machine-generated text. Furthermore, it examines the\nbenefits and risks of these tools from the perspectives of different users.",
                "Game of Tones: Faculty detection of GPT-4 generated content in\n  university assessments\nThis study explores the robustness of university assessments against the use\nof Open AI's Generative Pre-Trained Transformer 4 (GPT-4) generated content and\nevaluates the ability of academic staff to detect its use when supported by the\nTurnitin Artificial Intelligence (AI) detection tool. The research involved\ntwenty-two GPT-4 generated submissions being created and included in the\nassessment process to be marked by fifteen different faculty members. The study\nreveals that although the detection tool identified 91% of the experimental\nsubmissions as containing some AI-generated content, the total detected content\nwas only 54.8%. This suggests that the use of adversarial techniques regarding\nprompt engineering is an effective method in evading AI detection tools and\nhighlights that improvements to AI detection software are needed. Using the\nTurnitin AI detect tool, faculty reported 54.5% of the experimental submissions\nto the academic misconduct process, suggesting the need for increased awareness\nand training into these tools. Genuine submissions received a mean score of\n54.4, whereas AI-generated content scored 52.3, indicating the comparable\nperformance of GPT-4 in real-life situations. Recommendations include adjusting\nassessment strategies to make them more resistant to the use of AI tools, using\nAI-inclusive assessment where possible, and providing comprehensive training\nprograms for faculty and students. This research contributes to understanding\nthe relationship between AI-generated content and academic assessment, urging\nfurther investigation to preserve academic integrity."
            ],
            "interesting paper": 4
        },
        {
            "papers": [
                "AlphaBlock: Embodied Finetuning for Vision-Language Reasoning in Robot\n  Manipulation\nWe propose a novel framework for learning high-level cognitive capabilities\nin robot manipulation tasks, such as making a smiley face using building\nblocks. These tasks often involve complex multi-step reasoning, presenting\nsignificant challenges due to the limited paired data connecting human\ninstructions (e.g., making a smiley face) and robot actions (e.g., end-effector\nmovement). Existing approaches relieve this challenge by adopting an open-loop\nparadigm decomposing high-level instructions into simple sub-task plans, and\nexecuting them step-by-step using low-level control models. However, these\napproaches are short of instant observations in multi-step reasoning, leading\nto sub-optimal results. To address this issue, we propose to automatically\ncollect a cognitive robot dataset by Large Language Models (LLMs). The\nresulting dataset AlphaBlock consists of 35 comprehensive high-level tasks of\nmulti-step text plans and paired observation sequences. To enable efficient\ndata acquisition, we employ elaborated multi-round prompt designs that\neffectively reduce the burden of extensive human involvement. We further\npropose a closed-loop multi-modal embodied planning model that autoregressively\ngenerates plans by taking image observations as input. To facilitate effective\nlearning, we leverage MiniGPT-4 with a frozen visual encoder and LLM, and\nfinetune additional vision adapter and Q-former to enable fine-grained spatial\nperception for manipulation tasks. We conduct experiments to verify the\nsuperiority over existing open and closed-loop methods, and achieve a\nsignificant increase in success rate by 21.4% and 14.5% over ChatGPT and GPT-4\nbased robot tasks. Real-world demos are shown in\nhttps://www.youtube.com/watch?v=ayAzID1_qQk .",
                "An Emergency Disposal Decision-making Method with Human--Machine\n  Collaboration\nRapid developments in artificial intelligence technology have led to unmanned\nsystems replacing human beings in many fields requiring high-precision\npredictions and decisions. In modern operational environments, all job plans\nare affected by emergency events such as equipment failures and resource\nshortages, making a quick resolution critical. The use of unmanned systems to\nassist decision-making can improve resolution efficiency, but their\ndecision-making is not interpretable and may make the wrong decisions. Current\nunmanned systems require human supervision and control. Based on this, we\npropose a collaborative human--machine method for resolving unplanned events\nusing two phases: task filtering and task scheduling. In the task filtering\nphase, we propose a human--machine collaborative decision-making algorithm for\ndynamic tasks. The GACRNN model is used to predict the state of the job nodes,\nlocate the key nodes, and generate a machine-predicted resolution task list. A\nhuman decision-maker supervises the list in real time and modifies and confirms\nthe machine-predicted list through the human--machine interface. In the task\nscheduling phase, we propose a scheduling algorithm that integrates human\nexperience constraints. The steps to resolve an event are inserted into the\nnormal job sequence to schedule the resolution. We propose several\nhuman--machine collaboration methods in each phase to generate steps to resolve\nan unplanned event while minimizing the impact on the original job plan.",
                "Towards a Unifying Model of Rationality in Multiagent Systems\nMultiagent systems deployed in the real world need to cooperate with other\nagents (including humans) nearly as effectively as these agents cooperate with\none another. To design such AI, and provide guarantees of its effectiveness, we\nneed to clearly specify what types of agents our AI must be able to cooperate\nwith. In this work we propose a generic model of socially intelligent agents,\nwhich are individually rational learners that are also able to cooperate with\none another (in the sense that their joint behavior is Pareto efficient). We\ndefine rationality in terms of the regret incurred by each agent over its\nlifetime, and show how we can construct socially intelligent agents for\ndifferent forms of regret. We then discuss the implications of this model for\nthe development of \"robust\" MAS that can cooperate with a wide variety of\nsocially intelligent agents.",
                "Reason to explain: Interactive contrastive explanations (REASONX)\nMany high-performing machine learning models are not interpretable. As they\nare increasingly used in decision scenarios that can critically affect\nindividuals, it is necessary to develop tools to better understand their\noutputs. Popular explanation methods include contrastive explanations. However,\nthey suffer several shortcomings, among others an insufficient incorporation of\nbackground knowledge, and a lack of interactivity. While (dialogue-like)\ninteractivity is important to better communicate an explanation, background\nknowledge has the potential to significantly improve their quality, e.g., by\nadapting the explanation to the needs of the end-user. To close this gap, we\npresent REASONX, an explanation tool based on Constraint Logic Programming\n(CLP). REASONX provides interactive contrastive explanations that can be\naugmented by background knowledge, and allows to operate under a setting of\nunder-specified information, leading to increased flexibility in the provided\nexplanations. REASONX computes factual and constrative decision rules, as well\nas closest constrative examples. It provides explanations for decision trees,\nwhich can be the ML models under analysis, or global/local surrogate models of\nany ML model. While the core part of REASONX is built on CLP, we also provide a\nprogram layer that allows to compute the explanations via Python, making the\ntool accessible to a wider audience. We illustrate the capability of REASONX on\na synthetic data set, and on a a well-developed example in the credit domain.\nIn both cases, we can show how REASONX can be flexibly used and tailored to the\nneeds of the user.",
                "Contextual Object Detection with Multimodal Large Language Models\nRecent Multimodal Large Language Models (MLLMs) are remarkable in\nvision-language tasks, such as image captioning and question answering, but\nlack the essential perception ability, i.e., object detection. In this work, we\naddress this limitation by introducing a novel research problem of contextual\nobject detection -- understanding visible objects within different human-AI\ninteractive contexts. Three representative scenarios are investigated,\nincluding the language cloze test, visual captioning, and question answering.\nMoreover, we present ContextDET, a unified multimodal model that is capable of\nend-to-end differentiable modeling of visual-language contexts, so as to\nlocate, identify, and associate visual objects with language inputs for\nhuman-AI interaction. Our ContextDET involves three key submodels: (i) a visual\nencoder for extracting visual representations, (ii) a pre-trained LLM for\nmultimodal context decoding, and (iii) a visual decoder for predicting bounding\nboxes given contextual object words. The new generate-then-detect framework\nenables us to detect object words within human vocabulary. Extensive\nexperiments show the advantages of ContextDET on our proposed CODE benchmark,\nopen-vocabulary detection, and referring image segmentation. Github:\nhttps://github.com/yuhangzang/ContextDET.",
                "Code Prompting: a Neural Symbolic Method for Complex Reasoning in Large\n  Language Models\nLarge language models (LLMs) have scaled up to unlock a wide range of complex\nreasoning tasks with the aid of various prompting methods. However, current\nprompting methods generate natural language intermediate steps to help\nreasoning, which can cause imperfect task reduction and confusion. To mitigate\nsuch limitations, we explore code prompting, a neural symbolic prompting method\nwith both zero-shot and few-shot versions which triggers code as intermediate\nsteps. We conduct experiments on 7 widely-used benchmarks involving symbolic\nreasoning and arithmetic reasoning. Code prompting generally outperforms\nchain-of-thought (CoT) prompting. To further understand the performance and\nlimitations of code prompting, we perform extensive ablation studies and error\nanalyses, and identify several exclusive advantages of using symbolic\npromptings compared to natural language. We also consider the ensemble of code\nprompting and CoT prompting to combine the strengths of both. Finally, we show\nthrough experiments how code annotations and their locations affect code\nprompting."
            ],
            "interesting paper": 5
        },
        {
            "papers": [
                "Conceptual Design Generation Using Large Language Models\nConcept generation is a creative step in the conceptual design phase, where\ndesigners often turn to brainstorming, mindmapping, or crowdsourcing design\nideas to complement their own knowledge of the domain. Recent advances in\nnatural language processing (NLP) and machine learning (ML) have led to the\nrise of Large Language Models (LLMs) capable of generating seemingly creative\noutputs from textual prompts. The success of these models has led to their\nintegration and application across a variety of domains, including art,\nentertainment, and other creative work. In this paper, we leverage LLMs to\ngenerate solutions for a set of 12 design problems and compare them to a\nbaseline of crowdsourced solutions. We evaluate the differences between\ngenerated and crowdsourced design solutions through multiple perspectives,\nincluding human expert evaluations and computational metrics. Expert\nevaluations indicate that the LLM-generated solutions have higher average\nfeasibility and usefulness while the crowdsourced solutions have more novelty.\nWe experiment with prompt engineering and find that leveraging few-shot\nlearning can lead to the generation of solutions that are more similar to the\ncrowdsourced solutions. These findings provide insight into the quality of\ndesign solutions generated with LLMs and begins to evaluate prompt engineering\ntechniques that could be leveraged by practitioners to generate higher-quality\ndesign solutions synergistically with LLMs.",
                "Evaluating GPT's Programming Capability through CodeWars' Katas\nIn the burgeoning field of artificial intelligence (AI), understanding the\ncapabilities and limitations of programming-oriented models is crucial. This\npaper presents a novel evaluation of the programming proficiency of Generative\nPretrained Transformer (GPT) models, specifically GPT-3.5 and GPT-4, against\ncoding problems of varying difficulty levels drawn from Codewars. The\nexperiments reveal a distinct boundary at the 3kyu level, beyond which these\nGPT models struggle to provide solutions. These findings led to the proposal of\na measure for coding problem complexity that incorporates both problem\ndifficulty and the time required for solution. The research emphasizes the need\nfor validation and creative thinking capabilities in AI models to better\nemulate human problem-solving techniques. Future work aims to refine this\nproposed complexity measure, enhance AI models with these suggested\ncapabilities, and develop an objective measure for programming problem\ndifficulty. The results of this research offer invaluable insights for\nimproving AI programming capabilities and advancing the frontier of AI\nproblem-solving abilities.",
                "Intent-aligned AI systems deplete human agency: the need for agency\n  foundations research in AI safety\nThe rapid advancement of artificial intelligence (AI) systems suggests that\nartificial general intelligence (AGI) systems may soon arrive. Many researchers\nare concerned that AIs and AGIs will harm humans via intentional misuse\n(AI-misuse) or through accidents (AI-accidents). In respect of AI-accidents,\nthere is an increasing effort focused on developing algorithms and paradigms\nthat ensure AI systems are aligned to what humans intend, e.g. AI systems that\nyield actions or recommendations that humans might judge as consistent with\ntheir intentions and goals. Here we argue that alignment to human intent is\ninsufficient for safe AI systems and that preservation of long-term agency of\nhumans may be a more robust standard, and one that needs to be separated\nexplicitly and a priori during optimization. We argue that AI systems can\nreshape human intention and discuss the lack of biological and psychological\nmechanisms that protect humans from loss of agency. We provide the first formal\ndefinition of agency-preserving AI-human interactions which focuses on\nforward-looking agency evaluations and argue that AI systems - not humans -\nmust be increasingly tasked with making these evaluations. We show how agency\nloss can occur in simple environments containing embedded agents that use\ntemporal-difference learning to make action recommendations. Finally, we\npropose a new area of research called \"agency foundations\" and pose four\ninitial topics designed to improve our understanding of agency in AI-human\ninteractions: benevolent game theory, algorithmic foundations of human rights,\nmechanistic interpretability of agency representation in neural-networks and\nreinforcement learning from internal states.",
                "Towards Explainable and Language-Agnostic LLMs: Symbolic Reverse\n  Engineering of Language at Scale\nLarge language models (LLMs) have achieved a milestone that undenia-bly\nchanged many held beliefs in artificial intelligence (AI). However, there\nremains many limitations of these LLMs when it comes to true language\nunderstanding, limitations that are a byproduct of the under-lying architecture\nof deep neural networks. Moreover, and due to their subsymbolic nature,\nwhatever knowledge these models acquire about how language works will always be\nburied in billions of microfeatures (weights), none of which is meaningful on\nits own, making such models hopelessly unexplainable. To address these\nlimitations, we suggest com-bining the strength of symbolic representations\nwith what we believe to be the key to the success of LLMs, namely a successful\nbottom-up re-verse engineering of language at scale. As such we argue for a\nbottom-up reverse engineering of language in a symbolic setting. Hints on what\nthis project amounts to have been suggested by several authors, and we discuss\nin some detail here how this project could be accomplished.",
                "Less Likely Brainstorming: Using Language Models to Generate Alternative\n  Hypotheses\nA human decision-maker benefits the most from an AI assistant that corrects\nfor their biases. For problems such as generating interpretation of a radiology\nreport given findings, a system predicting only highly likely outcomes may be\nless useful, where such outcomes are already obvious to the user. To alleviate\nbiases in human decision-making, it is worth considering a broad differential\ndiagnosis, going beyond the most likely options. We introduce a new task, \"less\nlikely brainstorming,\" that asks a model to generate outputs that humans think\nare relevant but less likely to happen. We explore the task in two settings: a\nbrain MRI interpretation generation setting and an everyday commonsense\nreasoning setting. We found that a baseline approach of training with less\nlikely hypotheses as targets generates outputs that humans evaluate as either\nlikely or irrelevant nearly half of the time; standard MLE training is not\neffective. To tackle this problem, we propose a controlled text generation\nmethod that uses a novel contrastive learning strategy to encourage models to\ndifferentiate between generating likely and less likely outputs according to\nhumans. We compare our method with several state-of-the-art controlled text\ngeneration models via automatic and human evaluations and show that our models'\ncapability of generating less likely outputs is improved.",
                "Seeing Seeds Beyond Weeds: Green Teaming Generative AI for Beneficial\n  Uses\nLarge generative AI models (GMs) like GPT and DALL-E are trained to generate\ncontent for general, wide-ranging purposes. GM content filters are generalized\nto filter out content which has a risk of harm in many cases, e.g., hate\nspeech. However, prohibited content is not always harmful -- there are\ninstances where generating prohibited content can be beneficial. So, when GMs\nfilter out content, they preclude beneficial use cases along with harmful ones.\nWhich use cases are precluded reflects the values embedded in GM content\nfiltering. Recent work on red teaming proposes methods to bypass GM content\nfilters to generate harmful content. We coin the term green teaming to describe\nmethods of bypassing GM content filters to design for beneficial use cases. We\nshowcase green teaming by: 1) Using ChatGPT as a virtual patient to simulate a\nperson experiencing suicidal ideation, for suicide support training; 2) Using\nCodex to intentionally generate buggy solutions to train students on debugging;\nand 3) Examining an Instagram page using Midjourney to generate images of\nanti-LGBTQ+ politicians in drag. Finally, we discuss how our use cases\ndemonstrate green teaming as both a practical design method and a mode of\ncritique, which problematizes and subverts current understandings of harms and\nvalues in generative AI."
            ],
            "interesting paper": 1
        },
        {
            "papers": [
                "Intent-aligned AI systems deplete human agency: the need for agency\n  foundations research in AI safety\nThe rapid advancement of artificial intelligence (AI) systems suggests that\nartificial general intelligence (AGI) systems may soon arrive. Many researchers\nare concerned that AIs and AGIs will harm humans via intentional misuse\n(AI-misuse) or through accidents (AI-accidents). In respect of AI-accidents,\nthere is an increasing effort focused on developing algorithms and paradigms\nthat ensure AI systems are aligned to what humans intend, e.g. AI systems that\nyield actions or recommendations that humans might judge as consistent with\ntheir intentions and goals. Here we argue that alignment to human intent is\ninsufficient for safe AI systems and that preservation of long-term agency of\nhumans may be a more robust standard, and one that needs to be separated\nexplicitly and a priori during optimization. We argue that AI systems can\nreshape human intention and discuss the lack of biological and psychological\nmechanisms that protect humans from loss of agency. We provide the first formal\ndefinition of agency-preserving AI-human interactions which focuses on\nforward-looking agency evaluations and argue that AI systems - not humans -\nmust be increasingly tasked with making these evaluations. We show how agency\nloss can occur in simple environments containing embedded agents that use\ntemporal-difference learning to make action recommendations. Finally, we\npropose a new area of research called \"agency foundations\" and pose four\ninitial topics designed to improve our understanding of agency in AI-human\ninteractions: benevolent game theory, algorithmic foundations of human rights,\nmechanistic interpretability of agency representation in neural-networks and\nreinforcement learning from internal states.",
                "A Computational Account Of Self-Supervised Visual Learning From\n  Egocentric Object Play\nResearch in child development has shown that embodied experience handling\nphysical objects contributes to many cognitive abilities, including visual\nlearning. One characteristic of such experience is that the learner sees the\nsame object from several different viewpoints. In this paper, we study how\nlearning signals that equate different viewpoints -- e.g., assigning similar\nrepresentations to different views of a single object -- can support robust\nvisual learning. We use the Toybox dataset, which contains egocentric videos of\nhumans manipulating different objects, and conduct experiments using a computer\nvision framework for self-supervised contrastive learning. We find that\nrepresentations learned by equating different physical viewpoints of an object\nbenefit downstream image classification accuracy. Further experiments show that\nthis performance improvement is robust to variations in the gaps between\nviewpoints, and that the benefits transfer to several different image\nclassification tasks.",
                "Strategic Reasoning with Language Models\nStrategic reasoning enables agents to cooperate, communicate, and compete\nwith other agents in diverse situations. Existing approaches to solving\nstrategic games rely on extensive training, yielding strategies that do not\ngeneralize to new scenarios or games without retraining. Large Language Models\n(LLMs), with their ability to comprehend and generate complex, context-rich\nlanguage, could prove powerful as tools for strategic gameplay. This paper\nintroduces an approach that uses pretrained LLMs with few-shot chain-of-thought\nexamples to enable strategic reasoning for AI agents. Our approach uses\nsystematically generated demonstrations of reasoning about states, values, and\nbeliefs to prompt the model. Using extensive variations of simple matrix games,\nwe show that strategies that are derived based on systematically generated\nprompts generalize almost perfectly to new game structures, alternate\nobjectives, and hidden information. Additionally, we demonstrate our approach\ncan lead to human-like negotiation strategies in realistic scenarios without\nany extra training or fine-tuning. Our results highlight the ability of LLMs,\nguided by systematic reasoning demonstrations, to adapt and excel in diverse\nstrategic scenarios.",
                "Less Likely Brainstorming: Using Language Models to Generate Alternative\n  Hypotheses\nA human decision-maker benefits the most from an AI assistant that corrects\nfor their biases. For problems such as generating interpretation of a radiology\nreport given findings, a system predicting only highly likely outcomes may be\nless useful, where such outcomes are already obvious to the user. To alleviate\nbiases in human decision-making, it is worth considering a broad differential\ndiagnosis, going beyond the most likely options. We introduce a new task, \"less\nlikely brainstorming,\" that asks a model to generate outputs that humans think\nare relevant but less likely to happen. We explore the task in two settings: a\nbrain MRI interpretation generation setting and an everyday commonsense\nreasoning setting. We found that a baseline approach of training with less\nlikely hypotheses as targets generates outputs that humans evaluate as either\nlikely or irrelevant nearly half of the time; standard MLE training is not\neffective. To tackle this problem, we propose a controlled text generation\nmethod that uses a novel contrastive learning strategy to encourage models to\ndifferentiate between generating likely and less likely outputs according to\nhumans. We compare our method with several state-of-the-art controlled text\ngeneration models via automatic and human evaluations and show that our models'\ncapability of generating less likely outputs is improved.",
                "Bigger, Better, Faster: Human-level Atari with human-level efficiency\nWe introduce a value-based RL agent, which we call BBF, that achieves\nsuper-human performance in the Atari 100K benchmark. BBF relies on scaling the\nneural networks used for value estimation, as well as a number of other design\nchoices that enable this scaling in a sample-efficient manner. We conduct\nextensive analyses of these design choices and provide insights for future\nwork. We end with a discussion about updating the goalposts for\nsample-efficient RL research on the ALE. We make our code and data publicly\navailable at\nhttps://github.com/google-research/google-research/tree/master/bigger_better_faster.",
                "Language-Conditioned Imitation Learning with Base Skill Priors under\n  Unstructured Data\nThe growing interest in language-conditioned robot manipulation aims to\ndevelop robots capable of understanding and executing complex tasks, with the\nobjective of enabling robots to interpret language commands and manipulate\nobjects accordingly. While language-conditioned approaches demonstrate\nimpressive capabilities for addressing tasks in familiar environments, they\nencounter limitations in adapting to unfamiliar environment settings. In this\nstudy, we propose a general-purpose, language-conditioned approach that\ncombines base skill priors and imitation learning under unstructured data to\nenhance the algorithm's generalization in adapting to unfamiliar environments.\nWe assess our model's performance in both simulated and real-world environments\nusing a zero-shot setting. In the simulated environment, the proposed approach\nsurpasses previously reported scores for CALVIN benchmark, especially in the\nchallenging Zero-Shot Multi-Environment setting. The average completed task\nlength, indicating the average number of tasks the agent can continuously\ncomplete, improves more than 2.5 times compared to the state-of-the-art method\nHULC. In addition, we conduct a zero-shot evaluation of our policy in a\nreal-world setting, following training exclusively in simulated environments\nwithout additional specific adaptations. In this evaluation, we set up ten\ntasks and achieved an average 30% improvement in our approach compared to the\ncurrent state-of-the-art approach, demonstrating a high generalization\ncapability in both simulated environments and the real world. For further\ndetails, including access to our code and videos, please refer to\nhttps://hk-zh.github.io/spil/"
            ],
            "interesting paper": 1
        },
        {
            "papers": [
                "AI Imagery and the Overton Window\nAI-based text-to-image generation has undergone a significant leap in the\nproduction of visually comprehensive and aesthetic imagery over the past year,\nto the point where differentiating between a man-made piece of art and an\nAI-generated image is becoming more difficult. Generative Models such as Stable\nDiffusion, Midjourney and others are expected to affect several major\nindustries in technological and ethical aspects. Striking the balance between\nraising human standard of life and work vs exploiting one group of people to\nenrich another is a complex and crucial part of the discussion. Due to the\nrapid growth of this technology, the way in which its models operate, and gray\narea legalities, visual and artistic domains - including the video game\nindustry, are at risk of being taken over from creators by AI infrastructure\nowners. This paper is a literature review examining the concerns facing both AI\ndevelopers and users today, including identity theft, data laundering and more.\nIt discusses legalization challenges and ethical concerns, and concludes with\nhow AI generative models can be tremendously useful in streamlining the process\nof visual creativity in both static and interactive media given proper\nregulation.\n  Keywords: AI text-to-image generation, Midjourney, Stable Diffusion, AI\nEthics, Game Design, Digital Art, Data Laundering",
                "From Human-Centered to Social-Centered Artificial Intelligence:\n  Assessing ChatGPT's Impact through Disruptive Events\nLarge language models (LLMs) and dialogue agents have existed for years, but\nthe release of recent GPT models has been a watershed moment for artificial\nintelligence (AI) research and society at large. Immediately recognized for its\ngenerative capabilities and versatility, ChatGPT's impressive proficiency\nacross technical and creative domains led to its widespread adoption. While\nsociety grapples with the emerging cultural impacts of ChatGPT, critiques of\nChatGPT's impact within the machine learning community have coalesced around\nits performance or other conventional Responsible AI evaluations relating to\nbias, toxicity, and 'hallucination.' We argue that these latter critiques draw\nheavily on a particular conceptualization of the 'human-centered' framework,\nwhich tends to cast atomized individuals as the key recipients of both the\nbenefits and detriments of technology. In this article, we direct attention to\nanother dimension of LLMs and dialogue agents' impact: their effect on social\ngroups, institutions, and accompanying norms and practices. By illustrating\nChatGPT's social impact through three disruptive events, we challenge\nindividualistic approaches in AI development and contribute to ongoing debates\naround the ethical and responsible implementation of AI systems. We hope this\neffort will call attention to more comprehensive and longitudinal evaluation\ntools and compel technologists to go beyond human-centered thinking and ground\ntheir efforts through social-centered AI.",
                "EMOTE: An Explainable architecture for Modelling the Other Through\n  Empathy\nWe can usually assume others have goals analogous to our own. This assumption\ncan also, at times, be applied to multi-agent games - e.g. Agent 1's attraction\nto green pellets is analogous to Agent 2's attraction to red pellets. This\n\"analogy\" assumption is tied closely to the cognitive process known as empathy.\nInspired by empathy, we design a simple and explainable architecture to model\nanother agent's action-value function. This involves learning an \"Imagination\nNetwork\" to transform the other agent's observed state in order to produce a\nhuman-interpretable \"empathetic state\" which, when presented to the learning\nagent, produces behaviours that mimic the other agent. Our approach is\napplicable to multi-agent scenarios consisting of a single learning agent and\nother (independent) agents acting according to fixed policies. This\narchitecture is particularly beneficial for (but not limited to) algorithms\nusing a composite value or reward function. We show our method produces better\nperformance in multi-agent games, where it robustly estimates the other's model\nin different environment configurations. Additionally, we show that the\nempathetic states are human interpretable, and thus verifiable.",
                "Human or Not? A Gamified Approach to the Turing Test\nWe present \"Human or Not?\", an online game inspired by the Turing test, that\nmeasures the capability of AI chatbots to mimic humans in dialog, and of humans\nto tell bots from other humans. Over the course of a month, the game was played\nby over 1.5 million users who engaged in anonymous two-minute chat sessions\nwith either another human or an AI language model which was prompted to behave\nlike humans. The task of the players was to correctly guess whether they spoke\nto a person or to an AI. This largest scale Turing-style test conducted to date\nrevealed some interesting facts. For example, overall users guessed the\nidentity of their partners correctly in only 68% of the games. In the subset of\nthe games in which users faced an AI bot, users had even lower correct guess\nrates of 60% (that is, not much higher than chance). This white paper details\nthe development, deployment, and results of this unique experiment. While this\nexperiment calls for many extensions and refinements, these findings already\nbegin to shed light on the inevitable near future which will commingle humans\nand AI.",
                "Speaking the Language of Your Listener: Audience-Aware Adaptation via\n  Plug-and-Play Theory of Mind\nDialogue participants may have varying levels of knowledge about the topic\nunder discussion. In such cases, it is essential for speakers to adapt their\nutterances by taking their audience into account. Yet, it is an open question\nhow such adaptation can be modelled in computational agents. In this paper, we\nmodel a visually grounded referential game between a knowledgeable speaker and\na listener with more limited visual and linguistic experience. Inspired by\npsycholinguistic theories, we endow our speaker with the ability to adapt its\nreferring expressions via a simulation module that monitors the effectiveness\nof planned utterances from the listener's perspective. We propose an adaptation\nmechanism building on plug-and-play approaches to controlled language\ngeneration, where utterance generation is steered on the fly by the simulator\nwithout finetuning the speaker's underlying language model. Our results and\nanalyses show that our approach is effective: the speaker's utterances become\ncloser to the listener's domain of expertise, which leads to higher\ncommunicative success.",
                "Thought Cloning: Learning to Think while Acting by Imitating Human\n  Thinking\nLanguage is often considered a key aspect of human thinking, providing us\nwith exceptional abilities to generalize, explore, plan, replan, and adapt to\nnew situations. However, Reinforcement Learning (RL) agents are far from\nhuman-level performance in any of these abilities. We hypothesize one reason\nfor such cognitive deficiencies is that they lack the benefits of thinking in\nlanguage and that we can improve AI agents by training them to think like\nhumans do. We introduce a novel Imitation Learning framework, Thought Cloning,\nwhere the idea is to not just clone the behaviors of human demonstrators, but\nalso the thoughts humans have as they perform these behaviors. While we expect\nThought Cloning to truly shine at scale on internet-sized datasets of humans\nthinking out loud while acting (e.g. online videos with transcripts), here we\nconduct experiments in a domain where the thinking and action data are\nsynthetically generated. Results reveal that Thought Cloning learns much faster\nthan Behavioral Cloning and its performance advantage grows the further out of\ndistribution test tasks are, highlighting its ability to better handle novel\nsituations. Thought Cloning also provides important benefits for AI Safety and\nInterpretability, and makes it easier to debug and improve AI. Because we can\nobserve the agent's thoughts, we can (1) more easily diagnose why things are\ngoing wrong, making it easier to fix the problem, (2) steer the agent by\ncorrecting its thinking, or (3) prevent it from doing unsafe things it plans to\ndo. Overall, by training agents how to think as well as behave, Thought Cloning\ncreates safer, more powerful agents."
            ],
            "interesting paper": 1
        },
        {
            "papers": [
                "Human Control: Definitions and Algorithms\nHow can humans stay in control of advanced artificial intelligence systems?\nOne proposal is corrigibility, which requires the agent to follow the\ninstructions of a human overseer, without inappropriately influencing them. In\nthis paper, we formally define a variant of corrigibility called shutdown\ninstructability, and show that it implies appropriate shutdown behavior,\nretention of human autonomy, and avoidance of user harm. We also analyse the\nrelated concepts of non-obstruction and shutdown alignment, three previously\nproposed algorithms for human control, and one new algorithm.",
                "Human or Not? A Gamified Approach to the Turing Test\nWe present \"Human or Not?\", an online game inspired by the Turing test, that\nmeasures the capability of AI chatbots to mimic humans in dialog, and of humans\nto tell bots from other humans. Over the course of a month, the game was played\nby over 1.5 million users who engaged in anonymous two-minute chat sessions\nwith either another human or an AI language model which was prompted to behave\nlike humans. The task of the players was to correctly guess whether they spoke\nto a person or to an AI. This largest scale Turing-style test conducted to date\nrevealed some interesting facts. For example, overall users guessed the\nidentity of their partners correctly in only 68% of the games. In the subset of\nthe games in which users faced an AI bot, users had even lower correct guess\nrates of 60% (that is, not much higher than chance). This white paper details\nthe development, deployment, and results of this unique experiment. While this\nexperiment calls for many extensions and refinements, these findings already\nbegin to shed light on the inevitable near future which will commingle humans\nand AI.",
                "Decision-Oriented Dialogue for Human-AI Collaboration\nWe describe a class of tasks called decision-oriented dialogues, in which AI\nassistants such as large language models (LMs) must collaborate with one or\nmore humans via natural language to help them make complex decisions. We\nformalize three domains in which users face everyday decisions: (1) choosing an\nassignment of reviewers to conference papers, (2) planning a multi-step\nitinerary in a city, and (3) negotiating travel plans for a group of friends.\nIn each of these settings, AI assistants and users have disparate abilities\nthat they must combine to arrive at the best decision: assistants can access\nand process large amounts of information, while users have preferences and\nconstraints external to the system. For each task, we build a dialogue\nenvironment where agents receive a reward based on the quality of the final\ndecision they reach. We evaluate LMs in self-play and in collaboration with\nhumans and find that they fall short compared to human assistants, achieving\nmuch lower rewards despite engaging in longer dialogues. We highlight a number\nof challenges models face in decision-oriented dialogues, ranging from\ngoal-directed behavior to reasoning and optimization, and release our\nenvironments as a testbed for future work.",
                "From Human-Centered to Social-Centered Artificial Intelligence:\n  Assessing ChatGPT's Impact through Disruptive Events\nLarge language models (LLMs) and dialogue agents have existed for years, but\nthe release of recent GPT models has been a watershed moment for artificial\nintelligence (AI) research and society at large. Immediately recognized for its\ngenerative capabilities and versatility, ChatGPT's impressive proficiency\nacross technical and creative domains led to its widespread adoption. While\nsociety grapples with the emerging cultural impacts of ChatGPT, critiques of\nChatGPT's impact within the machine learning community have coalesced around\nits performance or other conventional Responsible AI evaluations relating to\nbias, toxicity, and 'hallucination.' We argue that these latter critiques draw\nheavily on a particular conceptualization of the 'human-centered' framework,\nwhich tends to cast atomized individuals as the key recipients of both the\nbenefits and detriments of technology. In this article, we direct attention to\nanother dimension of LLMs and dialogue agents' impact: their effect on social\ngroups, institutions, and accompanying norms and practices. By illustrating\nChatGPT's social impact through three disruptive events, we challenge\nindividualistic approaches in AI development and contribute to ongoing debates\naround the ethical and responsible implementation of AI systems. We hope this\neffort will call attention to more comprehensive and longitudinal evaluation\ntools and compel technologists to go beyond human-centered thinking and ground\ntheir efforts through social-centered AI.",
                "EMOTE: An Explainable architecture for Modelling the Other Through\n  Empathy\nWe can usually assume others have goals analogous to our own. This assumption\ncan also, at times, be applied to multi-agent games - e.g. Agent 1's attraction\nto green pellets is analogous to Agent 2's attraction to red pellets. This\n\"analogy\" assumption is tied closely to the cognitive process known as empathy.\nInspired by empathy, we design a simple and explainable architecture to model\nanother agent's action-value function. This involves learning an \"Imagination\nNetwork\" to transform the other agent's observed state in order to produce a\nhuman-interpretable \"empathetic state\" which, when presented to the learning\nagent, produces behaviours that mimic the other agent. Our approach is\napplicable to multi-agent scenarios consisting of a single learning agent and\nother (independent) agents acting according to fixed policies. This\narchitecture is particularly beneficial for (but not limited to) algorithms\nusing a composite value or reward function. We show our method produces better\nperformance in multi-agent games, where it robustly estimates the other's model\nin different environment configurations. Additionally, we show that the\nempathetic states are human interpretable, and thus verifiable.",
                "Speaking the Language of Your Listener: Audience-Aware Adaptation via\n  Plug-and-Play Theory of Mind\nDialogue participants may have varying levels of knowledge about the topic\nunder discussion. In such cases, it is essential for speakers to adapt their\nutterances by taking their audience into account. Yet, it is an open question\nhow such adaptation can be modelled in computational agents. In this paper, we\nmodel a visually grounded referential game between a knowledgeable speaker and\na listener with more limited visual and linguistic experience. Inspired by\npsycholinguistic theories, we endow our speaker with the ability to adapt its\nreferring expressions via a simulation module that monitors the effectiveness\nof planned utterances from the listener's perspective. We propose an adaptation\nmechanism building on plug-and-play approaches to controlled language\ngeneration, where utterance generation is steered on the fly by the simulator\nwithout finetuning the speaker's underlying language model. Our results and\nanalyses show that our approach is effective: the speaker's utterances become\ncloser to the listener's domain of expertise, which leads to higher\ncommunicative success."
            ],
            "interesting paper": 2
        },
        {
            "papers": [
                "AI and the creative realm: A short review of current and future\n  applications\nThis study explores the concept of creativity and artificial intelligence\n(AI) and their recent integration. While AI has traditionally been perceived as\nincapable of generating new ideas or creating art, the development of more\nsophisticated AI models and the proliferation of human-computer interaction\ntools have opened up new possibilities for AI in artistic creation. This study\ninvestigates the various applications of AI in a creative context,\ndifferentiating between the type of art, language, and algorithms used. It also\nconsiders the philosophical implications of AI and creativity, questioning\nwhether consciousness can be researched in machines and AI's potential\ninterests and decision-making capabilities. Overall, we aim to stimulate a\nreflection on AI's use and ethical implications in creative contexts.",
                "Exploring EFL students' prompt engineering in human-AI story writing: an\n  Activity Theory perspective\nThis study applies Activity Theory to investigate how English as a foreign\nlanguage (EFL) students prompt generative artificial intelligence (AI) tools\nduring short story writing. Sixty-seven Hong Kong secondary school students\ncreated generative-AI tools using open-source language models and wrote short\nstories with them. The study collected and analyzed the students' generative-AI\ntools, short stories, and written reflections on their conditions or purposes\nfor prompting. The research identified three main themes regarding the purposes\nfor which students prompt generative-AI tools during short story writing: a\nlack of awareness of purposes, overcoming writer's block, and developing,\nexpanding, and improving the story. The study also identified common\ncharacteristics of students' activity systems, including the sophistication of\ntheir generative-AI tools, the quality of their stories, and their school's\noverall academic achievement level, for their prompting of generative-AI tools\nfor the three purposes during short story writing. The study's findings suggest\nthat teachers should be aware of students' purposes for prompting generative-AI\ntools to provide tailored instructions and scaffolded guidance. The findings\nmay also help designers provide differentiated instructions for users at\nvarious levels of story development when using a generative-AI tool.",
                "The ethical ambiguity of AI data enrichment: Measuring gaps in research\n  ethics norms and practices\nThe technical progression of artificial intelligence (AI) research has been\nbuilt on breakthroughs in fields such as computer science, statistics, and\nmathematics. However, in the past decade AI researchers have increasingly\nlooked to the social sciences, turning to human interactions to solve the\nchallenges of model development. Paying crowdsourcing workers to generate or\ncurate data, or data enrichment, has become indispensable for many areas of AI\nresearch, from natural language processing to reinforcement learning from human\nfeedback (RLHF). Other fields that routinely interact with crowdsourcing\nworkers, such as Psychology, have developed common governance requirements and\nnorms to ensure research is undertaken ethically. This study explores how, and\nto what extent, comparable research ethics requirements and norms have\ndeveloped for AI research and data enrichment. We focus on the approach taken\nby two leading conferences: ICLR and NeurIPS, and journal publisher Springer.\nIn a longitudinal study of accepted papers, and via a comparison with\nPsychology and CHI papers, this work finds that leading AI venues have begun to\nestablish protocols for human data collection, but these are are inconsistently\nfollowed by authors. Whilst Psychology papers engaging with crowdsourcing\nworkers frequently disclose ethics reviews, payment data, demographic data and\nother information, similar disclosures are far less common in leading AI venues\ndespite similar guidance. The work concludes with hypotheses to explain these\ngaps in research ethics practices and considerations for its implications.",
                "AI Liability Insurance With an Example in AI-Powered E-diagnosis System\nArtificial Intelligence (AI) has received an increasing amount of attention\nin multiple areas. The uncertainties and risks in AI-powered systems have\ncreated reluctance in their wild adoption. As an economic solution to\ncompensate for potential damages, AI liability insurance is a promising market\nto enhance the integration of AI into daily life. In this work, we use an\nAI-powered E-diagnosis system as an example to study AI liability insurance. We\nprovide a quantitative risk assessment model with evidence-based numerical\nanalysis. We discuss the insurability criteria for AI technologies and suggest\nnecessary adjustments to accommodate the features of AI products. We show that\nAI liability insurance can act as a regulatory mechanism to incentivize\ncompliant behaviors and serve as a certificate of high-quality AI systems.\nFurthermore, we suggest premium adjustment to reflect the dynamic evolution of\nthe inherent uncertainty in AI. Moral hazard problems are discussed and\nsuggestions for AI liability insurance are provided.",
                "DeepfakeArt Challenge: A Benchmark Dataset for Generative AI Art Forgery\n  and Data Poisoning Detection\nThe tremendous recent advances in generative artificial intelligence\ntechniques have led to significant successes and promise in a wide range of\ndifferent applications ranging from conversational agents and textual content\ngeneration to voice and visual synthesis. Amid the rise in generative AI and\nits increasing widespread adoption, there has been significant growing concern\nover the use of generative AI for malicious purposes. In the realm of visual\ncontent synthesis using generative AI, key areas of significant concern has\nbeen image forgery (e.g., generation of images containing or derived from\ncopyright content), and data poisoning (i.e., generation of adversarially\ncontaminated images). Motivated to address these key concerns to encourage\nresponsible generative AI, we introduce the DeepfakeArt Challenge, a\nlarge-scale challenge benchmark dataset designed specifically to aid in the\nbuilding of machine learning algorithms for generative AI art forgery and data\npoisoning detection. Comprising of over 32,000 records across a variety of\ngenerative forgery and data poisoning techniques, each entry consists of a pair\nof images that are either forgeries / adversarially contaminated or not. Each\nof the generated images in the DeepfakeArt Challenge benchmark dataset\n\\footnote{The link to the dataset: http://anon\\_for\\_review.com} has been\nquality checked in a comprehensive manner.",
                "Egocentric Planning for Scalable Embodied Task Achievement\nEmbodied agents face significant challenges when tasked with performing\nactions in diverse environments, particularly in generalizing across object\ntypes and executing suitable actions to accomplish tasks. Furthermore, agents\nshould exhibit robustness, minimizing the execution of illegal actions. In this\nwork, we present Egocentric Planning, an innovative approach that combines\nsymbolic planning and Object-oriented POMDPs to solve tasks in complex\nenvironments, harnessing existing models for visual perception and natural\nlanguage processing. We evaluated our approach in ALFRED, a simulated\nenvironment designed for domestic tasks, and demonstrated its high scalability,\nachieving an impressive 36.07% unseen success rate in the ALFRED benchmark and\nwinning the ALFRED challenge at CVPR Embodied AI workshop. Our method requires\nreliable perception and the specification or learning of a symbolic description\nof the preconditions and effects of the agent's actions, as well as what object\ntypes reveal information about others. It is capable of naturally scaling to\nsolve new tasks beyond ALFRED, as long as they can be solved using the\navailable skills. This work offers a solid baseline for studying end-to-end and\nhybrid methods that aim to generalize to new tasks, including recent approaches\nrelying on LLMs, but often struggle to scale to long sequences of actions or\nproduce robust plans for novel tasks."
            ],
            "interesting paper": 3
        },
        {
            "papers": [
                "Egocentric Planning for Scalable Embodied Task Achievement\nEmbodied agents face significant challenges when tasked with performing\nactions in diverse environments, particularly in generalizing across object\ntypes and executing suitable actions to accomplish tasks. Furthermore, agents\nshould exhibit robustness, minimizing the execution of illegal actions. In this\nwork, we present Egocentric Planning, an innovative approach that combines\nsymbolic planning and Object-oriented POMDPs to solve tasks in complex\nenvironments, harnessing existing models for visual perception and natural\nlanguage processing. We evaluated our approach in ALFRED, a simulated\nenvironment designed for domestic tasks, and demonstrated its high scalability,\nachieving an impressive 36.07% unseen success rate in the ALFRED benchmark and\nwinning the ALFRED challenge at CVPR Embodied AI workshop. Our method requires\nreliable perception and the specification or learning of a symbolic description\nof the preconditions and effects of the agent's actions, as well as what object\ntypes reveal information about others. It is capable of naturally scaling to\nsolve new tasks beyond ALFRED, as long as they can be solved using the\navailable skills. This work offers a solid baseline for studying end-to-end and\nhybrid methods that aim to generalize to new tasks, including recent approaches\nrelying on LLMs, but often struggle to scale to long sequences of actions or\nproduce robust plans for novel tasks.",
                "AI and the creative realm: A short review of current and future\n  applications\nThis study explores the concept of creativity and artificial intelligence\n(AI) and their recent integration. While AI has traditionally been perceived as\nincapable of generating new ideas or creating art, the development of more\nsophisticated AI models and the proliferation of human-computer interaction\ntools have opened up new possibilities for AI in artistic creation. This study\ninvestigates the various applications of AI in a creative context,\ndifferentiating between the type of art, language, and algorithms used. It also\nconsiders the philosophical implications of AI and creativity, questioning\nwhether consciousness can be researched in machines and AI's potential\ninterests and decision-making capabilities. Overall, we aim to stimulate a\nreflection on AI's use and ethical implications in creative contexts.",
                "AI Liability Insurance With an Example in AI-Powered E-diagnosis System\nArtificial Intelligence (AI) has received an increasing amount of attention\nin multiple areas. The uncertainties and risks in AI-powered systems have\ncreated reluctance in their wild adoption. As an economic solution to\ncompensate for potential damages, AI liability insurance is a promising market\nto enhance the integration of AI into daily life. In this work, we use an\nAI-powered E-diagnosis system as an example to study AI liability insurance. We\nprovide a quantitative risk assessment model with evidence-based numerical\nanalysis. We discuss the insurability criteria for AI technologies and suggest\nnecessary adjustments to accommodate the features of AI products. We show that\nAI liability insurance can act as a regulatory mechanism to incentivize\ncompliant behaviors and serve as a certificate of high-quality AI systems.\nFurthermore, we suggest premium adjustment to reflect the dynamic evolution of\nthe inherent uncertainty in AI. Moral hazard problems are discussed and\nsuggestions for AI liability insurance are provided.",
                "STEVE-1: A Generative Model for Text-to-Behavior in Minecraft\nConstructing AI models that respond to text instructions is challenging,\nespecially for sequential decision-making tasks. This work introduces a\nmethodology, inspired by unCLIP, for instruction-tuning generative models of\nbehavior without relying on a large dataset of instruction-labeled\ntrajectories. Using this methodology, we create an instruction-tuned Video\nPretraining (VPT) model called STEVE-1, which can follow short-horizon\nopen-ended text and visual instructions in Minecraft. STEVE-1 is trained in two\nsteps: adapting the pretrained VPT model to follow commands in MineCLIP's\nlatent space, then training a prior to predict latent codes from text. This\nallows us to finetune VPT through self-supervised behavioral cloning and\nhindsight relabeling, reducing the need for costly human text annotations, and\nall for only $60 of compute. By leveraging pretrained models like VPT and\nMineCLIP and employing best practices from text-conditioned image generation,\nSTEVE-1 sets a new bar for open-ended instruction-following in Minecraft with\nlow-level controls (mouse and keyboard) and raw pixel inputs, far outperforming\nprevious baselines and robustly completing 12 of 13 tasks in our early-game\nevaluation suite. We provide experimental evidence highlighting key factors for\ndownstream performance, including pretraining, classifier-free guidance, and\ndata scaling. All resources, including our model weights, training scripts, and\nevaluation tools are made available for further research.",
                "The ethical ambiguity of AI data enrichment: Measuring gaps in research\n  ethics norms and practices\nThe technical progression of artificial intelligence (AI) research has been\nbuilt on breakthroughs in fields such as computer science, statistics, and\nmathematics. However, in the past decade AI researchers have increasingly\nlooked to the social sciences, turning to human interactions to solve the\nchallenges of model development. Paying crowdsourcing workers to generate or\ncurate data, or data enrichment, has become indispensable for many areas of AI\nresearch, from natural language processing to reinforcement learning from human\nfeedback (RLHF). Other fields that routinely interact with crowdsourcing\nworkers, such as Psychology, have developed common governance requirements and\nnorms to ensure research is undertaken ethically. This study explores how, and\nto what extent, comparable research ethics requirements and norms have\ndeveloped for AI research and data enrichment. We focus on the approach taken\nby two leading conferences: ICLR and NeurIPS, and journal publisher Springer.\nIn a longitudinal study of accepted papers, and via a comparison with\nPsychology and CHI papers, this work finds that leading AI venues have begun to\nestablish protocols for human data collection, but these are are inconsistently\nfollowed by authors. Whilst Psychology papers engaging with crowdsourcing\nworkers frequently disclose ethics reviews, payment data, demographic data and\nother information, similar disclosures are far less common in leading AI venues\ndespite similar guidance. The work concludes with hypotheses to explain these\ngaps in research ethics practices and considerations for its implications.",
                "LIV: Language-Image Representations and Rewards for Robotic Control\nWe present Language-Image Value learning (LIV), a unified objective for\nvision-language representation and reward learning from action-free videos with\ntext annotations. Exploiting a novel connection between dual reinforcement\nlearning and mutual information contrastive learning, the LIV objective trains\na multi-modal representation that implicitly encodes a universal value function\nfor tasks specified as language or image goals. We use LIV to pre-train the\nfirst control-centric vision-language representation from large human video\ndatasets such as EpicKitchen. Given only a language or image goal, the\npre-trained LIV model can assign dense rewards to each frame in videos of\nunseen robots or humans attempting that task in unseen environments. Further,\nwhen some target domain-specific data is available, the same objective can be\nused to fine-tune and improve LIV and even other pre-trained representations\nfor robotic control and reward specification in that domain. In our experiments\non several simulated and real-world robot environments, LIV models consistently\noutperform the best prior input state representations for imitation learning,\nas well as reward specification methods for policy synthesis. Our results\nvalidate the advantages of joint vision-language representation and reward\nlearning within the unified, compact LIV framework."
            ],
            "interesting paper": 4
        }
    ],
    "Jordan Clark": [
        {
            "papers": [
                "PDE+: Enhancing Generalization via PDE with Adaptive Distributional\n  Diffusion\nThe generalization of neural networks is a central challenge in machine\nlearning, especially concerning the performance under distributions that differ\nfrom training ones. Current methods, mainly based on the data-driven paradigm\nsuch as data augmentation, adversarial training, and noise injection, may\nencounter limited generalization due to model non-smoothness. In this paper, we\npropose to investigate generalization from a Partial Differential Equation\n(PDE) perspective, aiming to enhance it directly through the underlying\nfunction of neural networks, rather than focusing on adjusting input data.\nSpecifically, we first establish the connection between neural network\ngeneralization and the smoothness of the solution to a specific PDE, namely\n\"transport equation\". Building upon this, we propose a general framework that\nintroduces adaptive distributional diffusion into transport equation to enhance\nthe smoothness of its solution, thereby improving generalization. In the\ncontext of neural networks, we put this theoretical framework into practice as\n$\\textbf{PDE+}$ ($\\textbf{PDE}$ with $\\textbf{A}$daptive\n$\\textbf{D}$istributional $\\textbf{D}$iffusion) which diffuses each sample into\na distribution covering semantically similar inputs. This enables better\ncoverage of potentially unobserved distributions in training, thus improving\ngeneralization beyond merely data-driven methods. The effectiveness of PDE+ is\nvalidated through extensive experimental settings, demonstrating its superior\nperformance compared to SOTA methods.",
                "TLNets: Transformation Learning Networks for long-range time-series\n  prediction\nTime series prediction is a prevalent issue across various disciplines, such\nas meteorology, traffic surveillance, investment, and energy production and\nconsumption. Many statistical and machine-learning strategies have been\ndeveloped to tackle this problem. However, these approaches either lack\nexplainability or exhibit less satisfactory performance when the prediction\nhorizon increases. To this end, we propose a novel plan for the designing of\nnetworks' architecture based on transformations, possessing the potential to\nachieve an enhanced receptive field in learning which brings benefits to fuse\nfeatures across scales. In this context, we introduce four different\ntransformation mechanisms as bases to construct the learning model including\nFourier Transform (FT), Singular Value Decomposition (SVD), matrix\nmultiplication and Conv block. Hence, we develop four learning models based on\nthe above building blocks, namely, FT-Matrix, FT-SVD, FT-Conv, and Conv-SVD.\nNote that the FT and SVD blocks are capable of learning global information,\nwhile the Conv blocks focus on learning local information. The matrix block is\nsparsely designed to learn both global and local information simultaneously.\nThe above Transformation Learning Networks (TLNets) have been extensively\ntested and compared with multiple baseline models based on several real-world\ndatasets and showed clear potential in long-range time-series forecasting.",
                "Learning to Act through Evolution of Neural Diversity in Random Neural\n  Networks\nBiological nervous systems consist of networks of diverse, sophisticated\ninformation processors in the form of neurons of different classes. In most\nartificial neural networks (ANNs), neural computation is abstracted to an\nactivation function that is usually shared between all neurons within a layer\nor even the whole network; training of ANNs focuses on synaptic optimization.\nIn this paper, we propose the optimization of neuro-centric parameters to\nattain a set of diverse neurons that can perform complex computations.\nDemonstrating the promise of the approach, we show that evolving neural\nparameters alone allows agents to solve various reinforcement learning tasks\nwithout optimizing any synaptic weights. While not aiming to be an accurate\nbiological model, parameterizing neurons to a larger degree than the current\ncommon practice, allows us to ask questions about the computational abilities\nafforded by neural diversity in random neural networks. The presented results\nopen up interesting future research directions, such as combining evolved\nneural diversity with activity-dependent plasticity.",
                "Classic machine learning methods\nIn this chapter, we present the main classic machine learning methods. A\nlarge part of the chapter is devoted to supervised learning techniques for\nclassification and regression, including nearest-neighbor methods, linear and\nlogistic regressions, support vector machines and tree-based algorithms. We\nalso describe the problem of overfitting as well as strategies to overcome it.\nWe finally provide a brief overview of unsupervised learning methods, namely\nfor clustering and dimensionality reduction.",
                "Sharpness-Aware Minimization Revisited: Weighted Sharpness as a\n  Regularization Term\nDeep Neural Networks (DNNs) generalization is known to be closely related to\nthe flatness of minima, leading to the development of Sharpness-Aware\nMinimization (SAM) for seeking flatter minima and better generalization. In\nthis paper, we revisit the loss of SAM and propose a more general method,\ncalled WSAM, by incorporating sharpness as a regularization term. We prove its\ngeneralization bound through the combination of PAC and Bayes-PAC techniques,\nand evaluate its performance on various public datasets. The results\ndemonstrate that WSAM achieves improved generalization, or is at least highly\ncompetitive, compared to the vanilla optimizer, SAM and its variants. The code\nis available at\nhttps://github.com/intelligent-machine-learning/dlrover/tree/master/atorch/atorch/optimizers.",
                "Neural network reconstruction of cosmology using the Pantheon\n  compilation\nIn this work, we reconstruct the Hubble diagram using various data sets,\nincluding correlated ones, in Artificial Neural Networks (ANN). Using ReFANN,\nthat was built for data sets with independent uncertainties, we expand it to\ninclude non-Guassian data points, as well as data sets with covariance matrices\namong others. Furthermore, we compare our results with the existing ones\nderived from Gaussian processes and we also perform null tests in order to test\nthe validity of the concordance model of cosmology."
            ],
            "interesting paper": 3
        },
        {
            "papers": [
                "Dendritic Integration Based Quadratic Neural Networks Outperform\n  Traditional Aritificial Ones\nIncorporating biological neuronal properties into Artificial Neural Networks\n(ANNs) to enhance computational capabilities poses a formidable challenge in\nthe field of machine learning. Inspired by recent findings indicating that\ndendrites adhere to quadratic integration rules for synaptic inputs, we propose\na novel ANN model, Dendritic Integration-Based Quadratic Neural Network\n(DIQNN). This model shows superior performance over traditional ANNs in a\nvariety of classification tasks. To reduce the computational cost of DIQNN, we\nintroduce the Low-Rank DIQNN, while we find it can retain the performance of\nthe original DIQNN. We further propose a margin to characterize the\ngeneralization error and theoretically prove this margin will increase\nmonotonically during training. And we show the consistency between\ngeneralization and our margin using numerical experiments. Finally, by\nintegrating this margin into the loss function, the change of test accuracy is\nindeed accelerated. Our work contributes a novel, brain-inspired ANN model that\nsurpasses traditional ANNs and provides a theoretical framework to analyze the\ngeneralization error in classification tasks.",
                "SING: A Plug-and-Play DNN Learning Technique\nWe propose SING (StabIlized and Normalized Gradient), a plug-and-play\ntechnique that improves the stability and generalization of the Adam(W)\noptimizer. SING is straightforward to implement and has minimal computational\noverhead, requiring only a layer-wise standardization of the gradients fed to\nAdam(W) without introducing additional hyper-parameters. We support the\neffectiveness and practicality of the proposed approach by showing improved\nresults on a wide range of architectures, problems (such as image\nclassification, depth estimation, and natural language processing), and in\ncombination with other optimizers. We provide a theoretical analysis of the\nconvergence of the method, and we show that by virtue of the standardization,\nSING can escape local minima narrower than a threshold that is inversely\nproportional to the network's depth.",
                "XGrad: Boosting Gradient-Based Optimizers With Weight Prediction\nIn this paper, we propose a general deep learning training framework XGrad\nwhich introduces weight prediction into the popular gradient-based optimizers\nto boost their convergence and generalization when training the deep neural\nnetwork (DNN) models. In particular, ahead of each mini-batch training, the\nfuture weights are predicted according to the update rule of the used optimizer\nand are then applied to both the forward pass and backward propagation. In this\nway, during the whole training period, the optimizer always utilizes the\ngradients w.r.t. the future weights to update the DNN parameters, making the\ngradient-based optimizer achieve better convergence and generalization compared\nto the original optimizer without weight prediction. XGrad is rather\nstraightforward to implement yet pretty effective in boosting the convergence\nof gradient-based optimizers and the accuracy of DNN models. Empirical results\nconcerning five popular optimizers including SGD with momentum, Adam, AdamW,\nAdaBelief, and AdaM3 demonstrate the effectiveness of our proposal. The\nexperimental results validate that XGrad can attain higher model accuracy than\nthe baseline optimizers when training the DNN models. The code of XGrad will be\navailable at: https://github.com/guanleics/XGrad.",
                "Neural (Tangent Kernel) Collapse\nThis work bridges two important concepts: the Neural Tangent Kernel (NTK),\nwhich captures the evolution of deep neural networks (DNNs) during training,\nand the Neural Collapse (NC) phenomenon, which refers to the emergence of\nsymmetry and structure in the last-layer features of well-trained\nclassification DNNs. We adopt the natural assumption that the empirical NTK\ndevelops a block structure aligned with the class labels, i.e., samples within\nthe same class have stronger correlations than samples from different classes.\nUnder this assumption, we derive the dynamics of DNNs trained with mean squared\n(MSE) loss and break them into interpretable phases. Moreover, we identify an\ninvariant that captures the essence of the dynamics, and use it to prove the\nemergence of NC in DNNs with block-structured NTK. We provide large-scale\nnumerical experiments on three common DNN architectures and three benchmark\ndatasets to support our theory.",
                "Pruning Distorted Images in MNIST Handwritten Digits\nRecognizing handwritten digits is a challenging task primarily due to the\ndiversity of writing styles and the presence of noisy images. The widely used\nMNIST dataset, which is commonly employed as a benchmark for this task,\nincludes distorted digits with irregular shapes, incomplete strokes, and\nvarying skew in both the training and testing datasets. Consequently, these\nfactors contribute to reduced accuracy in digit recognition. To overcome this\nchallenge, we propose a two-stage deep learning approach. In the first stage,\nwe create a simple neural network to identify distorted digits within the\ntraining set. This model serves to detect and filter out such distorted and\nambiguous images. In the second stage, we exclude these identified images from\nthe training dataset and proceed to retrain the model using the filtered\ndataset. This process aims to improve the classification accuracy and\nconfidence levels while mitigating issues of underfitting and overfitting. Our\nexperimental results demonstrate the effectiveness of the proposed approach,\nachieving an accuracy rate of over 99.5% on the testing dataset. This\nsignificant improvement showcases the potential of our method in enhancing\ndigit classification accuracy. In our future work, we intend to explore the\nscalability of this approach and investigate techniques to further enhance\naccuracy by reducing the size of the training data.",
                "Graph Neural Convection-Diffusion with Heterophily\nGraph neural networks (GNNs) have shown promising results across various\ngraph learning tasks, but they often assume homophily, which can result in poor\nperformance on heterophilic graphs. The connected nodes are likely to be from\ndifferent classes or have dissimilar features on heterophilic graphs. In this\npaper, we propose a novel GNN that incorporates the principle of heterophily by\nmodeling the flow of information on nodes using the convection-diffusion\nequation (CDE). This allows the CDE to take into account both the diffusion of\ninformation due to homophily and the ``convection'' of information due to\nheterophily. We conduct extensive experiments, which suggest that our framework\ncan achieve competitive performance on node classification tasks for\nheterophilic graphs, compared to the state-of-the-art methods. The code is\navailable at \\url{https://github.com/zknus/Graph-Diffusion-CDE}."
            ],
            "interesting paper": 1
        },
        {
            "papers": [
                "Stability of implicit neural networks for long-term forecasting in\n  dynamical systems\nForecasting physical signals in long time range is among the most challenging\ntasks in Partial Differential Equations (PDEs) research. To circumvent\nlimitations of traditional solvers, many different Deep Learning methods have\nbeen proposed. They are all based on auto-regressive methods and exhibit\nstability issues. Drawing inspiration from the stability property of implicit\nnumerical schemes, we introduce a stable auto-regressive implicit neural\nnetwork. We develop a theory based on the stability definition of schemes to\nensure the stability in forecasting of this network. It leads us to introduce\nhard constraints on its weights and propagate the dynamics in the latent space.\nOur experimental results validate our stability property, and show improved\nresults at long-term forecasting for two transports PDEs.",
                "The Galaxy Assembly and Interaction Neural Networks (GAINN) for\n  high-redshift JWST observations\nWe present the Galaxy Assembly and Interaction Neural Networks (GAINN), a\nseries of artificial neural networks for predicting the redshift, stellar mass,\nhalo mass, and mass-weighted age of simulated galaxies based on JWST\nphotometry. Our goal is to determine the best neural network for predicting\nthese variables at $11.5 < z < 15$. The parameters of the optimal neural\nnetwork can then be used to estimate these variables for real, observed\ngalaxies. The inputs of the neural networks are JWST filter magnitudes of a\nsubset of five broadband filters (F150W, F200W, F277W, F356W, and F444W) and\ntwo medium-band filters (F162M and F182M). We compare the performance of the\nneural networks using different combinations of these filters, as well as\ndifferent activation functions and numbers of layers. The best neural network\npredicted redshift with normalized root mean squared error NRMS =\n$0.009_{-0.002}^{+0.003}$, stellar mass with RMS = $0.073_{-0.008}^{+0.017}$,\nhalo mass with MSE = $ 0.022_{-0.004}^{+0.006}$, and mass-weighted age with RMS\n= $10.866_{-1.410}^{+3.189}$. We also test the performance of GAINN on real\ndata from MACS0647-JD, an object observed by JWST. Predictions from GAINN for\nthe first projection of the object (JD1) have mean absolute errors $\\langle\n\\Delta z \\rangle <0.00228$, which is significantly smaller than with\ntemplate-fitting methods. We find that the optimal filter combination is F277W,\nF356W, F162M, and F182M when considering both theoretical accuracy and\nobservational resources from JWST.",
                "Exploiting Large Neuroimaging Datasets to Create Connectome-Constrained\n  Approaches for more Robust, Efficient, and Adaptable Artificial Intelligence\nDespite the progress in deep learning networks, efficient learning at the\nedge (enabling adaptable, low-complexity machine learning solutions) remains a\ncritical need for defense and commercial applications. We envision a pipeline\nto utilize large neuroimaging datasets, including maps of the brain which\ncapture neuron and synapse connectivity, to improve machine learning\napproaches. We have pursued different approaches within this pipeline\nstructure. First, as a demonstration of data-driven discovery, the team has\ndeveloped a technique for discovery of repeated subcircuits, or motifs. These\nwere incorporated into a neural architecture search approach to evolve network\narchitectures. Second, we have conducted analysis of the heading direction\ncircuit in the fruit fly, which performs fusion of visual and angular velocity\nfeatures, to explore augmenting existing computational models with new insight.\nOur team discovered a novel pattern of connectivity, implemented a new model,\nand demonstrated sensor fusion on a robotic platform. Third, the team analyzed\ncircuitry for memory formation in the fruit fly connectome, enabling the design\nof a novel generative replay approach. Finally, the team has begun analysis of\nconnectivity in mammalian cortex to explore potential improvements to\ntransformer networks. These constraints increased network robustness on the\nmost challenging examples in the CIFAR-10-C computer vision robustness\nbenchmark task, while reducing learnable attention parameters by over an order\nof magnitude. Taken together, these results demonstrate multiple potential\napproaches to utilize insight from neural systems for developing robust and\nefficient machine learning techniques.",
                "Statistically Significant Concept-based Explanation of Image Classifiers\n  via Model Knockoffs\nA concept-based classifier can explain the decision process of a deep\nlearning model by human-understandable concepts in image classification\nproblems. However, sometimes concept-based explanations may cause false\npositives, which misregards unrelated concepts as important for the prediction\ntask. Our goal is to find the statistically significant concept for\nclassification to prevent misinterpretation. In this study, we propose a method\nusing a deep learning model to learn the image concept and then using the\nKnockoff samples to select the important concepts for prediction by controlling\nthe False Discovery Rate (FDR) under a certain value. We evaluate the proposed\nmethod in our synthetic and real data experiments. Also, it shows that our\nmethod can control the FDR properly while selecting highly interpretable\nconcepts to improve the trustworthiness of the model.",
                "Are Deep Neural Networks Adequate Behavioural Models of Human Visual\n  Perception?\nDeep neural networks (DNNs) are machine learning algorithms that have\nrevolutionised computer vision due to their remarkable successes in tasks like\nobject classification and segmentation. The success of DNNs as computer vision\nalgorithms has led to the suggestion that DNNs may also be good models of human\nvisual perception. We here review evidence regarding current DNNs as adequate\nbehavioural models of human core object recognition. To this end, we argue that\nit is important to distinguish between statistical tools and computational\nmodels, and to understand model quality as a multidimensional concept where\nclarity about modelling goals is key. Reviewing a large number of\npsychophysical and computational explorations of core object recognition\nperformance in humans and DNNs, we argue that DNNs are highly valuable\nscientific tools but that as of today DNNs should only be regarded as promising\n-- but not yet adequate -- computational models of human core object\nrecognition behaviour. On the way we dispel a number of myths surrounding DNNs\nin vision science.",
                "Understanding Sparse Neural Networks from their Topology via\n  Multipartite Graph Representations\nPruning-at-Initialization (PaI) algorithms provide Sparse Neural Networks\n(SNNs) which are computationally more efficient than their dense counterparts,\nand try to avoid performance degradation. While much emphasis has been directed\ntowards \\emph{how} to prune, we still do not know \\emph{what topological\nmetrics} of the SNNs characterize \\emph{good performance}. From prior work, we\nhave layer-wise topological metrics by which SNN performance can be predicted:\nthe Ramanujan-based metrics. To exploit these metrics, proper ways to represent\nnetwork layers via Graph Encodings (GEs) are needed, with Bipartite Graph\nEncodings (BGEs) being the \\emph{de-facto} standard at the current stage.\nNevertheless, existing BGEs neglect the impact of the inputs, and do not\ncharacterize the SNN in an end-to-end manner. Additionally, thanks to a\nthorough study of the Ramanujan-based metrics, we discover that they are only\nas good as the \\emph{layer-wise density} as performance predictors, when paired\nwith BGEs. To close both gaps, we design a comprehensive topological analysis\nfor SNNs with both linear and convolutional layers, via (i) a new input-aware\nMultipartite Graph Encoding (MGE) for SNNs and (ii) the design of new\nend-to-end topological metrics over the MGE. With these novelties, we show the\nfollowing: (a) The proposed MGE allows to extract topological metrics that are\nmuch better predictors of the accuracy drop than metrics computed from current\ninput-agnostic BGEs; (b) Which metrics are important at different sparsity\nlevels and for different architectures; (c) A mixture of our topological\nmetrics can rank PaI algorithms more effectively than Ramanujan-based metrics.\nThe codebase is publicly available at https://github.com/eliacunegatti/mge-snn."
            ],
            "interesting paper": 1
        },
        {
            "papers": [
                "Emergent Modularity in Pre-trained Transformers\nThis work examines the presence of modularity in pre-trained Transformers, a\nfeature commonly found in human brains and thought to be vital for general\nintelligence. In analogy to human brains, we consider two main characteristics\nof modularity: (1) functional specialization of neurons: we evaluate whether\neach neuron is mainly specialized in a certain function, and find that the\nanswer is yes. (2) function-based neuron grouping: we explore finding a\nstructure that groups neurons into modules by function, and each module works\nfor its corresponding function. Given the enormous amount of possible\nstructures, we focus on Mixture-of-Experts as a promising candidate, which\npartitions neurons into experts and usually activates different experts for\ndifferent inputs. Experimental results show that there are functional experts,\nwhere clustered are the neurons specialized in a certain function. Moreover,\nperturbing the activations of functional experts significantly affects the\ncorresponding function. Finally, we study how modularity emerges during\npre-training, and find that the modular structure is stabilized at the early\nstage, which is faster than neuron stabilization. It suggests that Transformers\nfirst construct the modular structure and then learn fine-grained neuron\nfunctions. Our code and data are available at\nhttps://github.com/THUNLP/modularity-analysis.",
                "A Comprehensive Overview and Comparative Analysis on Deep Learning\n  Models: CNN, RNN, LSTM, GRU\nDeep learning (DL) has emerged as a powerful subset of machine learning (ML)\nand artificial intelligence (AI), outperforming traditional ML methods,\nespecially in handling unstructured and large datasets. Its impact spans across\nvarious domains, including speech recognition, healthcare, autonomous vehicles,\ncybersecurity, predictive analytics, and more. However, the complexity and\ndynamic nature of real-world problems present challenges in designing effective\ndeep learning models. Consequently, several deep learning models have been\ndeveloped to address different problems and applications. In this article, we\nconduct a comprehensive survey of various deep learning models, including\nConvolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs),\nGenerative Models, Deep Reinforcement Learning (DRL), and Deep Transfer\nLearning. We examine the structure, applications, benefits, and limitations of\neach model. Furthermore, we perform an analysis using three publicly available\ndatasets: IMDB, ARAS, and Fruit-360. We compare the performance of six renowned\ndeep learning models: CNN, Simple RNN, Long Short-Term Memory (LSTM),\nBidirectional LSTM, Gated Recurrent Unit (GRU), and Bidirectional GRU.",
                "Probing reaction channels via reinforcement learning\nWe propose a reinforcement learning based method to identify important\nconfigurations that connect reactant and product states along chemical reaction\npaths. By shooting multiple trajectories from these configurations, we can\ngenerate an ensemble of configurations that concentrate on the transition path\nensemble. This configuration ensemble can be effectively employed in a neural\nnetwork-based partial differential equation solver to obtain an approximation\nsolution of a restricted Backward Kolmogorov equation, even when the dimension\nof the problem is very high. The resulting solution, known as the committor\nfunction, encodes mechanistic information for the reaction and can in turn be\nused to evaluate reaction rates.",
                "One Network, Many Masks: Towards More Parameter-Efficient Transfer\n  Learning\nFine-tuning pre-trained language models for multiple tasks tends to be\nexpensive in terms of storage. To mitigate this, parameter-efficient transfer\nlearning (PETL) methods have been proposed to address this issue, but they\nstill require a significant number of parameters and storage when being applied\nto broader ranges of tasks. To achieve even greater storage reduction, we\npropose PROPETL, a novel method that enables efficient sharing of a single PETL\nmodule which we call prototype network (e.g., adapter, LoRA, and prefix-tuning)\nacross layers and tasks. We then learn binary masks to select different\nsub-networks from the shared prototype network and apply them as PETL modules\ninto different layers. We find that the binary masks can determine crucial\ninformation from the network, which is often ignored in previous studies. Our\nwork can also be seen as a type of pruning method, where we find that\noverparameterization also exists in the seemingly small PETL modules. We\nevaluate PROPETL on various downstream tasks and show that it can outperform\nother PETL methods with approximately 10% of the parameter storage required by\nthe latter.",
                "Amplification trojan network: Attack deep neural networks by amplifying\n  their inherent weakness\nRecent works found that deep neural networks (DNNs) can be fooled by\nadversarial examples, which are crafted by adding adversarial noise on clean\ninputs. The accuracy of DNNs on adversarial examples will decrease as the\nmagnitude of the adversarial noise increase. In this study, we show that DNNs\ncan be also fooled when the noise is very small under certain circumstances.\nThis new type of attack is called Amplification Trojan Attack (ATAttack).\nSpecifically, we use a trojan network to transform the inputs before sending\nthem to the target DNN. This trojan network serves as an amplifier to amplify\nthe inherent weakness of the target DNN. The target DNN, which is infected by\nthe trojan network, performs normally on clean data while being more vulnerable\nto adversarial examples. Since it only transforms the inputs, the trojan\nnetwork can hide in DNN-based pipelines, e.g. by infecting the pre-processing\nprocedure of the inputs before sending them to the DNNs. This new type of\nthreat should be considered in developing safe DNNs.",
                "Observation Denoising in CYRUS Soccer Simulation 2D Team For RoboCup\n  2023\nThe RoboCup competitions hold various leagues, and the Soccer Simulation 2D\nLeague is a major one among them. Soccer Simulation 2D (SS2D) match involves\ntwo teams, including 11 players and a coach, competing against each other. The\nplayers can only communicate with the Soccer Simulation Server during the game.\nThis paper presents the latest research of the CYRUS soccer simulation 2D team,\nthe champion of RoboCup 2021. We will explain our denoising idea powered by\nlong short-term memory networks (LSTM) and deep neural networks (DNN). The\nCYRUS team uses the CYRUS2D base code that was developed based on the Helios\nand Gliders bases."
            ],
            "interesting paper": 1
        },
        {
            "papers": [
                "On the impact of activation and normalization in obtaining isometric\n  embeddings at initialization\nIn this paper, we explore the structure of the penultimate Gram matrix in\ndeep neural networks, which contains the pairwise inner products of outputs\ncorresponding to a batch of inputs. In several architectures it has been\nobserved that this Gram matrix becomes degenerate with depth at initialization,\nwhich dramatically slows training. Normalization layers, such as batch or layer\nnormalization, play a pivotal role in preventing the rank collapse issue.\nDespite promising advances, the existing theoretical results do not extend to\nlayer normalization, which is widely used in transformers, and can not\nquantitatively characterize the role of non-linear activations. To bridge this\ngap, we prove that layer normalization, in conjunction with activation layers,\nbiases the Gram matrix of a multilayer perceptron towards the identity matrix\nat an exponential rate with depth at initialization. We quantify this rate\nusing the Hermite expansion of the activation function.",
                "Intelligent gradient amplification for deep neural networks\nDeep learning models offer superior performance compared to other machine\nlearning techniques for a variety of tasks and domains, but pose their own\nchallenges. In particular, deep learning models require larger training times\nas the depth of a model increases, and suffer from vanishing gradients. Several\nsolutions address these problems independently, but there have been minimal\nefforts to identify an integrated solution that improves the performance of a\nmodel by addressing vanishing gradients, as well as accelerates the training\nprocess to achieve higher performance at larger learning rates. In this work,\nwe intelligently determine which layers of a deep learning model to apply\ngradient amplification to, using a formulated approach that analyzes gradient\nfluctuations of layers during training. Detailed experiments are performed for\nsimpler and deeper neural networks using two different intelligent measures and\ntwo different thresholds that determine the amplification layers, and a\ntraining strategy where gradients are amplified only during certain epochs.\nResults show that our amplification offers better performance compared to the\noriginal models, and achieves accuracy improvement of around 2.5% on CIFAR- 10\nand around 4.5% on CIFAR-100 datasets, even when the models are trained with\nhigher learning rates.",
                "Deep Electron Cloud-activity and Field-activity Relationships\nChemists have been pursuing the general mathematical laws to explain and\npredict molecular properties for a long time. However, most of the traditional\nquantitative structure-activity relationship (QSAR) models have limited\napplication domains, e.g., they tend to have poor generalization performance\nwhen applied to molecules with parent structures different from those of the\ntrained molecules. This paper attempts to develop a new QSAR method that is\ntheoretically possible to predict various properties of molecules with diverse\nstructures. The proposed deep electron cloud-activity relationships (DECAR) and\ndeep field-activity relationships (DFAR) methods consist of three essentials:\n(1) A large number of molecule entities with activity data as training objects\nand responses; (2) three-dimensional electron cloud density (ECD) or related\nfield data by the accurate density functional theory methods as input\ndescriptors; (3) a deep learning model that is sufficiently flexible and\npowerful to learn the large data described above. DECAR and DFAR are used to\ndistinguish 977 sweet and 1965 non-sweet molecules (with 6-fold data\naugmentation) and the classification performance is demonstrated to be\nsignificantly better than the traditional least squares support vector machine\n(LS-SVM) models using traditional descriptors. DECAR and DFAR would provide a\npossible way to establish a widely applicable, cumulative, and shareable\nartificial intelligence-driven QSAR system. They are likely to promote the\ndevelopment of an interactive platform to collect and share the accurate ECD\nand field data of millions of molecules with annotated activities. With enough\ninput data, we envision the appearance of several deep networks trained for\nvarious molecular activities. Finally, we could anticipate a single DECAR or\nDFAR network to learn and infer various properties of interest for chemical\nmolecules.",
                "Bayesian inference and neural estimation of acoustic wave propagation\nIn this work, we introduce a novel framework which combines physics and\nmachine learning methods to analyse acoustic signals. Three methods are\ndeveloped for this task: a Bayesian inference approach for inferring the\nspectral acoustics characteristics, a neural-physical model which equips a\nneural network with forward and backward physical losses, and the non-linear\nleast squares approach which serves as benchmark. The inferred propagation\ncoefficient leads to the room impulse response (RIR) quantity which can be used\nfor relocalisation with uncertainty. The simplicity and efficiency of this\nframework is empirically validated on simulated data.",
                "Determinantal Point Process Attention Over Grid Cell Code Supports Out\n  of Distribution Generalization\nDeep neural networks have made tremendous gains in emulating human-like\nintelligence, and have been used increasingly as ways of understanding how the\nbrain may solve the complex computational problems on which this relies.\nHowever, these still fall short of, and therefore fail to provide insight into\nhow the brain supports strong forms of generalization of which humans are\ncapable. One such case is out-of-distribution (OOD) generalization-successful\nperformance on test examples that lie outside the distribution of the training\nset. Here, we identify properties of processing in the brain that may\ncontribute to this ability. We describe a two-part algorithm that draws on\nspecific features of neural computation to achieve OOD generalization, and\nprovide a proof of concept by evaluating performance on two challenging\ncognitive tasks. First we draw on the fact that the mammalian brain represents\nmetric spaces using grid cell code (e.g., in the entorhinal cortex): abstract\nrepresentations of relational structure, organized in recurring motifs that\ncover the representational space. Second, we propose an attentional mechanism\nthat operates over the grid cell code using Determinantal Point Process (DPP),\nthat we call DPP attention (DPP-A) -- a transformation that ensures maximum\nsparseness in the coverage of that space. We show that a loss function that\ncombines standard task-optimized error with DPP-A can exploit the recurring\nmotifs in the grid cell code, and can be integrated with common architectures\nto achieve strong OOD generalization performance on analogy and arithmetic\ntasks. This provides both an interpretation of how the grid cell code in the\nmammalian brain may contribute to generalization performance, and at the same\ntime a potential means for improving such capabilities in artificial neural\nnetworks.",
                "Employing Explainable Artificial Intelligence (XAI) Methodologies to\n  Analyze the Correlation between Input Variables and Tensile Strength in\n  Additively Manufactured Samples\nThis research paper explores the impact of various input parameters,\nincluding Infill percentage, Layer Height, Extrusion Temperature, and Print\nSpeed, on the resulting Tensile Strength in objects produced through additive\nmanufacturing. The main objective of this study is to enhance our understanding\nof the correlation between the input parameters and Tensile Strength, as well\nas to identify the key factors influencing the performance of the additive\nmanufacturing process. To achieve this objective, we introduced the utilization\nof Explainable Artificial Intelligence (XAI) techniques for the first time,\nwhich allowed us to analyze the data and gain valuable insights into the\nsystem's behavior. Specifically, we employed SHAP (SHapley Additive\nexPlanations), a widely adopted framework for interpreting machine learning\nmodel predictions, to provide explanations for the behavior of a machine\nlearning model trained on the data. Our findings reveal that the Infill\npercentage and Extrusion Temperature have the most significant influence on\nTensile Strength, while the impact of Layer Height and Print Speed is\nrelatively minor. Furthermore, we discovered that the relationship between the\ninput parameters and Tensile Strength is highly intricate and nonlinear, making\nit difficult to accurately describe using simple linear models."
            ],
            "interesting paper": 1
        },
        {
            "papers": [
                "Magnetic field regression using artificial neural networks for cold atom\n  experiments\nAccurately measuring magnetic fields is essential for magnetic-field\nsensitive experiments in fields like atomic, molecular, and optical physics,\ncondensed matter experiments, and other areas. However, since many experiments\nare conducted in an isolated vacuum environment that is inaccessible to\nexperimentalists, it can be challenging to accurately determine the magnetic\nfield. Here, we propose an efficient method for detecting magnetic fields with\nthe assistance of an artificial neural network (NN). Instead of measuring the\nmagnetic field directly at the desired location, we detect magnetic fields at\nseveral surrounding positions, and a trained NN can accurately predict the\nmagnetic field at the target location. After training, we achieve a relative\nerror of magnetic field magnitude (magnitude of error over the magnitude of\nmagnetic field) below 0.3$\\%$, and we successfully apply this method to our\nerbium quantum gas apparatus. This approach significantly simplifies the\nprocess of determining magnetic fields in isolated vacuum environments and can\nbe applied to various research fields across a wide range of magnetic field\nmagnitudes.",
                "Trainable and Explainable Simplicial Map Neural Networks\nSimplicial map neural networks (SMNNs) are topology-based neural networks\nwith interesting properties such as universal approximation ability and\nrobustness to adversarial examples under appropriate conditions. However, SMNNs\npresent some bottlenecks for their possible application in high-dimensional\ndatasets. First, SMNNs have precomputed fixed weight and no SMNN training\nprocess has been defined so far, so they lack generalization ability. Second,\nSMNNs require the construction of a convex polytope surrounding the input\ndataset. In this paper, we overcome these issues by proposing an SMNN training\nprocedure based on a support subset of the given dataset and replacing the\nconstruction of the convex polytope by a method based on projections to a\nhypersphere. In addition, the explainability capacity of SMNNs and an effective\nimplementation are also newly introduced in this paper.",
                "Understanding Predictive Coding as an Adaptive Trust-Region Method\nPredictive coding (PC) is a brain-inspired local learning algorithm that has\nrecently been suggested to provide advantages over backpropagation (BP) in\nbiologically relevant scenarios. While theoretical work has mainly focused on\nshowing how PC can approximate BP in various limits, the putative benefits of\n\"natural\" PC are less understood. Here we develop a theory of PC as an adaptive\ntrust-region (TR) algorithm that uses second-order information. We show that\nthe learning dynamics of PC can be interpreted as interpolating between BP's\nloss gradient direction and a TR direction found by the PC inference dynamics.\nOur theory suggests that PC should escape saddle points faster than BP, a\nprediction which we prove in a shallow linear model and support with\nexperiments on deeper networks. This work lays a foundation for understanding\nPC in deep and wide networks.",
                "Low Precision Quantization-aware Training in Spiking Neural Networks\n  with Differentiable Quantization Function\nDeep neural networks have been proven to be highly effective tools in various\ndomains, yet their computational and memory costs restrict them from being\nwidely deployed on portable devices. The recent rapid increase of edge\ncomputing devices has led to an active search for techniques to address the\nabove-mentioned limitations of machine learning frameworks. The quantization of\nartificial neural networks (ANNs), which converts the full-precision synaptic\nweights into low-bit versions, emerged as one of the solutions. At the same\ntime, spiking neural networks (SNNs) have become an attractive alternative to\nconventional ANNs due to their temporal information processing capability,\nenergy efficiency, and high biological plausibility. Despite being driven by\nthe same motivation, the simultaneous utilization of both concepts has yet to\nbe thoroughly studied. Therefore, this work aims to bridge the gap between\nrecent progress in quantized neural networks and SNNs. It presents an extensive\nstudy on the performance of the quantization function, represented as a linear\ncombination of sigmoid functions, exploited in low-bit weight quantization in\nSNNs. The presented quantization function demonstrates the state-of-the-art\nperformance on four popular benchmarks, CIFAR10-DVS, DVS128 Gesture,\nN-Caltech101, and N-MNIST, for binary networks (64.05\\%, 95.45\\%, 68.71\\%, and\n99.43\\% respectively) with small accuracy drops and up to 31$\\times$ memory\nsavings, which outperforms existing methods.",
                "E-PANNs: Sound Recognition Using Efficient Pre-trained Audio Neural\n  Networks\nSounds carry an abundance of information about activities and events in our\neveryday environment, such as traffic noise, road works, music, or people\ntalking. Recent machine learning methods, such as convolutional neural networks\n(CNNs), have been shown to be able to automatically recognize sound activities,\na task known as audio tagging. One such method, pre-trained audio neural\nnetworks (PANNs), provides a neural network which has been pre-trained on over\n500 sound classes from the publicly available AudioSet dataset, and can be used\nas a baseline or starting point for other tasks. However, the existing PANNs\nmodel has a high computational complexity and large storage requirement. This\ncould limit the potential for deploying PANNs on resource-constrained devices,\nsuch as on-the-edge sound sensors, and could lead to high energy consumption if\nmany such devices were deployed. In this paper, we reduce the computational\ncomplexity and memory requirement of the PANNs model by taking a pruning\napproach to eliminate redundant parameters from the PANNs model. The resulting\nEfficient PANNs (E-PANNs) model, which requires 36\\% less computations and 70\\%\nless memory, also slightly improves the sound recognition (audio tagging)\nperformance. The code for the E-PANNs model has been released under an open\nsource license.",
                "Gaussian Process Probes (GPP) for Uncertainty-Aware Probing\nUnderstanding which concepts models can and cannot represent has been\nfundamental to many tasks: from effective and responsible use of models to\ndetecting out of distribution data. We introduce Gaussian process probes (GPP),\na unified and simple framework for probing and measuring uncertainty about\nconcepts represented by models. As a Bayesian extension of linear probing\nmethods, GPP asks what kind of distribution over classifiers (of concepts) is\ninduced by the model. This distribution can be used to measure both what the\nmodel represents and how confident the probe is about what the model\nrepresents. GPP can be applied to any pre-trained model with vector\nrepresentations of inputs (e.g., activations). It does not require access to\ntraining data, gradients, or the architecture. We validate GPP on datasets\ncontaining both synthetic and real images. Our experiments show it can (1)\nprobe a model's representations of concepts even with a very small number of\nexamples, (2) accurately measure both epistemic uncertainty (how confident the\nprobe is) and aleatory uncertainty (how fuzzy the concepts are to the model),\nand (3) detect out of distribution data using those uncertainty measures as\nwell as classic methods do. By using Gaussian processes to expand what probing\ncan offer, GPP provides a data-efficient, versatile and uncertainty-aware tool\nfor understanding and evaluating the capabilities of machine learning models."
            ],
            "interesting paper": 2
        },
        {
            "papers": [
                "CVSNet: A Computer Implementation for Central Visual System of The Brain\nIn computer vision, different basic blocks are created around different\nmatrix operations, and models based on different basic blocks have achieved\ngood results. Good results achieved in vision tasks grants them rationality.\nHowever, these experimental-based models also make deep learning long\ncriticized for principle and interpretability. Deep learning originated from\nthe concept of neurons in neuroscience, but recent designs detached natural\nneural networks except for some simple concepts. In this paper, we build an\nartificial neural network, CVSNet, which can be seen as a computer\nimplementation for central visual system of the brain. Each block in CVSNet\nrepresents the same vision information as that in brains. In CVSNet, blocks\ndiffers from each other and visual information flows through three independent\npathways and five different blocks. Thus CVSNet is completely different from\nthe design of all previous models, in which basic blocks are repeated to build\nmodel and information between channels is mixed at the outset. In ablation\nexperiment, we show the information extracted by blocks in CVSNet and compare\nwith previous networks, proving effectiveness and rationality of blocks in\nCVSNet from experiment side. And in the experiment of object recognition,\nCVSNet achieves comparable results to ConvNets, Vision Transformers and MLPs.",
                "Probabilistic computation and uncertainty quantification with emerging\n  covariance\nBuilding robust, interpretable, and secure AI system requires quantifying and\nrepresenting uncertainty under a probabilistic perspective to mimic human\ncognitive abilities. However, probabilistic computation presents significant\nchallenges for most conventional artificial neural network, as they are\nessentially implemented in a deterministic manner. In this paper, we develop an\nefficient probabilistic computation framework by truncating the probabilistic\nrepresentation of neural activation up to its mean and covariance and construct\na moment neural network that encapsulates the nonlinear coupling between the\nmean and covariance of the underlying stochastic network. We reveal that when\nonly the mean but not the covariance is supervised during gradient-based\nlearning, the unsupervised covariance spontaneously emerges from its nonlinear\ncoupling with the mean and faithfully captures the uncertainty associated with\nmodel predictions. Our findings highlight the inherent simplicity of\nprobabilistic computation by seamlessly incorporating uncertainty into model\nprediction, paving the way for integrating it into large-scale AI systems.",
                "Adaptation of Tongue Ultrasound-Based Silent Speech Interfaces Using\n  Spatial Transformer Networks\nThanks to the latest deep learning algorithms, silent speech interfaces (SSI)\nare now able to synthesize intelligible speech from articulatory movement data\nunder certain conditions. However, the resulting models are rather\nspeaker-specific, making a quick switch between users troublesome. Even for the\nsame speaker, these models perform poorly cross-session, i.e. after dismounting\nand re-mounting the recording equipment. To aid quick speaker and session\nadaptation of ultrasound tongue imaging-based SSI models, we extend our deep\nnetworks with a spatial transformer network (STN) module, capable of performing\nan affine transformation on the input images. Although the STN part takes up\nonly about 10% of the network, our experiments show that adapting just the STN\nmodule might allow to reduce MSE by 88% on the average, compared to retraining\nthe whole network. The improvement is even larger (around 92%) when adapting\nthe network to different recording sessions from the same speaker.",
                "Catalysis distillation neural network for the few shot open catalyst\n  challenge\nThe integration of artificial intelligence and science has resulted in\nsubstantial progress in computational chemistry methods for the design and\ndiscovery of novel catalysts. Nonetheless, the challenges of electrocatalytic\nreactions and developing a large-scale language model in catalysis persist, and\nthe recent success of ChatGPT's (Chat Generative Pre-trained Transformer)\nfew-shot methods surpassing BERT (Bidirectional Encoder Representation from\nTransformers) underscores the importance of addressing limited data, expensive\ncomputations, time constraints and structure-activity relationship in research.\nHence, the development of few-shot techniques for catalysis is critical and\nessential, regardless of present and future requirements. This paper introduces\nthe Few-Shot Open Catalyst Challenge 2023, a competition aimed at advancing the\napplication of machine learning technology for predicting catalytic reactions\non catalytic surfaces, with a specific focus on dual-atom catalysts in hydrogen\nperoxide electrocatalysis. To address the challenge of limited data in\ncatalysis, we propose a machine learning approach based on MLP-Like and a\nframework called Catalysis Distillation Graph Neural Network (CDGNN). Our\nresults demonstrate that CDGNN effectively learns embeddings from catalytic\nstructures, enabling the capture of structure-adsorption relationships. This\naccomplishment has resulted in the utmost advanced and efficient determination\nof the reaction pathway for hydrogen peroxide, surpassing the current graph\nneural network approach by 16.1%.. Consequently, CDGNN presents a promising\napproach for few-shot learning in catalysis.",
                "Inverse Approximation Theory for Nonlinear Recurrent Neural Networks\nWe prove an inverse approximation theorem for the approximation of nonlinear\nsequence-to-sequence relationships using recurrent neural networks (RNNs). This\nis a so-called Bernstein-type result in approximation theory, which deduces\nproperties of a target function under the assumption that it can be effectively\napproximated by a hypothesis space. In particular, we show that nonlinear\nsequence relationships that can be stably approximated by nonlinear RNNs must\nhave an exponential decaying memory structure - a notion that can be made\nprecise. This extends the previously identified curse of memory in linear RNNs\ninto the general nonlinear setting, and quantifies the essential limitations of\nthe RNN architecture for learning sequential relationships with long-term\nmemory. Based on the analysis, we propose a principled reparameterization\nmethod to overcome the limitations. Our theoretical results are confirmed by\nnumerical experiments. The code has been released in\nhttps://github.com/radarFudan/Curse-of-memory",
                "Towards Explainable and Language-Agnostic LLMs: Symbolic Reverse\n  Engineering of Language at Scale\nLarge language models (LLMs) have achieved a milestone that undenia-bly\nchanged many held beliefs in artificial intelligence (AI). However, there\nremains many limitations of these LLMs when it comes to true language\nunderstanding, limitations that are a byproduct of the under-lying architecture\nof deep neural networks. Moreover, and due to their subsymbolic nature,\nwhatever knowledge these models acquire about how language works will always be\nburied in billions of microfeatures (weights), none of which is meaningful on\nits own, making such models hopelessly unexplainable. To address these\nlimitations, we suggest com-bining the strength of symbolic representations\nwith what we believe to be the key to the success of LLMs, namely a successful\nbottom-up re-verse engineering of language at scale. As such we argue for a\nbottom-up reverse engineering of language in a symbolic setting. Hints on what\nthis project amounts to have been suggested by several authors, and we discuss\nin some detail here how this project could be accomplished."
            ],
            "interesting paper": 2
        },
        {
            "papers": [
                "Beam Tree Recursive Cells\nWe propose Beam Tree Recursive Cell (BT-Cell) - a backpropagation-friendly\nframework to extend Recursive Neural Networks (RvNNs) with beam search for\nlatent structure induction. We further extend this framework by proposing a\nrelaxation of the hard top-k operators in beam search for better propagation of\ngradient signals. We evaluate our proposed models in different\nout-of-distribution splits in both synthetic and realistic data. Our\nexperiments show that BTCell achieves near-perfect performance on several\nchallenging structure-sensitive synthetic tasks like ListOps and logical\ninference while maintaining comparable performance in realistic data against\nother RvNN-based models. Additionally, we identify a previously unknown failure\ncase for neural models in generalization to unseen number of arguments in\nListOps. The code is available at:\nhttps://github.com/JRC1995/BeamTreeRecursiveCells.",
                "Breast Cancer Detection and Diagnosis: A comparative study of\n  state-of-the-arts deep learning architectures\nBreast cancer is a prevalent form of cancer among women, with over 1.5\nmillion women being diagnosed each year. Unfortunately, the survival rates for\nbreast cancer patients in certain third-world countries, like South Africa, are\nalarmingly low, with only 40% of diagnosed patients surviving beyond five\nyears. The inadequate availability of resources, including qualified\npathologists, delayed diagnoses, and ineffective therapy planning, contribute\nto this low survival rate. To address this pressing issue, medical specialists\nand researchers have turned to domain-specific AI approaches, specifically deep\nlearning models, to develop end-to-end solutions that can be integrated into\ncomputer-aided diagnosis (CAD) systems. By improving the workflow of\npathologists, these AI models have the potential to enhance the detection and\ndiagnosis of breast cancer. This research focuses on evaluating the performance\nof various cutting-edge convolutional neural network (CNN) architectures in\ncomparison to a relatively new model called the Vision Trans-former (ViT). The\nobjective is to determine the superiority of these models in terms of their\naccuracy and effectiveness. The experimental results reveal that the ViT models\noutperform the other selected state-of-the-art CNN architectures, achieving an\nimpressive accuracy rate of 95.15%. This study signifies a significant\nadvancement in the field, as it explores the utilization of data augmentation\nand other relevant preprocessing techniques in conjunction with deep learning\nmodels for the detection and diagnosis of breast cancer using datasets of\nBreast Cancer Histopathological Image Classification.",
                "Transformers learn to implement preconditioned gradient descent for\n  in-context learning\nSeveral recent works demonstrate that transformers can implement algorithms\nlike gradient descent. By a careful construction of weights, these works show\nthat multiple layers of transformers are expressive enough to simulate\niterations of gradient descent. Going beyond the question of expressivity, we\nask: Can transformers learn to implement such algorithms by training over\nrandom problem instances? To our knowledge, we make the first theoretical\nprogress on this question via an analysis of the loss landscape for linear\ntransformers trained over random instances of linear regression. For a single\nattention layer, we prove the global minimum of the training objective\nimplements a single iteration of preconditioned gradient descent. Notably, the\npreconditioning matrix not only adapts to the input distribution but also to\nthe variance induced by data inadequacy. For a transformer with $L$ attention\nlayers, we prove certain critical points of the training objective implement\n$L$ iterations of preconditioned gradient descent. Our results call for future\ntheoretical studies on learning algorithms by training transformers.",
                "Diffused Redundancy in Pre-trained Representations\nRepresentations learned by pre-training a neural network on a large dataset\nare increasingly used successfully to perform a variety of downstream tasks. In\nthis work, we take a closer look at how features are encoded in such\npre-trained representations. We find that learned representations in a given\nlayer exhibit a degree of diffuse redundancy, ie, any randomly chosen subset of\nneurons in the layer that is larger than a threshold size shares a large degree\nof similarity with the full layer and is able to perform similarly as the whole\nlayer on a variety of downstream tasks. For example, a linear probe trained on\n$20\\%$ of randomly picked neurons from the penultimate layer of a ResNet50\npre-trained on ImageNet1k achieves an accuracy within $5\\%$ of a linear probe\ntrained on the full layer of neurons for downstream CIFAR10 classification. We\nconduct experiments on different neural architectures (including CNNs and\nTransformers) pre-trained on both ImageNet1k and ImageNet21k and evaluate a\nvariety of downstream tasks taken from the VTAB benchmark. We find that the\nloss and dataset used during pre-training largely govern the degree of diffuse\nredundancy and the \"critical mass\" of neurons needed often depends on the\ndownstream task, suggesting that there is a task-inherent\nredundancy-performance Pareto frontier. Our findings shed light on the nature\nof representations learned by pre-trained deep neural networks and suggest that\nentire layers might not be necessary to perform many downstream tasks. We\ninvestigate the potential for exploiting this redundancy to achieve efficient\ngeneralization for downstream tasks and also draw caution to certain possible\nunintended consequences. Our code is available at\n\\url{https://github.com/nvedant07/diffused-redundancy}.",
                "On the Expressive Power of Neural Networks\nIn 1989 George Cybenko proved in a landmark paper that wide shallow neural\nnetworks can approximate arbitrary continuous functions on a compact set. This\nuniversal approximation theorem sparked a lot of follow-up research.\n  Shen, Yang and Zhang determined optimal approximation rates for ReLU-networks\nin $L^p$-norms with $p \\in [1,\\infty)$. Kidger and Lyons proved a universal\napproximation theorem for deep narrow ReLU-networks. Telgarsky gave an example\nof a deep narrow ReLU-network that cannot be approximated by a wide shallow\nReLU-network unless it has exponentially many neurons.\n  However, there are even more questions that still remain unresolved. Are\nthere any wide shallow ReLU-networks that cannot be approximated well by deep\nnarrow ReLU-networks? Is the universal approximation theorem still true for\nother norms like the Sobolev norm $W^{1,1}$? Do these results hold for\nactivation functions other than ReLU?\n  We will answer all of those questions and more with a framework of two\nexpressive powers. The first one is well-known and counts the maximal number of\nlinear regions of a function calculated by a ReLU-network. We will improve the\nbest known bounds for this expressive power. The second one is entirely new.",
                "Training-free Neural Architecture Search for RNNs and Transformers\nNeural architecture search (NAS) has allowed for the automatic creation of\nnew and effective neural network architectures, offering an alternative to the\nlaborious process of manually designing complex architectures. However,\ntraditional NAS algorithms are slow and require immense amounts of computing\npower. Recent research has investigated training-free NAS metrics for image\nclassification architectures, drastically speeding up search algorithms. In\nthis paper, we investigate training-free NAS metrics for recurrent neural\nnetwork (RNN) and BERT-based transformer architectures, targeted towards\nlanguage modeling tasks. First, we develop a new training-free metric, named\nhidden covariance, that predicts the trained performance of an RNN architecture\nand significantly outperforms existing training-free metrics. We experimentally\nevaluate the effectiveness of the hidden covariance metric on the NAS-Bench-NLP\nbenchmark. Second, we find that the current search space paradigm for\ntransformer architectures is not optimized for training-free neural\narchitecture search. Instead, a simple qualitative analysis can effectively\nshrink the search space to the best performing architectures. This conclusion\nis based on our investigation of existing training-free metrics and new metrics\ndeveloped from recent transformer pruning literature, evaluated on our own\nbenchmark of trained BERT architectures. Ultimately, our analysis shows that\nthe architecture search space and the training-free metric must be developed\ntogether in order to achieve effective results."
            ],
            "interesting paper": 3
        },
        {
            "papers": [
                "Smooth Min-Max Monotonic Networks\nMonotonicity constraints are powerful regularizers in statistical modelling.\nThey can support fairness in computer-aided decision making and increase\nplausibility in data-driven scientific models. The seminal min-max (MM) neural\nnetwork architecture ensures monotonicity, but often gets stuck in undesired\nlocal optima during training because of partial derivatives of the MM\nnonlinearities being zero. We propose a simple modification of the MM network\nusing strictly-increasing smooth minimum and maximum functions that alleviates\nthis problem. The resulting smooth min-max (SMM) network module inherits the\nasymptotic approximation properties from the MM architecture. It can be used\nwithin larger deep learning systems trained end-to-end. The SMM module is\nconceptually simple and computationally less demanding than state-of-the-art\nneural networks for monotonic modelling. Our experiments show that this does\nnot come with a loss in generalization performance compared to alternative\nneural and non-neural approaches.",
                "Interpretable and Explainable Logical Policies via Neurally Guided\n  Symbolic Abstraction\nThe limited priors required by neural networks make them the dominating\nchoice to encode and learn policies using reinforcement learning (RL). However,\nthey are also black-boxes, making it hard to understand the agent's behaviour,\nespecially when working on the image level. Therefore, neuro-symbolic RL aims\nat creating policies that are interpretable in the first place. Unfortunately,\ninterpretability is not explainability. To achieve both, we introduce Neurally\ngUided Differentiable loGic policiEs (NUDGE). NUDGE exploits trained neural\nnetwork-based agents to guide the search of candidate-weighted logic rules,\nthen uses differentiable logic to train the logic agents. Our experimental\nevaluation demonstrates that NUDGE agents can induce interpretable and\nexplainable policies while outperforming purely neural ones and showing good\nflexibility to environments of different initial states and problem sizes.",
                "Microstructure quality control of steels using deep learning\nIn quality control, microstructures are investigated rigorously to ensure\nstructural integrity, exclude the presence of critical volume defects, and\nvalidate the formation of the target microstructure. For quenched,\nhierarchically-structured steels, the morphology of the bainitic and\nmartensitic microstructures are of major concern to guarantee the reliability\nof the material under service conditions. Therefore, industries conduct small\nsample-size inspections of materials cross-sections through metallographers to\nvalidate the needle morphology of such microstructures. We demonstrate\nround-robin test results revealing that this visual grading is afflicted by\npronounced subjectivity despite the thorough training of personnel. Instead, we\npropose a deep learning image classification approach that distinguishes steels\nbased on their microstructure type and classifies their needle length alluding\nto the ISO 643 grain size assessment standard. This classification approach\nfacilitates the reliable, objective, and automated classification of\nhierarchically structured steels. Specifically, an accuracy of 96% and roughly\n91% is attained for the distinction of martensite/bainite subtypes and needle\nlength, respectively. This is achieved on an image dataset that contains\nsignificant variance and labeling noise as it is acquired over more than ten\nyears from multiple plants, alloys, etchant applications, and light optical\nmicroscopes by many metallographers (raters). Interpretability analysis gives\ninsights into the decision-making of these models and allows for estimating\ntheir generalization capability.",
                "Adversarial Attack Based on Prediction-Correction\nDeep neural networks (DNNs) are vulnerable to adversarial examples obtained\nby adding small perturbations to original examples. The added perturbations in\nexisting attacks are mainly determined by the gradient of the loss function\nwith respect to the inputs. In this paper, the close relationship between\ngradient-based attacks and the numerical methods for solving ordinary\ndifferential equation (ODE) is studied for the first time. Inspired by the\nnumerical solution of ODE, a new prediction-correction (PC) based adversarial\nattack is proposed. In our proposed PC-based attack, some existing attack can\nbe selected to produce a predicted example first, and then the predicted\nexample and the current example are combined together to determine the added\nperturbations. The proposed method possesses good extensibility and can be\napplied to all available gradient-based attacks easily. Extensive experiments\ndemonstrate that compared with the state-of-the-art gradient-based adversarial\nattacks, our proposed PC-based attacks have higher attack success rates, and\nexhibit better transferability.",
                "The Galerkin method beats Graph-Based Approaches for Spectral Algorithms\nHistorically, the machine learning community has derived spectral\ndecompositions from graph-based approaches. We break with this approach and\nprove the statistical and computational superiority of the Galerkin method,\nwhich consists in restricting the study to a small set of test functions. In\nparticular, we introduce implementation tricks to deal with differential\noperators in large dimensions with structured kernels. Finally, we extend on\nthe core principles beyond our approach to apply them to non-linear spaces of\nfunctions, such as the ones parameterized by deep neural networks, through\nloss-based optimization procedures.",
                "An FPGA Architecture for Online Learning using the Tsetlin Machine\nThere is a need for machine learning models to evolve in unsupervised\ncircumstances. New classifications may be introduced, unexpected faults may\noccur, or the initial dataset may be small compared to the data-points\npresented to the system during normal operation. Implementing such a system\nusing neural networks involves significant mathematical complexity, which is a\nmajor issue in power-critical edge applications.\n  This paper proposes a novel field-programmable gate-array infrastructure for\nonline learning, implementing a low-complexity machine learning algorithm\ncalled the Tsetlin Machine. This infrastructure features a custom-designed\narchitecture for run-time learning management, providing on-chip offline and\nonline learning. Using this architecture, training can be carried out on-demand\non the \\ac{FPGA} with pre-classified data before inference takes place.\nAdditionally, our architecture provisions online learning, where training can\nbe interleaved with inference during operation. Tsetlin Machine (TM) training\nnaturally descends to an optimum, with training also linked to a threshold\nhyper-parameter which is used to reduce the probability of issuing feedback as\nthe TM becomes trained further. The proposed architecture is modular, allowing\nthe data input source to be easily changed, whilst inbuilt cross-validation\ninfrastructure allows for reliable and representative results during system\ntesting. We present use cases for online learning using the proposed\ninfrastructure and demonstrate the energy/performance/accuracy trade-offs."
            ],
            "interesting paper": 2
        }
    ],
    "Jordan Hernandez": [
        {
            "papers": [
                "Deep Learning and Ethics\nThis article appears as chapter 21 of Prince (2023, Understanding Deep\nLearning); a complete draft of the textbook is available here:\nhttp://udlbook.com. This chapter considers potential harms arising from the\ndesign and use of AI systems. These include algorithmic bias, lack of\nexplainability, data privacy violations, militarization, fraud, and\nenvironmental concerns. The aim is not to provide advice on being more ethical.\nInstead, the goal is to express ideas and start conversations in key areas that\nhave received attention in philosophy, political science, and the broader\nsocial sciences.",
                "Towards a Capability Assessment Model for the Comprehension and Adoption\n  of AI in Organisations\nThe comprehension and adoption of Artificial Intelligence (AI) are beset with\npractical and ethical problems. This article presents a 5-level AI Capability\nAssessment Model (AI-CAM) and a related AI Capabilities Matrix (AI-CM) to\nassist practitioners in AI comprehension and adoption. These practical tools\nwere developed with business executives, technologists, and other\norganisational stakeholders in mind. They are founded on a comprehensive\nconception of AI compared to those in other AI adoption models and are also\nopen-source artefacts. Thus, the AI-CAM and AI-CM present an accessible\nresource to help inform organisational decision-makers on the capability\nrequirements for (1) AI-based data analytics use cases based on machine\nlearning technologies; (2) Knowledge representation to engineer and represent\ndata, information and knowledge using semantic technologies; and (3) AI-based\nsolutions that seek to emulate human reasoning and decision-making. The AI-CAM\ncovers the core capability dimensions (business, data, technology,\norganisation, AI skills, risks, and ethical considerations) required at the\nfive capability maturity levels to achieve optimal use of AI in organisations.",
                "Trends and Challenges Towards an Effective Data-Driven Decision Making\n  in UK SMEs: Case Studies and Lessons Learnt from the Analysis of 85 SMEs\nThe adoption of data science brings vast benefits to Small and Medium-sized\nEnterprises (SMEs) including business productivity, economic growth, innovation\nand jobs creation. Data Science can support SMEs to optimise production\nprocesses, anticipate customers' needs, predict machinery failures and deliver\nefficient smart services. Businesses can also harness the power of Artificial\nIntelligence (AI) and Big Data and the smart use of digital technologies to\nenhance productivity and performance, paving the way for innovation. However,\nintegrating data science decisions into an SME requires both skills and IT\ninvestments. In most cases, such expenses are beyond the means of SMEs due to\nlimited resources and restricted access to financing. This paper presents\ntrends and challenges towards an effective data-driven decision making for\norganisations based on a case study of 85 SMEs, mostly from the West Midlands\nregion of England. The work is supported as part of a 3 years ERDF (European\nRegional Development Funded project) in the areas of big data management,\nanalytics and business intelligence. We present two case studies that\ndemonstrates the potential of Digitisation, AI and Machine Learning and use\nthese as examples to unveil challenges and showcase the wealth of current\navailable opportunities for SMEs.",
                "Applications of Intelligent Systems in Green Technology\nIntelligent Systems (ISs) are technologically advanced machines, which\nperceive and respond to the environment around them. They are usually of\nvarious forms ranging from software to hardware. ISs are generally the fusion\nof Artificial Intelligence (AI), robotics and Internet of things (IoT). In\norder to strengthen ISs, one of the key technologies is green technology (GT).\nIt refers to the continuously advancing methods and materials, which cover\ntechniques for producing energy to non-toxic cleaning products. It may also be\nbroadened to saving energy, and reducing toxic and waste materials in the\nenvironment. The motto of GT can be achieved by using the ISs. In this paper,\nwe present various applications of ISs in GT. Moreover, we discuss various\npossible solutions using ISs in order to overcome the on-going real-life\nproblems.",
                "Model evaluation for extreme risks\nCurrent approaches to building general-purpose AI systems tend to produce\nsystems with both beneficial and harmful capabilities. Further progress in AI\ndevelopment could lead to capabilities that pose extreme risks, such as\noffensive cyber capabilities or strong manipulation skills. We explain why\nmodel evaluation is critical for addressing extreme risks. Developers must be\nable to identify dangerous capabilities (through \"dangerous capability\nevaluations\") and the propensity of models to apply their capabilities for harm\n(through \"alignment evaluations\"). These evaluations will become critical for\nkeeping policymakers and other stakeholders informed, and for making\nresponsible decisions about model training, deployment, and security.",
                "Distributed Online Rollout for Multivehicle Routing in Unmapped\n  Environments\nIn this work we consider a generalization of the well-known multivehicle\nrouting problem: given a network, a set of agents occupying a subset of its\nnodes, and a set of tasks, we seek a minimum cost sequence of movements subject\nto the constraint that each task is visited by some agent at least once. The\nclassical version of this problem assumes a central computational server that\nobserves the entire state of the system perfectly and directs individual agents\naccording to a centralized control scheme. In contrast, we assume that there is\nno centralized server and that each agent is an individual processor with no a\npriori knowledge of the underlying network (including task and agent\nlocations). Moreover, our agents possess strictly local communication and\nsensing capabilities (restricted to a fixed radius around their respective\nlocations), aligning more closely with several real-world multiagent\napplications. These restrictions introduce many challenges that are overcome\nthrough local information sharing and direct coordination between agents. We\npresent a fully distributed, online, and scalable reinforcement learning\nalgorithm for this problem whereby agents self-organize into local clusters and\nindependently apply a multiagent rollout scheme locally to each cluster. We\ndemonstrate empirically via extensive simulations that there exists a critical\nsensing radius beyond which the distributed rollout algorithm begins to improve\nover a greedy base policy. This critical sensing radius grows proportionally to\nthe $\\log^*$ function of the size of the network, and is, therefore, a small\nconstant for any relevant network. Our decentralized reinforcement learning\nalgorithm achieves approximately a factor of two cost improvement over the base\npolicy for a range of radii bounded from below and above by two and three times\nthe critical sensing radius, respectively."
            ],
            "interesting paper": 3
        },
        {
            "papers": [
                "Alert of the Second Decision-maker: An Introduction to Human-AI Conflict\nThe collaboration between humans and artificial intelligence (AI) is a\nsignificant feature in this digital age. However, humans and AI may have\nobservation, interpretation, and action conflicts when working synchronously.\nThis phenomenon is often masked by faults and, unfortunately, overlooked. This\npaper systematically introduces the human-AI conflict concept, causes,\nmeasurement methods, and risk assessment. The results highlight that there is a\npotential second decision-maker besides the human, which is the AI; the\nhuman-AI conflict is a unique and emerging risk in digitalized process systems;\nand this is an interdisciplinary field that needs to be distinguished from\ntraditional fault and failure analysis; the conflict risk is significant and\ncannot be ignored.",
                "AI Techniques in the Microservices Life-Cycle: A Survey\nMicroservices is a popular architectural style for the development of\ndistributed software, with an emphasis on modularity, scalability, and\nflexibility. Indeed, in microservice systems, functionalities are provided by\nloosely coupled, small services, each focusing on a specific business\ncapability. Building a system according to the microservices architectural\nstyle brings a number of challenges, mainly related to how the different\nmicroservices are deployed and coordinated and how they interact. In this\npaper, we provide a survey about how techniques in the area of Artificial\nIntelligence have been used to tackle these challenges.",
                "Attention Paper: How Generative AI Reshapes Digital Shadow Industry?\nThe rapid development of digital economy has led to the emergence of various\nblack and shadow internet industries, which pose potential risks that can be\nidentified and managed through digital risk management (DRM) that uses\ndifferent techniques such as machine learning and deep learning. The evolution\nof DRM architecture has been driven by changes in data forms. However, the\ndevelopment of AI-generated content (AIGC) technology, such as ChatGPT and\nStable Diffusion, has given black and shadow industries powerful tools to\npersonalize data and generate realistic images and conversations for fraudulent\nactivities. This poses a challenge for DRM systems to control risks from the\nsource of data generation and to respond quickly to the fast-changing risk\nenvironment. This paper aims to provide a technical analysis of the challenges\nand opportunities of AIGC from upstream, midstream, and downstream paths of\nblack/shadow industries and suggest future directions for improving existing\nrisk control systems. The paper will explore the new black and shadow\ntechniques triggered by generative AI technology and provide insights for\nbuilding the next-generation DRM system.",
                "On Computing Universal Plans for Partially Observable Multi-Agent Path\n  Finding\nMulti-agent routing problems have drawn significant attention nowadays due to\ntheir broad industrial applications in, e.g., warehouse robots, logistics\nautomation, and traffic control. Conventionally, they are modelled as classical\nplanning problems. In this paper, we argue that it is beneficial to formulate\nthem as universal planning problems. We therefore propose universal plans, also\nknown as policies, as the solution concepts, and implement a system called\nASP-MAUPF (Answer Set Programming for Multi-Agent Universal Plan Finding) for\ncomputing them. Given an arbitrary two-dimensional map and a profile of goals\nfor the agents, the system finds a feasible universal plan for each agent that\nensures no collision with others. We use the system to conduct some\nexperiments, and make some observations on the types of goal profiles and\nenvironments that will have feasible policies, and how they may depend on\nagents' sensors. We also demonstrate how users can customize action preferences\nto compute more efficient policies, even (near-)optimal ones.",
                "Enhancing Human Capabilities through Symbiotic Artificial Intelligence\n  with Shared Sensory Experiences\nThe merging of human intelligence and artificial intelligence has long been a\nsubject of interest in both science fiction and academia. In this paper, we\nintroduce a novel concept in Human-AI interaction called Symbiotic Artificial\nIntelligence with Shared Sensory Experiences (SAISSE), which aims to establish\na mutually beneficial relationship between AI systems and human users through\nshared sensory experiences. By integrating multiple sensory input channels and\nprocessing human experiences, SAISSE fosters a strong human-AI bond, enabling\nAI systems to learn from and adapt to individual users, providing personalized\nsupport, assistance, and enhancement. Furthermore, we discuss the incorporation\nof memory storage units for long-term growth and development of both the AI\nsystem and its human user. As we address user privacy and ethical guidelines\nfor responsible AI-human symbiosis, we also explore potential biases and\ninequalities in AI-human symbiosis and propose strategies to mitigate these\nchallenges. Our research aims to provide a comprehensive understanding of the\nSAISSE concept and its potential to effectively support and enhance individual\nhuman users through symbiotic AI systems. This position article aims at\ndiscussing poteintial AI-human interaction related topics within the scientific\ncommunity, rather than providing experimental or theoretical results.",
                "Applying Interdisciplinary Frameworks to Understand Algorithmic\n  Decision-Making\nWe argue that explanations for \"algorithmic decision-making\" (ADM) systems\ncan profit by adopting practices that are already used in the learning\nsciences. We shortly introduce the importance of explaining ADM systems, give a\nbrief overview of approaches drawing from other disciplines to improve\nexplanations, and present the results of our qualitative task-based study\nincorporating the \"six facets of understanding\" framework. We close with\nquestions guiding the discussion of how future studies can leverage an\ninterdisciplinary approach."
            ],
            "interesting paper": 3
        },
        {
            "papers": [
                "Towards Cognitive Bots: Architectural Research Challenges\nSoftware bots operating in multiple virtual digital platforms must understand\nthe platforms' affordances and behave like human users. Platform affordances or\nfeatures differ from one application platform to another or through a life\ncycle, requiring such bots to be adaptable. Moreover, bots in such platforms\ncould cooperate with humans or other software agents for work or to learn\nspecific behavior patterns. However, present-day bots, particularly chatbots,\nother than language processing and prediction, are far from reaching a human\nuser's behavior level within complex business information systems. They lack\nthe cognitive capabilities to sense and act in such virtual environments,\nrendering their development a challenge to artificial general intelligence\nresearch. In this study, we problematize and investigate assumptions in\nconceptualizing software bot architecture by directing attention to significant\narchitectural research challenges in developing cognitive bots endowed with\ncomplex behavior for operation on information systems. As an outlook, we\npropose alternate architectural assumptions to consider in future bot design\nand bot development frameworks.",
                "How Do UX Practitioners Communicate AI as a Design Material? Artifacts,\n  Conceptions, and Propositions\nUX practitioners (UXPs) face novel challenges when working with and\ncommunicating artificial intelligence (AI) as a design material. We explore how\nUXPs communicate AI concepts when given hands-on experience training and\nexperimenting with AI models. To do so, we conducted a task-based design study\nwith 27 UXPs in which they prototyped and created a design presentation for a\nAI-enabled interface while having access to a simple AI model training tool.\nThrough analyzing UXPs' design presentations and post-activity interviews, we\nfound that although UXPs struggled to clearly communicate some AI concepts,\ntinkering with AI broadened common ground when communicating with technical\nstakeholders. UXPs also identified key risks and benefits of AI in their\ndesigns, and proposed concrete next steps for both UX and AI work. We conclude\nwith a sensitizing concept and recommendations for design and AI tools to\nenhance multi-stakeholder communication and collaboration when crafting\nhuman-centered AI experiences.",
                "Mindstorms in Natural Language-Based Societies of Mind\nBoth Minsky's \"society of mind\" and Schmidhuber's \"learning to think\" inspire\ndiverse societies of large multimodal neural networks (NNs) that solve problems\nby interviewing each other in a \"mindstorm.\" Recent implementations of NN-based\nsocieties of minds consist of large language models (LLMs) and other NN-based\nexperts communicating through a natural language interface. In doing so, they\novercome the limitations of single LLMs, improving multimodal zero-shot\nreasoning. In these natural language-based societies of mind (NLSOMs), new\nagents -- all communicating through the same universal symbolic language -- are\neasily added in a modular fashion. To demonstrate the power of NLSOMs, we\nassemble and experiment with several of them (having up to 129 members),\nleveraging mindstorms in them to solve some practical AI tasks: visual question\nanswering, image captioning, text-to-image synthesis, 3D generation, egocentric\nretrieval, embodied AI, and general language-based task solving. We view this\nas a starting point towards much larger NLSOMs with billions of agents-some of\nwhich may be humans. And with this emergence of great societies of\nheterogeneous minds, many new research questions have suddenly become paramount\nto the future of artificial intelligence. What should be the social structure\nof an NLSOM? What would be the (dis)advantages of having a monarchical rather\nthan a democratic structure? How can principles of NN economies be used to\nmaximize the total reward of a reinforcement learning NLSOM? In this work, we\nidentify, discuss, and try to answer some of these questions.",
                "MADiff: Offline Multi-agent Learning with Diffusion Models\nDiffusion model (DM) recently achieved huge success in various scenarios\nincluding offline reinforcement learning, where the diffusion planner learn to\ngenerate desired trajectories during online evaluations. However, despite the\neffectiveness in single-agent learning, it remains unclear how DMs can operate\nin multi-agent problems, where agents can hardly complete teamwork without good\ncoordination by independently modeling each agent's trajectories. In this\npaper, we propose MADiff, a novel generative multi-agent learning framework to\ntackle this problem. MADiff is realized with an attention-based diffusion model\nto model the complex coordination among behaviors of multiple agents. To the\nbest of our knowledge, MADiff is the first diffusion-based multi-agent learning\nframework, which behaves as both a decentralized policy and a centralized\ncontroller. During decentralized executions, MADiff simultaneously performs\nteammate modeling, and the centralized controller can also be applied in\nmulti-agent trajectory predictions. Our experiments show the superior\nperformance of MADiff compared to baseline algorithms in a wide range of\nmulti-agent learning tasks, which emphasizes the effectiveness of MADiff in\nmodeling complex multi-agent interactions. Our code is available at\nhttps://github.com/zbzhu99/madiff.",
                "Attention Schema in Neural Agents\nAttention has become a common ingredient in deep learning architectures. It\nadds a dynamical selection of information on top of the static selection of\ninformation supported by weights. In the same way, we can imagine a\nhigher-order informational filter built on top of attention: an Attention\nSchema (AS), namely, a descriptive and predictive model of attention. In\ncognitive neuroscience, Attention Schema Theory (AST) supports this idea of\ndistinguishing attention from AS. A strong prediction of this theory is that an\nagent can use its own AS to also infer the states of other agents' attention\nand consequently enhance coordination with other agents. As such, multi-agent\nreinforcement learning would be an ideal setting to experimentally test the\nvalidity of AST. We explore different ways in which attention and AS interact\nwith each other. Our preliminary results indicate that agents that implement\nthe AS as a recurrent internal control achieve the best performance. In\ngeneral, these exploratory experiments suggest that equipping artificial agents\nwith a model of attention can enhance their social intelligence.",
                "A Categorical Representation Language and Computational System for\n  Knowledge-Based Planning\nClassical planning representation languages based on first-order logic have\npreliminarily been used to model and solve robotic task planning problems.\nWider adoption of these representation languages, however, is hindered by the\nlimitations present when managing implicit world changes with concise action\nmodels. To address this problem, we propose an alternative approach to\nrepresenting and managing updates to world states during planning. Based on the\ncategory-theoretic concepts of $\\mathsf{C}$-sets and double-pushout rewriting\n(DPO), our proposed representation can effectively handle structured knowledge\nabout world states that support domain abstractions at all levels. It\nformalizes the semantics of predicates according to a user-provided ontology\nand preserves the semantics when transitioning between world states. This\nmethod provides a formal semantics for using knowledge graphs and relational\ndatabases to model world states and updates in planning. In this paper, we\nconceptually compare our category-theoretic representation with the classical\nplanning representation. We show that our proposed representation has\nadvantages over the classical representation in terms of handling implicit\npreconditions and effects, and provides a more structured framework in which to\nmodel and solve planning problems."
            ],
            "interesting paper": 3
        },
        {
            "papers": [
                "AI Coach Assist: An Automated Approach for Call Recommendation in\n  Contact Centers for Agent Coaching\nIn recent years, the utilization of Artificial Intelligence (AI) in the\ncontact center industry is on the rise. One area where AI can have a\nsignificant impact is in the coaching of contact center agents. By analyzing\ncall transcripts using Natural Language Processing (NLP) techniques, it would\nbe possible to quickly determine which calls are most relevant for coaching\npurposes. In this paper, we present AI Coach Assist, which leverages the\npre-trained transformer-based language models to determine whether a given call\nis coachable or not based on the quality assurance (QA) questions asked by the\ncontact center managers or supervisors. The system was trained and evaluated on\na large dataset collected from real-world contact centers and provides an\neffective way to recommend calls to the contact center managers that are more\nlikely to contain coachable moments. Our experimental findings demonstrate the\npotential of AI Coach Assist to improve the coaching process, resulting in\nenhancing the performance of contact center agents.",
                "MLOps: A Step Forward to Enterprise Machine Learning\nMachine Learning Operations (MLOps) is becoming a highly crucial part of\nbusinesses looking to capitalize on the benefits of AI and ML models. This\nresearch presents a detailed review of MLOps, its benefits, difficulties,\nevolutions, and important underlying technologies such as MLOps frameworks,\nDocker, GitHub actions, and Kubernetes. The MLOps workflow, which includes\nmodel design, deployment, and operations, is explained in detail along with the\nvarious tools necessary for both model and data exploration and deployment.\nThis article also puts light on the end-to-end production of ML projects using\nvarious maturity levels of automated pipelines, with the least at no automation\nat all and the highest with complete CI/CD and CT capabilities. Furthermore, a\ndetailed example of an enterprise-level MLOps project for an object detection\nservice is used to explain the workflow of the technology in a real-world\nscenario. For this purpose, a web application hosting a pre-trained model from\nTensorFlow 2 Model Zoo is packaged and deployed to the internet making sure\nthat the system is scalable, reliable, and optimized for deployment at an\nenterprise level.",
                "Modeling Dynamic Environments with Scene Graph Memory\nEmbodied AI agents that search for objects in large environments such as\nhouseholds often need to make efficient decisions by predicting object\nlocations based on partial information. We pose this as a new type of link\nprediction problem: link prediction on partially observable dynamic graphs. Our\ngraph is a representation of a scene in which rooms and objects are nodes, and\ntheir relationships are encoded in the edges; only parts of the changing graph\nare known to the agent at each timestep. This partial observability poses a\nchallenge to existing link prediction approaches, which we address. We propose\na novel state representation -- Scene Graph Memory (SGM) -- with captures the\nagent's accumulated set of observations, as well as a neural net architecture\ncalled a Node Edge Predictor (NEP) that extracts information from the SGM to\nsearch efficiently. We evaluate our method in the Dynamic House Simulator, a\nnew benchmark that creates diverse dynamic graphs following the semantic\npatterns typically seen at homes, and show that NEP can be trained to predict\nthe locations of objects in a variety of environments with diverse object\nmovement dynamics, outperforming baselines both in terms of new scene\nadaptability and overall accuracy. The codebase and more can be found at\nhttps://www.scenegraphmemory.com.",
                "Integrating Action Knowledge and LLMs for Task Planning and Situation\n  Handling in Open Worlds\nTask planning systems have been developed to help robots use human knowledge\n(about actions) to complete long-horizon tasks. Most of them have been\ndeveloped for \"closed worlds\" while assuming the robot is provided with\ncomplete world knowledge. However, the real world is generally open, and the\nrobots frequently encounter unforeseen situations that can potentially break\nthe planner's completeness. Could we leverage the recent advances on\npre-trained Large Language Models (LLMs) to enable classical planning systems\nto deal with novel situations?\n  This paper introduces a novel framework, called COWP, for open-world task\nplanning and situation handling. COWP dynamically augments the robot's action\nknowledge, including the preconditions and effects of actions, with\ntask-oriented commonsense knowledge. COWP embraces the openness from LLMs, and\nis grounded to specific domains via action knowledge. For systematic\nevaluations, we collected a dataset that includes 1,085 execution-time\nsituations. Each situation corresponds to a state instance wherein a robot is\npotentially unable to complete a task using a solution that normally works.\nExperimental results show that our approach outperforms competitive baselines\nfrom the literature in the success rate of service tasks. Additionally, we have\ndemonstrated COWP using a mobile manipulator. Supplementary materials are\navailable at: https://cowplanning.github.io/",
                "On the Value of Myopic Behavior in Policy Reuse\nLeveraging learned strategies in unfamiliar scenarios is fundamental to human\nintelligence. In reinforcement learning, rationally reusing the policies\nacquired from other tasks or human experts is critical for tackling problems\nthat are difficult to learn from scratch. In this work, we present a framework\ncalled Selective Myopic bEhavior Control~(SMEC), which results from the insight\nthat the short-term behaviors of prior policies are sharable across tasks. By\nevaluating the behaviors of prior policies via a hybrid value function\narchitecture, SMEC adaptively aggregates the sharable short-term behaviors of\nprior policies and the long-term behaviors of the task policy, leading to\ncoordinated decisions. Empirical results on a collection of manipulation and\nlocomotion tasks demonstrate that SMEC outperforms existing methods, and\nvalidate the ability of SMEC to leverage related prior policies.",
                "Assumption Generation for the Verification of Learning-Enabled\n  Autonomous Systems\nProviding safety guarantees for autonomous systems is difficult as these\nsystems operate in complex environments that require the use of\nlearning-enabled components, such as deep neural networks (DNNs) for visual\nperception. DNNs are hard to analyze due to their size (they can have thousands\nor millions of parameters), lack of formal specifications (DNNs are typically\nlearnt from labeled data, in the absence of any formal requirements), and\nsensitivity to small changes in the environment. We present an assume-guarantee\nstyle compositional approach for the formal verification of system-level safety\nproperties of such autonomous systems. Our insight is that we can analyze the\nsystem in the absence of the DNN perception components by automatically\nsynthesizing assumptions on the DNN behaviour that guarantee the satisfaction\nof the required safety properties. The synthesized assumptions are the weakest\nin the sense that they characterize the output sequences of all the possible\nDNNs that, plugged into the autonomous system, guarantee the required safety\nproperties. The assumptions can be leveraged as run-time monitors over a\ndeployed DNN to guarantee the safety of the overall system; they can also be\nmined to extract local specifications for use during training and testing of\nDNNs. We illustrate our approach on a case study taken from the autonomous\nairplanes domain that uses a complex DNN for perception."
            ],
            "interesting paper": 4
        },
        {
            "papers": [
                "Re-imagining health and well-being in low resource African settings\n  using an augmented AI system and a 3D digital twin\nThis paper discusses and explores the potential and relevance of recent\ndevelopments in artificial intelligence (AI) and digital twins for health and\nwell-being in low-resource African countries. We use the case of public health\nemergency response to disease outbreaks and epidemic control. There is\npotential to take advantage of the increasing availability of data and\ndigitization to develop advanced AI methods for analysis and prediction. Using\nan AI systems perspective, we review emerging trends in AI systems and digital\ntwins and propose an initial augmented AI system architecture to illustrate how\nan AI system can work with a 3D digital twin to address public health goals. We\nhighlight scientific knowledge discovery, continual learning, pragmatic\ninteroperability, and interactive explanation and decision-making as essential\nresearch challenges for AI systems and digital twins.",
                "Towards Autonomous and Safe Last-mile Deliveries with AI-augmented\n  Self-driving Delivery Robots\nIn addition to its crucial impact on customer satisfaction, last-mile\ndelivery (LMD) is notorious for being the most time-consuming and costly stage\nof the shipping process. Pressing environmental concerns combined with the\nrecent surge of e-commerce sales have sparked renewed interest in automation\nand electrification of last-mile logistics. To address the hurdles faced by\nexisting robotic couriers, this paper introduces a customer-centric and\nsafety-conscious LMD system for small urban communities based on AI-assisted\nautonomous delivery robots. The presented framework enables end-to-end\nautomation and optimization of the logistic process while catering for\nreal-world imposed operational uncertainties, clients' preferred time\nschedules, and safety of pedestrians. To this end, the integrated optimization\ncomponent is modeled as a robust variant of the Cumulative Capacitated Vehicle\nRouting Problem with Time Windows, where routes are constructed under uncertain\ntravel times with an objective to minimize the total latency of deliveries\n(i.e., the overall waiting time of customers, which can negatively affect their\nsatisfaction). We demonstrate the proposed LMD system's utility through\nreal-world trials in a university campus with a single robotic courier.\nImplementation aspects as well as the findings and practical insights gained\nfrom the deployment are discussed in detail. Lastly, we round up the\ncontributions with numerical simulations to investigate the scalability of the\ndeveloped mathematical formulation with respect to the number of robotic\nvehicles and customers.",
                "The Digital Divide in Process Safety: Quantitative Risk Analysis of\n  Human-AI Collaboration\nDigital technologies have dramatically accelerated the digital transformation\nin process industries, boosted new industrial applications, upgraded the\nproduction system, and enhanced operational efficiency. In contrast, the\nchallenges and gaps between human and artificial intelligence (AI) have become\nmore and more prominent, whereas the digital divide in process safety is\naggregating. The study attempts to address the following questions: (i)What is\nAI in the process safety context? (ii)What is the difference between AI and\nhumans in process safety? (iii)How do AI and humans collaborate in process\nsafety? (iv)What are the challenges and gaps in human-AI collaboration? (v)How\nto quantify the risk of human-AI collaboration in process safety? Qualitative\nrisk analysis based on brainstorming and literature review, and quantitative\nrisk analysis based on layer of protection analysis (LOPA) and Bayesian network\n(BN), were applied to explore and model. The importance of human reliability\nshould be stressed in the digital age, not usually to increase the reliability\nof AI, and human-centered AI design in process safety needs to be propagated.",
                "Towards a Technology-Driven Adaptive Decision Support System for\n  Integrated Pavement and Maintenance strategies (TDADSS-IPM): focus on risk\n  assessment framework for climate change adaptation\nDecision Support Systems for pavement and maintenance strategies have\ntraditionally been designed as silos led to local optimum systems. Moreover,\nsince big data usage didn't exist as result of Industry 4.0 as of today, DSSs\nwere not initially designed adaptive to the sources of uncertainties led to\nrigid decisions. Motivated by the vulnerability of the road assets to the\nclimate phenomena, this paper takes a visionary step towards introducing a\nTechnology-Driven Adaptive Decision Support System for Integrated Pavement and\nMaintenance activities called TDADSS-IPM. As part of such DSS, a bottom-up risk\nassessment model is met via Bayesian Belief Networks (BBN) to realize the\nactual condition of the Danish roads due to weather condition. Such model fills\nthe gaps in the knowledge domain and develops a platform that can be trained\nover time, and applied in real-time to the actual event.",
                "AI Audit: A Card Game to Reflect on Everyday AI Systems\nAn essential element of K-12 AI literacy is educating learners about the\nethical and societal implications of AI systems. Previous work in AI ethics\nliteracy have developed curriculum and classroom activities that engage\nlearners in reflecting on the ethical implications of AI systems and developing\nresponsible AI. There is little work in using game-based learning methods in AI\nliteracy. Games are known to be compelling media to teach children about\ncomplex STEM concepts. In this work, we developed a competitive card game for\nmiddle and high school students called \"AI Audit\" where they play as AI\nstart-up founders building novel AI-powered technology. Players can challenge\nother players with potential harms of their technology or defend their own\nbusinesses by features that mitigate these harms. The game mechanics reward\nsystems that are ethically developed or that take steps to mitigate potential\nharms. In this paper, we present the game design, teacher resources for\nclassroom deployment and early playtesting results. We discuss our reflections\nabout using games as teaching tools for AI literacy in K-12 classrooms.",
                "IoT based Personal Voice Assistant\nToday, technological advancement is increasing day by day. Earlier, there was\nonly a computer system in which we could only perform a few tasks. But now,\nmachine learning, artificial intelligence, deep learning, and a few more\ntechnologies have made computer systems so advanced that we can perform any\ntype of task. In this era of advancement, if people are still struggling to\ninteract using various input devices, then it's not worth it. For this reason,\nwe developed a voice assistant using Python that allows the user to run any\ntype of command in Linux without interaction with the keyboard. The main task\nof the voice assistant is to minimize the use of input devices like the\nkeyboard and mouse. It will also reduce hardware space and cost."
            ],
            "interesting paper": 2
        },
        {
            "papers": [
                "RE-centric Recommendations for the Development of Trustworthy(er)\n  Autonomous Systems\nComplying with the EU AI Act (AIA) guidelines while developing and\nimplementing AI systems will soon be mandatory within the EU. However,\npractitioners lack actionable instructions to operationalise ethics during AI\nsystems development. A literature review of different ethical guidelines\nrevealed inconsistencies in the principles addressed and the terminology used\nto describe them. Furthermore, requirements engineering (RE), which is\nidentified to foster trustworthiness in the AI development process from the\nearly stages was observed to be absent in a lot of frameworks that support the\ndevelopment of ethical and trustworthy AI. This incongruous phrasing combined\nwith a lack of concrete development practices makes trustworthy AI development\nharder. To address this concern, we formulated a comparison table for the\nterminology used and the coverage of the ethical AI principles in major ethical\nAI guidelines. We then examined the applicability of ethical AI development\nframeworks for performing effective RE during the development of trustworthy AI\nsystems. A tertiary review and meta-analysis of literature discussing ethical\nAI frameworks revealed their limitations when developing trustworthy AI. Based\non our findings, we propose recommendations to address such limitations during\nthe development of trustworthy AI.",
                "An Emergency Disposal Decision-making Method with Human--Machine\n  Collaboration\nRapid developments in artificial intelligence technology have led to unmanned\nsystems replacing human beings in many fields requiring high-precision\npredictions and decisions. In modern operational environments, all job plans\nare affected by emergency events such as equipment failures and resource\nshortages, making a quick resolution critical. The use of unmanned systems to\nassist decision-making can improve resolution efficiency, but their\ndecision-making is not interpretable and may make the wrong decisions. Current\nunmanned systems require human supervision and control. Based on this, we\npropose a collaborative human--machine method for resolving unplanned events\nusing two phases: task filtering and task scheduling. In the task filtering\nphase, we propose a human--machine collaborative decision-making algorithm for\ndynamic tasks. The GACRNN model is used to predict the state of the job nodes,\nlocate the key nodes, and generate a machine-predicted resolution task list. A\nhuman decision-maker supervises the list in real time and modifies and confirms\nthe machine-predicted list through the human--machine interface. In the task\nscheduling phase, we propose a scheduling algorithm that integrates human\nexperience constraints. The steps to resolve an event are inserted into the\nnormal job sequence to schedule the resolution. We propose several\nhuman--machine collaboration methods in each phase to generate steps to resolve\nan unplanned event while minimizing the impact on the original job plan.",
                "Towards a Unifying Model of Rationality in Multiagent Systems\nMultiagent systems deployed in the real world need to cooperate with other\nagents (including humans) nearly as effectively as these agents cooperate with\none another. To design such AI, and provide guarantees of its effectiveness, we\nneed to clearly specify what types of agents our AI must be able to cooperate\nwith. In this work we propose a generic model of socially intelligent agents,\nwhich are individually rational learners that are also able to cooperate with\none another (in the sense that their joint behavior is Pareto efficient). We\ndefine rationality in terms of the regret incurred by each agent over its\nlifetime, and show how we can construct socially intelligent agents for\ndifferent forms of regret. We then discuss the implications of this model for\nthe development of \"robust\" MAS that can cooperate with a wide variety of\nsocially intelligent agents.",
                "Development of a ROS-based Architecture for Intelligent Autonomous on\n  Demand Last Mile Delivery\nThis paper presents the development of the JKU-ITS Last Mile Delivery Robot.\nThe proposed approach utilizes a combination of one 3D LIDAR, RGB-D camera, IMU\nand GPS sensor on top of a mobile robot slope mower. An embedded computer,\nrunning ROS1, is utilized to process the sensor data streams to enable 2D and\n3D Simultaneous Localization and Mapping, 2D localization and object detection\nusing a convolutional neural network.",
                "Graph Neural Network for spatiotemporal data: methods and applications\nIn the era of big data, there has been a surge in the availability of data\ncontaining rich spatial and temporal information, offering valuable insights\ninto dynamic systems and processes for applications such as weather\nforecasting, natural disaster management, intelligent transport systems, and\nprecision agriculture. Graph neural networks (GNNs) have emerged as a powerful\ntool for modeling and understanding data with dependencies to each other such\nas spatial and temporal dependencies. There is a large amount of existing work\nthat focuses on addressing the complex spatial and temporal dependencies in\nspatiotemporal data using GNNs. However, the strong interdisciplinary nature of\nspatiotemporal data has created numerous GNNs variants specifically designed\nfor distinct application domains. Although the techniques are generally\napplicable across various domains, cross-referencing these methods remains\nessential yet challenging due to the absence of a comprehensive literature\nreview on GNNs for spatiotemporal data. This article aims to provide a\nsystematic and comprehensive overview of the technologies and applications of\nGNNs in the spatiotemporal domain. First, the ways of constructing graphs from\nspatiotemporal data are summarized to help domain experts understand how to\ngenerate graphs from various types of spatiotemporal data. Then, a systematic\ncategorization and summary of existing spatiotemporal GNNs are presented to\nenable domain experts to identify suitable techniques and to support model\ndevelopers in advancing their research. Moreover, a comprehensive overview of\nsignificant applications in the spatiotemporal domain is offered to introduce a\nbroader range of applications to model developers and domain experts, assisting\nthem in exploring potential research topics and enhancing the impact of their\nwork. Finally, open challenges and future directions are discussed.",
                "Generalized Disparate Impact for Configurable Fairness Solutions in ML\nWe make two contributions in the field of AI fairness over continuous\nprotected attributes. First, we show that the Hirschfeld-Gebelein-Renyi (HGR)\nindicator (the only one currently available for such a case) is valuable but\nsubject to a few crucial limitations regarding semantics, interpretability, and\nrobustness. Second, we introduce a family of indicators that are: 1)\ncomplementary to HGR in terms of semantics; 2) fully interpretable and\ntransparent; 3) robust over finite samples; 4) configurable to suit specific\napplications. Our approach also allows us to define fine-grained constraints to\npermit certain types of dependence and forbid others selectively. By expanding\nthe available options for continuous protected attributes, our approach\nrepresents a significant contribution to the area of fair artificial\nintelligence."
            ],
            "interesting paper": 5
        },
        {
            "papers": [
                "Intent-aligned AI systems deplete human agency: the need for agency\n  foundations research in AI safety\nThe rapid advancement of artificial intelligence (AI) systems suggests that\nartificial general intelligence (AGI) systems may soon arrive. Many researchers\nare concerned that AIs and AGIs will harm humans via intentional misuse\n(AI-misuse) or through accidents (AI-accidents). In respect of AI-accidents,\nthere is an increasing effort focused on developing algorithms and paradigms\nthat ensure AI systems are aligned to what humans intend, e.g. AI systems that\nyield actions or recommendations that humans might judge as consistent with\ntheir intentions and goals. Here we argue that alignment to human intent is\ninsufficient for safe AI systems and that preservation of long-term agency of\nhumans may be a more robust standard, and one that needs to be separated\nexplicitly and a priori during optimization. We argue that AI systems can\nreshape human intention and discuss the lack of biological and psychological\nmechanisms that protect humans from loss of agency. We provide the first formal\ndefinition of agency-preserving AI-human interactions which focuses on\nforward-looking agency evaluations and argue that AI systems - not humans -\nmust be increasingly tasked with making these evaluations. We show how agency\nloss can occur in simple environments containing embedded agents that use\ntemporal-difference learning to make action recommendations. Finally, we\npropose a new area of research called \"agency foundations\" and pose four\ninitial topics designed to improve our understanding of agency in AI-human\ninteractions: benevolent game theory, algorithmic foundations of human rights,\nmechanistic interpretability of agency representation in neural-networks and\nreinforcement learning from internal states.",
                "Disentangling and Operationalizing AI Fairness at LinkedIn\nOperationalizing AI fairness at LinkedIn's scale is challenging not only\nbecause there are multiple mutually incompatible definitions of fairness but\nalso because determining what is fair depends on the specifics and context of\nthe product where AI is deployed. Moreover, AI practitioners need clarity on\nwhat fairness expectations need to be addressed at the AI level. In this paper,\nwe present the evolving AI fairness framework used at LinkedIn to address these\nthree challenges. The framework disentangles AI fairness by separating out\nequal treatment and equitable product expectations. Rather than imposing a\ntrade-off between these two commonly opposing interpretations of fairness, the\nframework provides clear guidelines for operationalizing equal AI treatment\ncomplemented with a product equity strategy. This paper focuses on the equal AI\ntreatment component of LinkedIn's AI fairness framework, shares the principles\nthat support it, and illustrates their application through a case study. We\nhope this paper will encourage other big tech companies to join us in sharing\ntheir approach to operationalizing AI fairness at scale, so that together we\ncan keep advancing this constantly evolving field.",
                "Evaluating GPT's Programming Capability through CodeWars' Katas\nIn the burgeoning field of artificial intelligence (AI), understanding the\ncapabilities and limitations of programming-oriented models is crucial. This\npaper presents a novel evaluation of the programming proficiency of Generative\nPretrained Transformer (GPT) models, specifically GPT-3.5 and GPT-4, against\ncoding problems of varying difficulty levels drawn from Codewars. The\nexperiments reveal a distinct boundary at the 3kyu level, beyond which these\nGPT models struggle to provide solutions. These findings led to the proposal of\na measure for coding problem complexity that incorporates both problem\ndifficulty and the time required for solution. The research emphasizes the need\nfor validation and creative thinking capabilities in AI models to better\nemulate human problem-solving techniques. Future work aims to refine this\nproposed complexity measure, enhance AI models with these suggested\ncapabilities, and develop an objective measure for programming problem\ndifficulty. The results of this research offer invaluable insights for\nimproving AI programming capabilities and advancing the frontier of AI\nproblem-solving abilities.",
                "Traffic Prediction using Artificial Intelligence: Review of Recent\n  Advances and Emerging Opportunities\nTraffic prediction plays a crucial role in alleviating traffic congestion\nwhich represents a critical problem globally, resulting in negative\nconsequences such as lost hours of additional travel time and increased fuel\nconsumption. Integrating emerging technologies into transportation systems\nprovides opportunities for improving traffic prediction significantly and\nbrings about new research problems. In order to lay the foundation for\nunderstanding the open research challenges in traffic prediction, this survey\naims to provide a comprehensive overview of traffic prediction methodologies.\nSpecifically, we focus on the recent advances and emerging research\nopportunities in Artificial Intelligence (AI)-based traffic prediction methods,\ndue to their recent success and potential in traffic prediction, with an\nemphasis on multivariate traffic time series modeling. We first provide a list\nand explanation of the various data types and resources used in the literature.\nNext, the essential data preprocessing methods within the traffic prediction\ncontext are categorized, and the prediction methods and applications are\nsubsequently summarized. Lastly, we present primary research challenges in\ntraffic prediction and discuss some directions for future research.",
                "Data and Knowledge for Overtaking Scenarios in Autonomous Driving\nAutonomous driving has become one of the most popular research topics within\nArtificial Intelligence. An autonomous vehicle is understood as a system that\ncombines perception, decision-making, planning, and control. All of those tasks\nrequire that the vehicle collects surrounding data in order to make a good\ndecision and action. In particular, the overtaking maneuver is one of the most\ncritical actions of driving. The process involves lane changes, acceleration\nand deceleration actions, and estimation of the speed and distance of the\nvehicle in front or in the lane in which it is moving. Despite the amount of\nwork available in the literature, just a few handle overtaking maneuvers and,\nbecause overtaking can be risky, no real-world dataset is available. This work\ncontributes in this area by presenting a new synthetic dataset whose focus is\nthe overtaking maneuver. We start by performing a thorough review of the state\nof the art in autonomous driving and then explore the main datasets found in\nthe literature (public and private, synthetic and real), highlighting their\nlimitations, and suggesting a new set of features whose focus is the overtaking\nmaneuver.",
                "Bigger, Better, Faster: Human-level Atari with human-level efficiency\nWe introduce a value-based RL agent, which we call BBF, that achieves\nsuper-human performance in the Atari 100K benchmark. BBF relies on scaling the\nneural networks used for value estimation, as well as a number of other design\nchoices that enable this scaling in a sample-efficient manner. We conduct\nextensive analyses of these design choices and provide insights for future\nwork. We end with a discussion about updating the goalposts for\nsample-efficient RL research on the ALE. We make our code and data publicly\navailable at\nhttps://github.com/google-research/google-research/tree/master/bigger_better_faster."
            ],
            "interesting paper": 3
        },
        {
            "papers": [
                "Citizen Perspectives on Necessary Safeguards to the Use of AI by Law\n  Enforcement Agencies\nIn the light of modern technological advances, Artificial Intelligence (AI)\nis relied upon to enhance performance, increase efficiency, and maximize gains.\nFor Law Enforcement Agencies (LEAs), it can prove valuable in optimizing\nevidence analysis and establishing proactive prevention measures. Nevertheless,\ncitizens raise legitimate concerns around privacy invasions, biases,\ninequalities, and inaccurate decisions. This study explores the views of 111\ncitizens towards AI use by police through interviews, and integrates societal\nconcerns along with propositions of safeguards from negative effects of AI use\nby LEAs in the context of cybercrime and terrorism.",
                "Traffic Road Congestion System using by the internet of vehicles (IoV)\nTraffic problems have increased in modern life due to a huge number of\nvehicles, big cities, and ignoring the traffic rules. Vehicular ad hoc network\n(VANET) has improved the traffic system in previous some and plays a vital role\nin the best traffic control system in big cities. But due to some limitations,\nit is not enough to control some problems in specific conditions. Now a day\ninvention of new technologies of the Internet of Things (IoT) is used for\ncollaboratively and efficiently performing tasks. This technology was also\nintroduced in the transportation system which makes it an intelligent\ntransportation system (ITS), this is called the Internet of vehicles (IOV). We\nwill elaborate on traffic problems in the traditional system and elaborate on\nthe benefits, enhancements, and reasons to better IOV by Systematic Literature\nReview (SLR). This technique will be implemented by targeting needed papers\nthrough many search phrases. A systematic literature review is used for 121\narticles between 2014 and 2023. The IoV technologies and tools are required to\ncreate the IoV and resolve some traffic rules through SUMO (simulation of urban\nmobility) which is used for the design and simulation the road traffic. We have\ntried to contribute to the best model of the traffic control system. This paper\nwill analysis two vehicular congestion control models in term of select the\noptimized and efficient model and elaborate on the reasons for efficiency by\nsearching the solution SLR based questions. Due to some efficient features, we\nhave suggested the IOV based on vehicular clouds. These efficient features make\nthis model the best and most effective than the traditional model which is a\ngreat reason to enhance the network system.",
                "Responsible Design Patterns for Machine Learning Pipelines\nIntegrating ethical practices into the AI development process for artificial\nintelligence (AI) is essential to ensure safe, fair, and responsible operation.\nAI ethics involves applying ethical principles to the entire life cycle of AI\nsystems. This is essential to mitigate potential risks and harms associated\nwith AI, such as algorithm biases. To achieve this goal, responsible design\npatterns (RDPs) are critical for Machine Learning (ML) pipelines to guarantee\nethical and fair outcomes. In this paper, we propose a comprehensive framework\nincorporating RDPs into ML pipelines to mitigate risks and ensure the ethical\ndevelopment of AI systems. Our framework comprises new responsible AI design\npatterns for ML pipelines identified through a survey of AI ethics and data\nmanagement experts and validated through real-world scenarios with expert\nfeedback. The framework guides AI developers, data scientists, and\npolicy-makers to implement ethical practices in AI development and deploy\nresponsible AI systems in production.",
                "Human or Not? A Gamified Approach to the Turing Test\nWe present \"Human or Not?\", an online game inspired by the Turing test, that\nmeasures the capability of AI chatbots to mimic humans in dialog, and of humans\nto tell bots from other humans. Over the course of a month, the game was played\nby over 1.5 million users who engaged in anonymous two-minute chat sessions\nwith either another human or an AI language model which was prompted to behave\nlike humans. The task of the players was to correctly guess whether they spoke\nto a person or to an AI. This largest scale Turing-style test conducted to date\nrevealed some interesting facts. For example, overall users guessed the\nidentity of their partners correctly in only 68% of the games. In the subset of\nthe games in which users faced an AI bot, users had even lower correct guess\nrates of 60% (that is, not much higher than chance). This white paper details\nthe development, deployment, and results of this unique experiment. While this\nexperiment calls for many extensions and refinements, these findings already\nbegin to shed light on the inevitable near future which will commingle humans\nand AI.",
                "Human Control: Definitions and Algorithms\nHow can humans stay in control of advanced artificial intelligence systems?\nOne proposal is corrigibility, which requires the agent to follow the\ninstructions of a human overseer, without inappropriately influencing them. In\nthis paper, we formally define a variant of corrigibility called shutdown\ninstructability, and show that it implies appropriate shutdown behavior,\nretention of human autonomy, and avoidance of user harm. We also analyse the\nrelated concepts of non-obstruction and shutdown alignment, three previously\nproposed algorithms for human control, and one new algorithm.",
                "From Human-Centered to Social-Centered Artificial Intelligence:\n  Assessing ChatGPT's Impact through Disruptive Events\nLarge language models (LLMs) and dialogue agents have existed for years, but\nthe release of recent GPT models has been a watershed moment for artificial\nintelligence (AI) research and society at large. Immediately recognized for its\ngenerative capabilities and versatility, ChatGPT's impressive proficiency\nacross technical and creative domains led to its widespread adoption. While\nsociety grapples with the emerging cultural impacts of ChatGPT, critiques of\nChatGPT's impact within the machine learning community have coalesced around\nits performance or other conventional Responsible AI evaluations relating to\nbias, toxicity, and 'hallucination.' We argue that these latter critiques draw\nheavily on a particular conceptualization of the 'human-centered' framework,\nwhich tends to cast atomized individuals as the key recipients of both the\nbenefits and detriments of technology. In this article, we direct attention to\nanother dimension of LLMs and dialogue agents' impact: their effect on social\ngroups, institutions, and accompanying norms and practices. By illustrating\nChatGPT's social impact through three disruptive events, we challenge\nindividualistic approaches in AI development and contribute to ongoing debates\naround the ethical and responsible implementation of AI systems. We hope this\neffort will call attention to more comprehensive and longitudinal evaluation\ntools and compel technologists to go beyond human-centered thinking and ground\ntheir efforts through social-centered AI."
            ],
            "interesting paper": 5
        },
        {
            "papers": [
                "AI Liability Insurance With an Example in AI-Powered E-diagnosis System\nArtificial Intelligence (AI) has received an increasing amount of attention\nin multiple areas. The uncertainties and risks in AI-powered systems have\ncreated reluctance in their wild adoption. As an economic solution to\ncompensate for potential damages, AI liability insurance is a promising market\nto enhance the integration of AI into daily life. In this work, we use an\nAI-powered E-diagnosis system as an example to study AI liability insurance. We\nprovide a quantitative risk assessment model with evidence-based numerical\nanalysis. We discuss the insurability criteria for AI technologies and suggest\nnecessary adjustments to accommodate the features of AI products. We show that\nAI liability insurance can act as a regulatory mechanism to incentivize\ncompliant behaviors and serve as a certificate of high-quality AI systems.\nFurthermore, we suggest premium adjustment to reflect the dynamic evolution of\nthe inherent uncertainty in AI. Moral hazard problems are discussed and\nsuggestions for AI liability insurance are provided.",
                "AI and the creative realm: A short review of current and future\n  applications\nThis study explores the concept of creativity and artificial intelligence\n(AI) and their recent integration. While AI has traditionally been perceived as\nincapable of generating new ideas or creating art, the development of more\nsophisticated AI models and the proliferation of human-computer interaction\ntools have opened up new possibilities for AI in artistic creation. This study\ninvestigates the various applications of AI in a creative context,\ndifferentiating between the type of art, language, and algorithms used. It also\nconsiders the philosophical implications of AI and creativity, questioning\nwhether consciousness can be researched in machines and AI's potential\ninterests and decision-making capabilities. Overall, we aim to stimulate a\nreflection on AI's use and ethical implications in creative contexts.",
                "Recent Advances in Graph-based Machine Learning for Applications in\n  Smart Urban Transportation Systems\nThe Intelligent Transportation System (ITS) is an important part of modern\ntransportation infrastructure, employing a combination of communication\ntechnology, information processing and control systems to manage transportation\nnetworks. This integration of various components such as roads, vehicles, and\ncommunication systems, is expected to improve efficiency and safety by\nproviding better information, services, and coordination of transportation\nmodes. In recent years, graph-based machine learning has become an increasingly\nimportant research focus in the field of ITS aiming at the development of\ncomplex, data-driven solutions to address various ITS-related challenges. This\nchapter presents background information on the key technical challenges for ITS\ndesign, along with a review of research methods ranging from classic\nstatistical approaches to modern machine learning and deep learning-based\napproaches. Specifically, we provide an in-depth review of graph-based machine\nlearning methods, including basic concepts of graphs, graph data\nrepresentation, graph neural network architectures and their relation to ITS\napplications. Additionally, two case studies of graph-based ITS applications\nproposed in our recent work are presented in detail to demonstrate the\npotential of graph-based machine learning in the ITS domain.",
                "An Overview on Generative AI at Scale with Edge-Cloud Computing\nAs a specific category of artificial intelligence (AI), generative artificial\nintelligence (GenAI) generates new content that resembles what is created by\nhumans. The rapid development of GenAI systems has created a huge amount of new\ndata on the Internet, posing new challenges to current computing and\ncommunication frameworks. Currently, GenAI services rely on the traditional\ncloud computing framework due to the need for large computation resources.\nHowever, such services will encounter high latency because of data transmission\nand a high volume of requests. On the other hand, edge-cloud computing can\nprovide adequate computation power and low latency at the same time through the\ncollaboration between edges and the cloud. Thus, it is attractive to build\nGenAI systems at scale by leveraging the edge-cloud computing paradigm. In this\noverview paper, we review recent developments in GenAI and edge-cloud\ncomputing, respectively. Then, we use two exemplary GenAI applications to\ndiscuss technical challenges in scaling up their solutions using edge-cloud\ncollaborative systems. Finally, we list design considerations for training and\ndeploying GenAI systems at scale and point out future research directions.",
                "The ethical ambiguity of AI data enrichment: Measuring gaps in research\n  ethics norms and practices\nThe technical progression of artificial intelligence (AI) research has been\nbuilt on breakthroughs in fields such as computer science, statistics, and\nmathematics. However, in the past decade AI researchers have increasingly\nlooked to the social sciences, turning to human interactions to solve the\nchallenges of model development. Paying crowdsourcing workers to generate or\ncurate data, or data enrichment, has become indispensable for many areas of AI\nresearch, from natural language processing to reinforcement learning from human\nfeedback (RLHF). Other fields that routinely interact with crowdsourcing\nworkers, such as Psychology, have developed common governance requirements and\nnorms to ensure research is undertaken ethically. This study explores how, and\nto what extent, comparable research ethics requirements and norms have\ndeveloped for AI research and data enrichment. We focus on the approach taken\nby two leading conferences: ICLR and NeurIPS, and journal publisher Springer.\nIn a longitudinal study of accepted papers, and via a comparison with\nPsychology and CHI papers, this work finds that leading AI venues have begun to\nestablish protocols for human data collection, but these are are inconsistently\nfollowed by authors. Whilst Psychology papers engaging with crowdsourcing\nworkers frequently disclose ethics reviews, payment data, demographic data and\nother information, similar disclosures are far less common in leading AI venues\ndespite similar guidance. The work concludes with hypotheses to explain these\ngaps in research ethics practices and considerations for its implications.",
                "Integrated Sensing-Communication-Computation for Edge Artificial\n  Intelligence\nEdge artificial intelligence (AI) has been a promising solution towards 6G to\nempower a series of advanced techniques such as digital twins, holographic\nprojection, semantic communications, and auto-driving, for achieving\nintelligence of everything. The performance of edge AI tasks, including edge\nlearning and edge AI inference, depends on the quality of three highly coupled\nprocesses, i.e., sensing for data acquisition, computation for information\nextraction, and communication for information transmission. However, these\nthree modules need to compete for network resources for enhancing their own\nquality-of-services. To this end, integrated sensing-communication-computation\n(ISCC) is of paramount significance for improving resource utilization as well\nas achieving the customized goals of edge AI tasks. By investigating the\ninterplay among the three modules, this article presents various kinds of ISCC\nschemes for federated edge learning tasks and edge AI inference tasks in both\napplication and physical layers."
            ],
            "interesting paper": 3
        }
    ],
    "Casey Lee": [
        {
            "papers": [
                "Deep Learning and Ethics\nThis article appears as chapter 21 of Prince (2023, Understanding Deep\nLearning); a complete draft of the textbook is available here:\nhttp://udlbook.com. This chapter considers potential harms arising from the\ndesign and use of AI systems. These include algorithmic bias, lack of\nexplainability, data privacy violations, militarization, fraud, and\nenvironmental concerns. The aim is not to provide advice on being more ethical.\nInstead, the goal is to express ideas and start conversations in key areas that\nhave received attention in philosophy, political science, and the broader\nsocial sciences.",
                "Applications of Intelligent Systems in Green Technology\nIntelligent Systems (ISs) are technologically advanced machines, which\nperceive and respond to the environment around them. They are usually of\nvarious forms ranging from software to hardware. ISs are generally the fusion\nof Artificial Intelligence (AI), robotics and Internet of things (IoT). In\norder to strengthen ISs, one of the key technologies is green technology (GT).\nIt refers to the continuously advancing methods and materials, which cover\ntechniques for producing energy to non-toxic cleaning products. It may also be\nbroadened to saving energy, and reducing toxic and waste materials in the\nenvironment. The motto of GT can be achieved by using the ISs. In this paper,\nwe present various applications of ISs in GT. Moreover, we discuss various\npossible solutions using ISs in order to overcome the on-going real-life\nproblems.",
                "Symbolic Regression via Control Variable Genetic Programming\nLearning symbolic expressions directly from experiment data is a vital step\nin AI-driven scientific discovery. Nevertheless, state-of-the-art approaches\nare limited to learning simple expressions. Regressing expressions involving\nmany independent variables still remain out of reach. Motivated by the control\nvariable experiments widely utilized in science, we propose Control Variable\nGenetic Programming (CVGP) for symbolic regression over many independent\nvariables. CVGP expedites symbolic expression discovery via customized\nexperiment design, rather than learning from a fixed dataset collected a\npriori. CVGP starts by fitting simple expressions involving a small set of\nindependent variables using genetic programming, under controlled experiments\nwhere other variables are held as constants. It then extends expressions\nlearned in previous generations by adding new independent variables, using new\ncontrol variable experiments in which these variables are allowed to vary.\nTheoretically, we show CVGP as an incremental building approach can yield an\nexponential reduction in the search space when learning a class of expressions.\nExperimentally, CVGP outperforms several baselines in learning symbolic\nexpressions involving multiple independent variables.",
                "TLNets: Transformation Learning Networks for long-range time-series\n  prediction\nTime series prediction is a prevalent issue across various disciplines, such\nas meteorology, traffic surveillance, investment, and energy production and\nconsumption. Many statistical and machine-learning strategies have been\ndeveloped to tackle this problem. However, these approaches either lack\nexplainability or exhibit less satisfactory performance when the prediction\nhorizon increases. To this end, we propose a novel plan for the designing of\nnetworks' architecture based on transformations, possessing the potential to\nachieve an enhanced receptive field in learning which brings benefits to fuse\nfeatures across scales. In this context, we introduce four different\ntransformation mechanisms as bases to construct the learning model including\nFourier Transform (FT), Singular Value Decomposition (SVD), matrix\nmultiplication and Conv block. Hence, we develop four learning models based on\nthe above building blocks, namely, FT-Matrix, FT-SVD, FT-Conv, and Conv-SVD.\nNote that the FT and SVD blocks are capable of learning global information,\nwhile the Conv blocks focus on learning local information. The matrix block is\nsparsely designed to learn both global and local information simultaneously.\nThe above Transformation Learning Networks (TLNets) have been extensively\ntested and compared with multiple baseline models based on several real-world\ndatasets and showed clear potential in long-range time-series forecasting.",
                "Lucy-SKG: Learning to Play Rocket League Efficiently Using Deep\n  Reinforcement Learning\nA successful tactic that is followed by the scientific community for\nadvancing AI is to treat games as problems, which has been proven to lead to\nvarious breakthroughs. We adapt this strategy in order to study Rocket League,\na widely popular but rather under-explored 3D multiplayer video game with a\ndistinct physics engine and complex dynamics that pose a significant challenge\nin developing efficient and high-performance game-playing agents. In this\npaper, we present Lucy-SKG, a Reinforcement Learning-based model that learned\nhow to play Rocket League in a sample-efficient manner, outperforming by a\nnotable margin the two highest-ranking bots in this game, namely Necto (2022\nbot champion) and its successor Nexto, thus becoming a state-of-the-art agent.\nOur contributions include: a) the development of a reward analysis and\nvisualization library, b) novel parameterizable reward shape functions that\ncapture the utility of complex reward types via our proposed Kinesthetic Reward\nCombination (KRC) technique, and c) design of auxiliary neural architectures\nfor training on reward prediction and state representation tasks in an\non-policy fashion for enhanced efficiency in learning speed and performance. By\nperforming thorough ablation studies for each component of Lucy-SKG, we showed\ntheir independent effectiveness in overall performance. In doing so, we\ndemonstrate the prospects and challenges of using sample-efficient\nReinforcement Learning techniques for controlling complex dynamical systems\nunder competitive team-based multiplayer conditions.",
                "Science in the Era of ChatGPT, Large Language Models and Generative AI:\n  Challenges for Research Ethics and How to Respond\nLarge language models of artificial intelligence (AI), such as ChatGPT, find\nremarkable but controversial applicability in science and research. This paper\nreviews epistemological challenges, ethical and integrity risks in science\nconduct in the advent of generative AI. This is with the aim to lay new timely\nfoundations for a high-quality research ethics review. The role of AI language\nmodels as a research instrument and subject is scrutinized along with ethical\nimplications for scientists, participants and reviewers. New emerging practices\nfor research ethics review are discussed, concluding with ten recommendations\nthat shape a response for a more responsible research conduct in the era of AI."
            ],
            "interesting paper": 2
        },
        {
            "papers": [
                "Alert of the Second Decision-maker: An Introduction to Human-AI Conflict\nThe collaboration between humans and artificial intelligence (AI) is a\nsignificant feature in this digital age. However, humans and AI may have\nobservation, interpretation, and action conflicts when working synchronously.\nThis phenomenon is often masked by faults and, unfortunately, overlooked. This\npaper systematically introduces the human-AI conflict concept, causes,\nmeasurement methods, and risk assessment. The results highlight that there is a\npotential second decision-maker besides the human, which is the AI; the\nhuman-AI conflict is a unique and emerging risk in digitalized process systems;\nand this is an interdisciplinary field that needs to be distinguished from\ntraditional fault and failure analysis; the conflict risk is significant and\ncannot be ignored.",
                "Attention Paper: How Generative AI Reshapes Digital Shadow Industry?\nThe rapid development of digital economy has led to the emergence of various\nblack and shadow internet industries, which pose potential risks that can be\nidentified and managed through digital risk management (DRM) that uses\ndifferent techniques such as machine learning and deep learning. The evolution\nof DRM architecture has been driven by changes in data forms. However, the\ndevelopment of AI-generated content (AIGC) technology, such as ChatGPT and\nStable Diffusion, has given black and shadow industries powerful tools to\npersonalize data and generate realistic images and conversations for fraudulent\nactivities. This poses a challenge for DRM systems to control risks from the\nsource of data generation and to respond quickly to the fast-changing risk\nenvironment. This paper aims to provide a technical analysis of the challenges\nand opportunities of AIGC from upstream, midstream, and downstream paths of\nblack/shadow industries and suggest future directions for improving existing\nrisk control systems. The paper will explore the new black and shadow\ntechniques triggered by generative AI technology and provide insights for\nbuilding the next-generation DRM system.",
                "AdaPlanner: Adaptive Planning from Feedback with Language Models\nLarge language models (LLMs) have recently demonstrated the potential in\nacting as autonomous agents for sequential decision-making tasks. However, most\nexisting methods either take actions greedily without planning or rely on\nstatic plans that are not adaptable to environmental feedback. Consequently,\nthe sequential decision-making performance of LLM agents degenerates with\nproblem complexity and plan horizons increase. We propose a closed-loop\napproach, AdaPlanner, which allows the LLM agent to refine its self-generated\nplan adaptively in response to environmental feedback. In AdaPlanner, the LLM\nagent adaptively refines its plan from feedback with both in-plan and\nout-of-plan refinement strategies. To mitigate hallucination, we develop a\ncode-style LLM prompt structure that facilitates plan generation across a\nvariety of tasks, environments, and agent capabilities. Furthermore, we propose\na skill discovery mechanism that leverages successful plans as few-shot\nexemplars, enabling the agent to plan and refine with fewer task\ndemonstrations. Our experiments in the ALFWorld and MiniWoB++ environments\ndemonstrate that AdaPlanner outperforms state-of-the-art baselines by 3.73% and\n4.11% while utilizing 2x and 600x fewer samples, respectively.",
                "Enhancing Human Capabilities through Symbiotic Artificial Intelligence\n  with Shared Sensory Experiences\nThe merging of human intelligence and artificial intelligence has long been a\nsubject of interest in both science fiction and academia. In this paper, we\nintroduce a novel concept in Human-AI interaction called Symbiotic Artificial\nIntelligence with Shared Sensory Experiences (SAISSE), which aims to establish\na mutually beneficial relationship between AI systems and human users through\nshared sensory experiences. By integrating multiple sensory input channels and\nprocessing human experiences, SAISSE fosters a strong human-AI bond, enabling\nAI systems to learn from and adapt to individual users, providing personalized\nsupport, assistance, and enhancement. Furthermore, we discuss the incorporation\nof memory storage units for long-term growth and development of both the AI\nsystem and its human user. As we address user privacy and ethical guidelines\nfor responsible AI-human symbiosis, we also explore potential biases and\ninequalities in AI-human symbiosis and propose strategies to mitigate these\nchallenges. Our research aims to provide a comprehensive understanding of the\nSAISSE concept and its potential to effectively support and enhance individual\nhuman users through symbiotic AI systems. This position article aims at\ndiscussing poteintial AI-human interaction related topics within the scientific\ncommunity, rather than providing experimental or theoretical results.",
                "Pedestrian Trajectory Forecasting Using Deep Ensembles Under Sensing\n  Uncertainty\nOne of the fundamental challenges in the prediction of dynamic agents is\nrobustness. Usually, most predictions are deterministic estimates of future\nstates which are over-confident and prone to error. Recently, few works have\naddressed capturing uncertainty during forecasting of future states. However,\nthese probabilistic estimation methods fail to account for the upstream noise\nin perception data during tracking. Sensors always have noise and state\nestimation becomes even more difficult under adverse weather conditions and\nocclusion. Traditionally, Bayes filters have been used to fuse information from\nnoisy sensors to update states with associated belief. But, they fail to\naddress non-linearities and long-term predictions. Therefore, we propose an\nend-to-end estimator that can take noisy sensor measurements and make robust\nfuture state predictions with uncertainty bounds while simultaneously taking\ninto consideration the upstream perceptual uncertainty. For the current\nresearch, we consider an encoder-decoder based deep ensemble network for\ncapturing both perception and predictive uncertainty simultaneously. We\ncompared the current model to other approximate Bayesian inference methods.\nOverall, deep ensembles provided more robust predictions and the consideration\nof upstream uncertainty further increased the estimation accuracy for the\nmodel.",
                "Exploring the Adaptive Behaviors of Particle Lenia: A\n  Perturbation-Response Analysis for Computational Agency\nA firm cognitive subject or ``individual'' is presupposed for the emergence\nof mind. However, with the development of recent information technology, the\n``individual'' has become more dispersed in society and the cognitive subject\nhas become increasingly unstable and adaptive, necessitating an update in our\nunderstanding of the ``individual''. Autopoiesis serves as a model of the\ncognitive subject, which is unstable and requires effort to maintain itself to\nadapt to the environment. In this study, we evaluated adaptivity for a highly\nextensible multi-particle system model Particle Lenia through the response\nperturbation. As a result, we found that Particle Lenia has a particle\nconfiguration that is both temporally unstable and has multiple stable states.\nThis result suggests that Particle Lenia can express adaptive characteristics\nand is expected to be used as a computational model toward building an\nautopoietic cognitive agent."
            ],
            "interesting paper": 2
        },
        {
            "papers": [
                "Mindstorms in Natural Language-Based Societies of Mind\nBoth Minsky's \"society of mind\" and Schmidhuber's \"learning to think\" inspire\ndiverse societies of large multimodal neural networks (NNs) that solve problems\nby interviewing each other in a \"mindstorm.\" Recent implementations of NN-based\nsocieties of minds consist of large language models (LLMs) and other NN-based\nexperts communicating through a natural language interface. In doing so, they\novercome the limitations of single LLMs, improving multimodal zero-shot\nreasoning. In these natural language-based societies of mind (NLSOMs), new\nagents -- all communicating through the same universal symbolic language -- are\neasily added in a modular fashion. To demonstrate the power of NLSOMs, we\nassemble and experiment with several of them (having up to 129 members),\nleveraging mindstorms in them to solve some practical AI tasks: visual question\nanswering, image captioning, text-to-image synthesis, 3D generation, egocentric\nretrieval, embodied AI, and general language-based task solving. We view this\nas a starting point towards much larger NLSOMs with billions of agents-some of\nwhich may be humans. And with this emergence of great societies of\nheterogeneous minds, many new research questions have suddenly become paramount\nto the future of artificial intelligence. What should be the social structure\nof an NLSOM? What would be the (dis)advantages of having a monarchical rather\nthan a democratic structure? How can principles of NN economies be used to\nmaximize the total reward of a reinforcement learning NLSOM? In this work, we\nidentify, discuss, and try to answer some of these questions.",
                "Learning Interpretable Models of Aircraft Handling Behaviour by\n  Reinforcement Learning from Human Feedback\nWe propose a method to capture the handling abilities of fast jet pilots in a\nsoftware model via reinforcement learning (RL) from human preference feedback.\nWe use pairwise preferences over simulated flight trajectories to learn an\ninterpretable rule-based model called a reward tree, which enables the\nautomated scoring of trajectories alongside an explanatory rationale. We train\nan RL agent to execute high-quality handling behaviour by using the reward tree\nas the objective, and thereby generate data for iterative preference collection\nand further refinement of both tree and agent. Experiments with synthetic\npreferences show reward trees to be competitive with uninterpretable neural\nnetwork reward models on quantitative and qualitative evaluations.",
                "How Do UX Practitioners Communicate AI as a Design Material? Artifacts,\n  Conceptions, and Propositions\nUX practitioners (UXPs) face novel challenges when working with and\ncommunicating artificial intelligence (AI) as a design material. We explore how\nUXPs communicate AI concepts when given hands-on experience training and\nexperimenting with AI models. To do so, we conducted a task-based design study\nwith 27 UXPs in which they prototyped and created a design presentation for a\nAI-enabled interface while having access to a simple AI model training tool.\nThrough analyzing UXPs' design presentations and post-activity interviews, we\nfound that although UXPs struggled to clearly communicate some AI concepts,\ntinkering with AI broadened common ground when communicating with technical\nstakeholders. UXPs also identified key risks and benefits of AI in their\ndesigns, and proposed concrete next steps for both UX and AI work. We conclude\nwith a sensitizing concept and recommendations for design and AI tools to\nenhance multi-stakeholder communication and collaboration when crafting\nhuman-centered AI experiences.",
                "MADiff: Offline Multi-agent Learning with Diffusion Models\nDiffusion model (DM) recently achieved huge success in various scenarios\nincluding offline reinforcement learning, where the diffusion planner learn to\ngenerate desired trajectories during online evaluations. However, despite the\neffectiveness in single-agent learning, it remains unclear how DMs can operate\nin multi-agent problems, where agents can hardly complete teamwork without good\ncoordination by independently modeling each agent's trajectories. In this\npaper, we propose MADiff, a novel generative multi-agent learning framework to\ntackle this problem. MADiff is realized with an attention-based diffusion model\nto model the complex coordination among behaviors of multiple agents. To the\nbest of our knowledge, MADiff is the first diffusion-based multi-agent learning\nframework, which behaves as both a decentralized policy and a centralized\ncontroller. During decentralized executions, MADiff simultaneously performs\nteammate modeling, and the centralized controller can also be applied in\nmulti-agent trajectory predictions. Our experiments show the superior\nperformance of MADiff compared to baseline algorithms in a wide range of\nmulti-agent learning tasks, which emphasizes the effectiveness of MADiff in\nmodeling complex multi-agent interactions. Our code is available at\nhttps://github.com/zbzhu99/madiff.",
                "SwiftSage: A Generative Agent with Fast and Slow Thinking for Complex\n  Interactive Tasks\nWe introduce SwiftSage, a novel agent framework inspired by the dual-process\ntheory of human cognition, designed to excel in action planning for complex\ninteractive reasoning tasks. SwiftSage integrates the strengths of behavior\ncloning and prompting large language models (LLMs) to enhance task completion\nperformance. The framework comprises two primary modules: the Swift module,\nrepresenting fast and intuitive thinking, and the Sage module, emulating\ndeliberate thought processes. The Swift module is a small encoder-decoder LM\nfine-tuned on the oracle agent's action trajectories, while the Sage module\nemploys LLMs such as GPT-4 for subgoal planning and grounding. We develop a\nheuristic method to harmoniously integrate the two modules, resulting in a more\nefficient and robust problem-solving process. In 30 tasks from the ScienceWorld\nbenchmark, SwiftSage significantly outperforms other methods such as SayCan,\nReAct, and Reflexion, demonstrating its effectiveness in solving complex\ninteractive tasks.",
                "Exploiting Large Neuroimaging Datasets to Create Connectome-Constrained\n  Approaches for more Robust, Efficient, and Adaptable Artificial Intelligence\nDespite the progress in deep learning networks, efficient learning at the\nedge (enabling adaptable, low-complexity machine learning solutions) remains a\ncritical need for defense and commercial applications. We envision a pipeline\nto utilize large neuroimaging datasets, including maps of the brain which\ncapture neuron and synapse connectivity, to improve machine learning\napproaches. We have pursued different approaches within this pipeline\nstructure. First, as a demonstration of data-driven discovery, the team has\ndeveloped a technique for discovery of repeated subcircuits, or motifs. These\nwere incorporated into a neural architecture search approach to evolve network\narchitectures. Second, we have conducted analysis of the heading direction\ncircuit in the fruit fly, which performs fusion of visual and angular velocity\nfeatures, to explore augmenting existing computational models with new insight.\nOur team discovered a novel pattern of connectivity, implemented a new model,\nand demonstrated sensor fusion on a robotic platform. Third, the team analyzed\ncircuitry for memory formation in the fruit fly connectome, enabling the design\nof a novel generative replay approach. Finally, the team has begun analysis of\nconnectivity in mammalian cortex to explore potential improvements to\ntransformer networks. These constraints increased network robustness on the\nmost challenging examples in the CIFAR-10-C computer vision robustness\nbenchmark task, while reducing learnable attention parameters by over an order\nof magnitude. Taken together, these results demonstrate multiple potential\napproaches to utilize insight from neural systems for developing robust and\nefficient machine learning techniques."
            ],
            "interesting paper": 1
        },
        {
            "papers": [
                "Probing reaction channels via reinforcement learning\nWe propose a reinforcement learning based method to identify important\nconfigurations that connect reactant and product states along chemical reaction\npaths. By shooting multiple trajectories from these configurations, we can\ngenerate an ensemble of configurations that concentrate on the transition path\nensemble. This configuration ensemble can be effectively employed in a neural\nnetwork-based partial differential equation solver to obtain an approximation\nsolution of a restricted Backward Kolmogorov equation, even when the dimension\nof the problem is very high. The resulting solution, known as the committor\nfunction, encodes mechanistic information for the reaction and can in turn be\nused to evaluate reaction rates.",
                "Synthesizing a Progression of Subtasks for Block-Based Visual\n  Programming Tasks\nBlock-based visual programming environments play an increasingly important\nrole in introducing computing concepts to K-12 students. In recent years, they\nhave also gained popularity in neuro-symbolic AI, serving as a benchmark to\nevaluate general problem-solving and logical reasoning skills. The open-ended\nand conceptual nature of these visual programming tasks make them challenging,\nboth for state-of-the-art AI agents as well as for novice programmers. A\nnatural approach to providing assistance for problem-solving is breaking down a\ncomplex task into a progression of simpler subtasks; however, this is not\ntrivial given that the solution codes are typically nested and have non-linear\nexecution behavior. In this paper, we formalize the problem of synthesizing\nsuch a progression for a given reference block-based visual programming task.\nWe propose a novel synthesis algorithm that generates a progression of subtasks\nthat are high-quality, well-spaced in terms of their complexity, and solving\nthis progression leads to solving the reference task. We show the utility of\nour synthesis algorithm in improving the efficacy of AI agents (in this case,\nneural program synthesizers) for solving tasks in the Karel programming\nenvironment. Then, we conduct a user study to demonstrate that our synthesized\nprogression of subtasks can assist a novice programmer in solving tasks in the\nHour of Code: Maze Challenge by Code-dot-org.",
                "Modeling Dynamic Environments with Scene Graph Memory\nEmbodied AI agents that search for objects in large environments such as\nhouseholds often need to make efficient decisions by predicting object\nlocations based on partial information. We pose this as a new type of link\nprediction problem: link prediction on partially observable dynamic graphs. Our\ngraph is a representation of a scene in which rooms and objects are nodes, and\ntheir relationships are encoded in the edges; only parts of the changing graph\nare known to the agent at each timestep. This partial observability poses a\nchallenge to existing link prediction approaches, which we address. We propose\na novel state representation -- Scene Graph Memory (SGM) -- with captures the\nagent's accumulated set of observations, as well as a neural net architecture\ncalled a Node Edge Predictor (NEP) that extracts information from the SGM to\nsearch efficiently. We evaluate our method in the Dynamic House Simulator, a\nnew benchmark that creates diverse dynamic graphs following the semantic\npatterns typically seen at homes, and show that NEP can be trained to predict\nthe locations of objects in a variety of environments with diverse object\nmovement dynamics, outperforming baselines both in terms of new scene\nadaptability and overall accuracy. The codebase and more can be found at\nhttps://www.scenegraphmemory.com.",
                "Integrating Action Knowledge and LLMs for Task Planning and Situation\n  Handling in Open Worlds\nTask planning systems have been developed to help robots use human knowledge\n(about actions) to complete long-horizon tasks. Most of them have been\ndeveloped for \"closed worlds\" while assuming the robot is provided with\ncomplete world knowledge. However, the real world is generally open, and the\nrobots frequently encounter unforeseen situations that can potentially break\nthe planner's completeness. Could we leverage the recent advances on\npre-trained Large Language Models (LLMs) to enable classical planning systems\nto deal with novel situations?\n  This paper introduces a novel framework, called COWP, for open-world task\nplanning and situation handling. COWP dynamically augments the robot's action\nknowledge, including the preconditions and effects of actions, with\ntask-oriented commonsense knowledge. COWP embraces the openness from LLMs, and\nis grounded to specific domains via action knowledge. For systematic\nevaluations, we collected a dataset that includes 1,085 execution-time\nsituations. Each situation corresponds to a state instance wherein a robot is\npotentially unable to complete a task using a solution that normally works.\nExperimental results show that our approach outperforms competitive baselines\nfrom the literature in the success rate of service tasks. Additionally, we have\ndemonstrated COWP using a mobile manipulator. Supplementary materials are\navailable at: https://cowplanning.github.io/",
                "A Comprehensive Overview and Comparative Analysis on Deep Learning\n  Models: CNN, RNN, LSTM, GRU\nDeep learning (DL) has emerged as a powerful subset of machine learning (ML)\nand artificial intelligence (AI), outperforming traditional ML methods,\nespecially in handling unstructured and large datasets. Its impact spans across\nvarious domains, including speech recognition, healthcare, autonomous vehicles,\ncybersecurity, predictive analytics, and more. However, the complexity and\ndynamic nature of real-world problems present challenges in designing effective\ndeep learning models. Consequently, several deep learning models have been\ndeveloped to address different problems and applications. In this article, we\nconduct a comprehensive survey of various deep learning models, including\nConvolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs),\nGenerative Models, Deep Reinforcement Learning (DRL), and Deep Transfer\nLearning. We examine the structure, applications, benefits, and limitations of\neach model. Furthermore, we perform an analysis using three publicly available\ndatasets: IMDB, ARAS, and Fruit-360. We compare the performance of six renowned\ndeep learning models: CNN, Simple RNN, Long Short-Term Memory (LSTM),\nBidirectional LSTM, Gated Recurrent Unit (GRU), and Bidirectional GRU.",
                "AI Coach Assist: An Automated Approach for Call Recommendation in\n  Contact Centers for Agent Coaching\nIn recent years, the utilization of Artificial Intelligence (AI) in the\ncontact center industry is on the rise. One area where AI can have a\nsignificant impact is in the coaching of contact center agents. By analyzing\ncall transcripts using Natural Language Processing (NLP) techniques, it would\nbe possible to quickly determine which calls are most relevant for coaching\npurposes. In this paper, we present AI Coach Assist, which leverages the\npre-trained transformer-based language models to determine whether a given call\nis coachable or not based on the quality assurance (QA) questions asked by the\ncontact center managers or supervisors. The system was trained and evaluated on\na large dataset collected from real-world contact centers and provides an\neffective way to recommend calls to the contact center managers that are more\nlikely to contain coachable moments. Our experimental findings demonstrate the\npotential of AI Coach Assist to improve the coaching process, resulting in\nenhancing the performance of contact center agents."
            ],
            "interesting paper": 4
        },
        {
            "papers": [
                "Towards a Technology-Driven Adaptive Decision Support System for\n  Integrated Pavement and Maintenance strategies (TDADSS-IPM): focus on risk\n  assessment framework for climate change adaptation\nDecision Support Systems for pavement and maintenance strategies have\ntraditionally been designed as silos led to local optimum systems. Moreover,\nsince big data usage didn't exist as result of Industry 4.0 as of today, DSSs\nwere not initially designed adaptive to the sources of uncertainties led to\nrigid decisions. Motivated by the vulnerability of the road assets to the\nclimate phenomena, this paper takes a visionary step towards introducing a\nTechnology-Driven Adaptive Decision Support System for Integrated Pavement and\nMaintenance activities called TDADSS-IPM. As part of such DSS, a bottom-up risk\nassessment model is met via Bayesian Belief Networks (BBN) to realize the\nactual condition of the Danish roads due to weather condition. Such model fills\nthe gaps in the knowledge domain and develops a platform that can be trained\nover time, and applied in real-time to the actual event.",
                "Re-imagining health and well-being in low resource African settings\n  using an augmented AI system and a 3D digital twin\nThis paper discusses and explores the potential and relevance of recent\ndevelopments in artificial intelligence (AI) and digital twins for health and\nwell-being in low-resource African countries. We use the case of public health\nemergency response to disease outbreaks and epidemic control. There is\npotential to take advantage of the increasing availability of data and\ndigitization to develop advanced AI methods for analysis and prediction. Using\nan AI systems perspective, we review emerging trends in AI systems and digital\ntwins and propose an initial augmented AI system architecture to illustrate how\nan AI system can work with a 3D digital twin to address public health goals. We\nhighlight scientific knowledge discovery, continual learning, pragmatic\ninteroperability, and interactive explanation and decision-making as essential\nresearch challenges for AI systems and digital twins.",
                "The Social Value of Dark Energy\nAstrophysics is a social enterprise exemplified here by the Dark Energy\nSurvey (DES) which completed its fieldwork in 2019 after 16 years of\npreparation and observation, while data analysis continues. Society funds\nastrophysics on a grand scale. For human capital and for governance the\ndiscipline draws on a self-governing \"republic of science\", while the funds\nwere provided by philanthropists in the past, and by governments today. The\nbenefits accrue initially to scientists themselves, in the form of a rewarding\nvocation. For the social benefit it is tempting to apply formal cost benefit\nanalysis, but that approach ignores the option value of science and imposes\nquestionable assumptions from welfare economics. Astrophysics generates some\nuseful spinoffs, offers attractive careers, appeals to the popular imagination,\nspeaks to metaphysical cravings and constitutes a good in itself. The rise of\nAI also suggests a role in exploring future habitats for intelligence and\ncognition.",
                "Diffusion Model is an Effective Planner and Data Synthesizer for\n  Multi-Task Reinforcement Learning\nDiffusion models have demonstrated highly-expressive generative capabilities\nin vision and NLP. Recent studies in reinforcement learning (RL) have shown\nthat diffusion models are also powerful in modeling complex policies or\ntrajectories in offline datasets. However, these works have been limited to\nsingle-task settings where a generalist agent capable of addressing multi-task\npredicaments is absent. In this paper, we aim to investigate the effectiveness\nof a single diffusion model in modeling large-scale multi-task offline data,\nwhich can be challenging due to diverse and multimodal data distribution.\nSpecifically, we propose Multi-Task Diffusion Model (\\textsc{MTDiff}), a\ndiffusion-based method that incorporates Transformer backbones and prompt\nlearning for generative planning and data synthesis in multi-task offline\nsettings. \\textsc{MTDiff} leverages vast amounts of knowledge available in\nmulti-task data and performs implicit knowledge sharing among tasks. For\ngenerative planning, we find \\textsc{MTDiff} outperforms state-of-the-art\nalgorithms across 50 tasks on Meta-World and 8 maps on Maze2D. For data\nsynthesis, \\textsc{MTDiff} generates high-quality data for testing tasks given\na single demonstration as a prompt, which enhances the low-quality datasets for\neven unseen tasks.",
                "The Digital Divide in Process Safety: Quantitative Risk Analysis of\n  Human-AI Collaboration\nDigital technologies have dramatically accelerated the digital transformation\nin process industries, boosted new industrial applications, upgraded the\nproduction system, and enhanced operational efficiency. In contrast, the\nchallenges and gaps between human and artificial intelligence (AI) have become\nmore and more prominent, whereas the digital divide in process safety is\naggregating. The study attempts to address the following questions: (i)What is\nAI in the process safety context? (ii)What is the difference between AI and\nhumans in process safety? (iii)How do AI and humans collaborate in process\nsafety? (iv)What are the challenges and gaps in human-AI collaboration? (v)How\nto quantify the risk of human-AI collaboration in process safety? Qualitative\nrisk analysis based on brainstorming and literature review, and quantitative\nrisk analysis based on layer of protection analysis (LOPA) and Bayesian network\n(BN), were applied to explore and model. The importance of human reliability\nshould be stressed in the digital age, not usually to increase the reliability\nof AI, and human-centered AI design in process safety needs to be propagated.",
                "A machine learning approach to the prediction of heat-transfer\n  coefficients in micro-channels\nThe accurate prediction of the two-phase heat transfer coefficient (HTC) as a\nfunction of working fluids, channel geometries and process conditions is key to\nthe optimal design and operation of compact heat exchangers. Advances in\nartificial intelligence research have recently boosted the application of\nmachine learning (ML) algorithms to obtain data-driven surrogate models for the\nHTC. For most supervised learning algorithms, the task is that of a nonlinear\nregression problem. Despite the fact that these models have been proven capable\nof outperforming traditional empirical correlations, they have key limitations\nsuch as overfitting the data, the lack of uncertainty estimation, and\ninterpretability of the results. To address these limitations, in this paper,\nwe use a multi-output Gaussian process regression (GPR) to estimate the HTC in\nmicrochannels as a function of the mass flow rate, heat flux, system pressure\nand channel diameter and length. The model is trained using the Brunel\nTwo-Phase Flow database of high-fidelity experimental data. The advantages of\nGPR are data efficiency, the small number of hyperparameters to be trained\n(typically of the same order of the number of input dimensions), and the\nautomatic trade-off between data fit and model complexity guaranteed by the\nmaximization of the marginal likelihood (Bayesian approach). Our paper proposes\nresearch directions to improve the performance of the GPR-based model in\nextrapolation."
            ],
            "interesting paper": 4
        },
        {
            "papers": [
                "Parity Calibration\nIn a sequential regression setting, a decision-maker may be primarily\nconcerned with whether the future observation will increase or decrease\ncompared to the current one, rather than the actual value of the future\nobservation. In this context, we introduce the notion of parity calibration,\nwhich captures the goal of calibrated forecasting for the increase-decrease (or\n\"parity\") event in a timeseries. Parity probabilities can be extracted from a\nforecasted distribution for the output, but we show that such a strategy leads\nto theoretical unpredictability and poor practical performance. We then observe\nthat although the original task was regression, parity calibration can be\nexpressed as binary calibration. Drawing on this connection, we use an online\nbinary calibration method to achieve parity calibration. We demonstrate the\neffectiveness of our approach on real-world case studies in epidemiology,\nweather forecasting, and model-based control in nuclear fusion.",
                "Graph Neural Network for spatiotemporal data: methods and applications\nIn the era of big data, there has been a surge in the availability of data\ncontaining rich spatial and temporal information, offering valuable insights\ninto dynamic systems and processes for applications such as weather\nforecasting, natural disaster management, intelligent transport systems, and\nprecision agriculture. Graph neural networks (GNNs) have emerged as a powerful\ntool for modeling and understanding data with dependencies to each other such\nas spatial and temporal dependencies. There is a large amount of existing work\nthat focuses on addressing the complex spatial and temporal dependencies in\nspatiotemporal data using GNNs. However, the strong interdisciplinary nature of\nspatiotemporal data has created numerous GNNs variants specifically designed\nfor distinct application domains. Although the techniques are generally\napplicable across various domains, cross-referencing these methods remains\nessential yet challenging due to the absence of a comprehensive literature\nreview on GNNs for spatiotemporal data. This article aims to provide a\nsystematic and comprehensive overview of the technologies and applications of\nGNNs in the spatiotemporal domain. First, the ways of constructing graphs from\nspatiotemporal data are summarized to help domain experts understand how to\ngenerate graphs from various types of spatiotemporal data. Then, a systematic\ncategorization and summary of existing spatiotemporal GNNs are presented to\nenable domain experts to identify suitable techniques and to support model\ndevelopers in advancing their research. Moreover, a comprehensive overview of\nsignificant applications in the spatiotemporal domain is offered to introduce a\nbroader range of applications to model developers and domain experts, assisting\nthem in exploring potential research topics and enhancing the impact of their\nwork. Finally, open challenges and future directions are discussed.",
                "Sit Back and Relax: Learning to Drive Incrementally in All Weather\n  Conditions\nIn autonomous driving scenarios, current object detection models show strong\nperformance when tested in clear weather. However, their performance\ndeteriorates significantly when tested in degrading weather conditions. In\naddition, even when adapted to perform robustly in a sequence of different\nweather conditions, they are often unable to perform well in all of them and\nsuffer from catastrophic forgetting. To efficiently mitigate forgetting, we\npropose Domain-Incremental Learning through Activation Matching (DILAM), which\nemploys unsupervised feature alignment to adapt only the affine parameters of a\nclear weather pre-trained network to different weather conditions. We propose\nto store these affine parameters as a memory bank for each weather condition\nand plug-in their weather-specific parameters during driving (i.e. test time)\nwhen the respective weather conditions are encountered. Our memory bank is\nextremely lightweight, since affine parameters account for less than 2% of a\ntypical object detector. Furthermore, contrary to previous domain-incremental\nlearning approaches, we do not require the weather label when testing and\npropose to automatically infer the weather condition by a majority voting\nlinear classifier.",
                "RE-centric Recommendations for the Development of Trustworthy(er)\n  Autonomous Systems\nComplying with the EU AI Act (AIA) guidelines while developing and\nimplementing AI systems will soon be mandatory within the EU. However,\npractitioners lack actionable instructions to operationalise ethics during AI\nsystems development. A literature review of different ethical guidelines\nrevealed inconsistencies in the principles addressed and the terminology used\nto describe them. Furthermore, requirements engineering (RE), which is\nidentified to foster trustworthiness in the AI development process from the\nearly stages was observed to be absent in a lot of frameworks that support the\ndevelopment of ethical and trustworthy AI. This incongruous phrasing combined\nwith a lack of concrete development practices makes trustworthy AI development\nharder. To address this concern, we formulated a comparison table for the\nterminology used and the coverage of the ethical AI principles in major ethical\nAI guidelines. We then examined the applicability of ethical AI development\nframeworks for performing effective RE during the development of trustworthy AI\nsystems. A tertiary review and meta-analysis of literature discussing ethical\nAI frameworks revealed their limitations when developing trustworthy AI. Based\non our findings, we propose recommendations to address such limitations during\nthe development of trustworthy AI.",
                "Towards a Unifying Model of Rationality in Multiagent Systems\nMultiagent systems deployed in the real world need to cooperate with other\nagents (including humans) nearly as effectively as these agents cooperate with\none another. To design such AI, and provide guarantees of its effectiveness, we\nneed to clearly specify what types of agents our AI must be able to cooperate\nwith. In this work we propose a generic model of socially intelligent agents,\nwhich are individually rational learners that are also able to cooperate with\none another (in the sense that their joint behavior is Pareto efficient). We\ndefine rationality in terms of the regret incurred by each agent over its\nlifetime, and show how we can construct socially intelligent agents for\ndifferent forms of regret. We then discuss the implications of this model for\nthe development of \"robust\" MAS that can cooperate with a wide variety of\nsocially intelligent agents.",
                "An Emergency Disposal Decision-making Method with Human--Machine\n  Collaboration\nRapid developments in artificial intelligence technology have led to unmanned\nsystems replacing human beings in many fields requiring high-precision\npredictions and decisions. In modern operational environments, all job plans\nare affected by emergency events such as equipment failures and resource\nshortages, making a quick resolution critical. The use of unmanned systems to\nassist decision-making can improve resolution efficiency, but their\ndecision-making is not interpretable and may make the wrong decisions. Current\nunmanned systems require human supervision and control. Based on this, we\npropose a collaborative human--machine method for resolving unplanned events\nusing two phases: task filtering and task scheduling. In the task filtering\nphase, we propose a human--machine collaborative decision-making algorithm for\ndynamic tasks. The GACRNN model is used to predict the state of the job nodes,\nlocate the key nodes, and generate a machine-predicted resolution task list. A\nhuman decision-maker supervises the list in real time and modifies and confirms\nthe machine-predicted list through the human--machine interface. In the task\nscheduling phase, we propose a scheduling algorithm that integrates human\nexperience constraints. The steps to resolve an event are inserted into the\nnormal job sequence to schedule the resolution. We propose several\nhuman--machine collaboration methods in each phase to generate steps to resolve\nan unplanned event while minimizing the impact on the original job plan."
            ],
            "interesting paper": 2
        },
        {
            "papers": [
                "Intent-aligned AI systems deplete human agency: the need for agency\n  foundations research in AI safety\nThe rapid advancement of artificial intelligence (AI) systems suggests that\nartificial general intelligence (AGI) systems may soon arrive. Many researchers\nare concerned that AIs and AGIs will harm humans via intentional misuse\n(AI-misuse) or through accidents (AI-accidents). In respect of AI-accidents,\nthere is an increasing effort focused on developing algorithms and paradigms\nthat ensure AI systems are aligned to what humans intend, e.g. AI systems that\nyield actions or recommendations that humans might judge as consistent with\ntheir intentions and goals. Here we argue that alignment to human intent is\ninsufficient for safe AI systems and that preservation of long-term agency of\nhumans may be a more robust standard, and one that needs to be separated\nexplicitly and a priori during optimization. We argue that AI systems can\nreshape human intention and discuss the lack of biological and psychological\nmechanisms that protect humans from loss of agency. We provide the first formal\ndefinition of agency-preserving AI-human interactions which focuses on\nforward-looking agency evaluations and argue that AI systems - not humans -\nmust be increasingly tasked with making these evaluations. We show how agency\nloss can occur in simple environments containing embedded agents that use\ntemporal-difference learning to make action recommendations. Finally, we\npropose a new area of research called \"agency foundations\" and pose four\ninitial topics designed to improve our understanding of agency in AI-human\ninteractions: benevolent game theory, algorithmic foundations of human rights,\nmechanistic interpretability of agency representation in neural-networks and\nreinforcement learning from internal states.",
                "Evaluating GPT's Programming Capability through CodeWars' Katas\nIn the burgeoning field of artificial intelligence (AI), understanding the\ncapabilities and limitations of programming-oriented models is crucial. This\npaper presents a novel evaluation of the programming proficiency of Generative\nPretrained Transformer (GPT) models, specifically GPT-3.5 and GPT-4, against\ncoding problems of varying difficulty levels drawn from Codewars. The\nexperiments reveal a distinct boundary at the 3kyu level, beyond which these\nGPT models struggle to provide solutions. These findings led to the proposal of\na measure for coding problem complexity that incorporates both problem\ndifficulty and the time required for solution. The research emphasizes the need\nfor validation and creative thinking capabilities in AI models to better\nemulate human problem-solving techniques. Future work aims to refine this\nproposed complexity measure, enhance AI models with these suggested\ncapabilities, and develop an objective measure for programming problem\ndifficulty. The results of this research offer invaluable insights for\nimproving AI programming capabilities and advancing the frontier of AI\nproblem-solving abilities.",
                "Probabilistic computation and uncertainty quantification with emerging\n  covariance\nBuilding robust, interpretable, and secure AI system requires quantifying and\nrepresenting uncertainty under a probabilistic perspective to mimic human\ncognitive abilities. However, probabilistic computation presents significant\nchallenges for most conventional artificial neural network, as they are\nessentially implemented in a deterministic manner. In this paper, we develop an\nefficient probabilistic computation framework by truncating the probabilistic\nrepresentation of neural activation up to its mean and covariance and construct\na moment neural network that encapsulates the nonlinear coupling between the\nmean and covariance of the underlying stochastic network. We reveal that when\nonly the mean but not the covariance is supervised during gradient-based\nlearning, the unsupervised covariance spontaneously emerges from its nonlinear\ncoupling with the mean and faithfully captures the uncertainty associated with\nmodel predictions. Our findings highlight the inherent simplicity of\nprobabilistic computation by seamlessly incorporating uncertainty into model\nprediction, paving the way for integrating it into large-scale AI systems.",
                "NetHack is Hard to Hack\nNeural policy learning methods have achieved remarkable results in various\ncontrol problems, ranging from Atari games to simulated locomotion. However,\nthese methods struggle in long-horizon tasks, especially in open-ended\nenvironments with multi-modal observations, such as the popular dungeon-crawler\ngame, NetHack. Intriguingly, the NeurIPS 2021 NetHack Challenge revealed that\nsymbolic agents outperformed neural approaches by over four times in median\ngame score. In this paper, we delve into the reasons behind this performance\ngap and present an extensive study on neural policy learning for NetHack. To\nconduct this study, we analyze the winning symbolic agent, extending its\ncodebase to track internal strategy selection in order to generate one of the\nlargest available demonstration datasets. Utilizing this dataset, we examine\n(i) the advantages of an action hierarchy; (ii) enhancements in neural\narchitecture; and (iii) the integration of reinforcement learning with\nimitation learning. Our investigations produce a state-of-the-art neural agent\nthat surpasses previous fully neural policies by 127% in offline settings and\n25% in online settings on median game score. However, we also demonstrate that\nmere scaling is insufficient to bridge the performance gap with the best\nsymbolic models or even the top human players.",
                "Non-convex Bayesian Learning via Stochastic Gradient Markov Chain Monte\n  Carlo\nThe rise of artificial intelligence (AI) hinges on the efficient training of\nmodern deep neural networks (DNNs) for non-convex optimization and uncertainty\nquantification, which boils down to a non-convex Bayesian learning problem. A\nstandard tool to handle the problem is Langevin Monte Carlo, which proposes to\napproximate the posterior distribution with theoretical guarantees. In this\nthesis, we start with the replica exchange Langevin Monte Carlo (also known as\nparallel tempering), which proposes appropriate swaps between exploration and\nexploitation to achieve accelerations. However, the na\\\"ive extension of swaps\nto big data problems leads to a large bias, and bias-corrected swaps are\nrequired. Such a mechanism leads to few effective swaps and insignificant\naccelerations. To alleviate this issue, we first propose a control variates\nmethod to reduce the variance of noisy energy estimators and show a potential\nto accelerate the exponential convergence. We also present the population-chain\nreplica exchange based on non-reversibility and obtain an optimal round-trip\nrate for deep learning. In the second part of the thesis, we study scalable\ndynamic importance sampling algorithms based on stochastic approximation.\nTraditional dynamic importance sampling algorithms have achieved success,\nhowever, the lack of scalability has greatly limited their extensions to big\ndata. To handle this scalability issue, we resolve the vanishing gradient\nproblem and propose two dynamic importance sampling algorithms. Theoretically,\nwe establish the stability condition for the underlying ordinary differential\nequation (ODE) system and guarantee the asymptotic convergence of the latent\nvariable to the desired fixed point. Interestingly, such a result still holds\ngiven non-convex energy landscapes.",
                "Towards Explainable and Language-Agnostic LLMs: Symbolic Reverse\n  Engineering of Language at Scale\nLarge language models (LLMs) have achieved a milestone that undenia-bly\nchanged many held beliefs in artificial intelligence (AI). However, there\nremains many limitations of these LLMs when it comes to true language\nunderstanding, limitations that are a byproduct of the under-lying architecture\nof deep neural networks. Moreover, and due to their subsymbolic nature,\nwhatever knowledge these models acquire about how language works will always be\nburied in billions of microfeatures (weights), none of which is meaningful on\nits own, making such models hopelessly unexplainable. To address these\nlimitations, we suggest com-bining the strength of symbolic representations\nwith what we believe to be the key to the success of LLMs, namely a successful\nbottom-up re-verse engineering of language at scale. As such we argue for a\nbottom-up reverse engineering of language in a symbolic setting. Hints on what\nthis project amounts to have been suggested by several authors, and we discuss\nin some detail here how this project could be accomplished."
            ],
            "interesting paper": 3
        },
        {
            "papers": [
                "Sea Ice Extraction via Remote Sensed Imagery: Algorithms, Datasets,\n  Applications and Challenges\nThe deep learning, which is a dominating technique in artificial\nintelligence, has completely changed the image understanding over the past\ndecade. As a consequence, the sea ice extraction (SIE) problem has reached a\nnew era. We present a comprehensive review of four important aspects of SIE,\nincluding algorithms, datasets, applications, and the future trends. Our review\nfocuses on researches published from 2016 to the present, with a specific focus\non deep learning-based approaches in the last five years. We divided all\nrelegated algorithms into 3 categories, including classical image segmentation\napproach, machine learning-based approach and deep learning-based methods. We\nreviewed the accessible ice datasets including SAR-based datasets, the\noptical-based datasets and others. The applications are presented in 4 aspects\nincluding climate research, navigation, geographic information systems (GIS)\nproduction and others. It also provides insightful observations and inspiring\nfuture research directions.",
                "BetaZero: Belief-State Planning for Long-Horizon POMDPs using Learned\n  Approximations\nReal-world planning problems, including autonomous driving and sustainable\nenergy applications like carbon storage and resource exploration, have recently\nbeen modeled as partially observable Markov decision processes (POMDPs) and\nsolved using approximate methods. To solve high-dimensional POMDPs in practice,\nstate-of-the-art methods use online planning with problem-specific heuristics\nto reduce planning horizons and make the problems tractable. Algorithms that\nlearn approximations to replace heuristics have recently found success in\nlarge-scale fully observable domains. The key insight is the combination of\nonline Monte Carlo tree search with offline neural network approximations of\nthe optimal policy and value function. In this work, we bring this insight to\npartially observable domains and propose BetaZero, a belief-state planning\nalgorithm for high-dimensional POMDPs. BetaZero learns offline approximations\nthat replace heuristics to enable online decision making in long-horizon\nproblems. We address several challenges inherent in large-scale partially\nobservable domains; namely challenges of transitioning in stochastic\nenvironments, prioritizing action branching with a limited search budget, and\nrepresenting beliefs as input to the network. To formalize the use of all\nlimited search information, we train against a novel $Q$-weighted visit counts\npolicy. We test BetaZero on various well-established POMDP benchmarks found in\nthe literature and a real-world problem of critical mineral exploration.\nExperiments show that BetaZero outperforms state-of-the-art POMDP solvers on a\nvariety of tasks.",
                "Improved flood mapping for efficient policy design by fusion of\n  Sentinel-1, Sentinel-2, and Landsat-9 imagery to identify population and\n  infrastructure exposed to floods\nA reliable yet inexpensive tool for the estimation of flood water spread is\nconducive for efficient disaster management. The application of optical and SAR\nimagery in tandem provides a means of extended availability and enhanced\nreliability of flood mapping. We propose a methodology to merge these two types\nof imagery into a common data space and demonstrate its use in the\nidentification of affected populations and infrastructure for the 2022 floods\nin Pakistan. The merging of optical and SAR data provides us with improved\nobservations in cloud-prone regions; that is then used to gain additional\ninsights into flood mapping applications. The use of open source datasets from\nWorldPop and OSM for population and roads respectively makes the exercise\nglobally replicable. The integration of flood maps with spatial data on\npopulation and infrastructure facilitates informed policy design. We have shown\nthat within the top five flood-affected districts in Sindh province, Pakistan,\nthe affected population accounts for 31 %, while the length of affected roads\nmeasures 1410.25 km out of a total of 7537.96 km.",
                "From Human-Centered to Social-Centered Artificial Intelligence:\n  Assessing ChatGPT's Impact through Disruptive Events\nLarge language models (LLMs) and dialogue agents have existed for years, but\nthe release of recent GPT models has been a watershed moment for artificial\nintelligence (AI) research and society at large. Immediately recognized for its\ngenerative capabilities and versatility, ChatGPT's impressive proficiency\nacross technical and creative domains led to its widespread adoption. While\nsociety grapples with the emerging cultural impacts of ChatGPT, critiques of\nChatGPT's impact within the machine learning community have coalesced around\nits performance or other conventional Responsible AI evaluations relating to\nbias, toxicity, and 'hallucination.' We argue that these latter critiques draw\nheavily on a particular conceptualization of the 'human-centered' framework,\nwhich tends to cast atomized individuals as the key recipients of both the\nbenefits and detriments of technology. In this article, we direct attention to\nanother dimension of LLMs and dialogue agents' impact: their effect on social\ngroups, institutions, and accompanying norms and practices. By illustrating\nChatGPT's social impact through three disruptive events, we challenge\nindividualistic approaches in AI development and contribute to ongoing debates\naround the ethical and responsible implementation of AI systems. We hope this\neffort will call attention to more comprehensive and longitudinal evaluation\ntools and compel technologists to go beyond human-centered thinking and ground\ntheir efforts through social-centered AI.",
                "Responsible Design Patterns for Machine Learning Pipelines\nIntegrating ethical practices into the AI development process for artificial\nintelligence (AI) is essential to ensure safe, fair, and responsible operation.\nAI ethics involves applying ethical principles to the entire life cycle of AI\nsystems. This is essential to mitigate potential risks and harms associated\nwith AI, such as algorithm biases. To achieve this goal, responsible design\npatterns (RDPs) are critical for Machine Learning (ML) pipelines to guarantee\nethical and fair outcomes. In this paper, we propose a comprehensive framework\nincorporating RDPs into ML pipelines to mitigate risks and ensure the ethical\ndevelopment of AI systems. Our framework comprises new responsible AI design\npatterns for ML pipelines identified through a survey of AI ethics and data\nmanagement experts and validated through real-world scenarios with expert\nfeedback. The framework guides AI developers, data scientists, and\npolicy-makers to implement ethical practices in AI development and deploy\nresponsible AI systems in production.",
                "TorchRL: A data-driven decision-making library for PyTorch\nPyTorch has ascended as a premier machine learning framework, yet it lacks a\nnative and comprehensive library for decision and control tasks suitable for\nlarge development teams dealing with complex real-world data and environments.\nTo address this issue, we propose TorchRL, a generalistic control library for\nPyTorch that provides well-integrated, yet standalone components. We introduce\na new and flexible PyTorch primitive, the TensorDict, which facilitates\nstreamlined algorithm development across the many branches of Reinforcement\nLearning (RL) and control. We provide a detailed description of the building\nblocks and an extensive overview of the library across domains and tasks.\nFinally, we experimentally demonstrate its reliability and flexibility and show\ncomparative benchmarks to demonstrate its computational efficiency. TorchRL\nfosters long-term support and is publicly available on GitHub for greater\nreproducibility and collaboration within the research community. The code is\nopen-sourced on GitHub."
            ],
            "interesting paper": 1
        },
        {
            "papers": [
                "AI Liability Insurance With an Example in AI-Powered E-diagnosis System\nArtificial Intelligence (AI) has received an increasing amount of attention\nin multiple areas. The uncertainties and risks in AI-powered systems have\ncreated reluctance in their wild adoption. As an economic solution to\ncompensate for potential damages, AI liability insurance is a promising market\nto enhance the integration of AI into daily life. In this work, we use an\nAI-powered E-diagnosis system as an example to study AI liability insurance. We\nprovide a quantitative risk assessment model with evidence-based numerical\nanalysis. We discuss the insurability criteria for AI technologies and suggest\nnecessary adjustments to accommodate the features of AI products. We show that\nAI liability insurance can act as a regulatory mechanism to incentivize\ncompliant behaviors and serve as a certificate of high-quality AI systems.\nFurthermore, we suggest premium adjustment to reflect the dynamic evolution of\nthe inherent uncertainty in AI. Moral hazard problems are discussed and\nsuggestions for AI liability insurance are provided.",
                "The ethical ambiguity of AI data enrichment: Measuring gaps in research\n  ethics norms and practices\nThe technical progression of artificial intelligence (AI) research has been\nbuilt on breakthroughs in fields such as computer science, statistics, and\nmathematics. However, in the past decade AI researchers have increasingly\nlooked to the social sciences, turning to human interactions to solve the\nchallenges of model development. Paying crowdsourcing workers to generate or\ncurate data, or data enrichment, has become indispensable for many areas of AI\nresearch, from natural language processing to reinforcement learning from human\nfeedback (RLHF). Other fields that routinely interact with crowdsourcing\nworkers, such as Psychology, have developed common governance requirements and\nnorms to ensure research is undertaken ethically. This study explores how, and\nto what extent, comparable research ethics requirements and norms have\ndeveloped for AI research and data enrichment. We focus on the approach taken\nby two leading conferences: ICLR and NeurIPS, and journal publisher Springer.\nIn a longitudinal study of accepted papers, and via a comparison with\nPsychology and CHI papers, this work finds that leading AI venues have begun to\nestablish protocols for human data collection, but these are are inconsistently\nfollowed by authors. Whilst Psychology papers engaging with crowdsourcing\nworkers frequently disclose ethics reviews, payment data, demographic data and\nother information, similar disclosures are far less common in leading AI venues\ndespite similar guidance. The work concludes with hypotheses to explain these\ngaps in research ethics practices and considerations for its implications.",
                "Counting Crowds in Bad Weather\nCrowd counting has recently attracted significant attention in the field of\ncomputer vision due to its wide applications to image understanding. Numerous\nmethods have been proposed and achieved state-of-the-art performance for\nreal-world tasks. However, existing approaches do not perform well under\nadverse weather such as haze, rain, and snow since the visual appearances of\ncrowds in such scenes are drastically different from those images in clear\nweather of typical datasets. In this paper, we propose a method for robust\ncrowd counting in adverse weather scenarios. Instead of using a two-stage\napproach that involves image restoration and crowd counting modules, our model\nlearns effective features and adaptive queries to account for large appearance\nvariations. With these weather queries, the proposed model can learn the\nweather information according to the degradation of the input image and\noptimize with the crowd counting module simultaneously. Experimental results\nshow that the proposed algorithm is effective in counting crowds under\ndifferent weather types on benchmark datasets. The source code and trained\nmodels will be made available to the public.",
                "Egocentric Planning for Scalable Embodied Task Achievement\nEmbodied agents face significant challenges when tasked with performing\nactions in diverse environments, particularly in generalizing across object\ntypes and executing suitable actions to accomplish tasks. Furthermore, agents\nshould exhibit robustness, minimizing the execution of illegal actions. In this\nwork, we present Egocentric Planning, an innovative approach that combines\nsymbolic planning and Object-oriented POMDPs to solve tasks in complex\nenvironments, harnessing existing models for visual perception and natural\nlanguage processing. We evaluated our approach in ALFRED, a simulated\nenvironment designed for domestic tasks, and demonstrated its high scalability,\nachieving an impressive 36.07% unseen success rate in the ALFRED benchmark and\nwinning the ALFRED challenge at CVPR Embodied AI workshop. Our method requires\nreliable perception and the specification or learning of a symbolic description\nof the preconditions and effects of the agent's actions, as well as what object\ntypes reveal information about others. It is capable of naturally scaling to\nsolve new tasks beyond ALFRED, as long as they can be solved using the\navailable skills. This work offers a solid baseline for studying end-to-end and\nhybrid methods that aim to generalize to new tasks, including recent approaches\nrelying on LLMs, but often struggle to scale to long sequences of actions or\nproduce robust plans for novel tasks.",
                "Interpretable and Explainable Logical Policies via Neurally Guided\n  Symbolic Abstraction\nThe limited priors required by neural networks make them the dominating\nchoice to encode and learn policies using reinforcement learning (RL). However,\nthey are also black-boxes, making it hard to understand the agent's behaviour,\nespecially when working on the image level. Therefore, neuro-symbolic RL aims\nat creating policies that are interpretable in the first place. Unfortunately,\ninterpretability is not explainability. To achieve both, we introduce Neurally\ngUided Differentiable loGic policiEs (NUDGE). NUDGE exploits trained neural\nnetwork-based agents to guide the search of candidate-weighted logic rules,\nthen uses differentiable logic to train the logic agents. Our experimental\nevaluation demonstrates that NUDGE agents can induce interpretable and\nexplainable policies while outperforming purely neural ones and showing good\nflexibility to environments of different initial states and problem sizes.",
                "Federated Learning Games for Reconfigurable Intelligent Surfaces via\n  Causal Representations\nIn this paper, we investigate the problem of robust Reconfigurable\nIntelligent Surface (RIS) phase-shifts configuration over heterogeneous\ncommunication environments. The problem is formulated as a distributed learning\nproblem over different environments in a Federated Learning (FL) setting.\nEquivalently, this corresponds to a game played between multiple RISs, as\nlearning agents, in heterogeneous environments. Using Invariant Risk\nMinimization (IRM) and its FL equivalent, dubbed FL Games, we solve the RIS\nconfiguration problem by learning invariant causal representations across\nmultiple environments and then predicting the phases. The solution corresponds\nto playing according to Best Response Dynamics (BRD) which yields the Nash\nEquilibrium of the FL game. The representation learner and the phase predictor\nare modeled by two neural networks, and their performance is validated via\nsimulations against other benchmarks from the literature. Our results show that\ncausality-based learning yields a predictor that is 15% more accurate in unseen\nOut-of-Distribution (OoD) environments."
            ],
            "interesting paper": 2
        }
    ],
    "Morgan Smith": [
        {
            "papers": [
                "Deep Learning and Ethics\nThis article appears as chapter 21 of Prince (2023, Understanding Deep\nLearning); a complete draft of the textbook is available here:\nhttp://udlbook.com. This chapter considers potential harms arising from the\ndesign and use of AI systems. These include algorithmic bias, lack of\nexplainability, data privacy violations, militarization, fraud, and\nenvironmental concerns. The aim is not to provide advice on being more ethical.\nInstead, the goal is to express ideas and start conversations in key areas that\nhave received attention in philosophy, political science, and the broader\nsocial sciences.",
                "Towards a Capability Assessment Model for the Comprehension and Adoption\n  of AI in Organisations\nThe comprehension and adoption of Artificial Intelligence (AI) are beset with\npractical and ethical problems. This article presents a 5-level AI Capability\nAssessment Model (AI-CAM) and a related AI Capabilities Matrix (AI-CM) to\nassist practitioners in AI comprehension and adoption. These practical tools\nwere developed with business executives, technologists, and other\norganisational stakeholders in mind. They are founded on a comprehensive\nconception of AI compared to those in other AI adoption models and are also\nopen-source artefacts. Thus, the AI-CAM and AI-CM present an accessible\nresource to help inform organisational decision-makers on the capability\nrequirements for (1) AI-based data analytics use cases based on machine\nlearning technologies; (2) Knowledge representation to engineer and represent\ndata, information and knowledge using semantic technologies; and (3) AI-based\nsolutions that seek to emulate human reasoning and decision-making. The AI-CAM\ncovers the core capability dimensions (business, data, technology,\norganisation, AI skills, risks, and ethical considerations) required at the\nfive capability maturity levels to achieve optimal use of AI in organisations.",
                "A Mini Review on the utilization of Reinforcement Learning with OPC UA\nReinforcement Learning (RL) is a powerful machine learning paradigm that has\nbeen applied in various fields such as robotics, natural language processing\nand game playing achieving state-of-the-art results. Targeted to solve\nsequential decision making problems, it is by design able to learn from\nexperience and therefore adapt to changing dynamic environments. These\ncapabilities make it a prime candidate for controlling and optimizing complex\nprocesses in industry. The key to fully exploiting this potential is the\nseamless integration of RL into existing industrial systems. The industrial\ncommunication standard Open Platform Communications UnifiedArchitecture (OPC\nUA) could bridge this gap. However, since RL and OPC UA are from different\nfields,there is a need for researchers to bridge the gap between the two\ntechnologies. This work serves to bridge this gap by providing a brief\ntechnical overview of both technologies and carrying out a semi-exhaustive\nliterature review to gain insights on how RL and OPC UA are applied in\ncombination. With this survey, three main research topics have been identified,\nfollowing the intersection of RL with OPC UA. The results of the literature\nreview show that RL is a promising technology for the control and optimization\nof industrial processes, but does not yet have the necessary standardized\ninterfaces to be deployed in real-world scenarios with reasonably low effort.",
                "ISLE: An Intelligent Streaming Framework for High-Throughput AI\n  Inference in Medical Imaging\nAs the adoption of Artificial Intelligence (AI) systems within the clinical\nenvironment grows, limitations in bandwidth and compute can create\ncommunication bottlenecks when streaming imaging data, leading to delays in\npatient care and increased cost. As such, healthcare providers and AI vendors\nwill require greater computational infrastructure, therefore dramatically\nincreasing costs. To that end, we developed ISLE, an intelligent streaming\nframework for high-throughput, compute- and bandwidth- optimized, and cost\neffective AI inference for clinical decision making at scale. In our\nexperiments, ISLE on average reduced data transmission by 98.02% and decoding\ntime by 98.09%, while increasing throughput by 2,730%. We show that ISLE\nresults in faster turnaround times, and reduced overall cost of data,\ntransmission, and compute, without negatively impacting clinical decision\nmaking using AI systems.",
                "Emergence of a phonological bias in ChatGPT\nCurrent large language models, such as OpenAI's ChatGPT, have captured the\npublic's attention because how remarkable they are in the use of language.\nHere, I demonstrate that ChatGPT displays phonological biases that are a\nhallmark of human language processing. More concretely, just like humans,\nChatGPT has a consonant bias. That is, the chatbot has a tendency to use\nconsonants over vowels to identify words. This is observed across languages\nthat differ in their relative distribution of consonants and vowels such as\nEnglish and Spanish. Despite the differences in how current artificial\nintelligence language models are trained to process linguistic stimuli and how\nhuman infants acquire language, such training seems to be enough for the\nemergence of a phonological bias in ChatGPT",
                "On the Planning Abilities of Large Language Models : A Critical\n  Investigation\nIntrigued by the claims of emergent reasoning capabilities in LLMs trained on\ngeneral web corpora, in this paper, we set out to investigate their planning\ncapabilities. We aim to evaluate (1) the effectiveness of LLMs in generating\nplans autonomously in commonsense planning tasks and (2) the potential of LLMs\nin LLM-Modulo settings where they act as a source of heuristic guidance for\nexternal planners and verifiers. We conduct a systematic study by generating a\nsuite of instances on domains similar to the ones employed in the International\nPlanning Competition and evaluate LLMs in two distinct modes: autonomous and\nheuristic. Our findings reveal that LLMs' ability to generate executable plans\nautonomously is rather limited, with the best model (GPT-4) having an average\nsuccess rate of ~12% across the domains. However, the results in the LLM-Modulo\nsetting show more promise. In the LLM-Modulo setting, we demonstrate that\nLLM-generated plans can improve the search process for underlying sound\nplanners and additionally show that external verifiers can help provide\nfeedback on the generated plans and back-prompt the LLM for better plan\ngeneration."
            ],
            "interesting paper": 4
        },
        {
            "papers": [
                "Attention Paper: How Generative AI Reshapes Digital Shadow Industry?\nThe rapid development of digital economy has led to the emergence of various\nblack and shadow internet industries, which pose potential risks that can be\nidentified and managed through digital risk management (DRM) that uses\ndifferent techniques such as machine learning and deep learning. The evolution\nof DRM architecture has been driven by changes in data forms. However, the\ndevelopment of AI-generated content (AIGC) technology, such as ChatGPT and\nStable Diffusion, has given black and shadow industries powerful tools to\npersonalize data and generate realistic images and conversations for fraudulent\nactivities. This poses a challenge for DRM systems to control risks from the\nsource of data generation and to respond quickly to the fast-changing risk\nenvironment. This paper aims to provide a technical analysis of the challenges\nand opportunities of AIGC from upstream, midstream, and downstream paths of\nblack/shadow industries and suggest future directions for improving existing\nrisk control systems. The paper will explore the new black and shadow\ntechniques triggered by generative AI technology and provide insights for\nbuilding the next-generation DRM system.",
                "Alert of the Second Decision-maker: An Introduction to Human-AI Conflict\nThe collaboration between humans and artificial intelligence (AI) is a\nsignificant feature in this digital age. However, humans and AI may have\nobservation, interpretation, and action conflicts when working synchronously.\nThis phenomenon is often masked by faults and, unfortunately, overlooked. This\npaper systematically introduces the human-AI conflict concept, causes,\nmeasurement methods, and risk assessment. The results highlight that there is a\npotential second decision-maker besides the human, which is the AI; the\nhuman-AI conflict is a unique and emerging risk in digitalized process systems;\nand this is an interdisciplinary field that needs to be distinguished from\ntraditional fault and failure analysis; the conflict risk is significant and\ncannot be ignored.",
                "AI Techniques in the Microservices Life-Cycle: A Survey\nMicroservices is a popular architectural style for the development of\ndistributed software, with an emphasis on modularity, scalability, and\nflexibility. Indeed, in microservice systems, functionalities are provided by\nloosely coupled, small services, each focusing on a specific business\ncapability. Building a system according to the microservices architectural\nstyle brings a number of challenges, mainly related to how the different\nmicroservices are deployed and coordinated and how they interact. In this\npaper, we provide a survey about how techniques in the area of Artificial\nIntelligence have been used to tackle these challenges.",
                "Applying Interdisciplinary Frameworks to Understand Algorithmic\n  Decision-Making\nWe argue that explanations for \"algorithmic decision-making\" (ADM) systems\ncan profit by adopting practices that are already used in the learning\nsciences. We shortly introduce the importance of explaining ADM systems, give a\nbrief overview of approaches drawing from other disciplines to improve\nexplanations, and present the results of our qualitative task-based study\nincorporating the \"six facets of understanding\" framework. We close with\nquestions guiding the discussion of how future studies can leverage an\ninterdisciplinary approach.",
                "Enhancing Human Capabilities through Symbiotic Artificial Intelligence\n  with Shared Sensory Experiences\nThe merging of human intelligence and artificial intelligence has long been a\nsubject of interest in both science fiction and academia. In this paper, we\nintroduce a novel concept in Human-AI interaction called Symbiotic Artificial\nIntelligence with Shared Sensory Experiences (SAISSE), which aims to establish\na mutually beneficial relationship between AI systems and human users through\nshared sensory experiences. By integrating multiple sensory input channels and\nprocessing human experiences, SAISSE fosters a strong human-AI bond, enabling\nAI systems to learn from and adapt to individual users, providing personalized\nsupport, assistance, and enhancement. Furthermore, we discuss the incorporation\nof memory storage units for long-term growth and development of both the AI\nsystem and its human user. As we address user privacy and ethical guidelines\nfor responsible AI-human symbiosis, we also explore potential biases and\ninequalities in AI-human symbiosis and propose strategies to mitigate these\nchallenges. Our research aims to provide a comprehensive understanding of the\nSAISSE concept and its potential to effectively support and enhance individual\nhuman users through symbiotic AI systems. This position article aims at\ndiscussing poteintial AI-human interaction related topics within the scientific\ncommunity, rather than providing experimental or theoretical results.",
                "Transformative Effects of ChatGPT on Modern Education: Emerging Era of\n  AI Chatbots\nChatGPT, an AI-based chatbot, was released to provide coherent and useful\nreplies based on analysis of large volumes of data. In this article, leading\nscientists, researchers and engineers discuss the transformative effects of\nChatGPT on modern education. This research seeks to improve our knowledge of\nChatGPT capabilities and its use in the education sector, identifying potential\nconcerns and challenges. Our preliminary evaluation concludes that ChatGPT\nperformed differently in each subject area including finance, coding and maths.\nWhile ChatGPT has the ability to help educators by creating instructional\ncontent, offering suggestions and acting as an online educator to learners by\nanswering questions and promoting group work, there are clear drawbacks in its\nuse, such as the possibility of producing inaccurate or false data and\ncircumventing duplicate content (plagiarism) detectors where originality is\nessential. The often reported hallucinations within Generative AI in general,\nand also relevant for ChatGPT, can render its use of limited benefit where\naccuracy is essential. What ChatGPT lacks is a stochastic measure to help\nprovide sincere and sensitive communication with its users. Academic\nregulations and evaluation practices used in educational institutions need to\nbe updated, should ChatGPT be used as a tool in education. To address the\ntransformative effects of ChatGPT on the learning environment, educating\nteachers and students alike about its capabilities and limitations will be\ncrucial."
            ],
            "interesting paper": 1
        },
        {
            "papers": [
                "How Do UX Practitioners Communicate AI as a Design Material? Artifacts,\n  Conceptions, and Propositions\nUX practitioners (UXPs) face novel challenges when working with and\ncommunicating artificial intelligence (AI) as a design material. We explore how\nUXPs communicate AI concepts when given hands-on experience training and\nexperimenting with AI models. To do so, we conducted a task-based design study\nwith 27 UXPs in which they prototyped and created a design presentation for a\nAI-enabled interface while having access to a simple AI model training tool.\nThrough analyzing UXPs' design presentations and post-activity interviews, we\nfound that although UXPs struggled to clearly communicate some AI concepts,\ntinkering with AI broadened common ground when communicating with technical\nstakeholders. UXPs also identified key risks and benefits of AI in their\ndesigns, and proposed concrete next steps for both UX and AI work. We conclude\nwith a sensitizing concept and recommendations for design and AI tools to\nenhance multi-stakeholder communication and collaboration when crafting\nhuman-centered AI experiences.",
                "Towards Cognitive Bots: Architectural Research Challenges\nSoftware bots operating in multiple virtual digital platforms must understand\nthe platforms' affordances and behave like human users. Platform affordances or\nfeatures differ from one application platform to another or through a life\ncycle, requiring such bots to be adaptable. Moreover, bots in such platforms\ncould cooperate with humans or other software agents for work or to learn\nspecific behavior patterns. However, present-day bots, particularly chatbots,\nother than language processing and prediction, are far from reaching a human\nuser's behavior level within complex business information systems. They lack\nthe cognitive capabilities to sense and act in such virtual environments,\nrendering their development a challenge to artificial general intelligence\nresearch. In this study, we problematize and investigate assumptions in\nconceptualizing software bot architecture by directing attention to significant\narchitectural research challenges in developing cognitive bots endowed with\ncomplex behavior for operation on information systems. As an outlook, we\npropose alternate architectural assumptions to consider in future bot design\nand bot development frameworks.",
                "Frontier AI developers need an internal audit function\nThis article argues that frontier artificial intelligence (AI) developers\nneed an internal audit function. First, it describes the role of internal audit\nin corporate governance: internal audit evaluates the adequacy and\neffectiveness of a company's risk management, control, and governance\nprocesses. It is organizationally independent from senior management and\nreports directly to the board of directors, typically its audit committee. In\nthe IIA's Three Lines Model, internal audit serves as the third line and is\nresponsible for providing assurance to the board, while the Combined Assurance\nFramework highlights the need to coordinate the activities of internal and\nexternal assurance providers. Next, the article provides an overview of key\ngovernance challenges in frontier AI development: dangerous capabilities can\narise unpredictably and undetected; it is difficult to prevent a deployed model\nfrom causing harm; frontier models can proliferate rapidly; it is inherently\ndifficult to assess frontier AI risks; and frontier AI developers do not seem\nto follow best practices in risk governance. Finally, the article discusses how\nan internal audit function could address some of these challenges: internal\naudit could identify ineffective risk management practices; it could ensure\nthat the board of directors has a more accurate understanding of the current\nlevel of risk and the adequacy of the developer's risk management practices;\nand it could serve as a contact point for whistleblowers. In light of rapid\nprogress in AI research and development, frontier AI developers need to\nstrengthen their risk governance. Instead of reinventing the wheel, they should\nfollow existing best practices. While this might not be sufficient, they should\nnot skip this obvious first step.",
                "Mindstorms in Natural Language-Based Societies of Mind\nBoth Minsky's \"society of mind\" and Schmidhuber's \"learning to think\" inspire\ndiverse societies of large multimodal neural networks (NNs) that solve problems\nby interviewing each other in a \"mindstorm.\" Recent implementations of NN-based\nsocieties of minds consist of large language models (LLMs) and other NN-based\nexperts communicating through a natural language interface. In doing so, they\novercome the limitations of single LLMs, improving multimodal zero-shot\nreasoning. In these natural language-based societies of mind (NLSOMs), new\nagents -- all communicating through the same universal symbolic language -- are\neasily added in a modular fashion. To demonstrate the power of NLSOMs, we\nassemble and experiment with several of them (having up to 129 members),\nleveraging mindstorms in them to solve some practical AI tasks: visual question\nanswering, image captioning, text-to-image synthesis, 3D generation, egocentric\nretrieval, embodied AI, and general language-based task solving. We view this\nas a starting point towards much larger NLSOMs with billions of agents-some of\nwhich may be humans. And with this emergence of great societies of\nheterogeneous minds, many new research questions have suddenly become paramount\nto the future of artificial intelligence. What should be the social structure\nof an NLSOM? What would be the (dis)advantages of having a monarchical rather\nthan a democratic structure? How can principles of NN economies be used to\nmaximize the total reward of a reinforcement learning NLSOM? In this work, we\nidentify, discuss, and try to answer some of these questions.",
                "A Knowledge Engineering Primer\nThe aim of this primer is to introduce the subject of knowledge engineering\nin a concise but synthetic way to develop the reader's intuition about the\narea.",
                "Attention Schema in Neural Agents\nAttention has become a common ingredient in deep learning architectures. It\nadds a dynamical selection of information on top of the static selection of\ninformation supported by weights. In the same way, we can imagine a\nhigher-order informational filter built on top of attention: an Attention\nSchema (AS), namely, a descriptive and predictive model of attention. In\ncognitive neuroscience, Attention Schema Theory (AST) supports this idea of\ndistinguishing attention from AS. A strong prediction of this theory is that an\nagent can use its own AS to also infer the states of other agents' attention\nand consequently enhance coordination with other agents. As such, multi-agent\nreinforcement learning would be an ideal setting to experimentally test the\nvalidity of AST. We explore different ways in which attention and AS interact\nwith each other. Our preliminary results indicate that agents that implement\nthe AS as a recurrent internal control achieve the best performance. In\ngeneral, these exploratory experiments suggest that equipping artificial agents\nwith a model of attention can enhance their social intelligence."
            ],
            "interesting paper": 4
        },
        {
            "papers": [
                "AI Coach Assist: An Automated Approach for Call Recommendation in\n  Contact Centers for Agent Coaching\nIn recent years, the utilization of Artificial Intelligence (AI) in the\ncontact center industry is on the rise. One area where AI can have a\nsignificant impact is in the coaching of contact center agents. By analyzing\ncall transcripts using Natural Language Processing (NLP) techniques, it would\nbe possible to quickly determine which calls are most relevant for coaching\npurposes. In this paper, we present AI Coach Assist, which leverages the\npre-trained transformer-based language models to determine whether a given call\nis coachable or not based on the quality assurance (QA) questions asked by the\ncontact center managers or supervisors. The system was trained and evaluated on\na large dataset collected from real-world contact centers and provides an\neffective way to recommend calls to the contact center managers that are more\nlikely to contain coachable moments. Our experimental findings demonstrate the\npotential of AI Coach Assist to improve the coaching process, resulting in\nenhancing the performance of contact center agents.",
                "Optimization's Neglected Normative Commitments\nOptimization is offered as an objective approach to resolving complex,\nreal-world decisions involving uncertainty and conflicting interests. It drives\nbusiness strategies as well as public policies and, increasingly, lies at the\nheart of sophisticated machine learning systems. A paradigm used to approach\npotentially high-stakes decisions, optimization relies on abstracting the real\nworld to a set of decision(s), objective(s) and constraint(s). Drawing from the\nmodeling process and a range of actual cases, this paper describes the\nnormative choices and assumptions that are necessarily part of using\noptimization. It then identifies six emergent problems that may be neglected:\n1) Misspecified values can yield optimizations that omit certain imperatives\naltogether or incorporate them incorrectly as a constraint or as part of the\nobjective, 2) Problematic decision boundaries can lead to faulty modularity\nassumptions and feedback loops, 3) Failing to account for multiple agents'\ndivergent goals and decisions can lead to policies that serve only certain\nnarrow interests, 4) Mislabeling and mismeasurement can introduce bias and\nimprecision, 5) Faulty use of relaxation and approximation methods,\nunaccompanied by formal characterizations and guarantees, can severely impede\napplicability, and 6) Treating optimization as a justification for action,\nwithout specifying the necessary contextual information, can lead to ethically\ndubious or faulty decisions. Suggestions are given to further understand and\ncurb the harms that can arise when optimization is used wrongfully.",
                "A Comprehensive Overview and Comparative Analysis on Deep Learning\n  Models: CNN, RNN, LSTM, GRU\nDeep learning (DL) has emerged as a powerful subset of machine learning (ML)\nand artificial intelligence (AI), outperforming traditional ML methods,\nespecially in handling unstructured and large datasets. Its impact spans across\nvarious domains, including speech recognition, healthcare, autonomous vehicles,\ncybersecurity, predictive analytics, and more. However, the complexity and\ndynamic nature of real-world problems present challenges in designing effective\ndeep learning models. Consequently, several deep learning models have been\ndeveloped to address different problems and applications. In this article, we\nconduct a comprehensive survey of various deep learning models, including\nConvolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs),\nGenerative Models, Deep Reinforcement Learning (DRL), and Deep Transfer\nLearning. We examine the structure, applications, benefits, and limitations of\neach model. Furthermore, we perform an analysis using three publicly available\ndatasets: IMDB, ARAS, and Fruit-360. We compare the performance of six renowned\ndeep learning models: CNN, Simple RNN, Long Short-Term Memory (LSTM),\nBidirectional LSTM, Gated Recurrent Unit (GRU), and Bidirectional GRU.",
                "Amplification trojan network: Attack deep neural networks by amplifying\n  their inherent weakness\nRecent works found that deep neural networks (DNNs) can be fooled by\nadversarial examples, which are crafted by adding adversarial noise on clean\ninputs. The accuracy of DNNs on adversarial examples will decrease as the\nmagnitude of the adversarial noise increase. In this study, we show that DNNs\ncan be also fooled when the noise is very small under certain circumstances.\nThis new type of attack is called Amplification Trojan Attack (ATAttack).\nSpecifically, we use a trojan network to transform the inputs before sending\nthem to the target DNN. This trojan network serves as an amplifier to amplify\nthe inherent weakness of the target DNN. The target DNN, which is infected by\nthe trojan network, performs normally on clean data while being more vulnerable\nto adversarial examples. Since it only transforms the inputs, the trojan\nnetwork can hide in DNN-based pipelines, e.g. by infecting the pre-processing\nprocedure of the inputs before sending them to the DNNs. This new type of\nthreat should be considered in developing safe DNNs.",
                "Observation Denoising in CYRUS Soccer Simulation 2D Team For RoboCup\n  2023\nThe RoboCup competitions hold various leagues, and the Soccer Simulation 2D\nLeague is a major one among them. Soccer Simulation 2D (SS2D) match involves\ntwo teams, including 11 players and a coach, competing against each other. The\nplayers can only communicate with the Soccer Simulation Server during the game.\nThis paper presents the latest research of the CYRUS soccer simulation 2D team,\nthe champion of RoboCup 2021. We will explain our denoising idea powered by\nlong short-term memory networks (LSTM) and deep neural networks (DNN). The\nCYRUS team uses the CYRUS2D base code that was developed based on the Helios\nand Gliders bases.",
                "Speech Intelligibility Assessment of Dysarthric Speech by using Goodness\n  of Pronunciation with Uncertainty Quantification\nThis paper proposes an improved Goodness of Pronunciation (GoP) that utilizes\nUncertainty Quantification (UQ) for automatic speech intelligibility assessment\nfor dysarthric speech. Current GoP methods rely heavily on neural\nnetwork-driven overconfident predictions, which is unsuitable for assessing\ndysarthric speech due to its significant acoustic differences from healthy\nspeech. To alleviate the problem, UQ techniques were used on GoP by 1)\nnormalizing the phoneme prediction (entropy, margin, maxlogit, logit-margin)\nand 2) modifying the scoring function (scaling, prior normalization). As a\nresult, prior-normalized maxlogit GoP achieves the best performance, with a\nrelative increase of 5.66%, 3.91%, and 23.65% compared to the baseline GoP for\nEnglish, Korean, and Tamil, respectively. Furthermore, phoneme analysis is\nconducted to identify which phoneme scores significantly correlate with\nintelligibility scores in each language."
            ],
            "interesting paper": 1
        },
        {
            "papers": [
                "The Digital Divide in Process Safety: Quantitative Risk Analysis of\n  Human-AI Collaboration\nDigital technologies have dramatically accelerated the digital transformation\nin process industries, boosted new industrial applications, upgraded the\nproduction system, and enhanced operational efficiency. In contrast, the\nchallenges and gaps between human and artificial intelligence (AI) have become\nmore and more prominent, whereas the digital divide in process safety is\naggregating. The study attempts to address the following questions: (i)What is\nAI in the process safety context? (ii)What is the difference between AI and\nhumans in process safety? (iii)How do AI and humans collaborate in process\nsafety? (iv)What are the challenges and gaps in human-AI collaboration? (v)How\nto quantify the risk of human-AI collaboration in process safety? Qualitative\nrisk analysis based on brainstorming and literature review, and quantitative\nrisk analysis based on layer of protection analysis (LOPA) and Bayesian network\n(BN), were applied to explore and model. The importance of human reliability\nshould be stressed in the digital age, not usually to increase the reliability\nof AI, and human-centered AI design in process safety needs to be propagated.",
                "Re-imagining health and well-being in low resource African settings\n  using an augmented AI system and a 3D digital twin\nThis paper discusses and explores the potential and relevance of recent\ndevelopments in artificial intelligence (AI) and digital twins for health and\nwell-being in low-resource African countries. We use the case of public health\nemergency response to disease outbreaks and epidemic control. There is\npotential to take advantage of the increasing availability of data and\ndigitization to develop advanced AI methods for analysis and prediction. Using\nan AI systems perspective, we review emerging trends in AI systems and digital\ntwins and propose an initial augmented AI system architecture to illustrate how\nan AI system can work with a 3D digital twin to address public health goals. We\nhighlight scientific knowledge discovery, continual learning, pragmatic\ninteroperability, and interactive explanation and decision-making as essential\nresearch challenges for AI systems and digital twins.",
                "IoT based Personal Voice Assistant\nToday, technological advancement is increasing day by day. Earlier, there was\nonly a computer system in which we could only perform a few tasks. But now,\nmachine learning, artificial intelligence, deep learning, and a few more\ntechnologies have made computer systems so advanced that we can perform any\ntype of task. In this era of advancement, if people are still struggling to\ninteract using various input devices, then it's not worth it. For this reason,\nwe developed a voice assistant using Python that allows the user to run any\ntype of command in Linux without interaction with the keyboard. The main task\nof the voice assistant is to minimize the use of input devices like the\nkeyboard and mouse. It will also reduce hardware space and cost.",
                "Taming AI Bots: Controllability of Neural States in Large Language\n  Models\nWe tackle the question of whether an agent can, by suitable choice of\nprompts, control an AI bot to any state. To that end, we first introduce a\nformal definition of ``meaning'' that is amenable to analysis. Then, we\ncharacterize ``meaningful data'' on which large language models (LLMs) are\nostensibly trained, and ``well-trained LLMs'' through conditions that are\nlargely met by today's LLMs. While a well-trained LLM constructs an embedding\nspace of meanings that is Euclidean, meanings themselves do not form a vector\n(linear) subspace, but rather a quotient space within. We then characterize the\nsubset of meanings that can be reached by the state of the LLMs for some input\nprompt, and show that a well-trained bot can reach any meaning albeit with\nsmall probability. We then introduce a stronger notion of controllability as\n{\\em almost certain reachability}, and show that, when restricted to the space\nof meanings, an AI bot is controllable. We do so after introducing a functional\ncharacterization of attentive AI bots, and finally derive necessary and\nsufficient conditions for controllability. The fact that AI bots are\ncontrollable means that an adversary could steer them towards any state.\nHowever, the sampling process can be designed to counteract adverse actions and\navoid reaching undesirable regions of state space before their boundary is\ncrossed.",
                "Towards a Technology-Driven Adaptive Decision Support System for\n  Integrated Pavement and Maintenance strategies (TDADSS-IPM): focus on risk\n  assessment framework for climate change adaptation\nDecision Support Systems for pavement and maintenance strategies have\ntraditionally been designed as silos led to local optimum systems. Moreover,\nsince big data usage didn't exist as result of Industry 4.0 as of today, DSSs\nwere not initially designed adaptive to the sources of uncertainties led to\nrigid decisions. Motivated by the vulnerability of the road assets to the\nclimate phenomena, this paper takes a visionary step towards introducing a\nTechnology-Driven Adaptive Decision Support System for Integrated Pavement and\nMaintenance activities called TDADSS-IPM. As part of such DSS, a bottom-up risk\nassessment model is met via Bayesian Belief Networks (BBN) to realize the\nactual condition of the Danish roads due to weather condition. Such model fills\nthe gaps in the knowledge domain and develops a platform that can be trained\nover time, and applied in real-time to the actual event.",
                "Key-Value Transformer\nTransformers have emerged as the prevailing standard solution for various AI\ntasks, including computer vision and natural language processing. The widely\nadopted Query, Key, and Value formulation (QKV) has played a significant role\nin this. Nevertheless, no research has examined the essentiality of these three\ncomponents for transformer performance. Therefore, we conducted an evaluation\nof the key-value formulation (KV), which generates symmetric attention maps,\nalong with an asymmetric version that incorporates a 2D positional encoding\ninto the attention matrix. Remarkably, this transformer requires fewer\nparameters and computation than the original one. Through experiments\nencompassing three task types -- synthetics (such as reversing or sorting a\nlist), vision (mnist or cifar classification), and NLP (character generation\nand translation) -- we discovered that the KV transformer occasionally\noutperforms the QKV transformer. However, it also exhibits instances of\nunderperformance compared to QKV, making it challenging to draw a definitive\nconclusion. Nonetheless, we consider the reported results to be encouraging and\nanticipate that they may pave the way for more efficient transformers in the\nfuture."
            ],
            "interesting paper": 4
        },
        {
            "papers": [
                "Towards a Unifying Model of Rationality in Multiagent Systems\nMultiagent systems deployed in the real world need to cooperate with other\nagents (including humans) nearly as effectively as these agents cooperate with\none another. To design such AI, and provide guarantees of its effectiveness, we\nneed to clearly specify what types of agents our AI must be able to cooperate\nwith. In this work we propose a generic model of socially intelligent agents,\nwhich are individually rational learners that are also able to cooperate with\none another (in the sense that their joint behavior is Pareto efficient). We\ndefine rationality in terms of the regret incurred by each agent over its\nlifetime, and show how we can construct socially intelligent agents for\ndifferent forms of regret. We then discuss the implications of this model for\nthe development of \"robust\" MAS that can cooperate with a wide variety of\nsocially intelligent agents.",
                "RE-centric Recommendations for the Development of Trustworthy(er)\n  Autonomous Systems\nComplying with the EU AI Act (AIA) guidelines while developing and\nimplementing AI systems will soon be mandatory within the EU. However,\npractitioners lack actionable instructions to operationalise ethics during AI\nsystems development. A literature review of different ethical guidelines\nrevealed inconsistencies in the principles addressed and the terminology used\nto describe them. Furthermore, requirements engineering (RE), which is\nidentified to foster trustworthiness in the AI development process from the\nearly stages was observed to be absent in a lot of frameworks that support the\ndevelopment of ethical and trustworthy AI. This incongruous phrasing combined\nwith a lack of concrete development practices makes trustworthy AI development\nharder. To address this concern, we formulated a comparison table for the\nterminology used and the coverage of the ethical AI principles in major ethical\nAI guidelines. We then examined the applicability of ethical AI development\nframeworks for performing effective RE during the development of trustworthy AI\nsystems. A tertiary review and meta-analysis of literature discussing ethical\nAI frameworks revealed their limitations when developing trustworthy AI. Based\non our findings, we propose recommendations to address such limitations during\nthe development of trustworthy AI.",
                "Split Federated Learning: Speed up Model Training in Resource-Limited\n  Wireless Networks\nIn this paper, we propose a novel distributed learning scheme, named\ngroup-based split federated learning (GSFL), to speed up artificial\nintelligence (AI) model training. Specifically, the GSFL operates in a\nsplit-then-federated manner, which consists of three steps: 1) Model\ndistribution, in which the access point (AP) splits the AI models and\ndistributes the client-side models to clients; 2) Model training, in which each\nclient executes forward propagation and transmit the smashed data to the edge\nserver. The edge server executes forward and backward propagation and then\nreturns the gradient to the clients for updating local client-side models; and\n3) Model aggregation, in which edge servers aggregate the server-side and\nclient-side models. Simulation results show that the GSFL outperforms vanilla\nsplit learning and federated learning schemes in terms of overall training\nlatency while achieving satisfactory accuracy.",
                "Perceived Trustworthiness of Natural Language Generators\nNatural Language Generation tools, such as chatbots that can generate\nhuman-like conversational text, are becoming more common both for personal and\nprofessional use. However, there are concerns about their trustworthiness and\nethical implications. The paper addresses the problem of understanding how\ndifferent users (e.g., linguists, engineers) perceive and adopt these tools and\ntheir perception of machine-generated text quality. It also discusses the\nperceived advantages and limitations of Natural Language Generation tools, as\nwell as users' beliefs on governance strategies. The main findings of this\nstudy include the impact of users' field and level of expertise on the\nperceived trust and adoption of Natural Language Generation tools, the users'\nassessment of the accuracy, fluency, and potential biases of machine-generated\ntext in comparison to human-written text, and an analysis of the advantages and\nethical risks associated with these tools as identified by the participants.\nMoreover, this paper discusses the potential implications of these findings for\nenhancing the AI development process. The paper sheds light on how different\nuser characteristics shape their beliefs on the quality and overall\ntrustworthiness of machine-generated text. Furthermore, it examines the\nbenefits and risks of these tools from the perspectives of different users.",
                "Chatbots put to the test in math and logic problems: A preliminary\n  comparison and assessment of ChatGPT-3.5, ChatGPT-4, and Google Bard\nA comparison between three chatbots which are based on large language models,\nnamely ChatGPT-3.5, ChatGPT-4 and Google Bard is presented, focusing on their\nability to give correct answers to mathematics and logic problems. In\nparticular, we check their ability to Understand the problem at hand; Apply\nappropriate algorithms or methods for its solution; and Generate a coherent\nresponse and a correct answer. We use 30 questions that are clear, without any\nambiguities, fully described with plain text only, and have a unique, well\ndefined correct answer. The questions are divided into two sets of 15 each. The\nquestions of Set A are 15 \"Original\" problems that cannot be found online,\nwhile Set B contains 15 \"Published\" problems that one can find online, usually\nwith their solution. Each question is posed three times to each chatbot. The\nanswers are recorded and discussed, highlighting their strengths and\nweaknesses. It has been found that for straightforward arithmetic, algebraic\nexpressions, or basic logic puzzles, chatbots may provide accurate solutions,\nalthough not in every attempt. However, for more complex mathematical problems\nor advanced logic tasks, their answers, although written in a usually\n\"convincing\" way, may not be reliable. Consistency is also an issue, as many\ntimes a chatbot will provide conflicting answers when given the same question\nmore than once. A comparative quantitative evaluation of the three chatbots is\nmade through scoring their final answers based on correctness. It was found\nthat ChatGPT-4 outperforms ChatGPT-3.5 in both sets of questions. Bard comes\nthird in the original questions of Set A, behind the other two chatbots, while\nit has the best performance (first place) in the published questions of Set B.\nThis is probably because Bard has direct access to the internet, in contrast to\nChatGPT chatbots which do not have any communication with the outside world.",
                "Game of Tones: Faculty detection of GPT-4 generated content in\n  university assessments\nThis study explores the robustness of university assessments against the use\nof Open AI's Generative Pre-Trained Transformer 4 (GPT-4) generated content and\nevaluates the ability of academic staff to detect its use when supported by the\nTurnitin Artificial Intelligence (AI) detection tool. The research involved\ntwenty-two GPT-4 generated submissions being created and included in the\nassessment process to be marked by fifteen different faculty members. The study\nreveals that although the detection tool identified 91% of the experimental\nsubmissions as containing some AI-generated content, the total detected content\nwas only 54.8%. This suggests that the use of adversarial techniques regarding\nprompt engineering is an effective method in evading AI detection tools and\nhighlights that improvements to AI detection software are needed. Using the\nTurnitin AI detect tool, faculty reported 54.5% of the experimental submissions\nto the academic misconduct process, suggesting the need for increased awareness\nand training into these tools. Genuine submissions received a mean score of\n54.4, whereas AI-generated content scored 52.3, indicating the comparable\nperformance of GPT-4 in real-life situations. Recommendations include adjusting\nassessment strategies to make them more resistant to the use of AI tools, using\nAI-inclusive assessment where possible, and providing comprehensive training\nprograms for faculty and students. This research contributes to understanding\nthe relationship between AI-generated content and academic assessment, urging\nfurther investigation to preserve academic integrity."
            ],
            "interesting paper": 3
        },
        {
            "papers": [
                "Intent-aligned AI systems deplete human agency: the need for agency\n  foundations research in AI safety\nThe rapid advancement of artificial intelligence (AI) systems suggests that\nartificial general intelligence (AGI) systems may soon arrive. Many researchers\nare concerned that AIs and AGIs will harm humans via intentional misuse\n(AI-misuse) or through accidents (AI-accidents). In respect of AI-accidents,\nthere is an increasing effort focused on developing algorithms and paradigms\nthat ensure AI systems are aligned to what humans intend, e.g. AI systems that\nyield actions or recommendations that humans might judge as consistent with\ntheir intentions and goals. Here we argue that alignment to human intent is\ninsufficient for safe AI systems and that preservation of long-term agency of\nhumans may be a more robust standard, and one that needs to be separated\nexplicitly and a priori during optimization. We argue that AI systems can\nreshape human intention and discuss the lack of biological and psychological\nmechanisms that protect humans from loss of agency. We provide the first formal\ndefinition of agency-preserving AI-human interactions which focuses on\nforward-looking agency evaluations and argue that AI systems - not humans -\nmust be increasingly tasked with making these evaluations. We show how agency\nloss can occur in simple environments containing embedded agents that use\ntemporal-difference learning to make action recommendations. Finally, we\npropose a new area of research called \"agency foundations\" and pose four\ninitial topics designed to improve our understanding of agency in AI-human\ninteractions: benevolent game theory, algorithmic foundations of human rights,\nmechanistic interpretability of agency representation in neural-networks and\nreinforcement learning from internal states.",
                "Disentangling and Operationalizing AI Fairness at LinkedIn\nOperationalizing AI fairness at LinkedIn's scale is challenging not only\nbecause there are multiple mutually incompatible definitions of fairness but\nalso because determining what is fair depends on the specifics and context of\nthe product where AI is deployed. Moreover, AI practitioners need clarity on\nwhat fairness expectations need to be addressed at the AI level. In this paper,\nwe present the evolving AI fairness framework used at LinkedIn to address these\nthree challenges. The framework disentangles AI fairness by separating out\nequal treatment and equitable product expectations. Rather than imposing a\ntrade-off between these two commonly opposing interpretations of fairness, the\nframework provides clear guidelines for operationalizing equal AI treatment\ncomplemented with a product equity strategy. This paper focuses on the equal AI\ntreatment component of LinkedIn's AI fairness framework, shares the principles\nthat support it, and illustrates their application through a case study. We\nhope this paper will encourage other big tech companies to join us in sharing\ntheir approach to operationalizing AI fairness at scale, so that together we\ncan keep advancing this constantly evolving field.",
                "Evaluating GPT's Programming Capability through CodeWars' Katas\nIn the burgeoning field of artificial intelligence (AI), understanding the\ncapabilities and limitations of programming-oriented models is crucial. This\npaper presents a novel evaluation of the programming proficiency of Generative\nPretrained Transformer (GPT) models, specifically GPT-3.5 and GPT-4, against\ncoding problems of varying difficulty levels drawn from Codewars. The\nexperiments reveal a distinct boundary at the 3kyu level, beyond which these\nGPT models struggle to provide solutions. These findings led to the proposal of\na measure for coding problem complexity that incorporates both problem\ndifficulty and the time required for solution. The research emphasizes the need\nfor validation and creative thinking capabilities in AI models to better\nemulate human problem-solving techniques. Future work aims to refine this\nproposed complexity measure, enhance AI models with these suggested\ncapabilities, and develop an objective measure for programming problem\ndifficulty. The results of this research offer invaluable insights for\nimproving AI programming capabilities and advancing the frontier of AI\nproblem-solving abilities.",
                "Traffic Prediction using Artificial Intelligence: Review of Recent\n  Advances and Emerging Opportunities\nTraffic prediction plays a crucial role in alleviating traffic congestion\nwhich represents a critical problem globally, resulting in negative\nconsequences such as lost hours of additional travel time and increased fuel\nconsumption. Integrating emerging technologies into transportation systems\nprovides opportunities for improving traffic prediction significantly and\nbrings about new research problems. In order to lay the foundation for\nunderstanding the open research challenges in traffic prediction, this survey\naims to provide a comprehensive overview of traffic prediction methodologies.\nSpecifically, we focus on the recent advances and emerging research\nopportunities in Artificial Intelligence (AI)-based traffic prediction methods,\ndue to their recent success and potential in traffic prediction, with an\nemphasis on multivariate traffic time series modeling. We first provide a list\nand explanation of the various data types and resources used in the literature.\nNext, the essential data preprocessing methods within the traffic prediction\ncontext are categorized, and the prediction methods and applications are\nsubsequently summarized. Lastly, we present primary research challenges in\ntraffic prediction and discuss some directions for future research.",
                "Towards Explainable and Language-Agnostic LLMs: Symbolic Reverse\n  Engineering of Language at Scale\nLarge language models (LLMs) have achieved a milestone that undenia-bly\nchanged many held beliefs in artificial intelligence (AI). However, there\nremains many limitations of these LLMs when it comes to true language\nunderstanding, limitations that are a byproduct of the under-lying architecture\nof deep neural networks. Moreover, and due to their subsymbolic nature,\nwhatever knowledge these models acquire about how language works will always be\nburied in billions of microfeatures (weights), none of which is meaningful on\nits own, making such models hopelessly unexplainable. To address these\nlimitations, we suggest com-bining the strength of symbolic representations\nwith what we believe to be the key to the success of LLMs, namely a successful\nbottom-up re-verse engineering of language at scale. As such we argue for a\nbottom-up reverse engineering of language in a symbolic setting. Hints on what\nthis project amounts to have been suggested by several authors, and we discuss\nin some detail here how this project could be accomplished.",
                "Unified Information Dynamic Analysis of Quantum Decision-Making and\n  Search Algorithms: Computational Intelligence Measure\nThere are important algorithms built upon a mixture of basic techniques\ndescribed; for example, the Fast Fourier Transform (FFT) employs both\nDivide-and-Conquer and Transform-and-Conquer techniques. In this article, the\nevolution of a quantum algorithm (QA) is examined from an information theory\nviewpoint. The complex vector entering the quantum algorithmic gate - QAG is\nconsidered as an information source both from the classical and the quantum\nlevel. The analysis of the classical and quantum information flow in\nDeutsch-Jozsa, Shor and Grover algorithms is used. It is shown that QAG, based\non superposition of states, quantum entanglement and interference, when acting\non the input vector, stores information into the system state, minimizing the\ngap between classical Shannon entropy and quantum von Neumann entropy.\nMinimizing of the gap between Shannon and von Neumann entropies is considered\nas a termination criterion of QA computational intelligence measure."
            ],
            "interesting paper": 2
        },
        {
            "papers": [
                "Human or Not? A Gamified Approach to the Turing Test\nWe present \"Human or Not?\", an online game inspired by the Turing test, that\nmeasures the capability of AI chatbots to mimic humans in dialog, and of humans\nto tell bots from other humans. Over the course of a month, the game was played\nby over 1.5 million users who engaged in anonymous two-minute chat sessions\nwith either another human or an AI language model which was prompted to behave\nlike humans. The task of the players was to correctly guess whether they spoke\nto a person or to an AI. This largest scale Turing-style test conducted to date\nrevealed some interesting facts. For example, overall users guessed the\nidentity of their partners correctly in only 68% of the games. In the subset of\nthe games in which users faced an AI bot, users had even lower correct guess\nrates of 60% (that is, not much higher than chance). This white paper details\nthe development, deployment, and results of this unique experiment. While this\nexperiment calls for many extensions and refinements, these findings already\nbegin to shed light on the inevitable near future which will commingle humans\nand AI.",
                "From Human-Centered to Social-Centered Artificial Intelligence:\n  Assessing ChatGPT's Impact through Disruptive Events\nLarge language models (LLMs) and dialogue agents have existed for years, but\nthe release of recent GPT models has been a watershed moment for artificial\nintelligence (AI) research and society at large. Immediately recognized for its\ngenerative capabilities and versatility, ChatGPT's impressive proficiency\nacross technical and creative domains led to its widespread adoption. While\nsociety grapples with the emerging cultural impacts of ChatGPT, critiques of\nChatGPT's impact within the machine learning community have coalesced around\nits performance or other conventional Responsible AI evaluations relating to\nbias, toxicity, and 'hallucination.' We argue that these latter critiques draw\nheavily on a particular conceptualization of the 'human-centered' framework,\nwhich tends to cast atomized individuals as the key recipients of both the\nbenefits and detriments of technology. In this article, we direct attention to\nanother dimension of LLMs and dialogue agents' impact: their effect on social\ngroups, institutions, and accompanying norms and practices. By illustrating\nChatGPT's social impact through three disruptive events, we challenge\nindividualistic approaches in AI development and contribute to ongoing debates\naround the ethical and responsible implementation of AI systems. We hope this\neffort will call attention to more comprehensive and longitudinal evaluation\ntools and compel technologists to go beyond human-centered thinking and ground\ntheir efforts through social-centered AI.",
                "Citizen Perspectives on Necessary Safeguards to the Use of AI by Law\n  Enforcement Agencies\nIn the light of modern technological advances, Artificial Intelligence (AI)\nis relied upon to enhance performance, increase efficiency, and maximize gains.\nFor Law Enforcement Agencies (LEAs), it can prove valuable in optimizing\nevidence analysis and establishing proactive prevention measures. Nevertheless,\ncitizens raise legitimate concerns around privacy invasions, biases,\ninequalities, and inaccurate decisions. This study explores the views of 111\ncitizens towards AI use by police through interviews, and integrates societal\nconcerns along with propositions of safeguards from negative effects of AI use\nby LEAs in the context of cybercrime and terrorism.",
                "Credit Card Fraud Detection Using Asexual Reproduction Optimization\nAs the number of credit card users has increased, detecting fraud in this\ndomain has become a vital issue. Previous literature has applied various\nsupervised and unsupervised machine learning methods to find an effective fraud\ndetection system. However, some of these methods require an enormous amount of\ntime to achieve reasonable accuracy. In this paper, an Asexual Reproduction\nOptimization (ARO) approach was employed, which is a supervised method to\ndetect credit card fraud. ARO refers to a kind of production in which one\nparent produces some offspring. By applying this method and sampling just from\nthe majority class, the effectiveness of the classification is increased. A\ncomparison to Artificial Immune Systems (AIS), which is one of the best methods\nimplemented on current datasets, has shown that the proposed method is able to\nremarkably reduce the required training time and at the same time increase the\nrecall that is important in fraud detection problems. The obtained results show\nthat ARO achieves the best cost in a short time, and consequently, it can be\nconsidered a real-time fraud detection system.",
                "Human Control: Definitions and Algorithms\nHow can humans stay in control of advanced artificial intelligence systems?\nOne proposal is corrigibility, which requires the agent to follow the\ninstructions of a human overseer, without inappropriately influencing them. In\nthis paper, we formally define a variant of corrigibility called shutdown\ninstructability, and show that it implies appropriate shutdown behavior,\nretention of human autonomy, and avoidance of user harm. We also analyse the\nrelated concepts of non-obstruction and shutdown alignment, three previously\nproposed algorithms for human control, and one new algorithm.",
                "Decision-Oriented Dialogue for Human-AI Collaboration\nWe describe a class of tasks called decision-oriented dialogues, in which AI\nassistants such as large language models (LMs) must collaborate with one or\nmore humans via natural language to help them make complex decisions. We\nformalize three domains in which users face everyday decisions: (1) choosing an\nassignment of reviewers to conference papers, (2) planning a multi-step\nitinerary in a city, and (3) negotiating travel plans for a group of friends.\nIn each of these settings, AI assistants and users have disparate abilities\nthat they must combine to arrive at the best decision: assistants can access\nand process large amounts of information, while users have preferences and\nconstraints external to the system. For each task, we build a dialogue\nenvironment where agents receive a reward based on the quality of the final\ndecision they reach. We evaluate LMs in self-play and in collaboration with\nhumans and find that they fall short compared to human assistants, achieving\nmuch lower rewards despite engaging in longer dialogues. We highlight a number\nof challenges models face in decision-oriented dialogues, ranging from\ngoal-directed behavior to reasoning and optimization, and release our\nenvironments as a testbed for future work."
            ],
            "interesting paper": 2
        },
        {
            "papers": [
                "AI Liability Insurance With an Example in AI-Powered E-diagnosis System\nArtificial Intelligence (AI) has received an increasing amount of attention\nin multiple areas. The uncertainties and risks in AI-powered systems have\ncreated reluctance in their wild adoption. As an economic solution to\ncompensate for potential damages, AI liability insurance is a promising market\nto enhance the integration of AI into daily life. In this work, we use an\nAI-powered E-diagnosis system as an example to study AI liability insurance. We\nprovide a quantitative risk assessment model with evidence-based numerical\nanalysis. We discuss the insurability criteria for AI technologies and suggest\nnecessary adjustments to accommodate the features of AI products. We show that\nAI liability insurance can act as a regulatory mechanism to incentivize\ncompliant behaviors and serve as a certificate of high-quality AI systems.\nFurthermore, we suggest premium adjustment to reflect the dynamic evolution of\nthe inherent uncertainty in AI. Moral hazard problems are discussed and\nsuggestions for AI liability insurance are provided.",
                "AI and the creative realm: A short review of current and future\n  applications\nThis study explores the concept of creativity and artificial intelligence\n(AI) and their recent integration. While AI has traditionally been perceived as\nincapable of generating new ideas or creating art, the development of more\nsophisticated AI models and the proliferation of human-computer interaction\ntools have opened up new possibilities for AI in artistic creation. This study\ninvestigates the various applications of AI in a creative context,\ndifferentiating between the type of art, language, and algorithms used. It also\nconsiders the philosophical implications of AI and creativity, questioning\nwhether consciousness can be researched in machines and AI's potential\ninterests and decision-making capabilities. Overall, we aim to stimulate a\nreflection on AI's use and ethical implications in creative contexts.",
                "Integrated Sensing-Communication-Computation for Edge Artificial\n  Intelligence\nEdge artificial intelligence (AI) has been a promising solution towards 6G to\nempower a series of advanced techniques such as digital twins, holographic\nprojection, semantic communications, and auto-driving, for achieving\nintelligence of everything. The performance of edge AI tasks, including edge\nlearning and edge AI inference, depends on the quality of three highly coupled\nprocesses, i.e., sensing for data acquisition, computation for information\nextraction, and communication for information transmission. However, these\nthree modules need to compete for network resources for enhancing their own\nquality-of-services. To this end, integrated sensing-communication-computation\n(ISCC) is of paramount significance for improving resource utilization as well\nas achieving the customized goals of edge AI tasks. By investigating the\ninterplay among the three modules, this article presents various kinds of ISCC\nschemes for federated edge learning tasks and edge AI inference tasks in both\napplication and physical layers.",
                "An Architecture for Deploying Reinforcement Learning in Industrial\n  Environments\nIndustry 4.0 is driven by demands like shorter time-to-market, mass\ncustomization of products, and batch size one production. Reinforcement\nLearning (RL), a machine learning paradigm shown to possess a great potential\nin improving and surpassing human level performance in numerous complex tasks,\nallows coping with the mentioned demands. In this paper, we present an OPC UA\nbased Operational Technology (OT)-aware RL architecture, which extends the\nstandard RL setting, combining it with the setting of digital twins. Moreover,\nwe define an OPC UA information model allowing for a generalized plug-and-play\nlike approach for exchanging the RL agent used. In conclusion, we demonstrate\nand evaluate the architecture, by creating a proof of concept. By means of\nsolving a toy example, we show that this architecture can be used to determine\nthe optimal policy using a real control system.",
                "Adaptation and Optimization of Automatic Speech Recognition (ASR) for\n  the Maritime Domain in the Field of VHF Communication\nThis paper introduces a multilingual automatic speech recognizer (ASR) for\nmaritime radio communi-cation that automatically converts received VHF radio\nsignals into text. The challenges of maritime radio communication are described\nat first, and the deep learning architecture of marFM consisting of audio\nprocessing techniques and machine learning algorithms is presented.\nSubsequently, maritime radio data of interest is analyzed and then used to\nevaluate the transcription performance of our ASR model for various maritime\nradio data.",
                "The ethical ambiguity of AI data enrichment: Measuring gaps in research\n  ethics norms and practices\nThe technical progression of artificial intelligence (AI) research has been\nbuilt on breakthroughs in fields such as computer science, statistics, and\nmathematics. However, in the past decade AI researchers have increasingly\nlooked to the social sciences, turning to human interactions to solve the\nchallenges of model development. Paying crowdsourcing workers to generate or\ncurate data, or data enrichment, has become indispensable for many areas of AI\nresearch, from natural language processing to reinforcement learning from human\nfeedback (RLHF). Other fields that routinely interact with crowdsourcing\nworkers, such as Psychology, have developed common governance requirements and\nnorms to ensure research is undertaken ethically. This study explores how, and\nto what extent, comparable research ethics requirements and norms have\ndeveloped for AI research and data enrichment. We focus on the approach taken\nby two leading conferences: ICLR and NeurIPS, and journal publisher Springer.\nIn a longitudinal study of accepted papers, and via a comparison with\nPsychology and CHI papers, this work finds that leading AI venues have begun to\nestablish protocols for human data collection, but these are are inconsistently\nfollowed by authors. Whilst Psychology papers engaging with crowdsourcing\nworkers frequently disclose ethics reviews, payment data, demographic data and\nother information, similar disclosures are far less common in leading AI venues\ndespite similar guidance. The work concludes with hypotheses to explain these\ngaps in research ethics practices and considerations for its implications."
            ],
            "interesting paper": 3
        }
    ],
    "Casey Lopez": [
        {
            "papers": [
                "Sharpness-Aware Minimization Revisited: Weighted Sharpness as a\n  Regularization Term\nDeep Neural Networks (DNNs) generalization is known to be closely related to\nthe flatness of minima, leading to the development of Sharpness-Aware\nMinimization (SAM) for seeking flatter minima and better generalization. In\nthis paper, we revisit the loss of SAM and propose a more general method,\ncalled WSAM, by incorporating sharpness as a regularization term. We prove its\ngeneralization bound through the combination of PAC and Bayes-PAC techniques,\nand evaluate its performance on various public datasets. The results\ndemonstrate that WSAM achieves improved generalization, or is at least highly\ncompetitive, compared to the vanilla optimizer, SAM and its variants. The code\nis available at\nhttps://github.com/intelligent-machine-learning/dlrover/tree/master/atorch/atorch/optimizers.",
                "Deep Learning and Ethics\nThis article appears as chapter 21 of Prince (2023, Understanding Deep\nLearning); a complete draft of the textbook is available here:\nhttp://udlbook.com. This chapter considers potential harms arising from the\ndesign and use of AI systems. These include algorithmic bias, lack of\nexplainability, data privacy violations, militarization, fraud, and\nenvironmental concerns. The aim is not to provide advice on being more ethical.\nInstead, the goal is to express ideas and start conversations in key areas that\nhave received attention in philosophy, political science, and the broader\nsocial sciences.",
                "PDE+: Enhancing Generalization via PDE with Adaptive Distributional\n  Diffusion\nThe generalization of neural networks is a central challenge in machine\nlearning, especially concerning the performance under distributions that differ\nfrom training ones. Current methods, mainly based on the data-driven paradigm\nsuch as data augmentation, adversarial training, and noise injection, may\nencounter limited generalization due to model non-smoothness. In this paper, we\npropose to investigate generalization from a Partial Differential Equation\n(PDE) perspective, aiming to enhance it directly through the underlying\nfunction of neural networks, rather than focusing on adjusting input data.\nSpecifically, we first establish the connection between neural network\ngeneralization and the smoothness of the solution to a specific PDE, namely\n\"transport equation\". Building upon this, we propose a general framework that\nintroduces adaptive distributional diffusion into transport equation to enhance\nthe smoothness of its solution, thereby improving generalization. In the\ncontext of neural networks, we put this theoretical framework into practice as\n$\\textbf{PDE+}$ ($\\textbf{PDE}$ with $\\textbf{A}$daptive\n$\\textbf{D}$istributional $\\textbf{D}$iffusion) which diffuses each sample into\na distribution covering semantically similar inputs. This enables better\ncoverage of potentially unobserved distributions in training, thus improving\ngeneralization beyond merely data-driven methods. The effectiveness of PDE+ is\nvalidated through extensive experimental settings, demonstrating its superior\nperformance compared to SOTA methods.",
                "READ: Recurrent Adaptation of Large Transformers\nFine-tuning large-scale Transformers has led to the explosion of many AI\napplications across Natural Language Processing and Computer Vision tasks.\nHowever, fine-tuning all pre-trained model parameters becomes impractical as\nthe model size and number of tasks increase. Parameter-efficient transfer\nlearning (PETL) methods aim to address these challenges. While effective in\nreducing the number of trainable parameters, PETL methods still require\nsignificant energy and computational resources to fine-tune. In this paper, we\nintroduce \\textbf{RE}current \\textbf{AD}aption (READ) -- a lightweight and\nmemory-efficient fine-tuning method -- to overcome the limitations of the\ncurrent PETL approaches. Specifically, READ inserts a small RNN network\nalongside the backbone model so that the model does not have to back-propagate\nthrough the large backbone network. Through comprehensive empirical evaluation\nof the GLUE benchmark, we demonstrate READ can achieve a $56\\%$ reduction in\nthe training memory consumption and an $84\\%$ reduction in the GPU energy usage\nwhile retraining high model quality compared to full-tuning. Additionally, the\nmodel size of READ does not grow with the backbone model size, making it a\nhighly scalable solution for fine-tuning large Transformers.",
                "Unifying gradient regularization for Heterogeneous Graph Neural Networks\nHeterogeneous Graph Neural Networks (HGNNs) are a class of powerful deep\nlearning methods widely used to learn representations of heterogeneous graphs.\nDespite the fast development of HGNNs, they still face some challenges such as\nover-smoothing, and non-robustness. Previous studies have shown that these\nproblems can be reduced by using gradient regularization methods. However, the\nexisting gradient regularization methods focus on either graph topology or node\nfeatures. There is no universal approach to integrate these features, which\nseverely affects the efficiency of regularization. In addition, the inclusion\nof gradient regularization into HGNNs sometimes leads to some problems, such as\nan unstable training process, increased complexity and insufficient coverage\nregularized information. Furthermore, there is still short of a complete\ntheoretical analysis of the effects of gradient regularization on HGNNs. In\nthis paper, we propose a novel gradient regularization method called Grug,\nwhich iteratively applies regularization to the gradients generated by both\npropagated messages and the node features during the message-passing process.\nGrug provides a unified framework integrating graph topology and node features,\nbased on which we conduct a detailed theoretical analysis of their\neffectiveness. Specifically, the theoretical analyses elaborate the advantages\nof Grug: 1) Decreasing sample variance during the training process (Stability);\n2) Enhancing the generalization of the model (Universality); 3) Reducing the\ncomplexity of the model (Simplicity); 4) Improving the integrity and diversity\nof graph information utilization (Diversity). As a result, Grug has the\npotential to surpass the theoretical upper bounds set by DropMessage (AAAI-23\nDistinguished Papers). In addition, we evaluate Grug on five public real-world\ndatasets with two downstream tasks...",
                "Deep Neural Networks in Video Human Action Recognition: A Review\nCurrently, video behavior recognition is one of the most foundational tasks\nof computer vision. The 2D neural networks of deep learning are built for\nrecognizing pixel-level information such as images with RGB, RGB-D, or optical\nflow formats, with the current increasingly wide usage of surveillance video\nand more tasks related to human action recognition. There are increasing tasks\nrequiring temporal information for frames dependency analysis. The researchers\nhave widely studied video-based recognition rather than\nimage-based(pixel-based) only to extract more informative elements from\ngeometry tasks. Our current related research addresses multiple novel proposed\nresearch works and compares their advantages and disadvantages between the\nderived deep learning frameworks rather than machine learning frameworks. The\ncomparison happened between existing frameworks and datasets, which are video\nformat data only. Due to the specific properties of human actions and the\nincreasingly wide usage of deep neural networks, we collected all research\nworks within the last three years between 2020 to 2022. In our article, the\nperformance of deep neural networks surpassed most of the techniques in the\nfeature learning and extraction tasks, especially video action recognition."
            ],
            "interesting paper": 3
        },
        {
            "papers": [
                "XGrad: Boosting Gradient-Based Optimizers With Weight Prediction\nIn this paper, we propose a general deep learning training framework XGrad\nwhich introduces weight prediction into the popular gradient-based optimizers\nto boost their convergence and generalization when training the deep neural\nnetwork (DNN) models. In particular, ahead of each mini-batch training, the\nfuture weights are predicted according to the update rule of the used optimizer\nand are then applied to both the forward pass and backward propagation. In this\nway, during the whole training period, the optimizer always utilizes the\ngradients w.r.t. the future weights to update the DNN parameters, making the\ngradient-based optimizer achieve better convergence and generalization compared\nto the original optimizer without weight prediction. XGrad is rather\nstraightforward to implement yet pretty effective in boosting the convergence\nof gradient-based optimizers and the accuracy of DNN models. Empirical results\nconcerning five popular optimizers including SGD with momentum, Adam, AdamW,\nAdaBelief, and AdaM3 demonstrate the effectiveness of our proposal. The\nexperimental results validate that XGrad can attain higher model accuracy than\nthe baseline optimizers when training the DNN models. The code of XGrad will be\navailable at: https://github.com/guanleics/XGrad.",
                "Pruning Distorted Images in MNIST Handwritten Digits\nRecognizing handwritten digits is a challenging task primarily due to the\ndiversity of writing styles and the presence of noisy images. The widely used\nMNIST dataset, which is commonly employed as a benchmark for this task,\nincludes distorted digits with irregular shapes, incomplete strokes, and\nvarying skew in both the training and testing datasets. Consequently, these\nfactors contribute to reduced accuracy in digit recognition. To overcome this\nchallenge, we propose a two-stage deep learning approach. In the first stage,\nwe create a simple neural network to identify distorted digits within the\ntraining set. This model serves to detect and filter out such distorted and\nambiguous images. In the second stage, we exclude these identified images from\nthe training dataset and proceed to retrain the model using the filtered\ndataset. This process aims to improve the classification accuracy and\nconfidence levels while mitigating issues of underfitting and overfitting. Our\nexperimental results demonstrate the effectiveness of the proposed approach,\nachieving an accuracy rate of over 99.5% on the testing dataset. This\nsignificant improvement showcases the potential of our method in enhancing\ndigit classification accuracy. In our future work, we intend to explore the\nscalability of this approach and investigate techniques to further enhance\naccuracy by reducing the size of the training data.",
                "Revisiting Structured Variational Autoencoders\nStructured variational autoencoders (SVAEs) combine probabilistic graphical\nmodel priors on latent variables, deep neural networks to link latent variables\nto observed data, and structure-exploiting algorithms for approximate posterior\ninference. These models are particularly appealing for sequential data, where\nthe prior can capture temporal dependencies. However, despite their conceptual\nelegance, SVAEs have proven difficult to implement, and more general approaches\nhave been favored in practice. Here, we revisit SVAEs using modern machine\nlearning tools and demonstrate their advantages over more general alternatives\nin terms of both accuracy and efficiency. First, we develop a modern\nimplementation for hardware acceleration, parallelization, and automatic\ndifferentiation of the message passing algorithms at the core of the SVAE.\nSecond, we show that by exploiting structure in the prior, the SVAE learns more\naccurate models and posterior distributions, which translate into improved\nperformance on prediction tasks. Third, we show how the SVAE can naturally\nhandle missing data, and we leverage this ability to develop a novel,\nself-supervised training approach. Altogether, these results show that the time\nis ripe to revisit structured variational autoencoders.",
                "SING: A Plug-and-Play DNN Learning Technique\nWe propose SING (StabIlized and Normalized Gradient), a plug-and-play\ntechnique that improves the stability and generalization of the Adam(W)\noptimizer. SING is straightforward to implement and has minimal computational\noverhead, requiring only a layer-wise standardization of the gradients fed to\nAdam(W) without introducing additional hyper-parameters. We support the\neffectiveness and practicality of the proposed approach by showing improved\nresults on a wide range of architectures, problems (such as image\nclassification, depth estimation, and natural language processing), and in\ncombination with other optimizers. We provide a theoretical analysis of the\nconvergence of the method, and we show that by virtue of the standardization,\nSING can escape local minima narrower than a threshold that is inversely\nproportional to the network's depth.",
                "Automated Search-Space Generation Neural Architecture Search\nTo search an optimal sub-network within a general deep neural network (DNN),\nexisting neural architecture search (NAS) methods typically rely on\nhandcrafting a search space beforehand. Such requirements make it challenging\nto extend them onto general scenarios without significant human expertise and\nmanual intervention. To overcome the limitations, we propose Automated\nSearch-Space Generation Neural Architecture Search (ASGNAS), perhaps the first\nautomated system to train general DNNs that cover all candidate connections and\noperations and produce high-performing sub-networks in the one shot manner.\nTechnologically, ASGNAS delivers three noticeable contributions to minimize\nhuman efforts: (i) automated search space generation for general DNNs; (ii) a\nHierarchical Half-Space Projected Gradient (H2SPG) that leverages the hierarchy\nand dependency within generated search space to ensure the network validity\nduring optimization, and reliably produces a solution with both high\nperformance and hierarchical group sparsity; and (iii) automated sub-network\nconstruction upon the H2SPG solution. Numerically, we demonstrate the\neffectiveness of ASGNAS on a variety of general DNNs, including RegNet,\nStackedUnets, SuperResNet, and DARTS, over benchmark datasets such as CIFAR10,\nFashion-MNIST, ImageNet, STL-10 , and SVNH. The sub-networks computed by ASGNAS\nachieve competitive even superior performance compared to the starting full\nDNNs and other state-of-the-arts. The library will be released at\nhttps://github.com/tianyic/only_train_once.",
                "Neural (Tangent Kernel) Collapse\nThis work bridges two important concepts: the Neural Tangent Kernel (NTK),\nwhich captures the evolution of deep neural networks (DNNs) during training,\nand the Neural Collapse (NC) phenomenon, which refers to the emergence of\nsymmetry and structure in the last-layer features of well-trained\nclassification DNNs. We adopt the natural assumption that the empirical NTK\ndevelops a block structure aligned with the class labels, i.e., samples within\nthe same class have stronger correlations than samples from different classes.\nUnder this assumption, we derive the dynamics of DNNs trained with mean squared\n(MSE) loss and break them into interpretable phases. Moreover, we identify an\ninvariant that captures the essence of the dynamics, and use it to prove the\nemergence of NC in DNNs with block-structured NTK. We provide large-scale\nnumerical experiments on three common DNN architectures and three benchmark\ndatasets to support our theory."
            ],
            "interesting paper": 1
        },
        {
            "papers": [
                "Statistically Significant Concept-based Explanation of Image Classifiers\n  via Model Knockoffs\nA concept-based classifier can explain the decision process of a deep\nlearning model by human-understandable concepts in image classification\nproblems. However, sometimes concept-based explanations may cause false\npositives, which misregards unrelated concepts as important for the prediction\ntask. Our goal is to find the statistically significant concept for\nclassification to prevent misinterpretation. In this study, we propose a method\nusing a deep learning model to learn the image concept and then using the\nKnockoff samples to select the important concepts for prediction by controlling\nthe False Discovery Rate (FDR) under a certain value. We evaluate the proposed\nmethod in our synthetic and real data experiments. Also, it shows that our\nmethod can control the FDR properly while selecting highly interpretable\nconcepts to improve the trustworthiness of the model.",
                "Are Deep Neural Networks Adequate Behavioural Models of Human Visual\n  Perception?\nDeep neural networks (DNNs) are machine learning algorithms that have\nrevolutionised computer vision due to their remarkable successes in tasks like\nobject classification and segmentation. The success of DNNs as computer vision\nalgorithms has led to the suggestion that DNNs may also be good models of human\nvisual perception. We here review evidence regarding current DNNs as adequate\nbehavioural models of human core object recognition. To this end, we argue that\nit is important to distinguish between statistical tools and computational\nmodels, and to understand model quality as a multidimensional concept where\nclarity about modelling goals is key. Reviewing a large number of\npsychophysical and computational explorations of core object recognition\nperformance in humans and DNNs, we argue that DNNs are highly valuable\nscientific tools but that as of today DNNs should only be regarded as promising\n-- but not yet adequate -- computational models of human core object\nrecognition behaviour. On the way we dispel a number of myths surrounding DNNs\nin vision science.",
                "Vision Transformers for Small Histological Datasets Learned through\n  Knowledge Distillation\nComputational Pathology (CPATH) systems have the potential to automate\ndiagnostic tasks. However, the artifacts on the digitized histological glass\nslides, known as Whole Slide Images (WSIs), may hamper the overall performance\nof CPATH systems. Deep Learning (DL) models such as Vision Transformers (ViTs)\nmay detect and exclude artifacts before running the diagnostic algorithm. A\nsimple way to develop robust and generalized ViTs is to train them on massive\ndatasets. Unfortunately, acquiring large medical datasets is expensive and\ninconvenient, prompting the need for a generalized artifact detection method\nfor WSIs. In this paper, we present a student-teacher recipe to improve the\nclassification performance of ViT for the air bubbles detection task. ViT,\ntrained under the student-teacher framework, boosts its performance by\ndistilling existing knowledge from the high-capacity teacher model. Our\nbest-performing ViT yields 0.961 and 0.911 F1-score and MCC, respectively,\nobserving a 7% gain in MCC against stand-alone training. The proposed method\npresents a new perspective of leveraging knowledge distillation over transfer\nlearning to encourage the use of customized transformers for efficient\npreprocessing pipelines in the CPATH systems.",
                "Green Runner: A tool for efficient model selection from model\n  repositories\nDeep learning models have become essential in software engineering, enabling\nintelligent features like image captioning and document generation. However,\ntheir popularity raises concerns about environmental impact and inefficient\nmodel selection. This paper introduces GreenRunnerGPT, a novel tool for\nefficiently selecting deep learning models based on specific use cases. It\nemploys a large language model to suggest weights for quality indicators,\noptimizing resource utilization. The tool utilizes a multi-armed bandit\nframework to evaluate models against target datasets, considering tradeoffs. We\ndemonstrate that GreenRunnerGPT is able to identify a model suited to a target\nuse case without wasteful computations that would occur under a brute-force\napproach to model selection.",
                "Stability of implicit neural networks for long-term forecasting in\n  dynamical systems\nForecasting physical signals in long time range is among the most challenging\ntasks in Partial Differential Equations (PDEs) research. To circumvent\nlimitations of traditional solvers, many different Deep Learning methods have\nbeen proposed. They are all based on auto-regressive methods and exhibit\nstability issues. Drawing inspiration from the stability property of implicit\nnumerical schemes, we introduce a stable auto-regressive implicit neural\nnetwork. We develop a theory based on the stability definition of schemes to\nensure the stability in forecasting of this network. It leads us to introduce\nhard constraints on its weights and propagate the dynamics in the latent space.\nOur experimental results validate our stability property, and show improved\nresults at long-term forecasting for two transports PDEs.",
                "Understanding Sparse Neural Networks from their Topology via\n  Multipartite Graph Representations\nPruning-at-Initialization (PaI) algorithms provide Sparse Neural Networks\n(SNNs) which are computationally more efficient than their dense counterparts,\nand try to avoid performance degradation. While much emphasis has been directed\ntowards \\emph{how} to prune, we still do not know \\emph{what topological\nmetrics} of the SNNs characterize \\emph{good performance}. From prior work, we\nhave layer-wise topological metrics by which SNN performance can be predicted:\nthe Ramanujan-based metrics. To exploit these metrics, proper ways to represent\nnetwork layers via Graph Encodings (GEs) are needed, with Bipartite Graph\nEncodings (BGEs) being the \\emph{de-facto} standard at the current stage.\nNevertheless, existing BGEs neglect the impact of the inputs, and do not\ncharacterize the SNN in an end-to-end manner. Additionally, thanks to a\nthorough study of the Ramanujan-based metrics, we discover that they are only\nas good as the \\emph{layer-wise density} as performance predictors, when paired\nwith BGEs. To close both gaps, we design a comprehensive topological analysis\nfor SNNs with both linear and convolutional layers, via (i) a new input-aware\nMultipartite Graph Encoding (MGE) for SNNs and (ii) the design of new\nend-to-end topological metrics over the MGE. With these novelties, we show the\nfollowing: (a) The proposed MGE allows to extract topological metrics that are\nmuch better predictors of the accuracy drop than metrics computed from current\ninput-agnostic BGEs; (b) Which metrics are important at different sparsity\nlevels and for different architectures; (c) A mixture of our topological\nmetrics can rank PaI algorithms more effectively than Ramanujan-based metrics.\nThe codebase is publicly available at https://github.com/eliacunegatti/mge-snn."
            ],
            "interesting paper": 4
        },
        {
            "papers": [
                "A Comprehensive Overview and Comparative Analysis on Deep Learning\n  Models: CNN, RNN, LSTM, GRU\nDeep learning (DL) has emerged as a powerful subset of machine learning (ML)\nand artificial intelligence (AI), outperforming traditional ML methods,\nespecially in handling unstructured and large datasets. Its impact spans across\nvarious domains, including speech recognition, healthcare, autonomous vehicles,\ncybersecurity, predictive analytics, and more. However, the complexity and\ndynamic nature of real-world problems present challenges in designing effective\ndeep learning models. Consequently, several deep learning models have been\ndeveloped to address different problems and applications. In this article, we\nconduct a comprehensive survey of various deep learning models, including\nConvolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs),\nGenerative Models, Deep Reinforcement Learning (DRL), and Deep Transfer\nLearning. We examine the structure, applications, benefits, and limitations of\neach model. Furthermore, we perform an analysis using three publicly available\ndatasets: IMDB, ARAS, and Fruit-360. We compare the performance of six renowned\ndeep learning models: CNN, Simple RNN, Long Short-Term Memory (LSTM),\nBidirectional LSTM, Gated Recurrent Unit (GRU), and Bidirectional GRU.",
                "USIM-DAL: Uncertainty-aware Statistical Image Modeling-based Dense\n  Active Learning for Super-resolution\nDense regression is a widely used approach in computer vision for tasks such\nas image super-resolution, enhancement, depth estimation, etc. However, the\nhigh cost of annotation and labeling makes it challenging to achieve accurate\nresults. We propose incorporating active learning into dense regression models\nto address this problem. Active learning allows models to select the most\ninformative samples for labeling, reducing the overall annotation cost while\nimproving performance. Despite its potential, active learning has not been\nwidely explored in high-dimensional computer vision regression tasks like\nsuper-resolution. We address this research gap and propose a new framework\ncalled USIM-DAL that leverages the statistical properties of colour images to\nlearn informative priors using probabilistic deep neural networks that model\nthe heteroscedastic predictive distribution allowing uncertainty\nquantification. Moreover, the aleatoric uncertainty from the network serves as\na proxy for error that is used for active learning. Our experiments on a wide\nvariety of datasets spanning applications in natural images (visual genome,\nBSD100), medical imaging (histopathology slides), and remote sensing (satellite\nimages) demonstrate the efficacy of the newly proposed USIM-DAL and superiority\nover several dense regression active learning methods.",
                "One Network, Many Masks: Towards More Parameter-Efficient Transfer\n  Learning\nFine-tuning pre-trained language models for multiple tasks tends to be\nexpensive in terms of storage. To mitigate this, parameter-efficient transfer\nlearning (PETL) methods have been proposed to address this issue, but they\nstill require a significant number of parameters and storage when being applied\nto broader ranges of tasks. To achieve even greater storage reduction, we\npropose PROPETL, a novel method that enables efficient sharing of a single PETL\nmodule which we call prototype network (e.g., adapter, LoRA, and prefix-tuning)\nacross layers and tasks. We then learn binary masks to select different\nsub-networks from the shared prototype network and apply them as PETL modules\ninto different layers. We find that the binary masks can determine crucial\ninformation from the network, which is often ignored in previous studies. Our\nwork can also be seen as a type of pruning method, where we find that\noverparameterization also exists in the seemingly small PETL modules. We\nevaluate PROPETL on various downstream tasks and show that it can outperform\nother PETL methods with approximately 10% of the parameter storage required by\nthe latter.",
                "Distill Gold from Massive Ores: Bi-level Data Pruning towards Efficient\n  Dataset Distillation\nData-efficient learning has garnered significant attention, especially given\nthe current trend of large multi-modal models. Recently, dataset distillation\nhas become an effective approach by synthesizing data samples that are\nessential for network training. However, it remains to be explored which\nsamples are essential for the dataset distillation process itself. In this\nwork, we study the data efficiency and selection for the dataset distillation\ntask. By re-formulating the dynamics of distillation, we provide insight into\nthe inherent redundancy in the real dataset, both theoretically and\nempirically. We propose to use the empirical loss value as a static data\npruning criterion. To further compensate for the variation of the data value in\ntraining, we find the most contributing samples based on their causal effects\non the distillation. The proposed selection strategy can efficiently exploit\nthe training dataset, outperform the previous SOTA distillation algorithms, and\nconsistently enhance the distillation algorithms, even on much larger-scale and\nmore heterogeneous datasets, e.g., full ImageNet-1K and Kinetics-400. We\nbelieve this paradigm will open up new avenues in the dynamics of distillation\nand pave the way for efficient dataset distillation. Our code is available on\nhttps://github.com/silicx/GoldFromOres-BiLP.",
                "Observation Denoising in CYRUS Soccer Simulation 2D Team For RoboCup\n  2023\nThe RoboCup competitions hold various leagues, and the Soccer Simulation 2D\nLeague is a major one among them. Soccer Simulation 2D (SS2D) match involves\ntwo teams, including 11 players and a coach, competing against each other. The\nplayers can only communicate with the Soccer Simulation Server during the game.\nThis paper presents the latest research of the CYRUS soccer simulation 2D team,\nthe champion of RoboCup 2021. We will explain our denoising idea powered by\nlong short-term memory networks (LSTM) and deep neural networks (DNN). The\nCYRUS team uses the CYRUS2D base code that was developed based on the Helios\nand Gliders bases.",
                "PuMer: Pruning and Merging Tokens for Efficient Vision Language Models\nLarge-scale vision language (VL) models use Transformers to perform\ncross-modal interactions between the input text and image. These cross-modal\ninteractions are computationally expensive and memory-intensive due to the\nquadratic complexity of processing the input image and text. We present PuMer:\na token reduction framework that uses text-informed Pruning and modality-aware\nMerging strategies to progressively reduce the tokens of input image and text,\nimproving model inference speed and reducing memory footprint. PuMer learns to\nkeep salient image tokens related to the input text and merges similar textual\nand visual tokens by adding lightweight token reducer modules at several\ncross-modal layers in the VL model. Training PuMer is mostly the same as\nfinetuning the original VL model but faster. Our evaluation for two vision\nlanguage models on four downstream VL tasks shows PuMer increases inference\nthroughput by up to 2x and reduces memory footprint by over 50% while incurring\nless than a 1% accuracy drop."
            ],
            "interesting paper": 2
        },
        {
            "papers": [
                "Intelligent gradient amplification for deep neural networks\nDeep learning models offer superior performance compared to other machine\nlearning techniques for a variety of tasks and domains, but pose their own\nchallenges. In particular, deep learning models require larger training times\nas the depth of a model increases, and suffer from vanishing gradients. Several\nsolutions address these problems independently, but there have been minimal\nefforts to identify an integrated solution that improves the performance of a\nmodel by addressing vanishing gradients, as well as accelerates the training\nprocess to achieve higher performance at larger learning rates. In this work,\nwe intelligently determine which layers of a deep learning model to apply\ngradient amplification to, using a formulated approach that analyzes gradient\nfluctuations of layers during training. Detailed experiments are performed for\nsimpler and deeper neural networks using two different intelligent measures and\ntwo different thresholds that determine the amplification layers, and a\ntraining strategy where gradients are amplified only during certain epochs.\nResults show that our amplification offers better performance compared to the\noriginal models, and achieves accuracy improvement of around 2.5% on CIFAR- 10\nand around 4.5% on CIFAR-100 datasets, even when the models are trained with\nhigher learning rates.",
                "Deep Electron Cloud-activity and Field-activity Relationships\nChemists have been pursuing the general mathematical laws to explain and\npredict molecular properties for a long time. However, most of the traditional\nquantitative structure-activity relationship (QSAR) models have limited\napplication domains, e.g., they tend to have poor generalization performance\nwhen applied to molecules with parent structures different from those of the\ntrained molecules. This paper attempts to develop a new QSAR method that is\ntheoretically possible to predict various properties of molecules with diverse\nstructures. The proposed deep electron cloud-activity relationships (DECAR) and\ndeep field-activity relationships (DFAR) methods consist of three essentials:\n(1) A large number of molecule entities with activity data as training objects\nand responses; (2) three-dimensional electron cloud density (ECD) or related\nfield data by the accurate density functional theory methods as input\ndescriptors; (3) a deep learning model that is sufficiently flexible and\npowerful to learn the large data described above. DECAR and DFAR are used to\ndistinguish 977 sweet and 1965 non-sweet molecules (with 6-fold data\naugmentation) and the classification performance is demonstrated to be\nsignificantly better than the traditional least squares support vector machine\n(LS-SVM) models using traditional descriptors. DECAR and DFAR would provide a\npossible way to establish a widely applicable, cumulative, and shareable\nartificial intelligence-driven QSAR system. They are likely to promote the\ndevelopment of an interactive platform to collect and share the accurate ECD\nand field data of millions of molecules with annotated activities. With enough\ninput data, we envision the appearance of several deep networks trained for\nvarious molecular activities. Finally, we could anticipate a single DECAR or\nDFAR network to learn and infer various properties of interest for chemical\nmolecules.",
                "On the impact of activation and normalization in obtaining isometric\n  embeddings at initialization\nIn this paper, we explore the structure of the penultimate Gram matrix in\ndeep neural networks, which contains the pairwise inner products of outputs\ncorresponding to a batch of inputs. In several architectures it has been\nobserved that this Gram matrix becomes degenerate with depth at initialization,\nwhich dramatically slows training. Normalization layers, such as batch or layer\nnormalization, play a pivotal role in preventing the rank collapse issue.\nDespite promising advances, the existing theoretical results do not extend to\nlayer normalization, which is widely used in transformers, and can not\nquantitatively characterize the role of non-linear activations. To bridge this\ngap, we prove that layer normalization, in conjunction with activation layers,\nbiases the Gram matrix of a multilayer perceptron towards the identity matrix\nat an exponential rate with depth at initialization. We quantify this rate\nusing the Hermite expansion of the activation function.",
                "Pedestrian detection with high-resolution event camera\nDespite the dynamic development of computer vision algorithms, the\nimplementation of perception and control systems for autonomous vehicles such\nas drones and self-driving cars still poses many challenges. A video stream\ncaptured by traditional cameras is often prone to problems such as motion blur\nor degraded image quality due to challenging lighting conditions. In addition,\nthe frame rate - typically 30 or 60 frames per second - can be a limiting\nfactor in certain scenarios. Event cameras (DVS -- Dynamic Vision Sensor) are a\npotentially interesting technology to address the above mentioned problems. In\nthis paper, we compare two methods of processing event data by means of deep\nlearning for the task of pedestrian detection. We used a representation in the\nform of video frames, convolutional neural networks and asynchronous sparse\nconvolutional neural networks. The results obtained illustrate the potential of\nevent cameras and allow the evaluation of the accuracy and efficiency of the\nmethods used for high-resolution (1280 x 720 pixels) footage.",
                "Dink-Net: Neural Clustering on Large Graphs\nDeep graph clustering, which aims to group the nodes of a graph into disjoint\nclusters with deep neural networks, has achieved promising progress in recent\nyears. However, the existing methods fail to scale to the large graph with\nmillion nodes. To solve this problem, a scalable deep graph clustering method\n(Dink-Net) is proposed with the idea of dilation and shrink. Firstly, by\ndiscriminating nodes, whether being corrupted by augmentations, representations\nare learned in a self-supervised manner. Meanwhile, the cluster centres are\ninitialized as learnable neural parameters. Subsequently, the clustering\ndistribution is optimized by minimizing the proposed cluster dilation loss and\ncluster shrink loss in an adversarial manner. By these settings, we unify the\ntwo-step clustering, i.e., representation learning and clustering optimization,\ninto an end-to-end framework, guiding the network to learn clustering-friendly\nfeatures. Besides, Dink-Net scales well to large graphs since the designed loss\nfunctions adopt the mini-batch data to optimize the clustering distribution\neven without performance drops. Both experimental results and theoretical\nanalyses demonstrate the superiority of our method. Compared to the runner-up,\nDink-Net achieves 9.62% NMI improvement on the ogbn-papers100M dataset with 111\nmillion nodes and 1.6 billion edges. The source code is released at\nhttps://github.com/yueliu1999/Dink-Net. Besides, a collection (papers, codes,\nand datasets) of deep graph clustering is shared at\nhttps://github.com/yueliu1999/Awesome-Deep-Graph-Clustering.",
                "InDL: A New Dataset and Benchmark for In-Diagram Logic Interpretation\n  based on Visual Illusion\nThis paper introduces a novel approach to evaluating deep learning models'\ncapacity for in-diagram logic interpretation. Leveraging the intriguing realm\nof visual illusions, we establish a unique dataset, InDL, designed to\nrigorously test and benchmark these models. Deep learning has witnessed\nremarkable progress in domains such as computer vision and natural language\nprocessing. However, models often stumble in tasks requiring logical reasoning\ndue to their inherent 'black box' characteristics, which obscure the\ndecision-making process. Our work presents a new lens to understand these\nmodels better by focusing on their handling of visual illusions -- a complex\ninterplay of perception and logic. We utilize six classic geometric optical\nillusions to create a comparative framework between human and machine visual\nperception. This methodology offers a quantifiable measure to rank models,\nelucidating potential weaknesses and providing actionable insights for model\nimprovements. Our experimental results affirm the efficacy of our benchmarking\nstrategy, demonstrating its ability to effectively rank models based on their\nlogic interpretation ability. As part of our commitment to reproducible\nresearch, the source code and datasets will be made publicly available at\nhttps://github.com/rabbit-magic-wh/InDL"
            ],
            "interesting paper": 1
        },
        {
            "papers": [
                "How Does Information Bottleneck Help Deep Learning?\nNumerous deep learning algorithms have been inspired by and understood via\nthe notion of information bottleneck, where unnecessary information is (often\nimplicitly) minimized while task-relevant information is maximized. However, a\nrigorous argument for justifying why it is desirable to control information\nbottlenecks has been elusive. In this paper, we provide the first rigorous\nlearning theory for justifying the benefit of information bottleneck in deep\nlearning by mathematically relating information bottleneck to generalization\nerrors. Our theory proves that controlling information bottleneck is one way to\ncontrol generalization errors in deep learning, although it is not the only or\nnecessary way. We investigate the merit of our new mathematical findings with\nexperiments across a range of architectures and learning settings. In many\ncases, generalization errors are shown to correlate with the degree of\ninformation bottleneck: i.e., the amount of the unnecessary information at\nhidden layers. This paper provides a theoretical foundation for current and\nfuture methods through the lens of information bottleneck. Our new\ngeneralization bounds scale with the degree of information bottleneck, unlike\nthe previous bounds that scale with the number of parameters, VC dimension,\nRademacher complexity, stability or robustness. Our code is publicly available\nat: https://github.com/xu-ji/information-bottleneck",
                "Stochastic Gradient Langevin Dynamics Based on Quantization with\n  Increasing Resolution\nStochastic learning dynamics based on Langevin or Levy stochastic\ndifferential equations (SDEs) in deep neural networks control the variance of\nnoise by varying the size of the mini-batch or directly those of injecting\nnoise. Since the noise variance affects the approximation performance, the\ndesign of the additive noise is significant in SDE-based learning and practical\nimplementation. In this paper, we propose an alternative stochastic descent\nlearning equation based on quantized optimization for non-convex objective\nfunctions, adopting a stochastic analysis perspective. The proposed method\nemploys a quantized optimization approach that utilizes Langevin SDE dynamics,\nallowing for controllable noise with an identical distribution without the need\nfor additive noise or adjusting the mini-batch size. Numerical experiments\ndemonstrate the effectiveness of the proposed algorithm on vanilla convolution\nneural network(CNN) models and the ResNet-50 architecture across various data\nsets. Furthermore, we provide a simple PyTorch implementation of the proposed\nalgorithm.",
                "E-PANNs: Sound Recognition Using Efficient Pre-trained Audio Neural\n  Networks\nSounds carry an abundance of information about activities and events in our\neveryday environment, such as traffic noise, road works, music, or people\ntalking. Recent machine learning methods, such as convolutional neural networks\n(CNNs), have been shown to be able to automatically recognize sound activities,\na task known as audio tagging. One such method, pre-trained audio neural\nnetworks (PANNs), provides a neural network which has been pre-trained on over\n500 sound classes from the publicly available AudioSet dataset, and can be used\nas a baseline or starting point for other tasks. However, the existing PANNs\nmodel has a high computational complexity and large storage requirement. This\ncould limit the potential for deploying PANNs on resource-constrained devices,\nsuch as on-the-edge sound sensors, and could lead to high energy consumption if\nmany such devices were deployed. In this paper, we reduce the computational\ncomplexity and memory requirement of the PANNs model by taking a pruning\napproach to eliminate redundant parameters from the PANNs model. The resulting\nEfficient PANNs (E-PANNs) model, which requires 36\\% less computations and 70\\%\nless memory, also slightly improves the sound recognition (audio tagging)\nperformance. The code for the E-PANNs model has been released under an open\nsource license.",
                "Simulation-Aided Deep Learning for Laser Ultrasonic Visualization\n  Testing\nIn recent years, laser ultrasonic visualization testing (LUVT) has attracted\nmuch attention because of its ability to efficiently perform non-contact\nultrasonic non-destructive testing.Despite many success reports of deep\nlearning based image analysis for widespread areas, attempts to apply deep\nlearning to defect detection in LUVT images face the difficulty of preparing a\nlarge dataset of LUVT images that is too expensive to scale. To compensate for\nthe scarcity of such training data, we propose a data augmentation method that\ngenerates artificial LUVT images by simulation and applies a style transfer to\nsimulated LUVT images.The experimental results showed that the effectiveness of\ndata augmentation based on the style-transformed simulated images improved the\nprediction performance of defects, rather than directly using the raw simulated\nimages for data augmentation.",
                "Vector-based Representation is the Key: A Study on Disentanglement and\n  Compositional Generalization\nRecognizing elementary underlying concepts from observations\n(disentanglement) and generating novel combinations of these concepts\n(compositional generalization) are fundamental abilities for humans to support\nrapid knowledge learning and generalize to new tasks, with which the deep\nlearning models struggle. Towards human-like intelligence, various works on\ndisentangled representation learning have been proposed, and recently some\nstudies on compositional generalization have been presented. However, few works\nstudy the relationship between disentanglement and compositional\ngeneralization, and the observed results are inconsistent. In this paper, we\nstudy several typical disentangled representation learning works in terms of\nboth disentanglement and compositional generalization abilities, and we provide\nan important insight: vector-based representation (using a vector instead of a\nscalar to represent a concept) is the key to empower both good disentanglement\nand strong compositional generalization. This insight also resonates the\nneuroscience research that the brain encodes information in neuron population\nactivity rather than individual neurons. Motivated by this observation, we\nfurther propose a method to reform the scalar-based disentanglement works\n($\\beta$-TCVAE and FactorVAE) to be vector-based to increase both capabilities.\nWe investigate the impact of the dimensions of vector-based representation and\none important question: whether better disentanglement indicates higher\ncompositional generalization. In summary, our study demonstrates that it is\npossible to achieve both good concept recognition and novel concept\ncomposition, contributing an important step towards human-like intelligence.",
                "Towards Machine Learning and Inference for Resource-constrained MCUs\nMachine learning (ML) is moving towards edge devices. However, ML models with\nhigh computational demands and energy consumption pose challenges for ML\ninference in resource-constrained environments, such as the deep sea. To\naddress these challenges, we propose a battery-free ML inference and model\npersonalization pipeline for microcontroller units (MCUs). As an example, we\nperformed fish image recognition in the ocean. We evaluated and compared the\naccuracy, runtime, power, and energy consumption of the model before and after\noptimization. The results demonstrate that, our pipeline can achieve 97.78%\naccuracy with 483.82 KB Flash, 70.32 KB RAM, 118 ms runtime, 4.83 mW power, and\n0.57 mJ energy consumption on MCUs, reducing by 64.17%, 12.31%, 52.42%, 63.74%,\nand 82.67%, compared to the baseline. The results indicate the feasibility of\nbattery-free ML inference on MCUs."
            ],
            "interesting paper": 1
        },
        {
            "papers": [
                "CVSNet: A Computer Implementation for Central Visual System of The Brain\nIn computer vision, different basic blocks are created around different\nmatrix operations, and models based on different basic blocks have achieved\ngood results. Good results achieved in vision tasks grants them rationality.\nHowever, these experimental-based models also make deep learning long\ncriticized for principle and interpretability. Deep learning originated from\nthe concept of neurons in neuroscience, but recent designs detached natural\nneural networks except for some simple concepts. In this paper, we build an\nartificial neural network, CVSNet, which can be seen as a computer\nimplementation for central visual system of the brain. Each block in CVSNet\nrepresents the same vision information as that in brains. In CVSNet, blocks\ndiffers from each other and visual information flows through three independent\npathways and five different blocks. Thus CVSNet is completely different from\nthe design of all previous models, in which basic blocks are repeated to build\nmodel and information between channels is mixed at the outset. In ablation\nexperiment, we show the information extracted by blocks in CVSNet and compare\nwith previous networks, proving effectiveness and rationality of blocks in\nCVSNet from experiment side. And in the experiment of object recognition,\nCVSNet achieves comparable results to ConvNets, Vision Transformers and MLPs.",
                "Are Large Kernels Better Teachers than Transformers for ConvNets?\nThis paper reveals a new appeal of the recently emerged large-kernel\nConvolutional Neural Networks (ConvNets): as the teacher in Knowledge\nDistillation (KD) for small-kernel ConvNets. While Transformers have led\nstate-of-the-art (SOTA) performance in various fields with ever-larger models\nand labeled data, small-kernel ConvNets are considered more suitable for\nresource-limited applications due to the efficient convolution operation and\ncompact weight sharing. KD is widely used to boost the performance of\nsmall-kernel ConvNets. However, previous research shows that it is not quite\neffective to distill knowledge (e.g., global information) from Transformers to\nsmall-kernel ConvNets, presumably due to their disparate architectures. We\nhereby carry out a first-of-its-kind study unveiling that modern large-kernel\nConvNets, a compelling competitor to Vision Transformers, are remarkably more\neffective teachers for small-kernel ConvNets, due to more similar\narchitectures. Our findings are backed up by extensive experiments on both\nlogit-level and feature-level KD ``out of the box\", with no dedicated\narchitectural nor training recipe modifications. Notably, we obtain the\n\\textbf{best-ever pure ConvNet} under 30M parameters with \\textbf{83.1\\%} top-1\naccuracy on ImageNet, outperforming current SOTA methods including ConvNeXt V2\nand Swin V2. We also find that beneficial characteristics of large-kernel\nConvNets, e.g., larger effective receptive fields, can be seamlessly\ntransferred to students through this large-to-small kernel distillation. Code\nis available at: \\url{https://github.com/VITA-Group/SLaK}.",
                "End-to-end Training of Deep Boltzmann Machines by Unbiased Contrastive\n  Divergence with Local Mode Initialization\nWe address the problem of biased gradient estimation in deep Boltzmann\nmachines (DBMs). The existing method to obtain an unbiased estimator uses a\nmaximal coupling based on a Gibbs sampler, but when the state is\nhigh-dimensional, it takes a long time to converge. In this study, we propose\nto use a coupling based on the Metropolis-Hastings (MH) and to initialize the\nstate around a local mode of the target distribution. Because of the propensity\nof MH to reject proposals, the coupling tends to converge in only one step with\na high probability, leading to high efficiency. We find that our method allows\nDBMs to be trained in an end-to-end fashion without greedy pretraining. We also\npropose some practical techniques to further improve the performance of DBMs.\nWe empirically demonstrate that our training algorithm enables DBMs to show\ncomparable generative performance to other deep generative models, achieving\nthe FID score of 10.33 for MNIST.",
                "Adaptation of Tongue Ultrasound-Based Silent Speech Interfaces Using\n  Spatial Transformer Networks\nThanks to the latest deep learning algorithms, silent speech interfaces (SSI)\nare now able to synthesize intelligible speech from articulatory movement data\nunder certain conditions. However, the resulting models are rather\nspeaker-specific, making a quick switch between users troublesome. Even for the\nsame speaker, these models perform poorly cross-session, i.e. after dismounting\nand re-mounting the recording equipment. To aid quick speaker and session\nadaptation of ultrasound tongue imaging-based SSI models, we extend our deep\nnetworks with a spatial transformer network (STN) module, capable of performing\nan affine transformation on the input images. Although the STN part takes up\nonly about 10% of the network, our experiments show that adapting just the STN\nmodule might allow to reduce MSE by 88% on the average, compared to retraining\nthe whole network. The improvement is even larger (around 92%) when adapting\nthe network to different recording sessions from the same speaker.",
                "Diagnosis and Prognosis of Head and Neck Cancer Patients using\n  Artificial Intelligence\nCancer is one of the most life-threatening diseases worldwide, and head and\nneck (H&N) cancer is a prevalent type with hundreds of thousands of new cases\nrecorded each year. Clinicians use medical imaging modalities such as computed\ntomography and positron emission tomography to detect the presence of a tumor,\nand they combine that information with clinical data for patient prognosis. The\nprocess is mostly challenging and time-consuming. Machine learning and deep\nlearning can automate these tasks to help clinicians with highly promising\nresults. This work studies two approaches for H&N tumor segmentation: (i)\nexploration and comparison of vision transformer (ViT)-based and convolutional\nneural network-based models; and (ii) proposal of a novel 2D perspective to\nworking with 3D data. Furthermore, this work proposes two new architectures for\nthe prognosis task. An ensemble of several models predicts patient outcomes\n(which won the HECKTOR 2021 challenge prognosis task), and a ViT-based\nframework concurrently performs patient outcome prediction and tumor\nsegmentation, which outperforms the ensemble model.",
                "KrADagrad: Kronecker Approximation-Domination Gradient Preconditioned\n  Stochastic Optimization\nSecond order stochastic optimizers allow parameter update step size and\ndirection to adapt to loss curvature, but have traditionally required too much\nmemory and compute for deep learning. Recently, Shampoo [Gupta et al., 2018]\nintroduced a Kronecker factored preconditioner to reduce these requirements: it\nis used for large deep models [Anil et al., 2020] and in production [Anil et\nal., 2022]. However, it takes inverse matrix roots of ill-conditioned matrices.\nThis requires 64-bit precision, imposing strong hardware constraints. In this\npaper, we propose a novel factorization, Kronecker Approximation-Domination\n(KrAD). Using KrAD, we update a matrix that directly approximates the inverse\nempirical Fisher matrix (like full matrix AdaGrad), avoiding inversion and\nhence 64-bit precision. We then propose KrADagrad$^\\star$, with similar\ncomputational costs to Shampoo and the same regret. Synthetic ill-conditioned\nexperiments show improved performance over Shampoo for 32-bit precision, while\nfor several real datasets we have comparable or better generalization."
            ],
            "interesting paper": 2
        },
        {
            "papers": [
                "Breast Cancer Detection and Diagnosis: A comparative study of\n  state-of-the-arts deep learning architectures\nBreast cancer is a prevalent form of cancer among women, with over 1.5\nmillion women being diagnosed each year. Unfortunately, the survival rates for\nbreast cancer patients in certain third-world countries, like South Africa, are\nalarmingly low, with only 40% of diagnosed patients surviving beyond five\nyears. The inadequate availability of resources, including qualified\npathologists, delayed diagnoses, and ineffective therapy planning, contribute\nto this low survival rate. To address this pressing issue, medical specialists\nand researchers have turned to domain-specific AI approaches, specifically deep\nlearning models, to develop end-to-end solutions that can be integrated into\ncomputer-aided diagnosis (CAD) systems. By improving the workflow of\npathologists, these AI models have the potential to enhance the detection and\ndiagnosis of breast cancer. This research focuses on evaluating the performance\nof various cutting-edge convolutional neural network (CNN) architectures in\ncomparison to a relatively new model called the Vision Trans-former (ViT). The\nobjective is to determine the superiority of these models in terms of their\naccuracy and effectiveness. The experimental results reveal that the ViT models\noutperform the other selected state-of-the-art CNN architectures, achieving an\nimpressive accuracy rate of 95.15%. This study signifies a significant\nadvancement in the field, as it explores the utilization of data augmentation\nand other relevant preprocessing techniques in conjunction with deep learning\nmodels for the detection and diagnosis of breast cancer using datasets of\nBreast Cancer Histopathological Image Classification.",
                "Sea Ice Extraction via Remote Sensed Imagery: Algorithms, Datasets,\n  Applications and Challenges\nThe deep learning, which is a dominating technique in artificial\nintelligence, has completely changed the image understanding over the past\ndecade. As a consequence, the sea ice extraction (SIE) problem has reached a\nnew era. We present a comprehensive review of four important aspects of SIE,\nincluding algorithms, datasets, applications, and the future trends. Our review\nfocuses on researches published from 2016 to the present, with a specific focus\non deep learning-based approaches in the last five years. We divided all\nrelegated algorithms into 3 categories, including classical image segmentation\napproach, machine learning-based approach and deep learning-based methods. We\nreviewed the accessible ice datasets including SAR-based datasets, the\noptical-based datasets and others. The applications are presented in 4 aspects\nincluding climate research, navigation, geographic information systems (GIS)\nproduction and others. It also provides insightful observations and inspiring\nfuture research directions.",
                "Training-free Neural Architecture Search for RNNs and Transformers\nNeural architecture search (NAS) has allowed for the automatic creation of\nnew and effective neural network architectures, offering an alternative to the\nlaborious process of manually designing complex architectures. However,\ntraditional NAS algorithms are slow and require immense amounts of computing\npower. Recent research has investigated training-free NAS metrics for image\nclassification architectures, drastically speeding up search algorithms. In\nthis paper, we investigate training-free NAS metrics for recurrent neural\nnetwork (RNN) and BERT-based transformer architectures, targeted towards\nlanguage modeling tasks. First, we develop a new training-free metric, named\nhidden covariance, that predicts the trained performance of an RNN architecture\nand significantly outperforms existing training-free metrics. We experimentally\nevaluate the effectiveness of the hidden covariance metric on the NAS-Bench-NLP\nbenchmark. Second, we find that the current search space paradigm for\ntransformer architectures is not optimized for training-free neural\narchitecture search. Instead, a simple qualitative analysis can effectively\nshrink the search space to the best performing architectures. This conclusion\nis based on our investigation of existing training-free metrics and new metrics\ndeveloped from recent transformer pruning literature, evaluated on our own\nbenchmark of trained BERT architectures. Ultimately, our analysis shows that\nthe architecture search space and the training-free metric must be developed\ntogether in order to achieve effective results.",
                "Diffused Redundancy in Pre-trained Representations\nRepresentations learned by pre-training a neural network on a large dataset\nare increasingly used successfully to perform a variety of downstream tasks. In\nthis work, we take a closer look at how features are encoded in such\npre-trained representations. We find that learned representations in a given\nlayer exhibit a degree of diffuse redundancy, ie, any randomly chosen subset of\nneurons in the layer that is larger than a threshold size shares a large degree\nof similarity with the full layer and is able to perform similarly as the whole\nlayer on a variety of downstream tasks. For example, a linear probe trained on\n$20\\%$ of randomly picked neurons from the penultimate layer of a ResNet50\npre-trained on ImageNet1k achieves an accuracy within $5\\%$ of a linear probe\ntrained on the full layer of neurons for downstream CIFAR10 classification. We\nconduct experiments on different neural architectures (including CNNs and\nTransformers) pre-trained on both ImageNet1k and ImageNet21k and evaluate a\nvariety of downstream tasks taken from the VTAB benchmark. We find that the\nloss and dataset used during pre-training largely govern the degree of diffuse\nredundancy and the \"critical mass\" of neurons needed often depends on the\ndownstream task, suggesting that there is a task-inherent\nredundancy-performance Pareto frontier. Our findings shed light on the nature\nof representations learned by pre-trained deep neural networks and suggest that\nentire layers might not be necessary to perform many downstream tasks. We\ninvestigate the potential for exploiting this redundancy to achieve efficient\ngeneralization for downstream tasks and also draw caution to certain possible\nunintended consequences. Our code is available at\n\\url{https://github.com/nvedant07/diffused-redundancy}.",
                "Adam Accumulation to Reduce Memory Footprints of both Activations and\n  Gradients for Large-scale DNN Training\nRunning out of GPU memory has become a main bottleneck for large-scale DNN\ntraining. How to reduce the memory footprint during training has received\nintensive research attention. We find that previous gradient accumulation\nreduces activation memory but fails to be compatible with gradient memory\nreduction due to a contradiction between preserving gradients and releasing\ngradients. To address this issue, we propose a novel optimizer accumulation\nmethod for Adam, named Adam Accumulation (AdamA), which enables reducing both\nactivation and gradient memory. Specifically, AdamA directly integrates\ngradients into optimizer states and accumulates optimizer states over\nmicro-batches, so that gradients can be released immediately after use. We\nmathematically and experimentally demonstrate AdamA yields the same convergence\nproperties as Adam. Evaluated on transformer-based models, AdamA achieves up to\n23% memory reduction compared to gradient accumulation with less than 2%\ndegradation in training throughput. Notably, AdamA can work together with\nmemory reduction methods for optimizer states to fit 1.26x~3.14x larger models\nover PyTorch and DeepSpeed baseline on GPUs with different memory capacities.",
                "Transformers learn to implement preconditioned gradient descent for\n  in-context learning\nSeveral recent works demonstrate that transformers can implement algorithms\nlike gradient descent. By a careful construction of weights, these works show\nthat multiple layers of transformers are expressive enough to simulate\niterations of gradient descent. Going beyond the question of expressivity, we\nask: Can transformers learn to implement such algorithms by training over\nrandom problem instances? To our knowledge, we make the first theoretical\nprogress on this question via an analysis of the loss landscape for linear\ntransformers trained over random instances of linear regression. For a single\nattention layer, we prove the global minimum of the training objective\nimplements a single iteration of preconditioned gradient descent. Notably, the\npreconditioning matrix not only adapts to the input distribution but also to\nthe variance induced by data inadequacy. For a transformer with $L$ attention\nlayers, we prove certain critical points of the training objective implement\n$L$ iterations of preconditioned gradient descent. Our results call for future\ntheoretical studies on learning algorithms by training transformers."
            ],
            "interesting paper": 3
        },
        {
            "papers": [
                "On the Weight Dynamics of Deep Normalized Networks\nRecent studies have shown that high disparities in effective learning rates\n(ELRs) across layers in deep neural networks can negatively affect\ntrainability. We formalize how these disparities evolve over time by modeling\nweight dynamics (evolution of expected gradient and weight norms) of networks\nwith normalization layers, predicting the evolution of layer-wise ELR ratios.\nWe prove that when training with any constant learning rate, ELR ratios\nconverge to 1, despite initial gradient explosion. We identify a ``critical\nlearning rate\" beyond which ELR disparities widen, which only depends on\ncurrent ELRs. To validate our findings, we devise a hyper-parameter-free\nwarm-up method that successfully minimizes ELR spread quickly in theory and\npractice. Our experiments link ELR spread with trainability, a relationship\nthat is most evident in very deep networks with significant gradient magnitude\nexcursions.",
                "Smooth Min-Max Monotonic Networks\nMonotonicity constraints are powerful regularizers in statistical modelling.\nThey can support fairness in computer-aided decision making and increase\nplausibility in data-driven scientific models. The seminal min-max (MM) neural\nnetwork architecture ensures monotonicity, but often gets stuck in undesired\nlocal optima during training because of partial derivatives of the MM\nnonlinearities being zero. We propose a simple modification of the MM network\nusing strictly-increasing smooth minimum and maximum functions that alleviates\nthis problem. The resulting smooth min-max (SMM) network module inherits the\nasymptotic approximation properties from the MM architecture. It can be used\nwithin larger deep learning systems trained end-to-end. The SMM module is\nconceptually simple and computationally less demanding than state-of-the-art\nneural networks for monotonic modelling. Our experiments show that this does\nnot come with a loss in generalization performance compared to alternative\nneural and non-neural approaches.",
                "Adaptation and Optimization of Automatic Speech Recognition (ASR) for\n  the Maritime Domain in the Field of VHF Communication\nThis paper introduces a multilingual automatic speech recognizer (ASR) for\nmaritime radio communi-cation that automatically converts received VHF radio\nsignals into text. The challenges of maritime radio communication are described\nat first, and the deep learning architecture of marFM consisting of audio\nprocessing techniques and machine learning algorithms is presented.\nSubsequently, maritime radio data of interest is analyzed and then used to\nevaluate the transcription performance of our ASR model for various maritime\nradio data.",
                "Rotating Features for Object Discovery\nThe binding problem in human cognition, concerning how the brain represents\nand connects objects within a fixed network of neural connections, remains a\nsubject of intense debate. Most machine learning efforts addressing this issue\nin an unsupervised setting have focused on slot-based methods, which may be\nlimiting due to their discrete nature and difficulty to express uncertainty.\nRecently, the Complex AutoEncoder was proposed as an alternative that learns\ncontinuous and distributed object-centric representations. However, it is only\napplicable to simple toy data. In this paper, we present Rotating Features, a\ngeneralization of complex-valued features to higher dimensions, and a new\nevaluation procedure for extracting objects from distributed representations.\nAdditionally, we show the applicability of our approach to pre-trained\nfeatures. Together, these advancements enable us to scale distributed\nobject-centric representations from simple toy to real-world data. We believe\nthis work advances a new paradigm for addressing the binding problem in machine\nlearning and has the potential to inspire further innovation in the field.",
                "The Galerkin method beats Graph-Based Approaches for Spectral Algorithms\nHistorically, the machine learning community has derived spectral\ndecompositions from graph-based approaches. We break with this approach and\nprove the statistical and computational superiority of the Galerkin method,\nwhich consists in restricting the study to a small set of test functions. In\nparticular, we introduce implementation tricks to deal with differential\noperators in large dimensions with structured kernels. Finally, we extend on\nthe core principles beyond our approach to apply them to non-linear spaces of\nfunctions, such as the ones parameterized by deep neural networks, through\nloss-based optimization procedures.",
                "Microstructure quality control of steels using deep learning\nIn quality control, microstructures are investigated rigorously to ensure\nstructural integrity, exclude the presence of critical volume defects, and\nvalidate the formation of the target microstructure. For quenched,\nhierarchically-structured steels, the morphology of the bainitic and\nmartensitic microstructures are of major concern to guarantee the reliability\nof the material under service conditions. Therefore, industries conduct small\nsample-size inspections of materials cross-sections through metallographers to\nvalidate the needle morphology of such microstructures. We demonstrate\nround-robin test results revealing that this visual grading is afflicted by\npronounced subjectivity despite the thorough training of personnel. Instead, we\npropose a deep learning image classification approach that distinguishes steels\nbased on their microstructure type and classifies their needle length alluding\nto the ISO 643 grain size assessment standard. This classification approach\nfacilitates the reliable, objective, and automated classification of\nhierarchically structured steels. Specifically, an accuracy of 96% and roughly\n91% is attained for the distinction of martensite/bainite subtypes and needle\nlength, respectively. This is achieved on an image dataset that contains\nsignificant variance and labeling noise as it is acquired over more than ten\nyears from multiple plants, alloys, etchant applications, and light optical\nmicroscopes by many metallographers (raters). Interpretability analysis gives\ninsights into the decision-making of these models and allows for estimating\ntheir generalization capability."
            ],
            "interesting paper": 4
        }
    ],
    "Jordan Martinez": [
        {
            "papers": [
                "Behavior quantification as the missing link between fields: Tools for\n  digital psychiatry and their role in the future of neurobiology\nThe great behavioral heterogeneity observed between individuals with the same\npsychiatric disorder and even within one individual over time complicates both\nclinical practice and biomedical research. However, modern technologies are an\nexciting opportunity to improve behavioral characterization. Existing\npsychiatry methods that are qualitative or unscalable, such as patient surveys\nor clinical interviews, can now be collected at a greater capacity and analyzed\nto produce new quantitative measures. Furthermore, recent capabilities for\ncontinuous collection of passive sensor streams, such as phone GPS or\nsmartwatch accelerometer, open avenues of novel questioning that were\npreviously entirely unrealistic. Their temporally dense nature enables a\ncohesive study of real-time neural and behavioral signals.\n  To develop comprehensive neurobiological models of psychiatric disease, it\nwill be critical to first develop strong methods for behavioral quantification.\nThere is huge potential in what can theoretically be captured by current\ntechnologies, but this in itself presents a large computational challenge --\none that will necessitate new data processing tools, new machine learning\ntechniques, and ultimately a shift in how interdisciplinary work is conducted.\nIn my thesis, I detail research projects that take different perspectives on\ndigital psychiatry, subsequently tying ideas together with a concluding\ndiscussion on the future of the field. I also provide software infrastructure\nwhere relevant, with extensive documentation.\n  Major contributions include scientific arguments and proof of concept results\nfor daily free-form audio journals as an underappreciated psychiatry research\ndatatype, as well as novel stability theorems and pilot empirical success for a\nproposed multi-area recurrent neural network architecture.",
                "Using Models Based on Cognitive Theory to Predict Human Behavior in\n  Traffic: A Case Study\nThe development of automated vehicles has the potential to revolutionize\ntransportation, but they are currently unable to ensure a safe and\ntime-efficient driving style. Reliable models predicting human behavior are\nessential for overcoming this issue. While data-driven models are commonly used\nto this end, they can be vulnerable in safety-critical edge cases. This has led\nto an interest in models incorporating cognitive theory, but as such models are\ncommonly developed for explanatory purposes, this approach's effectiveness in\nbehavior prediction has remained largely untested so far. In this article, we\ninvestigate the usefulness of the \\emph{Commotions} model -- a novel\ncognitively plausible model incorporating the latest theories of human\nperception, decision-making, and motor control -- for predicting human behavior\nin gap acceptance scenarios, which entail many important traffic interactions\nsuch as lane changes and intersections. We show that this model can compete\nwith or even outperform well-established data-driven prediction models across\nseveral naturalistic datasets. These results demonstrate the promise of\nincorporating cognitive theory in behavior prediction models for automated\nvehicles.",
                "On the Planning Abilities of Large Language Models : A Critical\n  Investigation\nIntrigued by the claims of emergent reasoning capabilities in LLMs trained on\ngeneral web corpora, in this paper, we set out to investigate their planning\ncapabilities. We aim to evaluate (1) the effectiveness of LLMs in generating\nplans autonomously in commonsense planning tasks and (2) the potential of LLMs\nin LLM-Modulo settings where they act as a source of heuristic guidance for\nexternal planners and verifiers. We conduct a systematic study by generating a\nsuite of instances on domains similar to the ones employed in the International\nPlanning Competition and evaluate LLMs in two distinct modes: autonomous and\nheuristic. Our findings reveal that LLMs' ability to generate executable plans\nautonomously is rather limited, with the best model (GPT-4) having an average\nsuccess rate of ~12% across the domains. However, the results in the LLM-Modulo\nsetting show more promise. In the LLM-Modulo setting, we demonstrate that\nLLM-generated plans can improve the search process for underlying sound\nplanners and additionally show that external verifiers can help provide\nfeedback on the generated plans and back-prompt the LLM for better plan\ngeneration.",
                "A Mini Review on the utilization of Reinforcement Learning with OPC UA\nReinforcement Learning (RL) is a powerful machine learning paradigm that has\nbeen applied in various fields such as robotics, natural language processing\nand game playing achieving state-of-the-art results. Targeted to solve\nsequential decision making problems, it is by design able to learn from\nexperience and therefore adapt to changing dynamic environments. These\ncapabilities make it a prime candidate for controlling and optimizing complex\nprocesses in industry. The key to fully exploiting this potential is the\nseamless integration of RL into existing industrial systems. The industrial\ncommunication standard Open Platform Communications UnifiedArchitecture (OPC\nUA) could bridge this gap. However, since RL and OPC UA are from different\nfields,there is a need for researchers to bridge the gap between the two\ntechnologies. This work serves to bridge this gap by providing a brief\ntechnical overview of both technologies and carrying out a semi-exhaustive\nliterature review to gain insights on how RL and OPC UA are applied in\ncombination. With this survey, three main research topics have been identified,\nfollowing the intersection of RL with OPC UA. The results of the literature\nreview show that RL is a promising technology for the control and optimization\nof industrial processes, but does not yet have the necessary standardized\ninterfaces to be deployed in real-world scenarios with reasonably low effort.",
                "Asking Before Acting: Gather Information in Embodied Decision Making\n  with Language Models\nWith strong capabilities of reasoning and a broad understanding of the world,\nLarge Language Models (LLMs) have demonstrated immense potential in building\nversatile embodied decision-making agents capable of executing a wide array of\ntasks. Nevertheless, when deployed in unfamiliar environments, we show that LLM\nagents encounter challenges in efficiently gathering essential information,\nleading to suboptimal performance. Conversely, human individuals often seek\nadditional information from their peers prior to taking action, harnessing\nexternal knowledge to avoid unnecessary trial and error. Drawing inspiration\nfrom this behavior, we propose \\textit{Asking Before Acting} (ABA), a method\nthat empowers the agent to proactively inquire with external sources for\npertinent information using natural language during their interactions within\nthe environment. In this way, the agent is able to enhance its efficiency and\nperformance by circumventing potentially laborious steps and combating the\ndifficulties associated with exploration in unfamiliar environments and\nvagueness of the instructions. We conduct extensive experiments involving a\nspectrum of environments including text-based household everyday tasks, robot\narm manipulation tasks, and real world open domain image based embodied tasks.\nThe experiments involve various models from Vicuna to GPT-4. The results\ndemonstrate that, even with modest prompts modifications, ABA exhibits\nsubstantial advantages on both performance and efficiency over baseline LLM\nagents. Further finetuning ABA with reformulated metadata (ABA-FT) faciliates\nlearning the rationale for asking and allows for additional enhancements\nespecially in tasks that baselines struggle to solve.",
                "ISLE: An Intelligent Streaming Framework for High-Throughput AI\n  Inference in Medical Imaging\nAs the adoption of Artificial Intelligence (AI) systems within the clinical\nenvironment grows, limitations in bandwidth and compute can create\ncommunication bottlenecks when streaming imaging data, leading to delays in\npatient care and increased cost. As such, healthcare providers and AI vendors\nwill require greater computational infrastructure, therefore dramatically\nincreasing costs. To that end, we developed ISLE, an intelligent streaming\nframework for high-throughput, compute- and bandwidth- optimized, and cost\neffective AI inference for clinical decision making at scale. In our\nexperiments, ISLE on average reduced data transmission by 98.02% and decoding\ntime by 98.09%, while increasing throughput by 2,730%. We show that ISLE\nresults in faster turnaround times, and reduced overall cost of data,\ntransmission, and compute, without negatively impacting clinical decision\nmaking using AI systems."
            ],
            "interesting paper": 3
        },
        {
            "papers": [
                "Exploring the Adaptive Behaviors of Particle Lenia: A\n  Perturbation-Response Analysis for Computational Agency\nA firm cognitive subject or ``individual'' is presupposed for the emergence\nof mind. However, with the development of recent information technology, the\n``individual'' has become more dispersed in society and the cognitive subject\nhas become increasingly unstable and adaptive, necessitating an update in our\nunderstanding of the ``individual''. Autopoiesis serves as a model of the\ncognitive subject, which is unstable and requires effort to maintain itself to\nadapt to the environment. In this study, we evaluated adaptivity for a highly\nextensible multi-particle system model Particle Lenia through the response\nperturbation. As a result, we found that Particle Lenia has a particle\nconfiguration that is both temporally unstable and has multiple stable states.\nThis result suggests that Particle Lenia can express adaptive characteristics\nand is expected to be used as a computational model toward building an\nautopoietic cognitive agent.",
                "Voyager: An Open-Ended Embodied Agent with Large Language Models\nWe introduce Voyager, the first LLM-powered embodied lifelong learning agent\nin Minecraft that continuously explores the world, acquires diverse skills, and\nmakes novel discoveries without human intervention. Voyager consists of three\nkey components: 1) an automatic curriculum that maximizes exploration, 2) an\never-growing skill library of executable code for storing and retrieving\ncomplex behaviors, and 3) a new iterative prompting mechanism that incorporates\nenvironment feedback, execution errors, and self-verification for program\nimprovement. Voyager interacts with GPT-4 via blackbox queries, which bypasses\nthe need for model parameter fine-tuning. The skills developed by Voyager are\ntemporally extended, interpretable, and compositional, which compounds the\nagent's abilities rapidly and alleviates catastrophic forgetting. Empirically,\nVoyager shows strong in-context lifelong learning capability and exhibits\nexceptional proficiency in playing Minecraft. It obtains 3.3x more unique\nitems, travels 2.3x longer distances, and unlocks key tech tree milestones up\nto 15.3x faster than prior SOTA. Voyager is able to utilize the learned skill\nlibrary in a new Minecraft world to solve novel tasks from scratch, while other\ntechniques struggle to generalize. We open-source our full codebase and prompts\nat https://voyager.minedojo.org/.",
                "A Decentralized Spike-based Learning Framework for Sequential Capture in\n  Discrete Perimeter Defense Problem\nThis paper proposes a novel Decentralized Spike-based Learning (DSL)\nframework for the discrete Perimeter Defense Problem (d-PDP). A team of\ndefenders is operating on the perimeter to protect the circular territory from\nradially incoming intruders. At first, the d-PDP is formulated as a\nspatio-temporal multi-task assignment problem (STMTA). The problem of STMTA is\nthen converted into a multi-label learning problem to obtain labels of segments\nthat defenders have to visit in order to protect the perimeter. The DSL\nframework uses a Multi-Label Classifier using Synaptic Efficacy Function\nspiking neuRON (MLC-SEFRON) network for deterministic multi-label learning.\nEach defender contains a single MLC-SEFRON network. Each MLC-SEFRON network is\ntrained independently using input from its own perspective for decentralized\noperations. The input spikes to the MLC-SEFRON network can be directly obtained\nfrom the spatio-temporal information of defenders and intruders without any\nextra pre-processing step. The output of MLC-SEFRON contains the labels of\nsegments that a defender has to visit in order to protect the perimeter. Based\non the multi-label output from the MLC-SEFRON a trajectory is generated for a\ndefender using a Consensus-Based Bundle Algorithm (CBBA) in order to capture\nthe intruders. The target multi-label output for training MLC-SEFRON is\nobtained from an expert policy. Also, the MLC-SEFRON trained for a defender can\nbe directly used for obtaining labels of segments assigned to another defender\nwithout any retraining. The performance of MLC-SEFRON has been evaluated for\nfull observation and partial observation scenarios of the defender. The overall\nperformance of the DSL framework is then compared with expert policy along with\nother existing learning algorithms. The scalability of the DSL has been\nevaluated using an increasing number of defenders.",
                "Exploiting Noise as a Resource for Computation and Learning in Spiking\n  Neural Networks\n$\\textbf{Formal version available at}$\nhttps://cell.com/patterns/fulltext/S2666-3899(23)00200-3\n  Networks of spiking neurons underpin the extraordinary information-processing\ncapabilities of the brain and have become pillar models in neuromorphic\nartificial intelligence. Despite extensive research on spiking neural networks\n(SNNs), most studies are established on deterministic models, overlooking the\ninherent non-deterministic, noisy nature of neural computations. This study\nintroduces the noisy spiking neural network (NSNN) and the noise-driven\nlearning rule (NDL) by incorporating noisy neuronal dynamics to exploit the\ncomputational advantages of noisy neural processing. NSNN provides a\ntheoretical framework that yields scalable, flexible, and reliable computation.\nWe demonstrate that NSNN leads to spiking neural models with competitive\nperformance, improved robustness against challenging perturbations than\ndeterministic SNNs, and better reproducing probabilistic computations in neural\ncoding. This study offers a powerful and easy-to-use tool for machine learning,\nneuromorphic intelligence practitioners, and computational neuroscience\nresearchers.",
                "Enhancing Human Capabilities through Symbiotic Artificial Intelligence\n  with Shared Sensory Experiences\nThe merging of human intelligence and artificial intelligence has long been a\nsubject of interest in both science fiction and academia. In this paper, we\nintroduce a novel concept in Human-AI interaction called Symbiotic Artificial\nIntelligence with Shared Sensory Experiences (SAISSE), which aims to establish\na mutually beneficial relationship between AI systems and human users through\nshared sensory experiences. By integrating multiple sensory input channels and\nprocessing human experiences, SAISSE fosters a strong human-AI bond, enabling\nAI systems to learn from and adapt to individual users, providing personalized\nsupport, assistance, and enhancement. Furthermore, we discuss the incorporation\nof memory storage units for long-term growth and development of both the AI\nsystem and its human user. As we address user privacy and ethical guidelines\nfor responsible AI-human symbiosis, we also explore potential biases and\ninequalities in AI-human symbiosis and propose strategies to mitigate these\nchallenges. Our research aims to provide a comprehensive understanding of the\nSAISSE concept and its potential to effectively support and enhance individual\nhuman users through symbiotic AI systems. This position article aims at\ndiscussing poteintial AI-human interaction related topics within the scientific\ncommunity, rather than providing experimental or theoretical results.",
                "Understanding the Capabilities of Large Language Models for Automated\n  Planning\nAutomated planning is concerned with developing efficient algorithms to\ngenerate plans or sequences of actions to achieve a specific goal in a given\nenvironment. Emerging Large Language Models (LLMs) can answer questions, write\nhigh-quality programming code, and predict protein folding, showcasing their\nversatility in solving various tasks beyond language-based problems. In this\npaper, we aim to explore how LLMs can also be used for automated planning. To\ndo so, we seek to answer four key questions. Firstly, we want to understand the\nextent to which LLMs can be used for plan generation. Secondly, we aim to\nidentify which pre-training data is most effective in facilitating plan\ngeneration. Thirdly, we investigate whether fine-tuning or prompting is a more\neffective approach for plan generation. Finally, we explore whether LLMs are\ncapable of plan generalization. By answering these questions, the study seeks\nto shed light on the capabilities of LLMs in solving complex planning problems\nand provide insights into the most effective approaches for using LLMs in this\ncontext."
            ],
            "interesting paper": 2
        },
        {
            "papers": [
                "Inferring the Future by Imagining the Past\nA single panel of a comic book can say a lot: it can depict not only where\nthe characters currently are, but also their motions, their motivations, their\nemotions, and what they might do next. More generally, humans routinely infer\ncomplex sequences of past and future events from a *static snapshot* of a\n*dynamic scene*, even in situations they have never seen before.\n  In this paper, we model how humans make such rapid and flexible inferences.\nBuilding on a long line of work in cognitive science, we offer a Monte Carlo\nalgorithm whose inferences correlate well with human intuitions in a wide\nvariety of domains, while only using a small, cognitively-plausible number of\nsamples. Our key technical insight is a surprising connection between our\ninference problem and Monte Carlo path tracing, which allows us to apply\ndecades of ideas from the computer graphics community to this\nseemingly-unrelated theory of mind task.",
                "Towards Cognitive Bots: Architectural Research Challenges\nSoftware bots operating in multiple virtual digital platforms must understand\nthe platforms' affordances and behave like human users. Platform affordances or\nfeatures differ from one application platform to another or through a life\ncycle, requiring such bots to be adaptable. Moreover, bots in such platforms\ncould cooperate with humans or other software agents for work or to learn\nspecific behavior patterns. However, present-day bots, particularly chatbots,\nother than language processing and prediction, are far from reaching a human\nuser's behavior level within complex business information systems. They lack\nthe cognitive capabilities to sense and act in such virtual environments,\nrendering their development a challenge to artificial general intelligence\nresearch. In this study, we problematize and investigate assumptions in\nconceptualizing software bot architecture by directing attention to significant\narchitectural research challenges in developing cognitive bots endowed with\ncomplex behavior for operation on information systems. As an outlook, we\npropose alternate architectural assumptions to consider in future bot design\nand bot development frameworks.",
                "SwiftSage: A Generative Agent with Fast and Slow Thinking for Complex\n  Interactive Tasks\nWe introduce SwiftSage, a novel agent framework inspired by the dual-process\ntheory of human cognition, designed to excel in action planning for complex\ninteractive reasoning tasks. SwiftSage integrates the strengths of behavior\ncloning and prompting large language models (LLMs) to enhance task completion\nperformance. The framework comprises two primary modules: the Swift module,\nrepresenting fast and intuitive thinking, and the Sage module, emulating\ndeliberate thought processes. The Swift module is a small encoder-decoder LM\nfine-tuned on the oracle agent's action trajectories, while the Sage module\nemploys LLMs such as GPT-4 for subgoal planning and grounding. We develop a\nheuristic method to harmoniously integrate the two modules, resulting in a more\nefficient and robust problem-solving process. In 30 tasks from the ScienceWorld\nbenchmark, SwiftSage significantly outperforms other methods such as SayCan,\nReAct, and Reflexion, demonstrating its effectiveness in solving complex\ninteractive tasks.",
                "Input-Aware Dynamic Timestep Spiking Neural Networks for Efficient\n  In-Memory Computing\nSpiking Neural Networks (SNNs) have recently attracted widespread research\ninterest as an efficient alternative to traditional Artificial Neural Networks\n(ANNs) because of their capability to process sparse and binary spike\ninformation and avoid expensive multiplication operations. Although the\nefficiency of SNNs can be realized on the In-Memory Computing (IMC)\narchitecture, we show that the energy cost and latency of SNNs scale linearly\nwith the number of timesteps used on IMC hardware. Therefore, in order to\nmaximize the efficiency of SNNs, we propose input-aware Dynamic Timestep SNN\n(DT-SNN), a novel algorithmic solution to dynamically determine the number of\ntimesteps during inference on an input-dependent basis. By calculating the\nentropy of the accumulated output after each timestep, we can compare it to a\npredefined threshold and decide if the information processed at the current\ntimestep is sufficient for a confident prediction. We deploy DT-SNN on an IMC\narchitecture and show that it incurs negligible computational overhead. We\ndemonstrate that our method only uses 1.46 average timesteps to achieve the\naccuracy of a 4-timestep static SNN while reducing the energy-delay-product by\n80%.",
                "Exploiting Large Neuroimaging Datasets to Create Connectome-Constrained\n  Approaches for more Robust, Efficient, and Adaptable Artificial Intelligence\nDespite the progress in deep learning networks, efficient learning at the\nedge (enabling adaptable, low-complexity machine learning solutions) remains a\ncritical need for defense and commercial applications. We envision a pipeline\nto utilize large neuroimaging datasets, including maps of the brain which\ncapture neuron and synapse connectivity, to improve machine learning\napproaches. We have pursued different approaches within this pipeline\nstructure. First, as a demonstration of data-driven discovery, the team has\ndeveloped a technique for discovery of repeated subcircuits, or motifs. These\nwere incorporated into a neural architecture search approach to evolve network\narchitectures. Second, we have conducted analysis of the heading direction\ncircuit in the fruit fly, which performs fusion of visual and angular velocity\nfeatures, to explore augmenting existing computational models with new insight.\nOur team discovered a novel pattern of connectivity, implemented a new model,\nand demonstrated sensor fusion on a robotic platform. Third, the team analyzed\ncircuitry for memory formation in the fruit fly connectome, enabling the design\nof a novel generative replay approach. Finally, the team has begun analysis of\nconnectivity in mammalian cortex to explore potential improvements to\ntransformer networks. These constraints increased network robustness on the\nmost challenging examples in the CIFAR-10-C computer vision robustness\nbenchmark task, while reducing learnable attention parameters by over an order\nof magnitude. Taken together, these results demonstrate multiple potential\napproaches to utilize insight from neural systems for developing robust and\nefficient machine learning techniques.",
                "Chain-of-Thought Hub: A Continuous Effort to Measure Large Language\n  Models' Reasoning Performance\nAs large language models (LLMs) are continuously being developed, their\nevaluation becomes increasingly important yet challenging. This work proposes\nChain-of-Thought Hub, an open-source evaluation suite on the multi-step\nreasoning capabilities of large language models. We are interested in this\nsetting for two reasons: (1) from the behavior of GPT and PaLM model family, we\nobserve that complex reasoning is likely to be a key differentiator between\nweaker and stronger LLMs; (2) we envisage large language models to become the\nnext-generation computational platform and foster an ecosystem of LLM-based new\napplications, this naturally requires the foundation models to perform complex\ntasks that often involve the composition of linguistic and logical operations.\nOur approach is to compile a suite of challenging reasoning benchmarks to track\nthe progress of LLMs. Our current results show that: (1) model scale clearly\ncorrelates with reasoning capabilities; (2) As of May 2023, Claude-v1.3 and\nPaLM-2 are the only two models that are comparable with GPT-4, while\nopen-sourced models still lag behind; (3) LLaMA-65B performs closely to\ncode-davinci-002, indicating that with successful further development such as\nreinforcement learning from human feedback (RLHF), it has great potential to be\nclose to GPT-3.5-Turbo. Our results also suggest that for the open-source\nefforts to catch up, the community may focus more on building better base\nmodels and exploring RLHF."
            ],
            "interesting paper": 3
        },
        {
            "papers": [
                "Synthesizing a Progression of Subtasks for Block-Based Visual\n  Programming Tasks\nBlock-based visual programming environments play an increasingly important\nrole in introducing computing concepts to K-12 students. In recent years, they\nhave also gained popularity in neuro-symbolic AI, serving as a benchmark to\nevaluate general problem-solving and logical reasoning skills. The open-ended\nand conceptual nature of these visual programming tasks make them challenging,\nboth for state-of-the-art AI agents as well as for novice programmers. A\nnatural approach to providing assistance for problem-solving is breaking down a\ncomplex task into a progression of simpler subtasks; however, this is not\ntrivial given that the solution codes are typically nested and have non-linear\nexecution behavior. In this paper, we formalize the problem of synthesizing\nsuch a progression for a given reference block-based visual programming task.\nWe propose a novel synthesis algorithm that generates a progression of subtasks\nthat are high-quality, well-spaced in terms of their complexity, and solving\nthis progression leads to solving the reference task. We show the utility of\nour synthesis algorithm in improving the efficacy of AI agents (in this case,\nneural program synthesizers) for solving tasks in the Karel programming\nenvironment. Then, we conduct a user study to demonstrate that our synthesized\nprogression of subtasks can assist a novice programmer in solving tasks in the\nHour of Code: Maze Challenge by Code-dot-org.",
                "Emergent Modularity in Pre-trained Transformers\nThis work examines the presence of modularity in pre-trained Transformers, a\nfeature commonly found in human brains and thought to be vital for general\nintelligence. In analogy to human brains, we consider two main characteristics\nof modularity: (1) functional specialization of neurons: we evaluate whether\neach neuron is mainly specialized in a certain function, and find that the\nanswer is yes. (2) function-based neuron grouping: we explore finding a\nstructure that groups neurons into modules by function, and each module works\nfor its corresponding function. Given the enormous amount of possible\nstructures, we focus on Mixture-of-Experts as a promising candidate, which\npartitions neurons into experts and usually activates different experts for\ndifferent inputs. Experimental results show that there are functional experts,\nwhere clustered are the neurons specialized in a certain function. Moreover,\nperturbing the activations of functional experts significantly affects the\ncorresponding function. Finally, we study how modularity emerges during\npre-training, and find that the modular structure is stabilized at the early\nstage, which is faster than neuron stabilization. It suggests that Transformers\nfirst construct the modular structure and then learn fine-grained neuron\nfunctions. Our code and data are available at\nhttps://github.com/THUNLP/modularity-analysis.",
                "In-Context Analogical Reasoning with Pre-Trained Language Models\nAnalogical reasoning is a fundamental capacity of human cognition that allows\nus to reason abstractly about novel situations by relating them to past\nexperiences. While it is thought to be essential for robust reasoning in AI\nsystems, conventional approaches require significant training and/or\nhard-coding of domain knowledge to be applied to benchmark tasks. Inspired by\ncognitive science research that has found connections between human language\nand analogy-making, we explore the use of intuitive language-based abstractions\nto support analogy in AI systems. Specifically, we apply large pre-trained\nlanguage models (PLMs) to visual Raven's Progressive Matrices (RPM), a common\nrelational reasoning test. By simply encoding the perceptual features of the\nproblem into language form, we find that PLMs exhibit a striking capacity for\nzero-shot relational reasoning, exceeding human performance and nearing\nsupervised vision-based methods. We explore different encodings that vary the\nlevel of abstraction over task features, finding that higher-level abstractions\nfurther strengthen PLMs' analogical reasoning. Our detailed analysis reveals\ninsights on the role of model complexity, in-context learning, and prior\nknowledge in solving RPM tasks.",
                "On the Value of Myopic Behavior in Policy Reuse\nLeveraging learned strategies in unfamiliar scenarios is fundamental to human\nintelligence. In reinforcement learning, rationally reusing the policies\nacquired from other tasks or human experts is critical for tackling problems\nthat are difficult to learn from scratch. In this work, we present a framework\ncalled Selective Myopic bEhavior Control~(SMEC), which results from the insight\nthat the short-term behaviors of prior policies are sharable across tasks. By\nevaluating the behaviors of prior policies via a hybrid value function\narchitecture, SMEC adaptively aggregates the sharable short-term behaviors of\nprior policies and the long-term behaviors of the task policy, leading to\ncoordinated decisions. Empirical results on a collection of manipulation and\nlocomotion tasks demonstrate that SMEC outperforms existing methods, and\nvalidate the ability of SMEC to leverage related prior policies.",
                "Evolving Connectivity for Recurrent Spiking Neural Networks\nRecurrent spiking neural networks (RSNNs) hold great potential for advancing\nartificial general intelligence, as they draw inspiration from the biological\nnervous system and show promise in modeling complex dynamics. However, the\nwidely-used surrogate gradient-based training methods for RSNNs are inherently\ninaccurate and unfriendly to neuromorphic hardware. To address these\nlimitations, we propose the evolving connectivity (EC) framework, an\ninference-only method for training RSNNs. The EC framework reformulates\nweight-tuning as a search into parameterized connection probability\ndistributions, and employs Natural Evolution Strategies (NES) for optimizing\nthese distributions. Our EC framework circumvents the need for gradients and\nfeatures hardware-friendly characteristics, including sparse boolean\nconnections and high scalability. We evaluate EC on a series of standard\nrobotic locomotion tasks, where it achieves comparable performance with deep\nneural networks and outperforms gradient-trained RSNNs, even solving the\ncomplex 17-DoF humanoid task. Additionally, the EC framework demonstrates a two\nto three fold speedup in efficiency compared to directly evolving parameters.\nBy providing a performant and hardware-friendly alternative, the EC framework\nlays the groundwork for further energy-efficient applications of RSNNs and\nadvances the development of neuromorphic devices.",
                "Optimization's Neglected Normative Commitments\nOptimization is offered as an objective approach to resolving complex,\nreal-world decisions involving uncertainty and conflicting interests. It drives\nbusiness strategies as well as public policies and, increasingly, lies at the\nheart of sophisticated machine learning systems. A paradigm used to approach\npotentially high-stakes decisions, optimization relies on abstracting the real\nworld to a set of decision(s), objective(s) and constraint(s). Drawing from the\nmodeling process and a range of actual cases, this paper describes the\nnormative choices and assumptions that are necessarily part of using\noptimization. It then identifies six emergent problems that may be neglected:\n1) Misspecified values can yield optimizations that omit certain imperatives\naltogether or incorporate them incorrectly as a constraint or as part of the\nobjective, 2) Problematic decision boundaries can lead to faulty modularity\nassumptions and feedback loops, 3) Failing to account for multiple agents'\ndivergent goals and decisions can lead to policies that serve only certain\nnarrow interests, 4) Mislabeling and mismeasurement can introduce bias and\nimprecision, 5) Faulty use of relaxation and approximation methods,\nunaccompanied by formal characterizations and guarantees, can severely impede\napplicability, and 6) Treating optimization as a justification for action,\nwithout specifying the necessary contextual information, can lead to ethically\ndubious or faulty decisions. Suggestions are given to further understand and\ncurb the harms that can arise when optimization is used wrongfully."
            ],
            "interesting paper": 2
        },
        {
            "papers": [
                "Determinantal Point Process Attention Over Grid Cell Code Supports Out\n  of Distribution Generalization\nDeep neural networks have made tremendous gains in emulating human-like\nintelligence, and have been used increasingly as ways of understanding how the\nbrain may solve the complex computational problems on which this relies.\nHowever, these still fall short of, and therefore fail to provide insight into\nhow the brain supports strong forms of generalization of which humans are\ncapable. One such case is out-of-distribution (OOD) generalization-successful\nperformance on test examples that lie outside the distribution of the training\nset. Here, we identify properties of processing in the brain that may\ncontribute to this ability. We describe a two-part algorithm that draws on\nspecific features of neural computation to achieve OOD generalization, and\nprovide a proof of concept by evaluating performance on two challenging\ncognitive tasks. First we draw on the fact that the mammalian brain represents\nmetric spaces using grid cell code (e.g., in the entorhinal cortex): abstract\nrepresentations of relational structure, organized in recurring motifs that\ncover the representational space. Second, we propose an attentional mechanism\nthat operates over the grid cell code using Determinantal Point Process (DPP),\nthat we call DPP attention (DPP-A) -- a transformation that ensures maximum\nsparseness in the coverage of that space. We show that a loss function that\ncombines standard task-optimized error with DPP-A can exploit the recurring\nmotifs in the grid cell code, and can be integrated with common architectures\nto achieve strong OOD generalization performance on analogy and arithmetic\ntasks. This provides both an interpretation of how the grid cell code in the\nmammalian brain may contribute to generalization performance, and at the same\ntime a potential means for improving such capabilities in artificial neural\nnetworks.",
                "Universal Mechanical Polycomputation in Granular Matter\nUnconventional computing devices are increasingly of interest as they can\noperate in environments hostile to silicon-based electronics, or compute in\nways that traditional electronics cannot. Mechanical computers, wherein\ninformation processing is a material property emerging from the interaction of\ncomponents with the environment, are one such class of devices. This\ninformation processing can be manifested in various physical substrates, one of\nwhich is granular matter. In a granular assembly, vibration can be treated as\nthe information-bearing mode. This can be exploited to realize \"polycomputing\":\nmaterials can be evolved such that a single grain within them can report the\nresult of multiple logical operations simultaneously at different frequencies,\nwithout recourse to quantum effects. Here, we demonstrate the evolution of a\nmaterial in which one grain acts simultaneously as two different NAND gates at\ntwo different frequencies. NAND gates are of interest as any logical operations\ncan be built from them. Moreover, they are nonlinear thus demonstrating a step\ntoward general-purpose, computationally dense mechanical computers.\nPolycomputation was found to be distributed across each evolved material,\nsuggesting the material's robustness. With recent advances in material\nsciences, hardware realization of these materials may eventually provide\ndevices that challenge the computational density of traditional computers.",
                "Key-Value Transformer\nTransformers have emerged as the prevailing standard solution for various AI\ntasks, including computer vision and natural language processing. The widely\nadopted Query, Key, and Value formulation (QKV) has played a significant role\nin this. Nevertheless, no research has examined the essentiality of these three\ncomponents for transformer performance. Therefore, we conducted an evaluation\nof the key-value formulation (KV), which generates symmetric attention maps,\nalong with an asymmetric version that incorporates a 2D positional encoding\ninto the attention matrix. Remarkably, this transformer requires fewer\nparameters and computation than the original one. Through experiments\nencompassing three task types -- synthetics (such as reversing or sorting a\nlist), vision (mnist or cifar classification), and NLP (character generation\nand translation) -- we discovered that the KV transformer occasionally\noutperforms the QKV transformer. However, it also exhibits instances of\nunderperformance compared to QKV, making it challenging to draw a definitive\nconclusion. Nonetheless, we consider the reported results to be encouraging and\nanticipate that they may pave the way for more efficient transformers in the\nfuture.",
                "The Digital Divide in Process Safety: Quantitative Risk Analysis of\n  Human-AI Collaboration\nDigital technologies have dramatically accelerated the digital transformation\nin process industries, boosted new industrial applications, upgraded the\nproduction system, and enhanced operational efficiency. In contrast, the\nchallenges and gaps between human and artificial intelligence (AI) have become\nmore and more prominent, whereas the digital divide in process safety is\naggregating. The study attempts to address the following questions: (i)What is\nAI in the process safety context? (ii)What is the difference between AI and\nhumans in process safety? (iii)How do AI and humans collaborate in process\nsafety? (iv)What are the challenges and gaps in human-AI collaboration? (v)How\nto quantify the risk of human-AI collaboration in process safety? Qualitative\nrisk analysis based on brainstorming and literature review, and quantitative\nrisk analysis based on layer of protection analysis (LOPA) and Bayesian network\n(BN), were applied to explore and model. The importance of human reliability\nshould be stressed in the digital age, not usually to increase the reliability\nof AI, and human-centered AI design in process safety needs to be propagated.",
                "Re-imagining health and well-being in low resource African settings\n  using an augmented AI system and a 3D digital twin\nThis paper discusses and explores the potential and relevance of recent\ndevelopments in artificial intelligence (AI) and digital twins for health and\nwell-being in low-resource African countries. We use the case of public health\nemergency response to disease outbreaks and epidemic control. There is\npotential to take advantage of the increasing availability of data and\ndigitization to develop advanced AI methods for analysis and prediction. Using\nan AI systems perspective, we review emerging trends in AI systems and digital\ntwins and propose an initial augmented AI system architecture to illustrate how\nan AI system can work with a 3D digital twin to address public health goals. We\nhighlight scientific knowledge discovery, continual learning, pragmatic\ninteroperability, and interactive explanation and decision-making as essential\nresearch challenges for AI systems and digital twins.",
                "Continual Task Allocation in Meta-Policy Network via Sparse Prompting\nHow to train a generalizable meta-policy by continually learning a sequence\nof tasks? It is a natural human skill yet challenging to achieve by current\nreinforcement learning: the agent is expected to quickly adapt to new tasks\n(plasticity) meanwhile retaining the common knowledge from previous tasks\n(stability). We address it by \"Continual Task Allocation via Sparse Prompting\n(CoTASP)\", which learns over-complete dictionaries to produce sparse masks as\nprompts extracting a sub-network for each task from a meta-policy network.\nCoTASP trains a policy for each task by optimizing the prompts and the\nsub-network weights alternatively. The dictionary is then updated to align the\noptimized prompts with tasks' embedding, thereby capturing tasks' semantic\ncorrelations. Hence, relevant tasks share more neurons in the meta-policy\nnetwork due to similar prompts while cross-task interference causing forgetting\nis effectively restrained. Given a meta-policy and dictionaries trained on\nprevious tasks, new task adaptation reduces to highly efficient sparse\nprompting and sub-network finetuning. In experiments, CoTASP achieves a\npromising plasticity-stability trade-off without storing or replaying any past\ntasks' experiences. It outperforms existing continual and multi-task RL methods\non all seen tasks, forgetting reduction, and generalization to unseen tasks."
            ],
            "interesting paper": 1
        },
        {
            "papers": [
                "Dissecting Chain-of-Thought: Compositionality through In-Context\n  Filtering and Learning\nChain-of-thought (CoT) is a method that enables language models to handle\ncomplex reasoning tasks by decomposing them into simpler steps. Despite its\nsuccess, the underlying mechanics of CoT are not yet fully understood. In an\nattempt to shed light on this, our study investigates the impact of CoT on the\nability of transformers to in-context learn a simple to study, yet general\nfamily of compositional functions: multi-layer perceptrons (MLPs). In this\nsetting, we find that the success of CoT can be attributed to breaking down\nin-context learning of a compositional function into two distinct phases:\nfocusing on and filtering data related to each step of the composition and\nin-context learning the single-step composition function. Through both\nexperimental and theoretical evidence, we demonstrate how CoT significantly\nreduces the sample complexity of in-context learning (ICL) and facilitates the\nlearning of complex functions that non-CoT methods struggle with. Furthermore,\nwe illustrate how transformers can transition from vanilla in-context learning\nto mastering a compositional function with CoT by simply incorporating\nadditional layers that perform the necessary data-filtering for CoT via the\nattention mechanism. In addition to these test-time benefits, we show CoT helps\naccelerate pretraining by learning shortcuts to represent complex functions and\nfiltering plays an important role in this process. These findings collectively\nprovide insights into the mechanics of CoT, inviting further investigation of\nits role in complex reasoning tasks.",
                "Code Prompting: a Neural Symbolic Method for Complex Reasoning in Large\n  Language Models\nLarge language models (LLMs) have scaled up to unlock a wide range of complex\nreasoning tasks with the aid of various prompting methods. However, current\nprompting methods generate natural language intermediate steps to help\nreasoning, which can cause imperfect task reduction and confusion. To mitigate\nsuch limitations, we explore code prompting, a neural symbolic prompting method\nwith both zero-shot and few-shot versions which triggers code as intermediate\nsteps. We conduct experiments on 7 widely-used benchmarks involving symbolic\nreasoning and arithmetic reasoning. Code prompting generally outperforms\nchain-of-thought (CoT) prompting. To further understand the performance and\nlimitations of code prompting, we perform extensive ablation studies and error\nanalyses, and identify several exclusive advantages of using symbolic\npromptings compared to natural language. We also consider the ensemble of code\nprompting and CoT prompting to combine the strengths of both. Finally, we show\nthrough experiments how code annotations and their locations affect code\nprompting.",
                "Towards a Unifying Model of Rationality in Multiagent Systems\nMultiagent systems deployed in the real world need to cooperate with other\nagents (including humans) nearly as effectively as these agents cooperate with\none another. To design such AI, and provide guarantees of its effectiveness, we\nneed to clearly specify what types of agents our AI must be able to cooperate\nwith. In this work we propose a generic model of socially intelligent agents,\nwhich are individually rational learners that are also able to cooperate with\none another (in the sense that their joint behavior is Pareto efficient). We\ndefine rationality in terms of the regret incurred by each agent over its\nlifetime, and show how we can construct socially intelligent agents for\ndifferent forms of regret. We then discuss the implications of this model for\nthe development of \"robust\" MAS that can cooperate with a wide variety of\nsocially intelligent agents.",
                "Low Precision Quantization-aware Training in Spiking Neural Networks\n  with Differentiable Quantization Function\nDeep neural networks have been proven to be highly effective tools in various\ndomains, yet their computational and memory costs restrict them from being\nwidely deployed on portable devices. The recent rapid increase of edge\ncomputing devices has led to an active search for techniques to address the\nabove-mentioned limitations of machine learning frameworks. The quantization of\nartificial neural networks (ANNs), which converts the full-precision synaptic\nweights into low-bit versions, emerged as one of the solutions. At the same\ntime, spiking neural networks (SNNs) have become an attractive alternative to\nconventional ANNs due to their temporal information processing capability,\nenergy efficiency, and high biological plausibility. Despite being driven by\nthe same motivation, the simultaneous utilization of both concepts has yet to\nbe thoroughly studied. Therefore, this work aims to bridge the gap between\nrecent progress in quantized neural networks and SNNs. It presents an extensive\nstudy on the performance of the quantization function, represented as a linear\ncombination of sigmoid functions, exploited in low-bit weight quantization in\nSNNs. The presented quantization function demonstrates the state-of-the-art\nperformance on four popular benchmarks, CIFAR10-DVS, DVS128 Gesture,\nN-Caltech101, and N-MNIST, for binary networks (64.05\\%, 95.45\\%, 68.71\\%, and\n99.43\\% respectively) with small accuracy drops and up to 31$\\times$ memory\nsavings, which outperforms existing methods.",
                "An Emergency Disposal Decision-making Method with Human--Machine\n  Collaboration\nRapid developments in artificial intelligence technology have led to unmanned\nsystems replacing human beings in many fields requiring high-precision\npredictions and decisions. In modern operational environments, all job plans\nare affected by emergency events such as equipment failures and resource\nshortages, making a quick resolution critical. The use of unmanned systems to\nassist decision-making can improve resolution efficiency, but their\ndecision-making is not interpretable and may make the wrong decisions. Current\nunmanned systems require human supervision and control. Based on this, we\npropose a collaborative human--machine method for resolving unplanned events\nusing two phases: task filtering and task scheduling. In the task filtering\nphase, we propose a human--machine collaborative decision-making algorithm for\ndynamic tasks. The GACRNN model is used to predict the state of the job nodes,\nlocate the key nodes, and generate a machine-predicted resolution task list. A\nhuman decision-maker supervises the list in real time and modifies and confirms\nthe machine-predicted list through the human--machine interface. In the task\nscheduling phase, we propose a scheduling algorithm that integrates human\nexperience constraints. The steps to resolve an event are inserted into the\nnormal job sequence to schedule the resolution. We propose several\nhuman--machine collaboration methods in each phase to generate steps to resolve\nan unplanned event while minimizing the impact on the original job plan.",
                "Chatbots put to the test in math and logic problems: A preliminary\n  comparison and assessment of ChatGPT-3.5, ChatGPT-4, and Google Bard\nA comparison between three chatbots which are based on large language models,\nnamely ChatGPT-3.5, ChatGPT-4 and Google Bard is presented, focusing on their\nability to give correct answers to mathematics and logic problems. In\nparticular, we check their ability to Understand the problem at hand; Apply\nappropriate algorithms or methods for its solution; and Generate a coherent\nresponse and a correct answer. We use 30 questions that are clear, without any\nambiguities, fully described with plain text only, and have a unique, well\ndefined correct answer. The questions are divided into two sets of 15 each. The\nquestions of Set A are 15 \"Original\" problems that cannot be found online,\nwhile Set B contains 15 \"Published\" problems that one can find online, usually\nwith their solution. Each question is posed three times to each chatbot. The\nanswers are recorded and discussed, highlighting their strengths and\nweaknesses. It has been found that for straightforward arithmetic, algebraic\nexpressions, or basic logic puzzles, chatbots may provide accurate solutions,\nalthough not in every attempt. However, for more complex mathematical problems\nor advanced logic tasks, their answers, although written in a usually\n\"convincing\" way, may not be reliable. Consistency is also an issue, as many\ntimes a chatbot will provide conflicting answers when given the same question\nmore than once. A comparative quantitative evaluation of the three chatbots is\nmade through scoring their final answers based on correctness. It was found\nthat ChatGPT-4 outperforms ChatGPT-3.5 in both sets of questions. Bard comes\nthird in the original questions of Set A, behind the other two chatbots, while\nit has the best performance (first place) in the published questions of Set B.\nThis is probably because Bard has direct access to the internet, in contrast to\nChatGPT chatbots which do not have any communication with the outside world."
            ],
            "interesting paper": 1
        },
        {
            "papers": [
                "Evaluating GPT's Programming Capability through CodeWars' Katas\nIn the burgeoning field of artificial intelligence (AI), understanding the\ncapabilities and limitations of programming-oriented models is crucial. This\npaper presents a novel evaluation of the programming proficiency of Generative\nPretrained Transformer (GPT) models, specifically GPT-3.5 and GPT-4, against\ncoding problems of varying difficulty levels drawn from Codewars. The\nexperiments reveal a distinct boundary at the 3kyu level, beyond which these\nGPT models struggle to provide solutions. These findings led to the proposal of\na measure for coding problem complexity that incorporates both problem\ndifficulty and the time required for solution. The research emphasizes the need\nfor validation and creative thinking capabilities in AI models to better\nemulate human problem-solving techniques. Future work aims to refine this\nproposed complexity measure, enhance AI models with these suggested\ncapabilities, and develop an objective measure for programming problem\ndifficulty. The results of this research offer invaluable insights for\nimproving AI programming capabilities and advancing the frontier of AI\nproblem-solving abilities.",
                "Uncovering multifunctional mechano-intelligence in and through phononic\n  metastructures harnessing physical reservoir computing\nThe recent advances in autonomous systems have prompted a strong demand for\nthe next generation of adaptive structures and materials to possess more\nbuilt-in intelligence in their mechanical domain, the so-called\nmechano-intelligence (MI). Previous MI attempts mainly focused on specific\ndesigns and case studies to realize limited aspects of MI, and there is a lack\nof a systematic foundation in constructing and integrating the different\nelements of intelligence in an effective and efficient manner. Here, we propose\na new approach to create the needed foundation in realizing integrated\nmultifunctional MI via a physical reservoir computing (PRC) framework. That is,\nto concurrently embody computing power and the various elements of\nintelligence, namely perception, decision-making, and commanding, directly in\nthe mechanical domain, advancing from conventional adaptive structures that\nrely solely on add-on digital computers and massive electronics to achieve\nintelligence. As an exemplar platform, we construct a mechanically intelligent\nphononic metastructure with the integrated elements of MI by harnessing the PRC\npower hidden in their high-degree-of-freedom nonlinear dynamics. Through\nanalyses and experimental investigations, we uncover multiple adaptive\nstructural functions ranging from self-tuning wave controls to wave-based logic\ngates. This research will provide the basis for creating future new structures\nthat would greatly surpass the state of the art - such as lower power\nconsumption, more direct interactions, and much better survivability in harsh\nenvironment or under cyberattacks. Moreover, it will enable the addition of new\nfunctions and autonomy to systems without overburdening the onboard computers.",
                "Probabilistic computation and uncertainty quantification with emerging\n  covariance\nBuilding robust, interpretable, and secure AI system requires quantifying and\nrepresenting uncertainty under a probabilistic perspective to mimic human\ncognitive abilities. However, probabilistic computation presents significant\nchallenges for most conventional artificial neural network, as they are\nessentially implemented in a deterministic manner. In this paper, we develop an\nefficient probabilistic computation framework by truncating the probabilistic\nrepresentation of neural activation up to its mean and covariance and construct\na moment neural network that encapsulates the nonlinear coupling between the\nmean and covariance of the underlying stochastic network. We reveal that when\nonly the mean but not the covariance is supervised during gradient-based\nlearning, the unsupervised covariance spontaneously emerges from its nonlinear\ncoupling with the mean and faithfully captures the uncertainty associated with\nmodel predictions. Our findings highlight the inherent simplicity of\nprobabilistic computation by seamlessly incorporating uncertainty into model\nprediction, paving the way for integrating it into large-scale AI systems.",
                "Recasting Self-Attention with Holographic Reduced Representations\nIn recent years, self-attention has become the dominant paradigm for sequence\nmodeling in a variety of domains. However, in domains with very long sequence\nlengths the $\\mathcal{O}(T^2)$ memory and $\\mathcal{O}(T^2 H)$ compute costs\ncan make using transformers infeasible. Motivated by problems in malware\ndetection, where sequence lengths of $T \\geq 100,000$ are a roadblock to deep\nlearning, we re-cast self-attention using the neuro-symbolic approach of\nHolographic Reduced Representations (HRR). In doing so we perform the same\nhigh-level strategy of the standard self-attention: a set of queries matching\nagainst a set of keys, and returning a weighted response of the values for each\nkey. Implemented as a ``Hrrformer'' we obtain several benefits including\n$\\mathcal{O}(T H \\log H)$ time complexity, $\\mathcal{O}(T H)$ space complexity,\nand convergence in $10\\times$ fewer epochs. Nevertheless, the Hrrformer\nachieves near state-of-the-art accuracy on LRA benchmarks and we are able to\nlearn with just a single layer. Combined, these benefits make our Hrrformer the\nfirst viable Transformer for such long malware classification sequences and up\nto $280\\times$ faster to train on the Long Range Arena benchmark. Code is\navailable at\n\\url{https://github.com/NeuromorphicComputationResearchProgram/Hrrformer}",
                "Unified Information Dynamic Analysis of Quantum Decision-Making and\n  Search Algorithms: Computational Intelligence Measure\nThere are important algorithms built upon a mixture of basic techniques\ndescribed; for example, the Fast Fourier Transform (FFT) employs both\nDivide-and-Conquer and Transform-and-Conquer techniques. In this article, the\nevolution of a quantum algorithm (QA) is examined from an information theory\nviewpoint. The complex vector entering the quantum algorithmic gate - QAG is\nconsidered as an information source both from the classical and the quantum\nlevel. The analysis of the classical and quantum information flow in\nDeutsch-Jozsa, Shor and Grover algorithms is used. It is shown that QAG, based\non superposition of states, quantum entanglement and interference, when acting\non the input vector, stores information into the system state, minimizing the\ngap between classical Shannon entropy and quantum von Neumann entropy.\nMinimizing of the gap between Shannon and von Neumann entropies is considered\nas a termination criterion of QA computational intelligence measure.",
                "SheetCopilot: Bringing Software Productivity to the Next Level through\n  Large Language Models\nComputer end users have spent billions of hours completing daily tasks like\ntabular data processing and project timeline scheduling. Most of these tasks\nare repetitive and error-prone, yet most end users lack the skill to automate\nthese burdensome works. With the advent of large language models (LLMs),\ndirecting software with natural language user requests become a reachable goal.\nIn this work, we propose a SheetCopilot agent that takes natural language task\nand control spreadsheet to fulfill the requirements. We propose a set of atomic\nactions as an abstraction of spreadsheet software functionalities. We further\ndesign a state machine-based task planning framework for LLMs to robustly\ninteract with spreadsheets. We curate a representative dataset containing 221\nspreadsheet control tasks and establish a fully automated evaluation pipeline\nfor rigorously benchmarking the ability of LLMs in software control tasks. Our\nSheetCopilot correctly completes 44.3\\% of tasks for a single generation,\noutperforming the strong code generation baseline by a wide margin. Our project\npage:https://sheetcopilot.github.io/."
            ],
            "interesting paper": 2
        },
        {
            "papers": [
                "Environmental Memory Boosts Group Formation of Clueless Individuals\nThe formation of groups of interacting individuals improves performance and\nfitness in many decentralised systems, from micro-organisms to social insects,\nfrom robotic swarms to artificial intelligence algorithms. Often, group\nformation and high-level coordination in these systems emerge from individuals\nwith limited information-processing capabilities implementing low-level rules\nof communication to signal to each other. Here, we show that, even in a\ncommunity of clueless individuals incapable of processing information and\ncommunicating, a dynamic environment can coordinate group formation by\ntransiently storing memory of the earlier passage of individuals. Our results\nidentify a new mechanism of indirect coordination via shared memory that is\nprimarily promoted and reinforced by dynamic environmental factors, thus\novershadowing the need for any form of explicit signalling between individuals.\nWe expect this pathway to group formation to be relevant for understanding and\ncontrolling self-organisation and collective decision making in both living and\nartificial active matter in real-life environments.",
                "Assessing Language Disorders using Artificial Intelligence: a Paradigm\n  Shift\nSpeech, language, and communication deficits are present in most\nneurodegenerative syndromes. They enable the early detection, diagnosis,\ntreatment planning, and monitoring of neurocognitive disease progression as\npart of traditional neurological assessment. Nevertheless, standard speech and\nlanguage evaluation is time-consuming and resource-intensive for clinicians. We\nargue that using machine learning methodologies, natural language processing,\nand modern artificial intelligence (AI) for Language Assessment is an\nimprovement over conventional manual assessment. Using these methodologies,\nComputational Language Assessment (CLA) accomplishes three goals: (i) provides\na neuro-cognitive evaluation of speech, language, and communication in elderly\nand high-risk individuals for dementia; (ii) facilitates the diagnosis,\nprognosis, and therapy efficacy in at-risk and language-impaired populations;\nand (iii) allows easier extensibility to assess patients from a wide range of\nlanguages. By employing AI models, CLA may inform neurocognitive theory on the\nrelationship between language symptoms and their neural bases. Finally, it\nsignals a paradigm shift by significantly advancing our ability to optimize the\nprevention and treatment of elderly individuals with communication disorders,\nallowing them to age gracefully with social engagement.",
                "Human Control: Definitions and Algorithms\nHow can humans stay in control of advanced artificial intelligence systems?\nOne proposal is corrigibility, which requires the agent to follow the\ninstructions of a human overseer, without inappropriately influencing them. In\nthis paper, we formally define a variant of corrigibility called shutdown\ninstructability, and show that it implies appropriate shutdown behavior,\nretention of human autonomy, and avoidance of user harm. We also analyse the\nrelated concepts of non-obstruction and shutdown alignment, three previously\nproposed algorithms for human control, and one new algorithm.",
                "Human or Not? A Gamified Approach to the Turing Test\nWe present \"Human or Not?\", an online game inspired by the Turing test, that\nmeasures the capability of AI chatbots to mimic humans in dialog, and of humans\nto tell bots from other humans. Over the course of a month, the game was played\nby over 1.5 million users who engaged in anonymous two-minute chat sessions\nwith either another human or an AI language model which was prompted to behave\nlike humans. The task of the players was to correctly guess whether they spoke\nto a person or to an AI. This largest scale Turing-style test conducted to date\nrevealed some interesting facts. For example, overall users guessed the\nidentity of their partners correctly in only 68% of the games. In the subset of\nthe games in which users faced an AI bot, users had even lower correct guess\nrates of 60% (that is, not much higher than chance). This white paper details\nthe development, deployment, and results of this unique experiment. While this\nexperiment calls for many extensions and refinements, these findings already\nbegin to shed light on the inevitable near future which will commingle humans\nand AI.",
                "EMOTE: An Explainable architecture for Modelling the Other Through\n  Empathy\nWe can usually assume others have goals analogous to our own. This assumption\ncan also, at times, be applied to multi-agent games - e.g. Agent 1's attraction\nto green pellets is analogous to Agent 2's attraction to red pellets. This\n\"analogy\" assumption is tied closely to the cognitive process known as empathy.\nInspired by empathy, we design a simple and explainable architecture to model\nanother agent's action-value function. This involves learning an \"Imagination\nNetwork\" to transform the other agent's observed state in order to produce a\nhuman-interpretable \"empathetic state\" which, when presented to the learning\nagent, produces behaviours that mimic the other agent. Our approach is\napplicable to multi-agent scenarios consisting of a single learning agent and\nother (independent) agents acting according to fixed policies. This\narchitecture is particularly beneficial for (but not limited to) algorithms\nusing a composite value or reward function. We show our method produces better\nperformance in multi-agent games, where it robustly estimates the other's model\nin different environment configurations. Additionally, we show that the\nempathetic states are human interpretable, and thus verifiable.",
                "Chain-Of-Thought Prompting Under Streaming Batch: A Case Study\nRecently, Large Language Models (LLMs) have demonstrated remarkable\ncapabilities. Chain-of-Thought (CoT) has been proposed as a way of assisting\nLLMs in performing complex reasoning. However, developing effective prompts can\nbe a challenging and labor-intensive task. Many studies come out of some way to\nautomatically construct CoT from test data. Most of them assume that all test\ndata is visible before testing and only select a small subset to generate\nrationales, which is an unrealistic assumption. In this paper, we present a\ncase study on how to construct and optimize chain-of-thought prompting using\nbatch data in streaming settings."
            ],
            "interesting paper": 3
        },
        {
            "papers": [
                "Integrated Sensing-Communication-Computation for Edge Artificial\n  Intelligence\nEdge artificial intelligence (AI) has been a promising solution towards 6G to\nempower a series of advanced techniques such as digital twins, holographic\nprojection, semantic communications, and auto-driving, for achieving\nintelligence of everything. The performance of edge AI tasks, including edge\nlearning and edge AI inference, depends on the quality of three highly coupled\nprocesses, i.e., sensing for data acquisition, computation for information\nextraction, and communication for information transmission. However, these\nthree modules need to compete for network resources for enhancing their own\nquality-of-services. To this end, integrated sensing-communication-computation\n(ISCC) is of paramount significance for improving resource utilization as well\nas achieving the customized goals of edge AI tasks. By investigating the\ninterplay among the three modules, this article presents various kinds of ISCC\nschemes for federated edge learning tasks and edge AI inference tasks in both\napplication and physical layers.",
                "Minding Language Models' (Lack of) Theory of Mind: A Plug-and-Play\n  Multi-Character Belief Tracker\nTheory of Mind (ToM)$\\unicode{x2014}$the ability to reason about the mental\nstates of other people$\\unicode{x2014}$is a key element of our social\nintelligence. Yet, despite their ever more impressive performance, large-scale\nneural language models still lack basic theory of mind capabilities\nout-of-the-box. We posit that simply scaling up models will not imbue them with\ntheory of mind due to the inherently symbolic and implicit nature of the\nphenomenon, and instead investigate an alternative: can we design a\ndecoding-time algorithm that enhances theory of mind of off-the-shelf neural\nlanguage models without explicit supervision? We present SymbolicToM, a\nplug-and-play approach to reason about the belief states of multiple characters\nin reading comprehension tasks via explicit symbolic representation. More\nconcretely, our approach tracks each entity's beliefs, their estimation of\nother entities' beliefs, and higher-order levels of reasoning, all through\ngraphical representations, allowing for more precise and interpretable\nreasoning than previous approaches. Empirical results on the well-known ToMi\nbenchmark (Le et al., 2019) demonstrate that SymbolicToM dramatically enhances\noff-the-shelf neural networks' theory of mind in a zero-shot setting while\nshowing robust out-of-distribution performance compared to supervised\nbaselines. Our work also reveals spurious patterns in existing theory of mind\nbenchmarks, emphasizing the importance of out-of-distribution evaluation and\nmethods that do not overfit a particular dataset.",
                "On the Existence of Information Bottlenecks in Living and Non-Living\n  Systems\nIn many complex systems, we observe that `interesting behaviour' is often the\nconsequence of a system exploiting the existence of an Information Bottleneck\n(IB). These bottlenecks can occur at different scales, between individuals or\ncomponents of a system, and sometimes within individuals themselves.\nOftentimes, we regard these bottlenecks negatively; as merely the limitations\nof the individual's physiology and something that ought to be overcome when\ndesigning and implementing artificial systems. However, we suggest instead that\nIBs may serve a purpose beyond merely providing a minimally-viable channel for\ncoordination in collective systems. More specifically, we suggest that\ninteresting or novel behaviour occurs when the individuals in a system are\nconstrained or limited in their ability to share information and must discover\nnovel ways to exploit existing mechanisms, which are inherently bottlenecked,\nrather than circumventing or otherwise avoiding those mechanisms entirely.",
                "An Architecture for Deploying Reinforcement Learning in Industrial\n  Environments\nIndustry 4.0 is driven by demands like shorter time-to-market, mass\ncustomization of products, and batch size one production. Reinforcement\nLearning (RL), a machine learning paradigm shown to possess a great potential\nin improving and surpassing human level performance in numerous complex tasks,\nallows coping with the mentioned demands. In this paper, we present an OPC UA\nbased Operational Technology (OT)-aware RL architecture, which extends the\nstandard RL setting, combining it with the setting of digital twins. Moreover,\nwe define an OPC UA information model allowing for a generalized plug-and-play\nlike approach for exchanging the RL agent used. In conclusion, we demonstrate\nand evaluate the architecture, by creating a proof of concept. By means of\nsolving a toy example, we show that this architecture can be used to determine\nthe optimal policy using a real control system.",
                "The feasibility of artificial consciousness through the lens of\n  neuroscience\nInteractions with large language models have led to the suggestion that these\nmodels may soon be conscious. From the perspective of neuroscience, this\nposition is difficult to defend. For one, the inputs to large language models\nlack the embodied, embedded information content characteristic of our sensory\ncontact with the world around us. Secondly, the architecture of large language\nmodels is missing key features of the thalamocortical system that have been\nlinked to conscious awareness in mammals. Finally, the evolutionary and\ndevelopmental trajectories that led to the emergence of living conscious\norganisms arguably have no parallels in artificial systems as envisioned today.\nThe existence of living organisms depends on their actions, and their survival\nis intricately linked to multi-level cellular, inter-cellular, and organismal\nprocesses culminating in agency and consciousness.",
                "Responsible Task Automation: Empowering Large Language Models as\n  Responsible Task Automators\nThe recent success of Large Language Models (LLMs) signifies an impressive\nstride towards artificial general intelligence. They have shown a promising\nprospect in automatically completing tasks upon user instructions, functioning\nas brain-like coordinators. The associated risks will be revealed as we\ndelegate an increasing number of tasks to machines for automated completion. A\nbig question emerges: how can we make machines behave responsibly when helping\nhumans automate tasks as personal copilots? In this paper, we explore this\nquestion in depth from the perspectives of feasibility, completeness and\nsecurity. In specific, we present Responsible Task Automation (ResponsibleTA)\nas a fundamental framework to facilitate responsible collaboration between\nLLM-based coordinators and executors for task automation with three empowered\ncapabilities: 1) predicting the feasibility of the commands for executors; 2)\nverifying the completeness of executors; 3) enhancing the security (e.g., the\nprotection of users' privacy). We further propose and compare two paradigms for\nimplementing the first two capabilities. One is to leverage the generic\nknowledge of LLMs themselves via prompt engineering while the other is to adopt\ndomain-specific learnable models. Moreover, we introduce a local memory\nmechanism for achieving the third capability. We evaluate our proposed\nResponsibleTA on UI task automation and hope it could bring more attentions to\nensuring LLMs more responsible in diverse scenarios."
            ],
            "interesting paper": 2
        }
    ],
    "Morgan Lopez": [
        {
            "papers": [
                "Prompt Evolution for Generative AI: A Classifier-Guided Approach\nSynthesis of digital artifacts conditioned on user prompts has become an\nimportant paradigm facilitating an explosion of use cases with generative AI.\nHowever, such models often fail to connect the generated outputs and desired\ntarget concepts/preferences implied by the prompts. Current research addressing\nthis limitation has largely focused on enhancing the prompts before output\ngeneration or improving the model's performance up front. In contrast, this\npaper conceptualizes prompt evolution, imparting evolutionary selection\npressure and variation during the generative process to produce multiple\noutputs that satisfy the target concepts/preferences better. We propose a\nmulti-objective instantiation of this broader idea that uses a multi-label\nimage classifier-guided approach. The predicted labels from the classifiers\nserve as multiple objectives to optimize, with the aim of producing diversified\nimages that meet user preferences. A novelty of our evolutionary algorithm is\nthat the pre-trained generative model gives us implicit mutation operations,\nleveraging the model's stochastic generative capability to automate the\ncreation of Pareto-optimized images more faithful to user preferences.",
                "Large Language Models for User Interest Journeys\nLarge language models (LLMs) have shown impressive capabilities in natural\nlanguage understanding and generation. Their potential for deeper user\nunderstanding and improved personalized user experience on recommendation\nplatforms is, however, largely untapped. This paper aims to address this gap.\nRecommender systems today capture users' interests through encoding their\nhistorical activities on the platforms. The generated user representations are\nhard to examine or interpret. On the other hand, if we were to ask people about\ninterests they pursue in their life, they might talk about their hobbies, like\nI just started learning the ukulele, or their relaxation routines, e.g., I like\nto watch Saturday Night Live, or I want to plant a vertical garden. We argue,\nand demonstrate through extensive experiments, that LLMs as foundation models\ncan reason through user activities, and describe their interests in nuanced and\ninteresting ways, similar to how a human would.\n  We define interest journeys as the persistent and overarching user interests,\nin other words, the non-transient ones. These are the interests that we believe\nwill benefit most from the nuanced and personalized descriptions. We introduce\na framework in which we first perform personalized extraction of interest\njourneys, and then summarize the extracted journeys via LLMs, using techniques\nlike few-shot prompting, prompt-tuning and fine-tuning. Together, our results\nin prompting LLMs to name extracted user journeys in a large-scale industrial\nplatform demonstrate great potential of these models in providing deeper, more\ninterpretable, and controllable user understanding. We believe LLM powered user\nunderstanding can be a stepping stone to entirely new user experiences on\nrecommendation platforms that are journey-aware, assistive, and enabling\nfrictionless conversation down the line.",
                "HuatuoGPT, towards Taming Language Model to Be a Doctor\nIn this paper, we present HuatuoGPT, a large language model (LLM) for medical\nconsultation. The core recipe of HuatuoGPT is to leverage both\n\\textit{distilled data from ChatGPT} and \\textit{real-world data from doctors}\nin the supervised fine-tuned stage. The responses of ChatGPT are usually\ndetailed, well-presented and informative while it cannot perform like a doctor\nin many aspects, e.g. for integrative diagnosis. We argue that real-world data\nfrom doctors would be complementary to distilled data in the sense the former\ncould tame a distilled language model to perform like doctors. To better\nleverage the strengths of both data, we train a reward model to align the\nlanguage model with the merits that both data bring, following an RLAIF\n(reinforced learning from AI feedback) fashion. To evaluate and benchmark the\nmodels, we propose a comprehensive evaluation scheme (including automatic and\nmanual metrics). Experimental results demonstrate that HuatuoGPT achieves\nstate-of-the-art results in performing medical consultation among open-source\nLLMs in GPT-4 evaluation, human evaluation, and medical benchmark datasets. It\nis worth noting that by using additional real-world data and RLAIF, the\ndistilled language model (i.e., HuatuoGPT) outperforms its teacher model\nChatGPT in most cases. Our code, data, and models are publicly available at\n\\url{https://github.com/FreedomIntelligence/HuatuoGPT}. The online demo is\navailable at \\url{https://www.HuatuoGPT.cn/}.",
                "An Experimental Investigation into the Evaluation of Explainability\n  Methods\nEXplainable Artificial Intelligence (XAI) aims to help users to grasp the\nreasoning behind the predictions of an Artificial Intelligence (AI) system.\nMany XAI approaches have emerged in recent years. Consequently, a subfield\nrelated to the evaluation of XAI methods has gained considerable attention,\nwith the aim to determine which methods provide the best explanation using\nvarious approaches and criteria. However, the literature lacks a comparison of\nthe evaluation metrics themselves, that one can use to evaluate XAI methods.\nThis work aims to fill this gap by comparing 14 different metrics when applied\nto nine state-of-the-art XAI methods and three dummy methods (e.g., random\nsaliency maps) used as references. Experimental results show which of these\nmetrics produces highly correlated results, indicating potential redundancy. We\nalso demonstrate the significant impact of varying the baseline hyperparameter\non the evaluation metric values. Finally, we use dummy methods to assess the\nreliability of metrics in terms of ranking, pointing out their limitations.",
                "Topic-Guided Self-Introduction Generation for Social Media Users\nMillions of users are active on social media. To allow users to better\nshowcase themselves and network with others, we explore the auto-generation of\nsocial media self-introduction, a short sentence outlining a user's personal\ninterests. While most prior work profiles users with tags (e.g., ages), we\ninvestigate sentence-level self-introductions to provide a more natural and\nengaging way for users to know each other. Here we exploit a user's tweeting\nhistory to generate their self-introduction. The task is non-trivial because\nthe history content may be lengthy, noisy, and exhibit various personal\ninterests. To address this challenge, we propose a novel unified topic-guided\nencoder-decoder (UTGED) framework; it models latent topics to reflect salient\nuser interest, whose topic mixture then guides encoding a user's history and\ntopic words control decoding their self-introduction. For experiments, we\ncollect a large-scale Twitter dataset, and extensive results show the\nsuperiority of our UTGED to the advanced encoder-decoder models without topic\nmodeling.",
                "Visually-Situated Natural Language Understanding with Contrastive\n  Reading Model and Frozen Large Language Models\nRecent advances in Large Language Models (LLMs) have stimulated a surge of\nresearch aimed at extending their applications to the visual domain. While\nthese models exhibit promise in generating abstract image captions and\nfacilitating natural conversations, their performance on text-rich images still\nrequires improvement. In this paper, we introduce Contrastive Reading Model\n(Cream), a novel neural architecture designed to enhance the language-image\nunderstanding capability of LLMs by capturing intricate details that are often\noverlooked in existing methods. Cream combines vision and auxiliary encoders,\nfortified by a contrastive feature alignment technique, to achieve a more\neffective comprehension of language information in visually situated contexts\nwithin the images. Our approach bridges the gap between vision and language\nunderstanding, paving the way for the development of more sophisticated\nDocument Intelligence Assistants. Through rigorous evaluations across diverse\nvisually-situated language understanding tasks that demand reasoning\ncapabilities, we demonstrate the compelling performance of Cream, positioning\nit as a prominent model in the field of visual document understanding. We\nprovide our codebase and newly-generated datasets at\nhttps://github.com/naver-ai/cream ."
            ],
            "interesting paper": 2
        },
        {
            "papers": [
                "Multimodal Recommendation Dialog with Subjective Preference: A New\n  Challenge and Benchmark\nExisting multimodal task-oriented dialog data fails to demonstrate the\ndiverse expressions of user subjective preferences and recommendation acts in\nthe real-life shopping scenario. This paper introduces a new dataset SURE\n(Multimodal Recommendation Dialog with SUbjective PREference), which contains\n12K shopping dialogs in complex store scenes. The data is built in two phases\nwith human annotations to ensure quality and diversity. SURE is well-annotated\nwith subjective preferences and recommendation acts proposed by sales experts.\nA comprehensive analysis is given to reveal the distinguishing features of\nSURE. Three benchmark tasks are then proposed on the data to evaluate the\ncapability of multimodal recommendation agents. Based on the SURE, we propose a\nbaseline model, powered by a state-of-the-art multimodal model, for these\ntasks.",
                "Enhancing Human Capabilities through Symbiotic Artificial Intelligence\n  with Shared Sensory Experiences\nThe merging of human intelligence and artificial intelligence has long been a\nsubject of interest in both science fiction and academia. In this paper, we\nintroduce a novel concept in Human-AI interaction called Symbiotic Artificial\nIntelligence with Shared Sensory Experiences (SAISSE), which aims to establish\na mutually beneficial relationship between AI systems and human users through\nshared sensory experiences. By integrating multiple sensory input channels and\nprocessing human experiences, SAISSE fosters a strong human-AI bond, enabling\nAI systems to learn from and adapt to individual users, providing personalized\nsupport, assistance, and enhancement. Furthermore, we discuss the incorporation\nof memory storage units for long-term growth and development of both the AI\nsystem and its human user. As we address user privacy and ethical guidelines\nfor responsible AI-human symbiosis, we also explore potential biases and\ninequalities in AI-human symbiosis and propose strategies to mitigate these\nchallenges. Our research aims to provide a comprehensive understanding of the\nSAISSE concept and its potential to effectively support and enhance individual\nhuman users through symbiotic AI systems. This position article aims at\ndiscussing poteintial AI-human interaction related topics within the scientific\ncommunity, rather than providing experimental or theoretical results.",
                "Explainability Techniques for Chemical Language Models\nExplainability techniques are crucial in gaining insights into the reasons\nbehind the predictions of deep learning models, which have not yet been applied\nto chemical language models. We propose an explainable AI technique that\nattributes the importance of individual atoms towards the predictions made by\nthese models. Our method backpropagates the relevance information towards the\nchemical input string and visualizes the importance of individual atoms. We\nfocus on self-attention Transformers operating on molecular string\nrepresentations and leverage a pretrained encoder for finetuning. We showcase\nthe method by predicting and visualizing solubility in water and organic\nsolvents. We achieve competitive model performance while obtaining\ninterpretable predictions, which we use to inspect the pretrained model.",
                "Mitigating Exploitation Bias in Learning to Rank with an\n  Uncertainty-aware Empirical Bayes Approach\nRanking is at the core of many artificial intelligence (AI) applications,\nincluding search engines, recommender systems, etc. Modern ranking systems are\noften constructed with learning-to-rank (LTR) models built from user behavior\nsignals. While previous studies have demonstrated the effectiveness of using\nuser behavior signals (e.g., clicks) as both features and labels of LTR\nalgorithms, we argue that existing LTR algorithms that indiscriminately treat\nbehavior and non-behavior signals in input features could lead to suboptimal\nperformance in practice. Particularly because user behavior signals often have\nstrong correlations with the ranking objective and can only be collected on\nitems that have already been shown to users, directly using behavior signals in\nLTR could create an exploitation bias that hurts the system performance in the\nlong run.\n  To address the exploitation bias, we propose EBRank, an empirical Bayes-based\nuncertainty-aware ranking algorithm. Specifically, to overcome exploitation\nbias brought by behavior features in ranking models, EBRank uses a sole\nnon-behavior feature based prior model to get a prior estimation of relevance.\nIn the dynamic training and serving of ranking systems, EBRank uses the\nobserved user behaviors to update posterior relevance estimation instead of\nconcatenating behaviors as features in ranking models. Besides, EBRank\nadditionally applies an uncertainty-aware exploration strategy to explore\nactively, collect user behaviors for empirical Bayesian modeling and improve\nranking performance. Experiments on three public datasets show that EBRank is\neffective, practical and significantly outperforms state-of-the-art ranking\nalgorithms.",
                "Multiview Identifiers Enhanced Generative Retrieval\nInstead of simply matching a query to pre-existing passages, generative\nretrieval generates identifier strings of passages as the retrieval target. At\na cost, the identifier must be distinctive enough to represent a passage.\nCurrent approaches use either a numeric ID or a text piece (such as a title or\nsubstrings) as the identifier. However, these identifiers cannot cover a\npassage's content well. As such, we are motivated to propose a new type of\nidentifier, synthetic identifiers, that are generated based on the content of a\npassage and could integrate contextualized information that text pieces lack.\nFurthermore, we simultaneously consider multiview identifiers, including\nsynthetic identifiers, titles, and substrings. These views of identifiers\ncomplement each other and facilitate the holistic ranking of passages from\nmultiple perspectives. We conduct a series of experiments on three public\ndatasets, and the results indicate that our proposed approach performs the best\nin generative retrieval, demonstrating its effectiveness and robustness.",
                "GenQ: Automated Question Generation to Support Caregivers While Reading\n  Stories with Children\nWhen caregivers ask open--ended questions to motivate dialogue with children,\nit facilitates the child's reading comprehension skills.Although there is scope\nfor use of technological tools, referred here as \"intelligent tutoring\nsystems\", to scaffold this process, it is currently unclear whether existing\nintelligent systems that generate human--language like questions is beneficial.\nAdditionally, training data used in the development of these automated question\ngeneration systems is typically sourced without attention to demographics, but\npeople with different cultural backgrounds may ask different questions. As a\npart of a broader project to design an intelligent reading support app for\nLatinx children, we crowdsourced questions from Latinx caregivers and\nnoncaregivers as well as caregivers and noncaregivers from other demographics.\nWe examine variations in question--asking within this dataset mediated by\nindividual, cultural, and contextual factors. We then design a system that\nautomatically extracts templates from this data to generate open--ended\nquestions that are representative of those asked by Latinx caregivers."
            ],
            "interesting paper": 1
        },
        {
            "papers": [
                "How Do UX Practitioners Communicate AI as a Design Material? Artifacts,\n  Conceptions, and Propositions\nUX practitioners (UXPs) face novel challenges when working with and\ncommunicating artificial intelligence (AI) as a design material. We explore how\nUXPs communicate AI concepts when given hands-on experience training and\nexperimenting with AI models. To do so, we conducted a task-based design study\nwith 27 UXPs in which they prototyped and created a design presentation for a\nAI-enabled interface while having access to a simple AI model training tool.\nThrough analyzing UXPs' design presentations and post-activity interviews, we\nfound that although UXPs struggled to clearly communicate some AI concepts,\ntinkering with AI broadened common ground when communicating with technical\nstakeholders. UXPs also identified key risks and benefits of AI in their\ndesigns, and proposed concrete next steps for both UX and AI work. We conclude\nwith a sensitizing concept and recommendations for design and AI tools to\nenhance multi-stakeholder communication and collaboration when crafting\nhuman-centered AI experiences.",
                "Towards Explainable Conversational Recommender Systems\nExplanations in conventional recommender systems have demonstrated benefits\nin helping the user understand the rationality of the recommendations and\nimproving the system's efficiency, transparency, and trustworthiness. In the\nconversational environment, multiple contextualized explanations need to be\ngenerated, which poses further challenges for explanations. To better measure\nexplainability in conversational recommender systems (CRS), we propose ten\nevaluation perspectives based on concepts from conventional recommender systems\ntogether with the characteristics of CRS. We assess five existing CRS benchmark\ndatasets using these metrics and observe the necessity of improving the\nexplanation quality of CRS. To achieve this, we conduct manual and automatic\napproaches to extend these dialogues and construct a new CRS dataset, namely\nExplainable Recommendation Dialogues (E-ReDial). It includes 756 dialogues with\nover 2,000 high-quality rewritten explanations. We compare two baseline\napproaches to perform explanation generation based on E-ReDial. Experimental\nresults suggest that models trained on E-ReDial can significantly improve\nexplainability while introducing knowledge into the models can further improve\nthe performance. GPT-3 in the in-context learning setting can generate more\nrealistic and diverse movie descriptions. In contrast, T5 training on E-ReDial\ncan better generate clear reasons for recommendations based on user\npreferences. E-ReDial is available at https://github.com/Superbooming/E-ReDial.",
                "Towards Cognitive Bots: Architectural Research Challenges\nSoftware bots operating in multiple virtual digital platforms must understand\nthe platforms' affordances and behave like human users. Platform affordances or\nfeatures differ from one application platform to another or through a life\ncycle, requiring such bots to be adaptable. Moreover, bots in such platforms\ncould cooperate with humans or other software agents for work or to learn\nspecific behavior patterns. However, present-day bots, particularly chatbots,\nother than language processing and prediction, are far from reaching a human\nuser's behavior level within complex business information systems. They lack\nthe cognitive capabilities to sense and act in such virtual environments,\nrendering their development a challenge to artificial general intelligence\nresearch. In this study, we problematize and investigate assumptions in\nconceptualizing software bot architecture by directing attention to significant\narchitectural research challenges in developing cognitive bots endowed with\ncomplex behavior for operation on information systems. As an outlook, we\npropose alternate architectural assumptions to consider in future bot design\nand bot development frameworks.",
                "BiomedGPT: A Generalist Vision-Language Foundation Model for Diverse\n  Biomedical Tasks\nTraditional biomedical artificial intelligence (AI) models, designed for\nspecific tasks or modalities, often exhibit limited flexibility in real-world\ndeployment and struggle to utilize holistic information. Generalist AI holds\nthe potential to address these limitations due to its versatility in\ninterpreting different data types and generating tailored outputs for diverse\nneeds. However, existing biomedical generalist AI solutions are typically\nheavyweight and closed source to researchers, practitioners, and patients.\nHere, we propose BiomedGPT, the first open-source and lightweight\nvision-language foundation model, designed as a generalist capable of\nperforming various biomedical tasks. BiomedGPT achieved state-of-the-art\nresults in 16 out of 25 experiments while maintaining a computing-friendly\nmodel scale. We also conducted human evaluations to assess the capabilities of\nBiomedGPT in radiology visual question answering, report generation, and\nsummarization. BiomedGPT exhibits robust prediction ability with a low error\nrate of 3.8% in question answering, satisfactory performance with an error rate\nof 8.3% in writing complex radiology reports, and competitive summarization\nability with a nearly equivalent preference score to human experts. Our method\ndemonstrates that effective training with diverse data can lead to more\npractical biomedical AI for improving diagnosis and workflow efficiency.",
                "Training Socially Aligned Language Models on Simulated Social\n  Interactions\nSocial alignment in AI systems aims to ensure that these models behave\naccording to established societal values. However, unlike humans, who derive\nconsensus on value judgments through social interaction, current language\nmodels (LMs) are trained to rigidly replicate their training corpus in\nisolation, leading to subpar generalization in unfamiliar scenarios and\nvulnerability to adversarial attacks. This work presents a novel training\nparadigm that permits LMs to learn from simulated social interactions. In\ncomparison to existing methodologies, our approach is considerably more\nscalable and efficient, demonstrating superior performance in alignment\nbenchmarks and human evaluations. This paradigm shift in the training of LMs\nbrings us a step closer to developing AI systems that can robustly and\naccurately reflect societal norms and values.",
                "Justification vs. Transparency: Why and How Visual Explanations in a\n  Scientific Literature Recommender System\nSignificant attention has been paid to enhancing recommender systems (RS)\nwith explanation facilities to help users make informed decisions and increase\ntrust in and satisfaction with the RS. Justification and transparency represent\ntwo crucial goals in explainable recommendation. Different from transparency,\nwhich faithfully exposes the reasoning behind the recommendation mechanism,\njustification conveys a conceptual model that may differ from that of the\nunderlying algorithm. An explanation is an answer to a question. In explainable\nrecommendation, a user would want to ask questions (referred to as\nintelligibility types) to understand results given by the RS. In this paper, we\nidentify relationships between Why and How explanation intelligibility types\nand the explanation goals of justification and transparency. We followed the\nHuman-Centered Design (HCD) approach and leveraged the What-Why-How\nvisualization framework to systematically design and implement Why and How\nvisual explanations in the transparent Recommendation and Interest Modeling\nApplication (RIMA). Furthermore, we conducted a qualitative user study (N=12)\nto investigate the potential effects of providing Why and How explanations\ntogether in an explainable RS on the users' perceptions regarding transparency,\ntrust, and satisfaction. Our study showed qualitative evidence confirming that\nthe choice of the explanation intelligibility types depends on the explanation\ngoal and user type."
            ],
            "interesting paper": 2
        },
        {
            "papers": [
                "AI Coach Assist: An Automated Approach for Call Recommendation in\n  Contact Centers for Agent Coaching\nIn recent years, the utilization of Artificial Intelligence (AI) in the\ncontact center industry is on the rise. One area where AI can have a\nsignificant impact is in the coaching of contact center agents. By analyzing\ncall transcripts using Natural Language Processing (NLP) techniques, it would\nbe possible to quickly determine which calls are most relevant for coaching\npurposes. In this paper, we present AI Coach Assist, which leverages the\npre-trained transformer-based language models to determine whether a given call\nis coachable or not based on the quality assurance (QA) questions asked by the\ncontact center managers or supervisors. The system was trained and evaluated on\na large dataset collected from real-world contact centers and provides an\neffective way to recommend calls to the contact center managers that are more\nlikely to contain coachable moments. Our experimental findings demonstrate the\npotential of AI Coach Assist to improve the coaching process, resulting in\nenhancing the performance of contact center agents.",
                "Data Minimization at Inference Time\nIn domains with high stakes such as law, recruitment, and healthcare,\nlearning models frequently rely on sensitive user data for inference,\nnecessitating the complete set of features. This not only poses significant\nprivacy risks for individuals but also demands substantial human effort from\norganizations to verify information accuracy. This paper asks whether it is\nnecessary to use \\emph{all} input features for accurate predictions at\ninference time. The paper demonstrates that, in a personalized setting,\nindividuals may only need to disclose a small subset of their features without\ncompromising decision-making accuracy. The paper also provides an efficient\nsequential algorithm to determine the appropriate attributes for each\nindividual to provide. Evaluations across various learning tasks show that\nindividuals can potentially report as little as 10\\% of their information while\nmaintaining the same accuracy level as a model that employs the full set of\nuser information.",
                "Counterfactual Formulation of Patient-Specific Root Causes of Disease\nRoot causes of disease intuitively correspond to root vertices that increase\nthe likelihood of a diagnosis. This description of a root cause nevertheless\nlacks the rigorous mathematical formulation needed for the development of\ncomputer algorithms designed to automatically detect root causes from data.\nPrior work defined patient-specific root causes of disease using an\ninterventionalist account that only climbs to the second rung of Pearl's Ladder\nof Causation. In this theoretical piece, we climb to the third rung by\nproposing a counterfactual definition matching clinical intuition based on\nfixed factual data alone. We then show how to assign a root causal contribution\nscore to each variable using Shapley values from explainable artificial\nintelligence. The proposed counterfactual formulation of patient-specific root\ncauses of disease accounts for noisy labels, adapts to disease prevalence and\nadmits fast computation without the need for counterfactual simulation.",
                "Synthesizing a Progression of Subtasks for Block-Based Visual\n  Programming Tasks\nBlock-based visual programming environments play an increasingly important\nrole in introducing computing concepts to K-12 students. In recent years, they\nhave also gained popularity in neuro-symbolic AI, serving as a benchmark to\nevaluate general problem-solving and logical reasoning skills. The open-ended\nand conceptual nature of these visual programming tasks make them challenging,\nboth for state-of-the-art AI agents as well as for novice programmers. A\nnatural approach to providing assistance for problem-solving is breaking down a\ncomplex task into a progression of simpler subtasks; however, this is not\ntrivial given that the solution codes are typically nested and have non-linear\nexecution behavior. In this paper, we formalize the problem of synthesizing\nsuch a progression for a given reference block-based visual programming task.\nWe propose a novel synthesis algorithm that generates a progression of subtasks\nthat are high-quality, well-spaced in terms of their complexity, and solving\nthis progression leads to solving the reference task. We show the utility of\nour synthesis algorithm in improving the efficacy of AI agents (in this case,\nneural program synthesizers) for solving tasks in the Karel programming\nenvironment. Then, we conduct a user study to demonstrate that our synthesized\nprogression of subtasks can assist a novice programmer in solving tasks in the\nHour of Code: Maze Challenge by Code-dot-org.",
                "Modeling Dynamic Environments with Scene Graph Memory\nEmbodied AI agents that search for objects in large environments such as\nhouseholds often need to make efficient decisions by predicting object\nlocations based on partial information. We pose this as a new type of link\nprediction problem: link prediction on partially observable dynamic graphs. Our\ngraph is a representation of a scene in which rooms and objects are nodes, and\ntheir relationships are encoded in the edges; only parts of the changing graph\nare known to the agent at each timestep. This partial observability poses a\nchallenge to existing link prediction approaches, which we address. We propose\na novel state representation -- Scene Graph Memory (SGM) -- with captures the\nagent's accumulated set of observations, as well as a neural net architecture\ncalled a Node Edge Predictor (NEP) that extracts information from the SGM to\nsearch efficiently. We evaluate our method in the Dynamic House Simulator, a\nnew benchmark that creates diverse dynamic graphs following the semantic\npatterns typically seen at homes, and show that NEP can be trained to predict\nthe locations of objects in a variety of environments with diverse object\nmovement dynamics, outperforming baselines both in terms of new scene\nadaptability and overall accuracy. The codebase and more can be found at\nhttps://www.scenegraphmemory.com.",
                "The Curse of Recursion: Training on Generated Data Makes Models Forget\nStable Diffusion revolutionised image creation from descriptive text. GPT-2,\nGPT-3(.5) and GPT-4 demonstrated astonishing performance across a variety of\nlanguage tasks. ChatGPT introduced such language models to the general public.\nIt is now clear that large language models (LLMs) are here to stay, and will\nbring about drastic change in the whole ecosystem of online text and images. In\nthis paper we consider what the future might hold. What will happen to GPT-{n}\nonce LLMs contribute much of the language found online? We find that use of\nmodel-generated content in training causes irreversible defects in the\nresulting models, where tails of the original content distribution disappear.\nWe refer to this effect as Model Collapse and show that it can occur in\nVariational Autoencoders, Gaussian Mixture Models and LLMs. We build\ntheoretical intuition behind the phenomenon and portray its ubiquity amongst\nall learned generative models. We demonstrate that it has to be taken seriously\nif we are to sustain the benefits of training from large-scale data scraped\nfrom the web. Indeed, the value of data collected about genuine human\ninteractions with systems will be increasingly valuable in the presence of\ncontent generated by LLMs in data crawled from the Internet."
            ],
            "interesting paper": 2
        },
        {
            "papers": [
                "Ask an Expert: Leveraging Language Models to Improve Strategic Reasoning\n  in Goal-Oriented Dialogue Models\nExisting dialogue models may encounter scenarios which are not\nwell-represented in the training data, and as a result generate responses that\nare unnatural, inappropriate, or unhelpful. We propose the \"Ask an Expert\"\nframework in which the model is trained with access to an \"expert\" which it can\nconsult at each turn. Advice is solicited via a structured dialogue with the\nexpert, and the model is optimized to selectively utilize (or ignore) it given\nthe context and dialogue history. In this work the expert takes the form of an\nLLM. We evaluate this framework in a mental health support domain, where the\nstructure of the expert conversation is outlined by pre-specified prompts which\nreflect a reasoning strategy taught to practitioners in the field. Blenderbot\nmodels utilizing \"Ask an Expert\" show quality improvements across all expert\nsizes, including those with fewer parameters than the dialogue model itself.\nOur best model provides a $\\sim 10\\%$ improvement over baselines, approaching\nhuman-level scores on \"engingingness\" and \"helpfulness\" metrics.",
                "Taming AI Bots: Controllability of Neural States in Large Language\n  Models\nWe tackle the question of whether an agent can, by suitable choice of\nprompts, control an AI bot to any state. To that end, we first introduce a\nformal definition of ``meaning'' that is amenable to analysis. Then, we\ncharacterize ``meaningful data'' on which large language models (LLMs) are\nostensibly trained, and ``well-trained LLMs'' through conditions that are\nlargely met by today's LLMs. While a well-trained LLM constructs an embedding\nspace of meanings that is Euclidean, meanings themselves do not form a vector\n(linear) subspace, but rather a quotient space within. We then characterize the\nsubset of meanings that can be reached by the state of the LLMs for some input\nprompt, and show that a well-trained bot can reach any meaning albeit with\nsmall probability. We then introduce a stronger notion of controllability as\n{\\em almost certain reachability}, and show that, when restricted to the space\nof meanings, an AI bot is controllable. We do so after introducing a functional\ncharacterization of attentive AI bots, and finally derive necessary and\nsufficient conditions for controllability. The fact that AI bots are\ncontrollable means that an adversary could steer them towards any state.\nHowever, the sampling process can be designed to counteract adverse actions and\navoid reaching undesirable regions of state space before their boundary is\ncrossed.",
                "Re-imagining health and well-being in low resource African settings\n  using an augmented AI system and a 3D digital twin\nThis paper discusses and explores the potential and relevance of recent\ndevelopments in artificial intelligence (AI) and digital twins for health and\nwell-being in low-resource African countries. We use the case of public health\nemergency response to disease outbreaks and epidemic control. There is\npotential to take advantage of the increasing availability of data and\ndigitization to develop advanced AI methods for analysis and prediction. Using\nan AI systems perspective, we review emerging trends in AI systems and digital\ntwins and propose an initial augmented AI system architecture to illustrate how\nan AI system can work with a 3D digital twin to address public health goals. We\nhighlight scientific knowledge discovery, continual learning, pragmatic\ninteroperability, and interactive explanation and decision-making as essential\nresearch challenges for AI systems and digital twins.",
                "Sequential Condition Evolved Interaction Knowledge Graph for Traditional\n  Chinese Medicine Recommendation\nTraditional Chinese Medicine (TCM) has a rich history of utilizing natural\nherbs to treat a diversity of illnesses. In practice, TCM diagnosis and\ntreatment are highly personalized and organically holistic, requiring\ncomprehensive consideration of the patient's state and symptoms over time.\nHowever, existing TCM recommendation approaches overlook the changes in patient\nstatus and only explore potential patterns between symptoms and prescriptions.\nIn this paper, we propose a novel Sequential Condition Evolved Interaction\nKnowledge Graph (SCEIKG), a framework that treats the model as a sequential\nprescription-making problem by considering the dynamics of the patient's\ncondition across multiple visits. In addition, we incorporate an interaction\nknowledge graph to enhance the accuracy of recommendations by considering the\ninteractions between different herbs and the patient's condition. Experimental\nresults on a real-world dataset demonstrate that our approach outperforms\nexisting TCM recommendation methods, achieving state-of-the-art performance.",
                "ContrastNER: Contrastive-based Prompt Tuning for Few-shot NER\nPrompt-based language models have produced encouraging results in numerous\napplications, including Named Entity Recognition (NER) tasks. NER aims to\nidentify entities in a sentence and provide their types. However, the strong\nperformance of most available NER approaches is heavily dependent on the design\nof discrete prompts and a verbalizer to map the model-predicted outputs to\nentity categories, which are complicated undertakings. To address these\nchallenges, we present ContrastNER, a prompt-based NER framework that employs\nboth discrete and continuous tokens in prompts and uses a contrastive learning\napproach to learn the continuous prompts and forecast entity types. The\nexperimental results demonstrate that ContrastNER obtains competitive\nperformance to the state-of-the-art NER methods in high-resource settings and\noutperforms the state-of-the-art models in low-resource circumstances without\nrequiring extensive manual prompt engineering and verbalizer design.",
                "Key-Value Transformer\nTransformers have emerged as the prevailing standard solution for various AI\ntasks, including computer vision and natural language processing. The widely\nadopted Query, Key, and Value formulation (QKV) has played a significant role\nin this. Nevertheless, no research has examined the essentiality of these three\ncomponents for transformer performance. Therefore, we conducted an evaluation\nof the key-value formulation (KV), which generates symmetric attention maps,\nalong with an asymmetric version that incorporates a 2D positional encoding\ninto the attention matrix. Remarkably, this transformer requires fewer\nparameters and computation than the original one. Through experiments\nencompassing three task types -- synthetics (such as reversing or sorting a\nlist), vision (mnist or cifar classification), and NLP (character generation\nand translation) -- we discovered that the KV transformer occasionally\noutperforms the QKV transformer. However, it also exhibits instances of\nunderperformance compared to QKV, making it challenging to draw a definitive\nconclusion. Nonetheless, we consider the reported results to be encouraging and\nanticipate that they may pave the way for more efficient transformers in the\nfuture."
            ],
            "interesting paper": 1
        },
        {
            "papers": [
                "Partially Personalized Federated Learning: Breaking the Curse of Data\n  Heterogeneity\nWe present a partially personalized formulation of Federated Learning (FL)\nthat strikes a balance between the flexibility of personalization and\ncooperativeness of global training. In our framework, we split the variables\ninto global parameters, which are shared across all clients, and individual\nlocal parameters, which are kept private. We prove that under the right split\nof parameters, it is possible to find global parameters that allow each client\nto fit their data perfectly, and refer to the obtained problem as\noverpersonalized. For instance, the shared global parameters can be used to\nlearn good data representations, whereas the personalized layers are fine-tuned\nfor a specific client. Moreover, we present a simple algorithm for the\npartially personalized formulation that offers significant benefits to all\nclients. In particular, it breaks the curse of data heterogeneity in several\nsettings, such as training with local steps, asynchronous training, and\nByzantine-robust training.",
                "An Annotated Dataset for Explainable Interpersonal Risk Factors of\n  Mental Disturbance in Social Media Posts\nWith a surge in identifying suicidal risk and its severity in social media\nposts, we argue that a more consequential and explainable research is required\nfor optimal impact on clinical psychology practice and personalized mental\nhealthcare. The success of computational intelligence techniques for inferring\nmental illness from social media resources, points to natural language\nprocessing as a lens for determining Interpersonal Risk Factors (IRF) in human\nwritings. Motivated with limited availability of datasets for social NLP\nresearch community, we construct and release a new annotated dataset with\nhuman-labelled explanations and classification of IRF affecting mental\ndisturbance on social media: (i) Thwarted Belongingness (TBe), and (ii)\nPerceived Burdensomeness (PBu). We establish baseline models on our dataset\nfacilitating future research directions to develop real-time personalized AI\nmodels by detecting patterns of TBe and PBu in emotional spectrum of user's\nhistorical social media profile.",
                "Contextual Bandits with Budgeted Information Reveal\nContextual bandit algorithms are commonly used in digital health to recommend\npersonalized treatments. However, to ensure the effectiveness of the\ntreatments, patients are often requested to take actions that have no immediate\nbenefit to them, which we refer to as pro-treatment actions. In practice,\nclinicians have a limited budget to encourage patients to take these actions\nand collect additional information. We introduce a novel optimization and\nlearning algorithm to address this problem. This algorithm effectively combines\nthe strengths of two algorithmic approaches in a seamless manner, including 1)\nan online primal-dual algorithm for deciding the optimal timing to reach out to\npatients, and 2) a contextual bandit learning algorithm to deliver personalized\ntreatment to the patient. We prove that this algorithm admits a sub-linear\nregret bound. We illustrate the usefulness of this algorithm on both synthetic\nand real-world data.",
                "Perceived Trustworthiness of Natural Language Generators\nNatural Language Generation tools, such as chatbots that can generate\nhuman-like conversational text, are becoming more common both for personal and\nprofessional use. However, there are concerns about their trustworthiness and\nethical implications. The paper addresses the problem of understanding how\ndifferent users (e.g., linguists, engineers) perceive and adopt these tools and\ntheir perception of machine-generated text quality. It also discusses the\nperceived advantages and limitations of Natural Language Generation tools, as\nwell as users' beliefs on governance strategies. The main findings of this\nstudy include the impact of users' field and level of expertise on the\nperceived trust and adoption of Natural Language Generation tools, the users'\nassessment of the accuracy, fluency, and potential biases of machine-generated\ntext in comparison to human-written text, and an analysis of the advantages and\nethical risks associated with these tools as identified by the participants.\nMoreover, this paper discusses the potential implications of these findings for\nenhancing the AI development process. The paper sheds light on how different\nuser characteristics shape their beliefs on the quality and overall\ntrustworthiness of machine-generated text. Furthermore, it examines the\nbenefits and risks of these tools from the perspectives of different users.",
                "Code Prompting: a Neural Symbolic Method for Complex Reasoning in Large\n  Language Models\nLarge language models (LLMs) have scaled up to unlock a wide range of complex\nreasoning tasks with the aid of various prompting methods. However, current\nprompting methods generate natural language intermediate steps to help\nreasoning, which can cause imperfect task reduction and confusion. To mitigate\nsuch limitations, we explore code prompting, a neural symbolic prompting method\nwith both zero-shot and few-shot versions which triggers code as intermediate\nsteps. We conduct experiments on 7 widely-used benchmarks involving symbolic\nreasoning and arithmetic reasoning. Code prompting generally outperforms\nchain-of-thought (CoT) prompting. To further understand the performance and\nlimitations of code prompting, we perform extensive ablation studies and error\nanalyses, and identify several exclusive advantages of using symbolic\npromptings compared to natural language. We also consider the ensemble of code\nprompting and CoT prompting to combine the strengths of both. Finally, we show\nthrough experiments how code annotations and their locations affect code\nprompting.",
                "Shapley Based Residual Decomposition for Instance Analysis\nIn this paper, we introduce the idea of decomposing the residuals of\nregression with respect to the data instances instead of features. This allows\nus to determine the effects of each individual instance on the model and each\nother, and in doing so makes for a model-agnostic method of identifying\ninstances of interest. In doing so, we can also determine the appropriateness\nof the model and data in the wider context of a given study. The paper focuses\non the possible applications that such a framework brings to the relatively\nunexplored field of instance analysis in the context of Explainable AI tasks."
            ],
            "interesting paper": 1
        },
        {
            "papers": [
                "Less Likely Brainstorming: Using Language Models to Generate Alternative\n  Hypotheses\nA human decision-maker benefits the most from an AI assistant that corrects\nfor their biases. For problems such as generating interpretation of a radiology\nreport given findings, a system predicting only highly likely outcomes may be\nless useful, where such outcomes are already obvious to the user. To alleviate\nbiases in human decision-making, it is worth considering a broad differential\ndiagnosis, going beyond the most likely options. We introduce a new task, \"less\nlikely brainstorming,\" that asks a model to generate outputs that humans think\nare relevant but less likely to happen. We explore the task in two settings: a\nbrain MRI interpretation generation setting and an everyday commonsense\nreasoning setting. We found that a baseline approach of training with less\nlikely hypotheses as targets generates outputs that humans evaluate as either\nlikely or irrelevant nearly half of the time; standard MLE training is not\neffective. To tackle this problem, we propose a controlled text generation\nmethod that uses a novel contrastive learning strategy to encourage models to\ndifferentiate between generating likely and less likely outputs according to\nhumans. We compare our method with several state-of-the-art controlled text\ngeneration models via automatic and human evaluations and show that our models'\ncapability of generating less likely outputs is improved.",
                "Conceptual Design Generation Using Large Language Models\nConcept generation is a creative step in the conceptual design phase, where\ndesigners often turn to brainstorming, mindmapping, or crowdsourcing design\nideas to complement their own knowledge of the domain. Recent advances in\nnatural language processing (NLP) and machine learning (ML) have led to the\nrise of Large Language Models (LLMs) capable of generating seemingly creative\noutputs from textual prompts. The success of these models has led to their\nintegration and application across a variety of domains, including art,\nentertainment, and other creative work. In this paper, we leverage LLMs to\ngenerate solutions for a set of 12 design problems and compare them to a\nbaseline of crowdsourced solutions. We evaluate the differences between\ngenerated and crowdsourced design solutions through multiple perspectives,\nincluding human expert evaluations and computational metrics. Expert\nevaluations indicate that the LLM-generated solutions have higher average\nfeasibility and usefulness while the crowdsourced solutions have more novelty.\nWe experiment with prompt engineering and find that leveraging few-shot\nlearning can lead to the generation of solutions that are more similar to the\ncrowdsourced solutions. These findings provide insight into the quality of\ndesign solutions generated with LLMs and begins to evaluate prompt engineering\ntechniques that could be leveraged by practitioners to generate higher-quality\ndesign solutions synergistically with LLMs.",
                "Probabilistic computation and uncertainty quantification with emerging\n  covariance\nBuilding robust, interpretable, and secure AI system requires quantifying and\nrepresenting uncertainty under a probabilistic perspective to mimic human\ncognitive abilities. However, probabilistic computation presents significant\nchallenges for most conventional artificial neural network, as they are\nessentially implemented in a deterministic manner. In this paper, we develop an\nefficient probabilistic computation framework by truncating the probabilistic\nrepresentation of neural activation up to its mean and covariance and construct\na moment neural network that encapsulates the nonlinear coupling between the\nmean and covariance of the underlying stochastic network. We reveal that when\nonly the mean but not the covariance is supervised during gradient-based\nlearning, the unsupervised covariance spontaneously emerges from its nonlinear\ncoupling with the mean and faithfully captures the uncertainty associated with\nmodel predictions. Our findings highlight the inherent simplicity of\nprobabilistic computation by seamlessly incorporating uncertainty into model\nprediction, paving the way for integrating it into large-scale AI systems.",
                "Intent-aligned AI systems deplete human agency: the need for agency\n  foundations research in AI safety\nThe rapid advancement of artificial intelligence (AI) systems suggests that\nartificial general intelligence (AGI) systems may soon arrive. Many researchers\nare concerned that AIs and AGIs will harm humans via intentional misuse\n(AI-misuse) or through accidents (AI-accidents). In respect of AI-accidents,\nthere is an increasing effort focused on developing algorithms and paradigms\nthat ensure AI systems are aligned to what humans intend, e.g. AI systems that\nyield actions or recommendations that humans might judge as consistent with\ntheir intentions and goals. Here we argue that alignment to human intent is\ninsufficient for safe AI systems and that preservation of long-term agency of\nhumans may be a more robust standard, and one that needs to be separated\nexplicitly and a priori during optimization. We argue that AI systems can\nreshape human intention and discuss the lack of biological and psychological\nmechanisms that protect humans from loss of agency. We provide the first formal\ndefinition of agency-preserving AI-human interactions which focuses on\nforward-looking agency evaluations and argue that AI systems - not humans -\nmust be increasingly tasked with making these evaluations. We show how agency\nloss can occur in simple environments containing embedded agents that use\ntemporal-difference learning to make action recommendations. Finally, we\npropose a new area of research called \"agency foundations\" and pose four\ninitial topics designed to improve our understanding of agency in AI-human\ninteractions: benevolent game theory, algorithmic foundations of human rights,\nmechanistic interpretability of agency representation in neural-networks and\nreinforcement learning from internal states.",
                "Unsupervised Melody-to-Lyric Generation\nAutomatic melody-to-lyric generation is a task in which song lyrics are\ngenerated to go with a given melody. It is of significant practical interest\nand more challenging than unconstrained lyric generation as the music imposes\nadditional constraints onto the lyrics. The training data is limited as most\nsongs are copyrighted, resulting in models that underfit the complicated\ncross-modal relationship between melody and lyrics. In this work, we propose a\nmethod for generating high-quality lyrics without training on any aligned\nmelody-lyric data. Specifically, we design a hierarchical lyric generation\nframework that first generates a song outline and second the complete lyrics.\nThe framework enables disentanglement of training (based purely on text) from\ninference (melody-guided text generation) to circumvent the shortage of\nparallel data.\n  We leverage the segmentation and rhythm alignment between melody and lyrics\nto compile the given melody into decoding constraints as guidance during\ninference. The two-step hierarchical design also enables content control via\nthe lyric outline, a much-desired feature for democratizing collaborative song\ncreation. Experimental results show that our model can generate high-quality\nlyrics that are more on-topic, singable, intelligible, and coherent than strong\nbaselines, for example SongMASS, a SOTA model trained on a parallel dataset,\nwith a 24% relative overall quality improvement based on human ratings.",
                "Towards Explainable and Language-Agnostic LLMs: Symbolic Reverse\n  Engineering of Language at Scale\nLarge language models (LLMs) have achieved a milestone that undenia-bly\nchanged many held beliefs in artificial intelligence (AI). However, there\nremains many limitations of these LLMs when it comes to true language\nunderstanding, limitations that are a byproduct of the under-lying architecture\nof deep neural networks. Moreover, and due to their subsymbolic nature,\nwhatever knowledge these models acquire about how language works will always be\nburied in billions of microfeatures (weights), none of which is meaningful on\nits own, making such models hopelessly unexplainable. To address these\nlimitations, we suggest com-bining the strength of symbolic representations\nwith what we believe to be the key to the success of LLMs, namely a successful\nbottom-up re-verse engineering of language at scale. As such we argue for a\nbottom-up reverse engineering of language in a symbolic setting. Hints on what\nthis project amounts to have been suggested by several authors, and we discuss\nin some detail here how this project could be accomplished."
            ],
            "interesting paper": 2
        },
        {
            "papers": [
                "Decision-Oriented Dialogue for Human-AI Collaboration\nWe describe a class of tasks called decision-oriented dialogues, in which AI\nassistants such as large language models (LMs) must collaborate with one or\nmore humans via natural language to help them make complex decisions. We\nformalize three domains in which users face everyday decisions: (1) choosing an\nassignment of reviewers to conference papers, (2) planning a multi-step\nitinerary in a city, and (3) negotiating travel plans for a group of friends.\nIn each of these settings, AI assistants and users have disparate abilities\nthat they must combine to arrive at the best decision: assistants can access\nand process large amounts of information, while users have preferences and\nconstraints external to the system. For each task, we build a dialogue\nenvironment where agents receive a reward based on the quality of the final\ndecision they reach. We evaluate LMs in self-play and in collaboration with\nhumans and find that they fall short compared to human assistants, achieving\nmuch lower rewards despite engaging in longer dialogues. We highlight a number\nof challenges models face in decision-oriented dialogues, ranging from\ngoal-directed behavior to reasoning and optimization, and release our\nenvironments as a testbed for future work.",
                "Speaking the Language of Your Listener: Audience-Aware Adaptation via\n  Plug-and-Play Theory of Mind\nDialogue participants may have varying levels of knowledge about the topic\nunder discussion. In such cases, it is essential for speakers to adapt their\nutterances by taking their audience into account. Yet, it is an open question\nhow such adaptation can be modelled in computational agents. In this paper, we\nmodel a visually grounded referential game between a knowledgeable speaker and\na listener with more limited visual and linguistic experience. Inspired by\npsycholinguistic theories, we endow our speaker with the ability to adapt its\nreferring expressions via a simulation module that monitors the effectiveness\nof planned utterances from the listener's perspective. We propose an adaptation\nmechanism building on plug-and-play approaches to controlled language\ngeneration, where utterance generation is steered on the fly by the simulator\nwithout finetuning the speaker's underlying language model. Our results and\nanalyses show that our approach is effective: the speaker's utterances become\ncloser to the listener's domain of expertise, which leads to higher\ncommunicative success.",
                "TransAct: Transformer-based Realtime User Action Model for\n  Recommendation at Pinterest\nSequential models that encode user activity for next action prediction have\nbecome a popular design choice for building web-scale personalized\nrecommendation systems. Traditional methods of sequential recommendation either\nutilize end-to-end learning on realtime user actions, or learn user\nrepresentations separately in an offline batch-generated manner. This paper (1)\npresents Pinterest's ranking architecture for Homefeed, our personalized\nrecommendation product and the largest engagement surface; (2) proposes\nTransAct, a sequential model that extracts users' short-term preferences from\ntheir realtime activities; (3) describes our hybrid approach to ranking, which\ncombines end-to-end sequential modeling via TransAct with batch-generated user\nembeddings. The hybrid approach allows us to combine the advantages of\nresponsiveness from learning directly on realtime user activity with the\ncost-effectiveness of batch user representations learned over a longer time\nperiod. We describe the results of ablation studies, the challenges we faced\nduring productionization, and the outcome of an online A/B experiment, which\nvalidates the effectiveness of our hybrid ranking model. We further demonstrate\nthe effectiveness of TransAct on other surfaces such as contextual\nrecommendations and search. Our model has been deployed to production in\nHomefeed, Related Pins, Notifications, and Search at Pinterest.",
                "AI Imagery and the Overton Window\nAI-based text-to-image generation has undergone a significant leap in the\nproduction of visually comprehensive and aesthetic imagery over the past year,\nto the point where differentiating between a man-made piece of art and an\nAI-generated image is becoming more difficult. Generative Models such as Stable\nDiffusion, Midjourney and others are expected to affect several major\nindustries in technological and ethical aspects. Striking the balance between\nraising human standard of life and work vs exploiting one group of people to\nenrich another is a complex and crucial part of the discussion. Due to the\nrapid growth of this technology, the way in which its models operate, and gray\narea legalities, visual and artistic domains - including the video game\nindustry, are at risk of being taken over from creators by AI infrastructure\nowners. This paper is a literature review examining the concerns facing both AI\ndevelopers and users today, including identity theft, data laundering and more.\nIt discusses legalization challenges and ethical concerns, and concludes with\nhow AI generative models can be tremendously useful in streamlining the process\nof visual creativity in both static and interactive media given proper\nregulation.\n  Keywords: AI text-to-image generation, Midjourney, Stable Diffusion, AI\nEthics, Game Design, Digital Art, Data Laundering",
                "Human Control: Definitions and Algorithms\nHow can humans stay in control of advanced artificial intelligence systems?\nOne proposal is corrigibility, which requires the agent to follow the\ninstructions of a human overseer, without inappropriately influencing them. In\nthis paper, we formally define a variant of corrigibility called shutdown\ninstructability, and show that it implies appropriate shutdown behavior,\nretention of human autonomy, and avoidance of user harm. We also analyse the\nrelated concepts of non-obstruction and shutdown alignment, three previously\nproposed algorithms for human control, and one new algorithm.",
                "Enrichment of the NLST and NSCLC-Radiomics computed tomography\n  collections with AI-derived annotations\nPublic imaging datasets are critical for the development and evaluation of\nautomated tools in cancer imaging. Unfortunately, many do not include\nannotations or image-derived features, complicating their downstream analysis.\nArtificial intelligence-based annotation tools have been shown to achieve\nacceptable performance and thus can be used to automatically annotate large\ndatasets. As part of the effort to enrich public data available within NCI\nImaging Data Commons (IDC), here we introduce AI-generated annotations for two\ncollections of computed tomography images of the chest, NSCLC-Radiomics, and\nthe National Lung Screening Trial. Using publicly available AI algorithms we\nderived volumetric annotations of thoracic organs at risk, their corresponding\nradiomics features, and slice-level annotations of anatomical landmarks and\nregions. The resulting annotations are publicly available within IDC, where the\nDICOM format is used to harmonize the data and achieve FAIR principles. The\nannotations are accompanied by cloud-enabled notebooks demonstrating their use.\nThis study reinforces the need for large, publicly accessible curated datasets\nand demonstrates how AI can be used to aid in cancer imaging."
            ],
            "interesting paper": 3
        },
        {
            "papers": [
                "AI Liability Insurance With an Example in AI-Powered E-diagnosis System\nArtificial Intelligence (AI) has received an increasing amount of attention\nin multiple areas. The uncertainties and risks in AI-powered systems have\ncreated reluctance in their wild adoption. As an economic solution to\ncompensate for potential damages, AI liability insurance is a promising market\nto enhance the integration of AI into daily life. In this work, we use an\nAI-powered E-diagnosis system as an example to study AI liability insurance. We\nprovide a quantitative risk assessment model with evidence-based numerical\nanalysis. We discuss the insurability criteria for AI technologies and suggest\nnecessary adjustments to accommodate the features of AI products. We show that\nAI liability insurance can act as a regulatory mechanism to incentivize\ncompliant behaviors and serve as a certificate of high-quality AI systems.\nFurthermore, we suggest premium adjustment to reflect the dynamic evolution of\nthe inherent uncertainty in AI. Moral hazard problems are discussed and\nsuggestions for AI liability insurance are provided.",
                "Self Contrastive Learning for Session-based Recommendation\nSession-based recommendation, which aims to predict the next item of users'\ninterest as per an existing sequence interaction of items, has attracted\ngrowing applications of Contrastive Learning (CL) with improved user and item\nrepresentations. However, these contrastive objectives: (1) serve a similar\nrole as the cross-entropy loss while ignoring the item representation space\noptimisation; and (2) commonly require complicated modelling, including complex\npositive/negative sample constructions and extra data augmentation. In this\nwork, we introduce Self-Contrastive Learning (SCL), which simplifies the\napplication of CL and enhances the performance of state-of-the-art CL-based\nrecommendation techniques. Specifically, SCL is formulated as an objective\nfunction that directly promotes a uniform distribution among item\nrepresentations and efficiently replaces all the existing contrastive objective\ncomponents of state-of-the-art models. Unlike previous works, SCL eliminates\nthe need for any positive/negative sample construction or data augmentation,\nleading to enhanced interpretability of the item representation space and\nfacilitating its extensibility to existing recommender systems. Through\nexperiments on three benchmark datasets, we demonstrate that SCL consistently\nimproves the performance of state-of-the-art models with statistical\nsignificance. Notably, our experiments show that SCL improves the performance\nof two best-performing models by 8.2% and 9.5% in P@10 (Precision) and 9.9% and\n11.2% in MRR@10 (Mean Reciprocal Rank) on average across different benchmarks.\nAdditionally, our analysis elucidates the improvement in terms of alignment and\nuniformity of representations, as well as the effectiveness of SCL with a low\ncomputational cost.",
                "Cook-Gen: Robust Generative Modeling of Cooking Actions from Recipes\nAs people become more aware of their food choices, food computation models\nhave become increasingly popular in assisting people in maintaining healthy\neating habits. For example, food recommendation systems analyze recipe\ninstructions to assess nutritional contents and provide recipe recommendations.\nThe recent and remarkable successes of generative AI methods, such as\nauto-regressive large language models, can lead to robust methods for a more\ncomprehensive understanding of recipes for healthy food recommendations beyond\nsurface-level nutrition content assessments. In this study, we explore the use\nof generative AI methods to extend current food computation models, primarily\ninvolving the analysis of nutrition and ingredients, to also incorporate\ncooking actions (e.g., add salt, fry the meat, boil the vegetables, etc.).\nCooking actions are notoriously hard to model using statistical learning\nmethods due to irregular data patterns - significantly varying natural language\ndescriptions for the same action (e.g., marinate the meat vs. marinate the meat\nand leave overnight) and infrequently occurring patterns (e.g., add salt occurs\nfar more frequently than marinating the meat). The prototypical approach to\nhandling irregular data patterns is to increase the volume of data that the\nmodel ingests by orders of magnitude. Unfortunately, in the cooking domain,\nthese problems are further compounded with larger data volumes presenting a\nunique challenge that is not easily handled by simply scaling up. In this work,\nwe propose novel aggregation-based generative AI methods, Cook-Gen, that\nreliably generate cooking actions from recipes, despite difficulties with\nirregular data patterns, while also outperforming Large Language Models and\nother strong baselines.",
                "ViCo: Plug-and-play Visual Condition for Personalized Text-to-image\n  Generation\nPersonalized text-to-image generation using diffusion models has recently\nemerged and garnered significant interest. This task learns a novel concept\n(e.g., a unique toy), illustrated in a handful of images, into a generative\nmodel that captures fine visual details and generates photorealistic images\nbased on textual embeddings. In this paper, we present ViCo, a novel\nlightweight plug-and-play method that seamlessly integrates visual condition\ninto personalized text-to-image generation. ViCo stands out for its unique\nfeature of not requiring any fine-tuning of the original diffusion model\nparameters, thereby facilitating more flexible and scalable model deployment.\nThis key advantage distinguishes ViCo from most existing models that\nnecessitate partial or full diffusion fine-tuning. ViCo incorporates an image\nattention module that conditions the diffusion process on patch-wise visual\nsemantics, and an attention-based object mask that comes at no extra cost from\nthe attention module. Despite only requiring light parameter training (~6%\ncompared to the diffusion U-Net), ViCo delivers performance that is on par\nwith, or even surpasses, all state-of-the-art models, both qualitatively and\nquantitatively. This underscores the efficacy of ViCo, making it a highly\npromising solution for personalized text-to-image generation without the need\nfor diffusion model fine-tuning. Code: https://github.com/haoosz/ViCo",
                "KL-Divergence Guided Temperature Sampling\nTemperature sampling is a conventional approach to diversify large language\nmodel predictions. As temperature increases, the prediction becomes diverse but\nalso vulnerable to hallucinations -- generating tokens that are sensible but\nnot factual. One common approach to mitigate hallucinations is to provide\nsource/grounding documents and the model is trained to produce predictions that\nbind to and are attributable to the provided source. It appears that there is a\ntrade-off between diversity and attribution. To mitigate any such trade-off, we\npropose to relax the constraint of having a fixed temperature over decoding\nsteps, and a mechanism to guide the dynamic temperature according to its\nrelevance to the source through KL-divergence. Our experiments justifies the\ntrade-off, and shows that our sampling algorithm outperforms the conventional\ntop-k and top-p algorithms in conversational question-answering and\nsummarization tasks.",
                "Modeling and Analyzing Scorer Preferences in Short-Answer Math Questions\nAutomated scoring of student responses to open-ended questions, including\nshort-answer questions, has great potential to scale to a large number of\nresponses. Recent approaches for automated scoring rely on supervised learning,\ni.e., training classifiers or fine-tuning language models on a small number of\nresponses with human-provided score labels. However, since scoring is a\nsubjective process, these human scores are noisy and can be highly variable,\ndepending on the scorer. In this paper, we investigate a collection of models\nthat account for the individual preferences and tendencies of each human scorer\nin the automated scoring task. We apply these models to a short-answer math\nresponse dataset where each response is scored (often differently) by multiple\ndifferent human scorers. We conduct quantitative experiments to show that our\nscorer models lead to improved automated scoring accuracy. We also conduct\nquantitative experiments and case studies to analyze the individual preferences\nand tendencies of scorers. We found that scorers can be grouped into several\nobvious clusters, with each cluster having distinct features, and analyzed them\nin detail."
            ],
            "interesting paper": 2
        }
    ],
    "Bailey Hernandez": [
        {
            "papers": [
                "Towards a Capability Assessment Model for the Comprehension and Adoption\n  of AI in Organisations\nThe comprehension and adoption of Artificial Intelligence (AI) are beset with\npractical and ethical problems. This article presents a 5-level AI Capability\nAssessment Model (AI-CAM) and a related AI Capabilities Matrix (AI-CM) to\nassist practitioners in AI comprehension and adoption. These practical tools\nwere developed with business executives, technologists, and other\norganisational stakeholders in mind. They are founded on a comprehensive\nconception of AI compared to those in other AI adoption models and are also\nopen-source artefacts. Thus, the AI-CAM and AI-CM present an accessible\nresource to help inform organisational decision-makers on the capability\nrequirements for (1) AI-based data analytics use cases based on machine\nlearning technologies; (2) Knowledge representation to engineer and represent\ndata, information and knowledge using semantic technologies; and (3) AI-based\nsolutions that seek to emulate human reasoning and decision-making. The AI-CAM\ncovers the core capability dimensions (business, data, technology,\norganisation, AI skills, risks, and ethical considerations) required at the\nfive capability maturity levels to achieve optimal use of AI in organisations.",
                "Learning-Based Automatic Synthesis of Software Code and Configuration\nIncreasing demands in software industry and scarcity of software engineers\nmotivates researchers and practitioners to automate the process of software\ngeneration and configuration. Large scale automatic software generation and\nconfiguration is a very complex and challenging task. In this proposal, we set\nout to investigate this problem by breaking down automatic software generation\nand configuration into two different tasks. In first task, we propose to\nsynthesize software automatically with input output specifications. This task\nis further broken down into two sub-tasks. The first sub-task is about\nsynthesizing programs with a genetic algorithm which is driven by a neural\nnetwork based fitness function trained with program traces and specifications.\nFor the second sub-task, we formulate program synthesis as a continuous\noptimization problem and synthesize programs with covariance matrix adaption\nevolutionary strategy (a state-of-the-art continuous optimization method).\nFinally, for the second task, we propose to synthesize configurations of large\nscale software from different input files (e.g. software manuals,\nconfigurations files, online blogs, etc.) using a sequence-to-sequence deep\nlearning mechanism.",
                "A Mini Review on the utilization of Reinforcement Learning with OPC UA\nReinforcement Learning (RL) is a powerful machine learning paradigm that has\nbeen applied in various fields such as robotics, natural language processing\nand game playing achieving state-of-the-art results. Targeted to solve\nsequential decision making problems, it is by design able to learn from\nexperience and therefore adapt to changing dynamic environments. These\ncapabilities make it a prime candidate for controlling and optimizing complex\nprocesses in industry. The key to fully exploiting this potential is the\nseamless integration of RL into existing industrial systems. The industrial\ncommunication standard Open Platform Communications UnifiedArchitecture (OPC\nUA) could bridge this gap. However, since RL and OPC UA are from different\nfields,there is a need for researchers to bridge the gap between the two\ntechnologies. This work serves to bridge this gap by providing a brief\ntechnical overview of both technologies and carrying out a semi-exhaustive\nliterature review to gain insights on how RL and OPC UA are applied in\ncombination. With this survey, three main research topics have been identified,\nfollowing the intersection of RL with OPC UA. The results of the literature\nreview show that RL is a promising technology for the control and optimization\nof industrial processes, but does not yet have the necessary standardized\ninterfaces to be deployed in real-world scenarios with reasonably low effort.",
                "Trends and Challenges Towards an Effective Data-Driven Decision Making\n  in UK SMEs: Case Studies and Lessons Learnt from the Analysis of 85 SMEs\nThe adoption of data science brings vast benefits to Small and Medium-sized\nEnterprises (SMEs) including business productivity, economic growth, innovation\nand jobs creation. Data Science can support SMEs to optimise production\nprocesses, anticipate customers' needs, predict machinery failures and deliver\nefficient smart services. Businesses can also harness the power of Artificial\nIntelligence (AI) and Big Data and the smart use of digital technologies to\nenhance productivity and performance, paving the way for innovation. However,\nintegrating data science decisions into an SME requires both skills and IT\ninvestments. In most cases, such expenses are beyond the means of SMEs due to\nlimited resources and restricted access to financing. This paper presents\ntrends and challenges towards an effective data-driven decision making for\norganisations based on a case study of 85 SMEs, mostly from the West Midlands\nregion of England. The work is supported as part of a 3 years ERDF (European\nRegional Development Funded project) in the areas of big data management,\nanalytics and business intelligence. We present two case studies that\ndemonstrates the potential of Digitisation, AI and Machine Learning and use\nthese as examples to unveil challenges and showcase the wealth of current\navailable opportunities for SMEs.",
                "Symbolic Regression via Control Variable Genetic Programming\nLearning symbolic expressions directly from experiment data is a vital step\nin AI-driven scientific discovery. Nevertheless, state-of-the-art approaches\nare limited to learning simple expressions. Regressing expressions involving\nmany independent variables still remain out of reach. Motivated by the control\nvariable experiments widely utilized in science, we propose Control Variable\nGenetic Programming (CVGP) for symbolic regression over many independent\nvariables. CVGP expedites symbolic expression discovery via customized\nexperiment design, rather than learning from a fixed dataset collected a\npriori. CVGP starts by fitting simple expressions involving a small set of\nindependent variables using genetic programming, under controlled experiments\nwhere other variables are held as constants. It then extends expressions\nlearned in previous generations by adding new independent variables, using new\ncontrol variable experiments in which these variables are allowed to vary.\nTheoretically, we show CVGP as an incremental building approach can yield an\nexponential reduction in the search space when learning a class of expressions.\nExperimentally, CVGP outperforms several baselines in learning symbolic\nexpressions involving multiple independent variables.",
                "Learning to Act through Evolution of Neural Diversity in Random Neural\n  Networks\nBiological nervous systems consist of networks of diverse, sophisticated\ninformation processors in the form of neurons of different classes. In most\nartificial neural networks (ANNs), neural computation is abstracted to an\nactivation function that is usually shared between all neurons within a layer\nor even the whole network; training of ANNs focuses on synaptic optimization.\nIn this paper, we propose the optimization of neuro-centric parameters to\nattain a set of diverse neurons that can perform complex computations.\nDemonstrating the promise of the approach, we show that evolving neural\nparameters alone allows agents to solve various reinforcement learning tasks\nwithout optimizing any synaptic weights. While not aiming to be an accurate\nbiological model, parameterizing neurons to a larger degree than the current\ncommon practice, allows us to ask questions about the computational abilities\nafforded by neural diversity in random neural networks. The presented results\nopen up interesting future research directions, such as combining evolved\nneural diversity with activity-dependent plasticity."
            ],
            "interesting paper": 3
        },
        {
            "papers": [
                "Reverse Engineering Self-Supervised Learning\nSelf-supervised learning (SSL) is a powerful tool in machine learning, but\nunderstanding the learned representations and their underlying mechanisms\nremains a challenge. This paper presents an in-depth empirical analysis of\nSSL-trained representations, encompassing diverse models, architectures, and\nhyperparameters. Our study reveals an intriguing aspect of the SSL training\nprocess: it inherently facilitates the clustering of samples with respect to\nsemantic labels, which is surprisingly driven by the SSL objective's\nregularization term. This clustering process not only enhances downstream\nclassification but also compresses the data information. Furthermore, we\nestablish that SSL-trained representations align more closely with semantic\nclasses rather than random classes. Remarkably, we show that learned\nrepresentations align with semantic classes across various hierarchical levels,\nand this alignment increases during training and when moving deeper into the\nnetwork. Our findings provide valuable insights into SSL's representation\nlearning mechanisms and their impact on performance across different sets of\nclasses.",
                "Classic machine learning methods\nIn this chapter, we present the main classic machine learning methods. A\nlarge part of the chapter is devoted to supervised learning techniques for\nclassification and regression, including nearest-neighbor methods, linear and\nlogistic regressions, support vector machines and tree-based algorithms. We\nalso describe the problem of overfitting as well as strategies to overcome it.\nWe finally provide a brief overview of unsupervised learning methods, namely\nfor clustering and dimensionality reduction.",
                "Rethinking the Evaluation Protocol of Domain Generalization\nDomain generalization aims to solve the challenge of Out-of-Distribution\n(OOD) generalization by leveraging common knowledge learned from multiple\ntraining domains to generalize to unseen test domains. To accurately evaluate\nthe OOD generalization ability, it is required that test data information is\nunavailable. However, the current domain generalization protocol may still have\npotential test data information leakage. This paper examines the risks of test\ndata information leakage from two aspects of the current evaluation protocol:\nsupervised pretraining on ImageNet and oracle model selection. We propose\nmodifications to the current protocol that we should employ self-supervised\npretraining or train from scratch instead of employing the current supervised\npretraining, and we should use multiple test domains. These would result in a\nmore precise evaluation of OOD generalization ability. We also rerun the\nalgorithms with the modified protocol and introduce new leaderboards to\nencourage future research in domain generalization with a fairer comparison.",
                "Differentiable Clustering with Perturbed Spanning Forests\nWe introduce a differentiable clustering method based on stochastic\nperturbations of minimum-weight spanning forests. This allows us to include\nclustering in end-to-end trainable pipelines, with efficient gradients. We show\nthat our method performs well even in difficult settings, such as data sets\nwith high noise and challenging geometries. We also formulate an ad hoc loss to\nefficiently learn from partial clustering data using this operation. We\ndemonstrate its performance on several data sets for supervised and\nsemi-supervised tasks.",
                "How to escape sharp minima with random perturbations\nModern machine learning applications have witnessed the remarkable success of\noptimization algorithms that are designed to find flat minima. Motivated by\nthis design choice, we undertake a formal study that (i) formulates the notion\nof flat minima, and (ii) studies the complexity of finding them. Specifically,\nwe adopt the trace of the Hessian of the cost function as a measure of\nflatness, and use it to formally define the notion of approximate flat minima.\nUnder this notion, we then analyze algorithms that find approximate flat minima\nefficiently. For general cost functions, we discuss a gradient-based algorithm\nthat finds an approximate flat local minimum efficiently. The main component of\nthe algorithm is to use gradients computed from randomly perturbed iterates to\nestimate a direction that leads to flatter minima. For the setting where the\ncost function is an empirical risk over training data, we present a faster\nalgorithm that is inspired by a recently proposed practical algorithm called\nsharpness-aware minimization, supporting its success in practice.",
                "Generalizable Low-Resource Activity Recognition with Diverse and\n  Discriminative Representation Learning\nHuman activity recognition (HAR) is a time series classification task that\nfocuses on identifying the motion patterns from human sensor readings. Adequate\ndata is essential but a major bottleneck for training a generalizable HAR\nmodel, which assists customization and optimization of online web applications.\nHowever, it is costly in time and economy to collect large-scale labeled data\nin reality, i.e., the low-resource challenge. Meanwhile, data collected from\ndifferent persons have distribution shifts due to different living habits, body\nshapes, age groups, etc. The low-resource and distribution shift challenges are\ndetrimental to HAR when applying the trained model to new unseen subjects. In\nthis paper, we propose a novel approach called Diverse and Discriminative\nrepresentation Learning (DDLearn) for generalizable low-resource HAR. DDLearn\nsimultaneously considers diversity and discrimination learning. With the\nconstructed self-supervised learning task, DDLearn enlarges the data diversity\nand explores the latent activity properties. Then, we propose a diversity\npreservation module to preserve the diversity of learned features by enlarging\nthe distribution divergence between the original and augmented domains.\nMeanwhile, DDLearn also enhances semantic discrimination by learning\ndiscriminative representations with supervised contrastive learning. Extensive\nexperiments on three public HAR datasets demonstrate that our method\nsignificantly outperforms state-of-art methods by an average accuracy\nimprovement of 9.5% under the low-resource distribution shift scenarios, while\nbeing a generic, explainable, and flexible framework. Code is available at:\nhttps://github.com/microsoft/robustlearn."
            ],
            "interesting paper": 1
        },
        {
            "papers": [
                "Attention Paper: How Generative AI Reshapes Digital Shadow Industry?\nThe rapid development of digital economy has led to the emergence of various\nblack and shadow internet industries, which pose potential risks that can be\nidentified and managed through digital risk management (DRM) that uses\ndifferent techniques such as machine learning and deep learning. The evolution\nof DRM architecture has been driven by changes in data forms. However, the\ndevelopment of AI-generated content (AIGC) technology, such as ChatGPT and\nStable Diffusion, has given black and shadow industries powerful tools to\npersonalize data and generate realistic images and conversations for fraudulent\nactivities. This poses a challenge for DRM systems to control risks from the\nsource of data generation and to respond quickly to the fast-changing risk\nenvironment. This paper aims to provide a technical analysis of the challenges\nand opportunities of AIGC from upstream, midstream, and downstream paths of\nblack/shadow industries and suggest future directions for improving existing\nrisk control systems. The paper will explore the new black and shadow\ntechniques triggered by generative AI technology and provide insights for\nbuilding the next-generation DRM system.",
                "On Computing Universal Plans for Partially Observable Multi-Agent Path\n  Finding\nMulti-agent routing problems have drawn significant attention nowadays due to\ntheir broad industrial applications in, e.g., warehouse robots, logistics\nautomation, and traffic control. Conventionally, they are modelled as classical\nplanning problems. In this paper, we argue that it is beneficial to formulate\nthem as universal planning problems. We therefore propose universal plans, also\nknown as policies, as the solution concepts, and implement a system called\nASP-MAUPF (Answer Set Programming for Multi-Agent Universal Plan Finding) for\ncomputing them. Given an arbitrary two-dimensional map and a profile of goals\nfor the agents, the system finds a feasible universal plan for each agent that\nensures no collision with others. We use the system to conduct some\nexperiments, and make some observations on the types of goal profiles and\nenvironments that will have feasible policies, and how they may depend on\nagents' sensors. We also demonstrate how users can customize action preferences\nto compute more efficient policies, even (near-)optimal ones.",
                "AI Techniques in the Microservices Life-Cycle: A Survey\nMicroservices is a popular architectural style for the development of\ndistributed software, with an emphasis on modularity, scalability, and\nflexibility. Indeed, in microservice systems, functionalities are provided by\nloosely coupled, small services, each focusing on a specific business\ncapability. Building a system according to the microservices architectural\nstyle brings a number of challenges, mainly related to how the different\nmicroservices are deployed and coordinated and how they interact. In this\npaper, we provide a survey about how techniques in the area of Artificial\nIntelligence have been used to tackle these challenges.",
                "Generating Synergistic Formulaic Alpha Collections via Reinforcement\n  Learning\nIn the field of quantitative trading, it is common practice to transform raw\nhistorical stock data into indicative signals for the market trend. Such\nsignals are called alpha factors. Alphas in formula forms are more\ninterpretable and thus favored by practitioners concerned with risk. In\npractice, a set of formulaic alphas is often used together for better modeling\nprecision, so we need to find synergistic formulaic alpha sets that work well\ntogether. However, most traditional alpha generators mine alphas one by one\nseparately, overlooking the fact that the alphas would be combined later. In\nthis paper, we propose a new alpha-mining framework that prioritizes mining a\nsynergistic set of alphas, i.e., it directly uses the performance of the\ndownstream combination model to optimize the alpha generator. Our framework\nalso leverages the strong exploratory capabilities of reinforcement\nlearning~(RL) to better explore the vast search space of formulaic alphas. The\ncontribution to the combination models' performance is assigned to be the\nreturn used in the RL process, driving the alpha generator to find better\nalphas that improve upon the current set. Experimental evaluations on\nreal-world stock market data demonstrate both the effectiveness and the\nefficiency of our framework for stock trend forecasting. The investment\nsimulation results show that our framework is able to achieve higher returns\ncompared to previous approaches.",
                "Alert of the Second Decision-maker: An Introduction to Human-AI Conflict\nThe collaboration between humans and artificial intelligence (AI) is a\nsignificant feature in this digital age. However, humans and AI may have\nobservation, interpretation, and action conflicts when working synchronously.\nThis phenomenon is often masked by faults and, unfortunately, overlooked. This\npaper systematically introduces the human-AI conflict concept, causes,\nmeasurement methods, and risk assessment. The results highlight that there is a\npotential second decision-maker besides the human, which is the AI; the\nhuman-AI conflict is a unique and emerging risk in digitalized process systems;\nand this is an interdisciplinary field that needs to be distinguished from\ntraditional fault and failure analysis; the conflict risk is significant and\ncannot be ignored.",
                "C-MCTS: Safe Planning with Monte Carlo Tree Search\nThe Constrained Markov Decision Process (CMDP) formulation allows to solve\nsafety-critical decision making tasks that are subject to constraints. While\nCMDPs have been extensively studied in the Reinforcement Learning literature,\nlittle attention has been given to sampling-based planning algorithms such as\nMCTS for solving them. Previous approaches perform conservatively with respect\nto costs as they avoid constraint violations by using Monte Carlo cost\nestimates that suffer from high variance. We propose Constrained MCTS (C-MCTS),\nwhich estimates cost using a safety critic that is trained with Temporal\nDifference learning in an offline phase prior to agent deployment. The critic\nlimits exploration by pruning unsafe trajectories within MCTS during\ndeployment. C-MCTS satisfies cost constraints but operates closer to the\nconstraint boundary, achieving higher rewards than previous work. As a nice\nbyproduct, the planner is more efficient w.r.t. planning steps. Most\nimportantly, under model mismatch between the planner and the real world,\nC-MCTS is less susceptible to cost violations than previous work."
            ],
            "interesting paper": 2
        },
        {
            "papers": [
                "Revisiting Structured Variational Autoencoders\nStructured variational autoencoders (SVAEs) combine probabilistic graphical\nmodel priors on latent variables, deep neural networks to link latent variables\nto observed data, and structure-exploiting algorithms for approximate posterior\ninference. These models are particularly appealing for sequential data, where\nthe prior can capture temporal dependencies. However, despite their conceptual\nelegance, SVAEs have proven difficult to implement, and more general approaches\nhave been favored in practice. Here, we revisit SVAEs using modern machine\nlearning tools and demonstrate their advantages over more general alternatives\nin terms of both accuracy and efficiency. First, we develop a modern\nimplementation for hardware acceleration, parallelization, and automatic\ndifferentiation of the message passing algorithms at the core of the SVAE.\nSecond, we show that by exploiting structure in the prior, the SVAE learns more\naccurate models and posterior distributions, which translate into improved\nperformance on prediction tasks. Third, we show how the SVAE can naturally\nhandle missing data, and we leverage this ability to develop a novel,\nself-supervised training approach. Altogether, these results show that the time\nis ripe to revisit structured variational autoencoders.",
                "Detecting Heart Disease from Multi-View Ultrasound Images via Supervised\n  Attention Multiple Instance Learning\nAortic stenosis (AS) is a degenerative valve condition that causes\nsubstantial morbidity and mortality. This condition is under-diagnosed and\nunder-treated. In clinical practice, AS is diagnosed with expert review of\ntransthoracic echocardiography, which produces dozens of ultrasound images of\nthe heart. Only some of these views show the aortic valve. To automate\nscreening for AS, deep networks must learn to mimic a human expert's ability to\nidentify views of the aortic valve then aggregate across these relevant images\nto produce a study-level diagnosis. We find previous approaches to AS detection\nyield insufficient accuracy due to relying on inflexible averages across\nimages. We further find that off-the-shelf attention-based multiple instance\nlearning (MIL) performs poorly. We contribute a new end-to-end MIL approach\nwith two key methodological innovations. First, a supervised attention\ntechnique guides the learned attention mechanism to favor relevant views.\nSecond, a novel self-supervised pretraining strategy applies contrastive\nlearning on the representation of the whole study instead of individual images\nas commonly done in prior literature. Experiments on an open-access dataset and\nan external validation set show that our approach yields higher accuracy while\nreducing model size.",
                "Scan and Snap: Understanding Training Dynamics and Token Composition in\n  1-layer Transformer\nTransformer architecture has shown impressive performance in multiple\nresearch domains and has become the backbone of many neural network models.\nHowever, there is limited understanding on how it works. In particular, with a\nsimple predictive loss, how the representation emerges from the gradient\n\\emph{training dynamics} remains a mystery. In this paper, for 1-layer\ntransformer with one self-attention layer plus one decoder layer, we analyze\nits SGD training dynamics for the task of next token prediction in a\nmathematically rigorous manner. We open the black box of the dynamic process of\nhow the self-attention layer combines input tokens, and reveal the nature of\nunderlying inductive bias. More specifically, with the assumption (a) no\npositional encoding, (b) long input sequence, and (c) the decoder layer learns\nfaster than the self-attention layer, we prove that self-attention acts as a\n\\emph{discriminative scanning algorithm}: starting from uniform attention, it\ngradually attends more to distinct key tokens for a specific next token to be\npredicted, and pays less attention to common key tokens that occur across\ndifferent next tokens. Among distinct tokens, it progressively drops attention\nweights, following the order of low to high co-occurrence between the key and\nthe query token in the training set. Interestingly, this procedure does not\nlead to winner-takes-all, but decelerates due to a \\emph{phase transition} that\nis controllable by the learning rates of the two layers, leaving (almost) fixed\ntoken combination. We verify this \\textbf{\\emph{scan and snap}} dynamics on\nsynthetic and real-world data (WikiText).",
                "Diversify Your Vision Datasets with Automatic Diffusion-Based\n  Augmentation\nMany fine-grained classification tasks, like rare animal identification, have\nlimited training data and consequently classifiers trained on these datasets\noften fail to generalize to variations in the domain like changes in weather or\nlocation. As such, we explore how natural language descriptions of the domains\nseen in training data can be used with large vision models trained on diverse\npretraining datasets to generate useful variations of the training data. We\nintroduce ALIA (Automated Language-guided Image Augmentation), a method which\nutilizes large vision and language models to automatically generate natural\nlanguage descriptions of a dataset's domains and augment the training data via\nlanguage-guided image editing. To maintain data integrity, a model trained on\nthe original dataset filters out minimal image edits and those which corrupt\nclass-relevant information. The resulting dataset is visually consistent with\nthe original training data and offers significantly enhanced diversity. We show\nthat ALIA is able to surpasses traditional data augmentation and text-to-image\ngenerated data on fine-grained classification tasks, including cases of domain\ngeneralization and contextual bias. Code is available at\nhttps://github.com/lisadunlap/ALIA.",
                "On convex decision regions in deep network representations\nCurrent work on human-machine alignment aims at understanding machine-learned\nlatent spaces and their correspondence to human representations.\nG{\\\"a}rdenfors' conceptual spaces is a prominent framework for understanding\nhuman representations. Convexity of object regions in conceptual spaces is\nargued to promote generalizability, few-shot learning, and interpersonal\nalignment. Based on these insights, we investigate the notion of convexity of\nconcept regions in machine-learned latent spaces. We develop a set of tools for\nmeasuring convexity in sampled data and evaluate emergent convexity in layered\nrepresentations of state-of-the-art deep networks. We show that convexity is\nrobust to basic re-parametrization and, hence, meaningful as a quality of\nmachine-learned latent spaces. We find that approximate convexity is pervasive\nin neural representations in multiple application domains, including models of\nimages, audio, human activity, text, and medical images. Generally, we observe\nthat fine-tuning increases the convexity of label regions. We find evidence\nthat pretraining convexity of class label regions predicts subsequent\nfine-tuning performance.",
                "Differentiable Random Partition Models\nPartitioning a set of elements into an unknown number of mutually exclusive\nsubsets is essential in many machine learning problems. However, assigning\nelements, such as samples in a dataset or neurons in a network layer, to an\nunknown and discrete number of subsets is inherently non-differentiable,\nprohibiting end-to-end gradient-based optimization of parameters. We overcome\nthis limitation by proposing a novel two-step method for inferring partitions,\nwhich allows its usage in variational inference tasks. This new approach\nenables reparameterized gradients with respect to the parameters of the new\nrandom partition model. Our method works by inferring the number of elements\nper subset and, second, by filling these subsets in a learned order. We\nhighlight the versatility of our general-purpose approach on three different\nchallenging experiments: variational clustering, inference of shared and\nindependent generative factors under weak supervision, and multitask learning."
            ],
            "interesting paper": 3
        },
        {
            "papers": [
                "Optimization for truss design using Bayesian optimization\nIn this work, geometry optimization of mechanical truss using computer-aided\nfinite element analysis is presented. The shape of the truss is a dominant\nfactor in determining the capacity of load it can bear. At a given parameter\nspace, our goal is to find the parameters of a hull that maximize the\nload-bearing capacity and also don't yield to the induced stress. We rely on\nfinite element analysis, which is a computationally costly design analysis tool\nfor design evaluation. For such expensive to-evaluate functions, we chose\nBayesian optimization as our optimization framework which has empirically\nproven sample efficient than other simulation-based optimization methods.\n  By utilizing Bayesian optimization algorithms, the truss design involves\niteratively evaluating a set of candidate truss designs and updating a\nprobabilistic model of the design space based on the results. The model is used\nto predict the performance of each candidate design, and the next candidate\ndesign is selected based on the prediction and an acquisition function that\nbalances exploration and exploitation of the design space. Our result can be\nused as a baseline for future study on AI-based optimization in expensive\nengineering domains especially in finite element Analysis.",
                "Frontier AI developers need an internal audit function\nThis article argues that frontier artificial intelligence (AI) developers\nneed an internal audit function. First, it describes the role of internal audit\nin corporate governance: internal audit evaluates the adequacy and\neffectiveness of a company's risk management, control, and governance\nprocesses. It is organizationally independent from senior management and\nreports directly to the board of directors, typically its audit committee. In\nthe IIA's Three Lines Model, internal audit serves as the third line and is\nresponsible for providing assurance to the board, while the Combined Assurance\nFramework highlights the need to coordinate the activities of internal and\nexternal assurance providers. Next, the article provides an overview of key\ngovernance challenges in frontier AI development: dangerous capabilities can\narise unpredictably and undetected; it is difficult to prevent a deployed model\nfrom causing harm; frontier models can proliferate rapidly; it is inherently\ndifficult to assess frontier AI risks; and frontier AI developers do not seem\nto follow best practices in risk governance. Finally, the article discusses how\nan internal audit function could address some of these challenges: internal\naudit could identify ineffective risk management practices; it could ensure\nthat the board of directors has a more accurate understanding of the current\nlevel of risk and the adequacy of the developer's risk management practices;\nand it could serve as a contact point for whistleblowers. In light of rapid\nprogress in AI research and development, frontier AI developers need to\nstrengthen their risk governance. Instead of reinventing the wheel, they should\nfollow existing best practices. While this might not be sufficient, they should\nnot skip this obvious first step.",
                "Improved Sales Forecasting using Trend and Seasonality Decomposition\n  with LightGBM\nRetail sales forecasting presents a significant challenge for large retailers\nsuch as Walmart and Amazon, due to the vast assortment of products,\ngeographical location heterogeneity, seasonality, and external factors\nincluding weather, local economic conditions, and geopolitical events. Various\nmethods have been employed to tackle this challenge, including traditional time\nseries models, machine learning models, and neural network mechanisms, but the\ndifficulty persists. Categorizing data into relevant groups has been shown to\nimprove sales forecast accuracy as time series from different categories may\nexhibit distinct patterns. In this paper, we propose a new measure to indicate\nthe unique impacts of the trend and seasonality components on a time series and\nsuggest grouping time series based on this measure. We apply this approach to\nWalmart sales data from 01/29/2011 to 05/22/2016 and generate sales forecasts\nfrom 05/23/2016 to 06/19/2016. Our experiments show that the proposed strategy\ncan achieve improved accuracy. Furthermore, we present a robust pipeline for\nconducting retail sales forecasting.",
                "How Do UX Practitioners Communicate AI as a Design Material? Artifacts,\n  Conceptions, and Propositions\nUX practitioners (UXPs) face novel challenges when working with and\ncommunicating artificial intelligence (AI) as a design material. We explore how\nUXPs communicate AI concepts when given hands-on experience training and\nexperimenting with AI models. To do so, we conducted a task-based design study\nwith 27 UXPs in which they prototyped and created a design presentation for a\nAI-enabled interface while having access to a simple AI model training tool.\nThrough analyzing UXPs' design presentations and post-activity interviews, we\nfound that although UXPs struggled to clearly communicate some AI concepts,\ntinkering with AI broadened common ground when communicating with technical\nstakeholders. UXPs also identified key risks and benefits of AI in their\ndesigns, and proposed concrete next steps for both UX and AI work. We conclude\nwith a sensitizing concept and recommendations for design and AI tools to\nenhance multi-stakeholder communication and collaboration when crafting\nhuman-centered AI experiences.",
                "Let the Flows Tell: Solving Graph Combinatorial Optimization Problems\n  with GFlowNets\nCombinatorial optimization (CO) problems are often NP-hard and thus out of\nreach for exact algorithms, making them a tempting domain to apply machine\nlearning methods. The highly structured constraints in these problems can\nhinder either optimization or sampling directly in the solution space. On the\nother hand, GFlowNets have recently emerged as a powerful machinery to\nefficiently sample from composite unnormalized densities sequentially and have\nthe potential to amortize such solution-searching processes in CO, as well as\ngenerate diverse solution candidates. In this paper, we design Markov decision\nprocesses (MDPs) for different combinatorial problems and propose to train\nconditional GFlowNets to sample from the solution space. Efficient training\ntechniques are also developed to benefit long-range credit assignment. Through\nextensive experiments on a variety of different CO tasks with synthetic and\nrealistic data, we demonstrate that GFlowNet policies can efficiently find\nhigh-quality solutions. Our implementation is open-sourced at\nhttps://github.com/zdhNarsil/GFlowNet-CombOpt.",
                "Learning Interpretable Models of Aircraft Handling Behaviour by\n  Reinforcement Learning from Human Feedback\nWe propose a method to capture the handling abilities of fast jet pilots in a\nsoftware model via reinforcement learning (RL) from human preference feedback.\nWe use pairwise preferences over simulated flight trajectories to learn an\ninterpretable rule-based model called a reward tree, which enables the\nautomated scoring of trajectories alongside an explanatory rationale. We train\nan RL agent to execute high-quality handling behaviour by using the reward tree\nas the objective, and thereby generate data for iterative preference collection\nand further refinement of both tree and agent. Experiments with synthetic\npreferences show reward trees to be competitive with uninterpretable neural\nnetwork reward models on quantitative and qualitative evaluations."
            ],
            "interesting paper": 5
        },
        {
            "papers": [
                "Instance-based Max-margin for Practical Few-shot Recognition\nIn order to mimic the human few-shot learning (FSL) ability better and to\nmake FSL closer to real-world applications, this paper proposes a practical FSL\n(pFSL) setting. pFSL is based on unsupervised pretrained models (analogous to\nhuman prior knowledge) and recognizes many novel classes simultaneously.\nCompared to traditional FSL, pFSL is simpler in its formulation, easier to\nevaluate, more challenging and more practical. To cope with the rarity of\ntraining examples, this paper proposes IbM2, an instance-based max-margin\nmethod not only for the new pFSL setting, but also works well in traditional\nFSL scenarios. Based on the Gaussian Annulus Theorem, IbM2 converts random\nnoise applied to the instances into a mechanism to achieve maximum margin in\nthe many-way pFSL (or traditional FSL) recognition task. Experiments with\nvarious self-supervised pretraining methods and diverse many- or few-way FSL\ntasks show that IbM2 almost always leads to improvements compared to its\nrespective baseline methods, and in most cases the improvements are\nsignificant. With both the new pFSL setting and novel IbM2 method, this paper\nshows that practical few-shot learning is both viable and promising.",
                "Self-Supervised Reinforcement Learning that Transfers using Random\n  Features\nModel-free reinforcement learning algorithms have exhibited great potential\nin solving single-task sequential decision-making problems with\nhigh-dimensional observations and long horizons, but are known to be hard to\ngeneralize across tasks. Model-based RL, on the other hand, learns\ntask-agnostic models of the world that naturally enables transfer across\ndifferent reward functions, but struggles to scale to complex environments due\nto the compounding error. To get the best of both worlds, we propose a\nself-supervised reinforcement learning method that enables the transfer of\nbehaviors across tasks with different rewards, while circumventing the\nchallenges of model-based RL. In particular, we show self-supervised\npre-training of model-free reinforcement learning with a number of random\nfeatures as rewards allows implicit modeling of long-horizon environment\ndynamics. Then, planning techniques like model-predictive control using these\nimplicit models enable fast adaptation to problems with new reward functions.\nOur method is self-supervised in that it can be trained on offline datasets\nwithout reward labels, but can then be quickly deployed on new tasks. We\nvalidate that our proposed method enables transfer across tasks on a variety of\nmanipulation and locomotion domains in simulation, opening the door to\ngeneralist decision-making agents.",
                "Matrix Information Theory for Self-Supervised Learning\nThe maximum entropy encoding framework provides a unified perspective for\nmany non-contrastive learning methods like SimSiam, Barlow Twins, and MEC.\nInspired by this framework, we introduce Matrix-SSL, a novel approach that\nleverages matrix information theory to interpret the maximum entropy encoding\nloss as matrix uniformity loss. Furthermore, Matrix-SSL enhances the maximum\nentropy encoding method by seamlessly incorporating matrix alignment loss,\ndirectly aligning covariance matrices in different branches. Experimental\nresults reveal that Matrix-SSL outperforms state-of-the-art methods on the\nImageNet dataset under linear evaluation settings and on MS-COCO for transfer\nlearning tasks. Specifically, when performing transfer learning tasks on\nMS-COCO, our method outperforms previous SOTA methods such as MoCo v2 and BYOL\nup to 3.3% with only 400 epochs compared to 800 epochs pre-training. We also\ntry to introduce representation learning into the language modeling regime by\nfine-tuning a 7B model using matrix cross-entropy loss, with a margin of 3.1%\non the GSM8K dataset over the standard cross-entropy loss. Code available at\nhttps://github.com/yifanzhang-pro/Matrix-SSL.",
                "Statistically Significant Concept-based Explanation of Image Classifiers\n  via Model Knockoffs\nA concept-based classifier can explain the decision process of a deep\nlearning model by human-understandable concepts in image classification\nproblems. However, sometimes concept-based explanations may cause false\npositives, which misregards unrelated concepts as important for the prediction\ntask. Our goal is to find the statistically significant concept for\nclassification to prevent misinterpretation. In this study, we propose a method\nusing a deep learning model to learn the image concept and then using the\nKnockoff samples to select the important concepts for prediction by controlling\nthe False Discovery Rate (FDR) under a certain value. We evaluate the proposed\nmethod in our synthetic and real data experiments. Also, it shows that our\nmethod can control the FDR properly while selecting highly interpretable\nconcepts to improve the trustworthiness of the model.",
                "Visualizing Self-Regulated Learner Profiles in Dashboards: Design\n  Insights from Teachers\nFlipped Classrooms (FC) are a promising teaching strategy, where students\nengage with the learning material before attending face-to-face sessions. While\npre-class activities are critical for course success, many students struggle to\nengage effectively in them due to inadequate of self-regulated learning (SRL)\nskills. Thus, tools enabling teachers to monitor students' SRL and provide\npersonalized guidance have the potential to improve learning outcomes. However,\nexisting dashboards mostly focus on aggregated information, disregarding recent\nwork leveraging machine learning (ML) approaches that have identified\ncomprehensive, multi-dimensional SRL behaviors. Unfortunately, the complexity\nof such findings makes them difficult to communicate and act on. In this paper,\nwe follow a teacher-centered approach to study how to make thorough findings\naccessible to teachers. We design and implement FlippED, a dashboard for\nmonitoring students' SRL behavior. We evaluate the usability and actionability\nof the tool in semi-structured interviews with ten university teachers. We find\nthat communicating ML-based profiles spark a range of potential interventions\nfor students and course modifications.",
                "Causal Component Analysis\nIndependent Component Analysis (ICA) aims to recover independent latent\nvariables from observed mixtures thereof. Causal Representation Learning (CRL)\naims instead to infer causally related (thus often statistically dependent)\nlatent variables, together with the unknown graph encoding their causal\nrelationships. We introduce an intermediate problem termed Causal Component\nAnalysis (CauCA). CauCA can be viewed as a generalization of ICA, modelling the\ncausal dependence among the latent components, and as a special case of CRL. In\ncontrast to CRL, it presupposes knowledge of the causal graph, focusing solely\non learning the unmixing function and the causal mechanisms. Any impossibility\nresults regarding the recovery of the ground truth in CauCA also apply for CRL,\nwhile possibility results may serve as a stepping stone for extensions to CRL.\nWe characterize CauCA identifiability from multiple datasets generated through\ndifferent types of interventions on the latent causal variables. As a\ncorollary, this interventional perspective also leads to new identifiability\nresults for nonlinear ICA -- a special case of CauCA with an empty graph --\nrequiring strictly fewer datasets than previous results. We introduce a\nlikelihood-based approach using normalizing flows to estimate both the unmixing\nfunction and the causal mechanisms, and demonstrate its effectiveness through\nextensive synthetic experiments in the CauCA and ICA setting."
            ],
            "interesting paper": 3
        },
        {
            "papers": [
                "Optimization's Neglected Normative Commitments\nOptimization is offered as an objective approach to resolving complex,\nreal-world decisions involving uncertainty and conflicting interests. It drives\nbusiness strategies as well as public policies and, increasingly, lies at the\nheart of sophisticated machine learning systems. A paradigm used to approach\npotentially high-stakes decisions, optimization relies on abstracting the real\nworld to a set of decision(s), objective(s) and constraint(s). Drawing from the\nmodeling process and a range of actual cases, this paper describes the\nnormative choices and assumptions that are necessarily part of using\noptimization. It then identifies six emergent problems that may be neglected:\n1) Misspecified values can yield optimizations that omit certain imperatives\naltogether or incorporate them incorrectly as a constraint or as part of the\nobjective, 2) Problematic decision boundaries can lead to faulty modularity\nassumptions and feedback loops, 3) Failing to account for multiple agents'\ndivergent goals and decisions can lead to policies that serve only certain\nnarrow interests, 4) Mislabeling and mismeasurement can introduce bias and\nimprecision, 5) Faulty use of relaxation and approximation methods,\nunaccompanied by formal characterizations and guarantees, can severely impede\napplicability, and 6) Treating optimization as a justification for action,\nwithout specifying the necessary contextual information, can lead to ethically\ndubious or faulty decisions. Suggestions are given to further understand and\ncurb the harms that can arise when optimization is used wrongfully.",
                "Probing reaction channels via reinforcement learning\nWe propose a reinforcement learning based method to identify important\nconfigurations that connect reactant and product states along chemical reaction\npaths. By shooting multiple trajectories from these configurations, we can\ngenerate an ensemble of configurations that concentrate on the transition path\nensemble. This configuration ensemble can be effectively employed in a neural\nnetwork-based partial differential equation solver to obtain an approximation\nsolution of a restricted Backward Kolmogorov equation, even when the dimension\nof the problem is very high. The resulting solution, known as the committor\nfunction, encodes mechanistic information for the reaction and can in turn be\nused to evaluate reaction rates.",
                "MLOps: A Step Forward to Enterprise Machine Learning\nMachine Learning Operations (MLOps) is becoming a highly crucial part of\nbusinesses looking to capitalize on the benefits of AI and ML models. This\nresearch presents a detailed review of MLOps, its benefits, difficulties,\nevolutions, and important underlying technologies such as MLOps frameworks,\nDocker, GitHub actions, and Kubernetes. The MLOps workflow, which includes\nmodel design, deployment, and operations, is explained in detail along with the\nvarious tools necessary for both model and data exploration and deployment.\nThis article also puts light on the end-to-end production of ML projects using\nvarious maturity levels of automated pipelines, with the least at no automation\nat all and the highest with complete CI/CD and CT capabilities. Furthermore, a\ndetailed example of an enterprise-level MLOps project for an object detection\nservice is used to explain the workflow of the technology in a real-world\nscenario. For this purpose, a web application hosting a pre-trained model from\nTensorFlow 2 Model Zoo is packaged and deployed to the internet making sure\nthat the system is scalable, reliable, and optimized for deployment at an\nenterprise level.",
                "AI Coach Assist: An Automated Approach for Call Recommendation in\n  Contact Centers for Agent Coaching\nIn recent years, the utilization of Artificial Intelligence (AI) in the\ncontact center industry is on the rise. One area where AI can have a\nsignificant impact is in the coaching of contact center agents. By analyzing\ncall transcripts using Natural Language Processing (NLP) techniques, it would\nbe possible to quickly determine which calls are most relevant for coaching\npurposes. In this paper, we present AI Coach Assist, which leverages the\npre-trained transformer-based language models to determine whether a given call\nis coachable or not based on the quality assurance (QA) questions asked by the\ncontact center managers or supervisors. The system was trained and evaluated on\na large dataset collected from real-world contact centers and provides an\neffective way to recommend calls to the contact center managers that are more\nlikely to contain coachable moments. Our experimental findings demonstrate the\npotential of AI Coach Assist to improve the coaching process, resulting in\nenhancing the performance of contact center agents.",
                "Synthesizing a Progression of Subtasks for Block-Based Visual\n  Programming Tasks\nBlock-based visual programming environments play an increasingly important\nrole in introducing computing concepts to K-12 students. In recent years, they\nhave also gained popularity in neuro-symbolic AI, serving as a benchmark to\nevaluate general problem-solving and logical reasoning skills. The open-ended\nand conceptual nature of these visual programming tasks make them challenging,\nboth for state-of-the-art AI agents as well as for novice programmers. A\nnatural approach to providing assistance for problem-solving is breaking down a\ncomplex task into a progression of simpler subtasks; however, this is not\ntrivial given that the solution codes are typically nested and have non-linear\nexecution behavior. In this paper, we formalize the problem of synthesizing\nsuch a progression for a given reference block-based visual programming task.\nWe propose a novel synthesis algorithm that generates a progression of subtasks\nthat are high-quality, well-spaced in terms of their complexity, and solving\nthis progression leads to solving the reference task. We show the utility of\nour synthesis algorithm in improving the efficacy of AI agents (in this case,\nneural program synthesizers) for solving tasks in the Karel programming\nenvironment. Then, we conduct a user study to demonstrate that our synthesized\nprogression of subtasks can assist a novice programmer in solving tasks in the\nHour of Code: Maze Challenge by Code-dot-org.",
                "PFNs4BO: In-Context Learning for Bayesian Optimization\nIn this paper, we use Prior-data Fitted Networks (PFNs) as a flexible\nsurrogate for Bayesian Optimization (BO). PFNs are neural processes that are\ntrained to approximate the posterior predictive distribution (PPD) through\nin-context learning on any prior distribution that can be efficiently sampled\nfrom. We describe how this flexibility can be exploited for surrogate modeling\nin BO. We use PFNs to mimic a naive Gaussian process (GP), an advanced GP, and\na Bayesian Neural Network (BNN). In addition, we show how to incorporate\nfurther information into the prior, such as allowing hints about the position\nof optima (user priors), ignoring irrelevant dimensions, and performing\nnon-myopic BO by learning the acquisition function. The flexibility underlying\nthese extensions opens up vast possibilities for using PFNs for BO. We\ndemonstrate the usefulness of PFNs for BO in a large-scale evaluation on\nartificial GP samples and three different hyperparameter optimization testbeds:\nHPO-B, Bayesmark, and PD1. We publish code alongside trained models at\ngithub.com/automl/PFNs4BO."
            ],
            "interesting paper": 6
        },
        {
            "papers": [
                "Data Minimization at Inference Time\nIn domains with high stakes such as law, recruitment, and healthcare,\nlearning models frequently rely on sensitive user data for inference,\nnecessitating the complete set of features. This not only poses significant\nprivacy risks for individuals but also demands substantial human effort from\norganizations to verify information accuracy. This paper asks whether it is\nnecessary to use \\emph{all} input features for accurate predictions at\ninference time. The paper demonstrates that, in a personalized setting,\nindividuals may only need to disclose a small subset of their features without\ncompromising decision-making accuracy. The paper also provides an efficient\nsequential algorithm to determine the appropriate attributes for each\nindividual to provide. Evaluations across various learning tasks show that\nindividuals can potentially report as little as 10\\% of their information while\nmaintaining the same accuracy level as a model that employs the full set of\nuser information.",
                "Distill Gold from Massive Ores: Bi-level Data Pruning towards Efficient\n  Dataset Distillation\nData-efficient learning has garnered significant attention, especially given\nthe current trend of large multi-modal models. Recently, dataset distillation\nhas become an effective approach by synthesizing data samples that are\nessential for network training. However, it remains to be explored which\nsamples are essential for the dataset distillation process itself. In this\nwork, we study the data efficiency and selection for the dataset distillation\ntask. By re-formulating the dynamics of distillation, we provide insight into\nthe inherent redundancy in the real dataset, both theoretically and\nempirically. We propose to use the empirical loss value as a static data\npruning criterion. To further compensate for the variation of the data value in\ntraining, we find the most contributing samples based on their causal effects\non the distillation. The proposed selection strategy can efficiently exploit\nthe training dataset, outperform the previous SOTA distillation algorithms, and\nconsistently enhance the distillation algorithms, even on much larger-scale and\nmore heterogeneous datasets, e.g., full ImageNet-1K and Kinetics-400. We\nbelieve this paradigm will open up new avenues in the dynamics of distillation\nand pave the way for efficient dataset distillation. Our code is available on\nhttps://github.com/silicx/GoldFromOres-BiLP.",
                "The Curse of Recursion: Training on Generated Data Makes Models Forget\nStable Diffusion revolutionised image creation from descriptive text. GPT-2,\nGPT-3(.5) and GPT-4 demonstrated astonishing performance across a variety of\nlanguage tasks. ChatGPT introduced such language models to the general public.\nIt is now clear that large language models (LLMs) are here to stay, and will\nbring about drastic change in the whole ecosystem of online text and images. In\nthis paper we consider what the future might hold. What will happen to GPT-{n}\nonce LLMs contribute much of the language found online? We find that use of\nmodel-generated content in training causes irreversible defects in the\nresulting models, where tails of the original content distribution disappear.\nWe refer to this effect as Model Collapse and show that it can occur in\nVariational Autoencoders, Gaussian Mixture Models and LLMs. We build\ntheoretical intuition behind the phenomenon and portray its ubiquity amongst\nall learned generative models. We demonstrate that it has to be taken seriously\nif we are to sustain the benefits of training from large-scale data scraped\nfrom the web. Indeed, the value of data collected about genuine human\ninteractions with systems will be increasingly valuable in the presence of\ncontent generated by LLMs in data crawled from the Internet.",
                "One Network, Many Masks: Towards More Parameter-Efficient Transfer\n  Learning\nFine-tuning pre-trained language models for multiple tasks tends to be\nexpensive in terms of storage. To mitigate this, parameter-efficient transfer\nlearning (PETL) methods have been proposed to address this issue, but they\nstill require a significant number of parameters and storage when being applied\nto broader ranges of tasks. To achieve even greater storage reduction, we\npropose PROPETL, a novel method that enables efficient sharing of a single PETL\nmodule which we call prototype network (e.g., adapter, LoRA, and prefix-tuning)\nacross layers and tasks. We then learn binary masks to select different\nsub-networks from the shared prototype network and apply them as PETL modules\ninto different layers. We find that the binary masks can determine crucial\ninformation from the network, which is often ignored in previous studies. Our\nwork can also be seen as a type of pruning method, where we find that\noverparameterization also exists in the seemingly small PETL modules. We\nevaluate PROPETL on various downstream tasks and show that it can outperform\nother PETL methods with approximately 10% of the parameter storage required by\nthe latter.",
                "Diagnosing Transformers: Illuminating Feature Spaces for Clinical\n  Decision-Making\nPre-trained transformers are often fine-tuned to aid clinical decision-making\nusing limited clinical notes. Model interpretability is crucial, especially in\nhigh-stakes domains like medicine, to establish trust and ensure safety, which\nrequires human engagement. We introduce SUFO, a systematic framework that\nenhances interpretability of fine-tuned transformer feature spaces. SUFO\nutilizes a range of analytic and visualization techniques, including Supervised\nprobing, Unsupervised similarity analysis, Feature dynamics, and Outlier\nanalysis to address key questions about model trust and interpretability. We\nconduct a case study investigating the impact of pre-training data where we\nfocus on real-world pathology classification tasks, and validate our findings\non MedNLI. We evaluate five 110M-sized pre-trained transformer models,\ncategorized into general-domain (BERT, TNLR), mixed-domain (BioBERT, Clinical\nBioBERT), and domain-specific (PubMedBERT) groups. Our SUFO analyses reveal\nthat: (1) while PubMedBERT, the domain-specific model, contains valuable\ninformation for fine-tuning, it can overfit to minority classes when class\nimbalances exist. In contrast, mixed-domain models exhibit greater resistance\nto overfitting, suggesting potential improvements in domain-specific model\nrobustness; (2) in-domain pre-training accelerates feature disambiguation\nduring fine-tuning; and (3) feature spaces undergo significant sparsification\nduring this process, enabling clinicians to identify common outlier modes among\nfine-tuned models as demonstrated in this paper. These findings showcase the\nutility of SUFO in enhancing trust and safety when using transformers in\nmedicine, and we believe SUFO can aid practitioners in evaluating fine-tuned\nlanguage models for other applications in medicine and in more critical\ndomains.",
                "USIM-DAL: Uncertainty-aware Statistical Image Modeling-based Dense\n  Active Learning for Super-resolution\nDense regression is a widely used approach in computer vision for tasks such\nas image super-resolution, enhancement, depth estimation, etc. However, the\nhigh cost of annotation and labeling makes it challenging to achieve accurate\nresults. We propose incorporating active learning into dense regression models\nto address this problem. Active learning allows models to select the most\ninformative samples for labeling, reducing the overall annotation cost while\nimproving performance. Despite its potential, active learning has not been\nwidely explored in high-dimensional computer vision regression tasks like\nsuper-resolution. We address this research gap and propose a new framework\ncalled USIM-DAL that leverages the statistical properties of colour images to\nlearn informative priors using probabilistic deep neural networks that model\nthe heteroscedastic predictive distribution allowing uncertainty\nquantification. Moreover, the aleatoric uncertainty from the network serves as\na proxy for error that is used for active learning. Our experiments on a wide\nvariety of datasets spanning applications in natural images (visual genome,\nBSD100), medical imaging (histopathology slides), and remote sensing (satellite\nimages) demonstrate the efficacy of the newly proposed USIM-DAL and superiority\nover several dense regression active learning methods."
            ],
            "interesting paper": 2
        },
        {
            "papers": [
                "Improving Confidence in Evolutionary Mine Scheduling via Uncertainty\n  Discounting\nMine planning is a complex task that involves many uncertainties. During\nearly stage feasibility, available mineral resources can only be estimated\nbased on limited sampling of ore grades from sparse drilling, leading to large\nuncertainty in under-sampled parts of the deposit. Planning the extraction\nschedule of ore over the life of a mine is crucial for its economic viability.\nWe introduce a new approach for determining an \"optimal schedule under\nuncertainty\" that provides probabilistic bounds on the profits obtained in each\nperiod. This treatment of uncertainty within an economic framework reduces\npreviously difficult-to-use models of variability into actionable insights. The\nnew method discounts profits based on uncertainty within an evolutionary\nalgorithm, sacrificing economic optimality of a single geological model for\nimproving the downside risk over an ensemble of equally likely models. We\nprovide experimental studies using Maptek's mine planning software Evolution.\nOur results show that our new approach is successful for effectively making use\nof uncertainty information in the mine planning process.",
                "Towards Autonomous and Safe Last-mile Deliveries with AI-augmented\n  Self-driving Delivery Robots\nIn addition to its crucial impact on customer satisfaction, last-mile\ndelivery (LMD) is notorious for being the most time-consuming and costly stage\nof the shipping process. Pressing environmental concerns combined with the\nrecent surge of e-commerce sales have sparked renewed interest in automation\nand electrification of last-mile logistics. To address the hurdles faced by\nexisting robotic couriers, this paper introduces a customer-centric and\nsafety-conscious LMD system for small urban communities based on AI-assisted\nautonomous delivery robots. The presented framework enables end-to-end\nautomation and optimization of the logistic process while catering for\nreal-world imposed operational uncertainties, clients' preferred time\nschedules, and safety of pedestrians. To this end, the integrated optimization\ncomponent is modeled as a robust variant of the Cumulative Capacitated Vehicle\nRouting Problem with Time Windows, where routes are constructed under uncertain\ntravel times with an objective to minimize the total latency of deliveries\n(i.e., the overall waiting time of customers, which can negatively affect their\nsatisfaction). We demonstrate the proposed LMD system's utility through\nreal-world trials in a university campus with a single robotic courier.\nImplementation aspects as well as the findings and practical insights gained\nfrom the deployment are discussed in detail. Lastly, we round up the\ncontributions with numerical simulations to investigate the scalability of the\ndeveloped mathematical formulation with respect to the number of robotic\nvehicles and customers.",
                "Employing Explainable Artificial Intelligence (XAI) Methodologies to\n  Analyze the Correlation between Input Variables and Tensile Strength in\n  Additively Manufactured Samples\nThis research paper explores the impact of various input parameters,\nincluding Infill percentage, Layer Height, Extrusion Temperature, and Print\nSpeed, on the resulting Tensile Strength in objects produced through additive\nmanufacturing. The main objective of this study is to enhance our understanding\nof the correlation between the input parameters and Tensile Strength, as well\nas to identify the key factors influencing the performance of the additive\nmanufacturing process. To achieve this objective, we introduced the utilization\nof Explainable Artificial Intelligence (XAI) techniques for the first time,\nwhich allowed us to analyze the data and gain valuable insights into the\nsystem's behavior. Specifically, we employed SHAP (SHapley Additive\nexPlanations), a widely adopted framework for interpreting machine learning\nmodel predictions, to provide explanations for the behavior of a machine\nlearning model trained on the data. Our findings reveal that the Infill\npercentage and Extrusion Temperature have the most significant influence on\nTensile Strength, while the impact of Layer Height and Print Speed is\nrelatively minor. Furthermore, we discovered that the relationship between the\ninput parameters and Tensile Strength is highly intricate and nonlinear, making\nit difficult to accurately describe using simple linear models.",
                "The Digital Divide in Process Safety: Quantitative Risk Analysis of\n  Human-AI Collaboration\nDigital technologies have dramatically accelerated the digital transformation\nin process industries, boosted new industrial applications, upgraded the\nproduction system, and enhanced operational efficiency. In contrast, the\nchallenges and gaps between human and artificial intelligence (AI) have become\nmore and more prominent, whereas the digital divide in process safety is\naggregating. The study attempts to address the following questions: (i)What is\nAI in the process safety context? (ii)What is the difference between AI and\nhumans in process safety? (iii)How do AI and humans collaborate in process\nsafety? (iv)What are the challenges and gaps in human-AI collaboration? (v)How\nto quantify the risk of human-AI collaboration in process safety? Qualitative\nrisk analysis based on brainstorming and literature review, and quantitative\nrisk analysis based on layer of protection analysis (LOPA) and Bayesian network\n(BN), were applied to explore and model. The importance of human reliability\nshould be stressed in the digital age, not usually to increase the reliability\nof AI, and human-centered AI design in process safety needs to be propagated.",
                "ProcessGPT: Transforming Business Process Management with Generative\n  Artificial Intelligence\nGenerative Pre-trained Transformer (GPT) is a state-of-the-art machine\nlearning model capable of generating human-like text through natural language\nprocessing (NLP). GPT is trained on massive amounts of text data and uses deep\nlearning techniques to learn patterns and relationships within the data,\nenabling it to generate coherent and contextually appropriate text. This\nposition paper proposes using GPT technology to generate new process models\nwhen/if needed. We introduce ProcessGPT as a new technology that has the\npotential to enhance decision-making in data-centric and knowledge-intensive\nprocesses. ProcessGPT can be designed by training a generative pre-trained\ntransformer model on a large dataset of business process data. This model can\nthen be fine-tuned on specific process domains and trained to generate process\nflows and make decisions based on context and user input. The model can be\nintegrated with NLP and machine learning techniques to provide insights and\nrecommendations for process improvement. Furthermore, the model can automate\nrepetitive tasks and improve process efficiency while enabling knowledge\nworkers to communicate analysis findings, supporting evidence, and make\ndecisions. ProcessGPT can revolutionize business process management (BPM) by\noffering a powerful tool for process augmentation, automation and improvement.\nFinally, we demonstrate how ProcessGPT can be a powerful tool for augmenting\ndata engineers in maintaining data ecosystem processes within large bank\norganizations. Our scenario highlights the potential of this approach to\nimprove efficiency, reduce costs, and enhance the quality of business\noperations through the automation of data-centric and knowledge-intensive\nprocesses. These results underscore the promise of ProcessGPT as a\ntransformative technology for organizations looking to improve their process\nworkflows.",
                "Optimizing Airbnb Search Journey with Multi-task Learning\nAt Airbnb, an online marketplace for stays and experiences, guests often\nspend weeks exploring and comparing multiple items before making a final\nreservation request. Each reservation request may then potentially be rejected\nor cancelled by the host prior to check-in. The long and exploratory nature of\nthe search journey, as well as the need to balance both guest and host\npreferences, present unique challenges for Airbnb search ranking. In this\npaper, we present Journey Ranker, a new multi-task deep learning model\narchitecture that addresses these challenges. Journey Ranker leverages\nintermediate guest actions as milestones, both positive and negative, to better\nprogress the guest towards a successful booking. It also uses contextual\ninformation such as guest state and search query to balance guest and host\npreferences. Its modular and extensible design, consisting of four modules with\nclear separation of concerns, allows for easy application to use cases beyond\nthe Airbnb search ranking context. We conducted offline and online testing of\nthe Journey Ranker and successfully deployed it in production to four different\nAirbnb products with significant business metrics improvements."
            ],
            "interesting paper": 2
        },
        {
            "papers": [
                "Full High-Dimensional Intelligible Learning In 2-D Lossless\n  Visualization Space\nThis study explores a new methodology for machine learning classification\ntasks in 2-dimensional visualization space (2-D ML) using Visual knowledge\nDiscovery in lossless General Line Coordinates. It is shown that this is a full\nmachine learning approach that does not require processing n-dimensional data\nin an abstract n-dimensional space. It enables discovering n-D patterns in 2-D\nspace without loss of n-D information using graph representations of n-D data\nin 2-D. Specifically, this study shows that it can be done with static and\ndynamic In-line Based Coordinates in different modifications, which are a\ncategory of General Line Coordinates. Based on these inline coordinates,\nclassification and regression methods were developed. The viability of the\nstrategy was shown by two case studies based on benchmark datasets (Wisconsin\nBreast Cancer and Page Block Classification datasets). The characteristics of\npage block classification data led to the development of an algorithm for\nimbalanced high-resolution data with multiple classes, which exploits the\ndecision trees as a model design facilitator producing a model, which is more\ngeneral than a decision tree. This work accelerates the ongoing consolidation\nof an emerging field of full 2-D machine learning and its methodology. Within\nthis methodology the end users can discover models and justify them as\nself-service. Providing interpretable ML models is another benefit of this\napproach.",
                "Mitigating Label Biases for In-context Learning\nVarious design settings for in-context learning (ICL), such as the choice and\norder of the in-context examples, can bias a model toward a particular\nprediction without being reflective of an understanding of the task. While many\nstudies discuss these design choices, there have been few systematic\ninvestigations into categorizing them and mitigating their impact. In this\nwork, we define a typology for three types of label biases in ICL for text\nclassification: vanilla-label bias, context-label bias, and domain-label bias\n(which we conceptualize and detect for the first time).\n  Our analysis demonstrates that prior label bias calibration methods fall\nshort of addressing all three types of biases. Specifically, domain-label bias\nrestricts LLMs to random-level performance on many tasks regardless of the\nchoice of in-context examples. To mitigate the effect of these biases, we\npropose a simple bias calibration method that estimates a language model's\nlabel bias using random in-domain words from the task corpus. After controlling\nfor this estimated bias when making predictions, our novel domain-context\ncalibration significantly improves the ICL performance of GPT-J and GPT-3 on a\nwide range of tasks. The gain is substantial on tasks with large domain-label\nbias (up to 37% in Macro-F1). Furthermore, our results generalize to models\nwith different scales, pretraining methods, and manually-designed task\ninstructions, showing the prevalence of label biases in ICL.",
                "Learning to Learn from APIs: Black-Box Data-Free Meta-Learning\nData-free meta-learning (DFML) aims to enable efficient learning of new tasks\nby meta-learning from a collection of pre-trained models without access to the\ntraining data. Existing DFML work can only meta-learn from (i) white-box and\n(ii) small-scale pre-trained models (iii) with the same architecture,\nneglecting the more practical setting where the users only have inference\naccess to the APIs with arbitrary model architectures and model scale inside.\nTo solve this issue, we propose a Bi-level Data-free Meta Knowledge\nDistillation (BiDf-MKD) framework to transfer more general meta knowledge from\na collection of black-box APIs to one single meta model. Specifically, by just\nquerying APIs, we inverse each API to recover its training data via a\nzero-order gradient estimator and then perform meta-learning via a novel\nbi-level meta knowledge distillation structure, in which we design a boundary\nquery set recovery technique to recover a more informative query set near the\ndecision boundary. In addition, to encourage better generalization within the\nsetting of limited API budgets, we propose task memory replay to diversify the\nunderlying task distribution by covering more interpolated tasks. Extensive\nexperiments in various real-world scenarios show the superior performance of\nour BiDf-MKD framework.",
                "A Meta-learning Framework for Tuning Parameters of Protection Mechanisms\n  in Trustworthy Federated Learning\nTrustworthy Federated Learning (TFL) typically leverages protection\nmechanisms to guarantee privacy. However, protection mechanisms inevitably\nintroduce utility loss or efficiency reduction while protecting data privacy.\nTherefore, protection mechanisms and their parameters should be carefully\nchosen to strike an optimal tradeoff between \\textit{privacy leakage},\n\\textit{utility loss}, and \\textit{efficiency reduction}. To this end,\nfederated learning practitioners need tools to measure the three factors and\noptimize the tradeoff between them to choose the protection mechanism that is\nmost appropriate to the application at hand. Motivated by this requirement, we\npropose a framework that (1) formulates TFL as a problem of finding a\nprotection mechanism to optimize the tradeoff between privacy leakage, utility\nloss, and efficiency reduction and (2) formally defines bounded measurements of\nthe three factors. We then propose a meta-learning algorithm to approximate\nthis optimization problem and find optimal protection parameters for\nrepresentative protection mechanisms, including Randomization, Homomorphic\nEncryption, Secret Sharing, and Compression. We further design estimation\nalgorithms to quantify these found optimal protection parameters in a practical\nhorizontal federated learning setting and provide a theoretical analysis of the\nestimation error.",
                "JutePestDetect: An Intelligent Approach for Jute Pest Identification\n  Using Fine-Tuned Transfer Learning\nIn certain Asian countries, Jute is one of the primary sources of income and\nGross Domestic Product (GDP) for the agricultural sector. Like many other\ncrops, Jute is prone to pest infestations, and its identification is typically\nmade visually in countries like Bangladesh, India, Myanmar, and China. In\naddition, this method is time-consuming, challenging, and somewhat imprecise,\nwhich poses a substantial financial risk. To address this issue, the study\nproposes a high-performing and resilient transfer learning (TL) based\nJutePestDetect model to identify jute pests at the early stage. Firstly, we\nprepared jute pest dataset containing 17 classes and around 380 photos per pest\nclass, which were evaluated after manual and automatic pre-processing and\ncleaning, such as background removal and resizing. Subsequently, five prominent\npre-trained models -DenseNet201, InceptionV3, MobileNetV2, VGG19, and ResNet50\nwere selected from a previous study to design the JutePestDetect model. Each\nmodel was revised by replacing the classification layer with a global average\npooling layer and incorporating a dropout layer for regularization. To evaluate\nthe models performance, various metrics such as precision, recall, F1 score,\nROC curve, and confusion matrix were employed. These analyses provided\nadditional insights for determining the efficacy of the models. Among them, the\ncustomized regularized DenseNet201-based proposed JutePestDetect model\noutperformed the others, achieving an impressive accuracy of 99%. As a result,\nour proposed method and strategy offer an enhanced approach to pest\nidentification in the case of Jute, which can significantly benefit farmers\nworldwide.",
                "Dink-Net: Neural Clustering on Large Graphs\nDeep graph clustering, which aims to group the nodes of a graph into disjoint\nclusters with deep neural networks, has achieved promising progress in recent\nyears. However, the existing methods fail to scale to the large graph with\nmillion nodes. To solve this problem, a scalable deep graph clustering method\n(Dink-Net) is proposed with the idea of dilation and shrink. Firstly, by\ndiscriminating nodes, whether being corrupted by augmentations, representations\nare learned in a self-supervised manner. Meanwhile, the cluster centres are\ninitialized as learnable neural parameters. Subsequently, the clustering\ndistribution is optimized by minimizing the proposed cluster dilation loss and\ncluster shrink loss in an adversarial manner. By these settings, we unify the\ntwo-step clustering, i.e., representation learning and clustering optimization,\ninto an end-to-end framework, guiding the network to learn clustering-friendly\nfeatures. Besides, Dink-Net scales well to large graphs since the designed loss\nfunctions adopt the mini-batch data to optimize the clustering distribution\neven without performance drops. Both experimental results and theoretical\nanalyses demonstrate the superiority of our method. Compared to the runner-up,\nDink-Net achieves 9.62% NMI improvement on the ogbn-papers100M dataset with 111\nmillion nodes and 1.6 billion edges. The source code is released at\nhttps://github.com/yueliu1999/Dink-Net. Besides, a collection (papers, codes,\nand datasets) of deep graph clustering is shared at\nhttps://github.com/yueliu1999/Awesome-Deep-Graph-Clustering."
            ],
            "interesting paper": 3
        },
        {
            "papers": [
                "Design of the Reverse Logistics System for Medical Waste Recycling Part\n  II: Route Optimization with Case Study under COVID-19 Pandemic\nMedical waste recycling and treatment has gradually drawn concerns from the\nwhole society, as the amount of medical waste generated is increasing\ndramatically, especially during the pandemic of COVID-19. To tackle the\nemerging challenges, this study designs a reverse logistics system architecture\nwith three modules, i.e., medical waste classification & monitoring module,\ntemporary storage & disposal site (disposal site for short) selection module,\nas well as route optimization module. This overall solution design won the\nGrand Prize of the \"YUNFENG CUP\" China National Contest on Green Supply and\nReverse Logistics Design ranking 1st. This paper focuses on the design of the\nroute optimization module. In this module, a route optimization problem is\ndesigned considering transportation costs and multiple risk costs (e.g.,\nenvironment risk, population risk, property risk, and other accident-related\nrisks). The Analytic Hierarchy Process is employed to determine the weights for\neach risk element, and a customized genetic algorithm is developed to solve the\nroute optimization problem. A case study under the COVID-19 pandemic is further\nprovided to verify the proposed model. Limited by length, detailed descriptions\nof the whole system and the other modules can be found at\nhttps://shorturl.at/cdY59.",
                "Towards a Unifying Model of Rationality in Multiagent Systems\nMultiagent systems deployed in the real world need to cooperate with other\nagents (including humans) nearly as effectively as these agents cooperate with\none another. To design such AI, and provide guarantees of its effectiveness, we\nneed to clearly specify what types of agents our AI must be able to cooperate\nwith. In this work we propose a generic model of socially intelligent agents,\nwhich are individually rational learners that are also able to cooperate with\none another (in the sense that their joint behavior is Pareto efficient). We\ndefine rationality in terms of the regret incurred by each agent over its\nlifetime, and show how we can construct socially intelligent agents for\ndifferent forms of regret. We then discuss the implications of this model for\nthe development of \"robust\" MAS that can cooperate with a wide variety of\nsocially intelligent agents.",
                "Generalized Disparate Impact for Configurable Fairness Solutions in ML\nWe make two contributions in the field of AI fairness over continuous\nprotected attributes. First, we show that the Hirschfeld-Gebelein-Renyi (HGR)\nindicator (the only one currently available for such a case) is valuable but\nsubject to a few crucial limitations regarding semantics, interpretability, and\nrobustness. Second, we introduce a family of indicators that are: 1)\ncomplementary to HGR in terms of semantics; 2) fully interpretable and\ntransparent; 3) robust over finite samples; 4) configurable to suit specific\napplications. Our approach also allows us to define fine-grained constraints to\npermit certain types of dependence and forbid others selectively. By expanding\nthe available options for continuous protected attributes, our approach\nrepresents a significant contribution to the area of fair artificial\nintelligence.",
                "RE-centric Recommendations for the Development of Trustworthy(er)\n  Autonomous Systems\nComplying with the EU AI Act (AIA) guidelines while developing and\nimplementing AI systems will soon be mandatory within the EU. However,\npractitioners lack actionable instructions to operationalise ethics during AI\nsystems development. A literature review of different ethical guidelines\nrevealed inconsistencies in the principles addressed and the terminology used\nto describe them. Furthermore, requirements engineering (RE), which is\nidentified to foster trustworthiness in the AI development process from the\nearly stages was observed to be absent in a lot of frameworks that support the\ndevelopment of ethical and trustworthy AI. This incongruous phrasing combined\nwith a lack of concrete development practices makes trustworthy AI development\nharder. To address this concern, we formulated a comparison table for the\nterminology used and the coverage of the ethical AI principles in major ethical\nAI guidelines. We then examined the applicability of ethical AI development\nframeworks for performing effective RE during the development of trustworthy AI\nsystems. A tertiary review and meta-analysis of literature discussing ethical\nAI frameworks revealed their limitations when developing trustworthy AI. Based\non our findings, we propose recommendations to address such limitations during\nthe development of trustworthy AI.",
                "What is Essential for Unseen Goal Generalization of Offline\n  Goal-conditioned RL?\nOffline goal-conditioned RL (GCRL) offers a way to train general-purpose\nagents from fully offline datasets. In addition to being conservative within\nthe dataset, the generalization ability to achieve unseen goals is another\nfundamental challenge for offline GCRL. However, to the best of our knowledge,\nthis problem has not been well studied yet. In this paper, we study\nout-of-distribution (OOD) generalization of offline GCRL both theoretically and\nempirically to identify factors that are important. In a number of experiments,\nwe observe that weighted imitation learning enjoys better generalization than\npessimism-based offline RL method. Based on this insight, we derive a theory\nfor OOD generalization, which characterizes several important design choices.\nWe then propose a new offline GCRL method, Generalizable Offline\ngoAl-condiTioned RL (GOAT), by combining the findings from our theoretical and\nempirical studies. On a new benchmark containing 9 independent identically\ndistributed (IID) tasks and 17 OOD tasks, GOAT outperforms current\nstate-of-the-art methods by a large margin.",
                "Maximize to Explore: One Objective Function Fusing Estimation, Planning,\n  and Exploration\nIn online reinforcement learning (online RL), balancing exploration and\nexploitation is crucial for finding an optimal policy in a sample-efficient\nway. To achieve this, existing sample-efficient online RL algorithms typically\nconsist of three components: estimation, planning, and exploration. However, in\norder to cope with general function approximators, most of them involve\nimpractical algorithmic components to incentivize exploration, such as\noptimization within data-dependent level-sets or complicated sampling\nprocedures. To address this challenge, we propose an easy-to-implement RL\nframework called \\textit{Maximize to Explore} (\\texttt{MEX}), which only needs\nto optimize \\emph{unconstrainedly} a single objective that integrates the\nestimation and planning components while balancing exploration and exploitation\nautomatically. Theoretically, we prove that \\texttt{MEX} achieves a sublinear\nregret with general function approximations for Markov decision processes (MDP)\nand is further extendable to two-player zero-sum Markov games (MG). Meanwhile,\nwe adapt deep RL baselines to design practical versions of \\texttt{MEX}, in\nboth model-free and model-based manners, which can outperform baselines by a\nstable margin in various MuJoCo environments with sparse rewards. Compared with\nexisting sample-efficient online RL algorithms with general function\napproximations, \\texttt{MEX} achieves similar sample efficiency while enjoying\na lower computational cost and is more compatible with modern deep RL methods."
            ],
            "interesting paper": 5
        },
        {
            "papers": [
                "Learning Off-Road Terrain Traversability with Self-Supervisions Only\nEstimating the traversability of terrain should be reliable and accurate in\ndiverse conditions for autonomous driving in off-road environments. However,\nlearning-based approaches often yield unreliable results when confronted with\nunfamiliar contexts, and it is challenging to obtain manual annotations\nfrequently for new circumstances. In this paper, we introduce a method for\nlearning traversability from images that utilizes only self-supervision and no\nmanual labels, enabling it to easily learn traversability in new circumstances.\nTo this end, we first generate self-supervised traversability labels from past\ndriving trajectories by labeling regions traversed by the vehicle as highly\ntraversable. Using the self-supervised labels, we then train a neural network\nthat identifies terrains that are safe to traverse from an image using a\none-class classification algorithm. Additionally, we supplement the limitations\nof self-supervised labels by incorporating methods of self-supervised learning\nof visual representations. To conduct a comprehensive evaluation, we collect\ndata in a variety of driving environments and perceptual conditions and show\nthat our method produces reliable estimations in various environments. In\naddition, the experimental results validate that our method outperforms other\nself-supervised traversability estimation methods and achieves comparable\nperformances with supervised learning methods trained on manually labeled data.",
                "Partially Personalized Federated Learning: Breaking the Curse of Data\n  Heterogeneity\nWe present a partially personalized formulation of Federated Learning (FL)\nthat strikes a balance between the flexibility of personalization and\ncooperativeness of global training. In our framework, we split the variables\ninto global parameters, which are shared across all clients, and individual\nlocal parameters, which are kept private. We prove that under the right split\nof parameters, it is possible to find global parameters that allow each client\nto fit their data perfectly, and refer to the obtained problem as\noverpersonalized. For instance, the shared global parameters can be used to\nlearn good data representations, whereas the personalized layers are fine-tuned\nfor a specific client. Moreover, we present a simple algorithm for the\npartially personalized formulation that offers significant benefits to all\nclients. In particular, it breaks the curse of data heterogeneity in several\nsettings, such as training with local steps, asynchronous training, and\nByzantine-robust training.",
                "Task-Equivariant Graph Few-shot Learning\nAlthough Graph Neural Networks (GNNs) have been successful in node\nclassification tasks, their performance heavily relies on the availability of a\nsufficient number of labeled nodes per class. In real-world situations, not all\nclasses have many labeled nodes and there may be instances where the model\nneeds to classify new classes, making manual labeling difficult. To solve this\nproblem, it is important for GNNs to be able to classify nodes with a limited\nnumber of labeled nodes, known as few-shot node classification. Previous\nepisodic meta-learning based methods have demonstrated success in few-shot node\nclassification, but our findings suggest that optimal performance can only be\nachieved with a substantial amount of diverse training meta-tasks. To address\nthis challenge of meta-learning based few-shot learning (FSL), we propose a new\napproach, the Task-Equivariant Graph few-shot learning (TEG) framework. Our TEG\nframework enables the model to learn transferable task-adaptation strategies\nusing a limited number of training meta-tasks, allowing it to acquire\nmeta-knowledge for a wide range of meta-tasks. By incorporating equivariant\nneural networks, TEG can utilize their strong generalization abilities to learn\nhighly adaptable task-specific strategies. As a result, TEG achieves\nstate-of-the-art performance with limited training meta-tasks. Our experiments\non various benchmark datasets demonstrate TEG's superiority in terms of\naccuracy and generalization ability, even when using minimal meta-training\ndata, highlighting the effectiveness of our proposed approach in addressing the\nchallenges of meta-learning based few-shot node classification. Our code is\navailable at the following link: https://github.com/sung-won-kim/TEG",
                "Controllable Path of Destruction\nPath of Destruction (PoD) is a self-supervised method for learning iterative\ngenerators. The core idea is to produce a training set by destroying a set of\nartifacts, and for each destructive step create a training instance based on\nthe corresponding repair action. A generator trained on this dataset can then\ngenerate new artifacts by repairing from arbitrary states. The PoD method is\nvery data-efficient in terms of original training examples and well-suited to\nfunctional artifacts composed of categorical data, such as game levels and\ndiscrete 3D structures. In this paper, we extend the Path of Destruction method\nto allow designer control over aspects of the generated artifacts.\nControllability is introduced by adding conditional inputs to the state-action\npairs that make up the repair trajectories. We test the controllable PoD method\nin a 2D dungeon setting, as well as in the domain of small 3D Lego cars.",
                "Autoencoding Conditional Neural Processes for Representation Learning\nConditional neural processes (CNPs) are a flexible and efficient family of\nmodels that learn to learn a stochastic process from data. They have seen\nparticular application in contextual image completion - observing pixel values\nat some locations to predict a distribution over values at other unobserved\nlocations. However, the choice of pixels in learning CNPs is typically either\nrandom or derived from a simple statistical measure (e.g. pixel variance).\nHere, we turn the problem on its head and ask: which pixels would a CNP like to\nobserve - do they facilitate fitting better CNPs, and do such pixels tell us\nsomething meaningful about the underlying image? To this end we develop the\nPartial Pixel Space Variational Autoencoder (PPS-VAE), an amortised variational\nframework that casts CNP context as latent variables learnt simultaneously with\nthe CNP. We evaluate PPS-VAE over a number of tasks across different visual\ndata, and find that not only can it facilitate better-fit CNPs, but also that\nthe spatial arrangement and values meaningfully characterise image information\n- evaluated through the lens of classification on both within and out-of-data\ndistributions. Our model additionally allows for dynamic adaption of\ncontext-set size and the ability to scale-up to larger images, providing a\npromising avenue to explore learning meaningful and effective visual\nrepresentations.",
                "DMS: Differentiable Mean Shift for Dataset Agnostic Task Specific\n  Clustering Using Side Information\nWe present a novel approach, in which we learn to cluster data directly from\nside information, in the form of a small set of pairwise examples. Unlike\nprevious methods, with or without side information, we do not need to know the\nnumber of clusters, their centers or any kind of distance metric for\nsimilarity. Our method is able to divide the same data points in various ways\ndependant on the needs of a specific task, defined by the side information.\nContrastingly, other work generally finds only the intrinsic, most obvious,\nclusters. Inspired by the mean shift algorithm, we implement our new clustering\napproach using a custom iterative neural network to create Differentiable Mean\nShift (DMS), a state of the art, dataset agnostic, clustering method. We found\nthat it was possible to train a strong cluster definition without enforcing a\nconstraint that each cluster must be presented during training. DMS outperforms\ncurrent methods in both the intrinsic and non-intrinsic dataset tasks."
            ],
            "interesting paper": 1
        },
        {
            "papers": [
                "Towards Omni-generalizable Neural Methods for Vehicle Routing Problems\nLearning heuristics for vehicle routing problems (VRPs) has gained much\nattention due to the less reliance on hand-crafted rules. However, existing\nmethods are typically trained and tested on the same task with a fixed size and\ndistribution (of nodes), and hence suffer from limited generalization\nperformance. This paper studies a challenging yet realistic setting, which\nconsiders generalization across both size and distribution in VRPs. We propose\na generic meta-learning framework, which enables effective training of an\ninitialized model with the capability of fast adaptation to new tasks during\ninference. We further develop a simple yet efficient approximation method to\nreduce the training overhead. Extensive experiments on both synthetic and\nbenchmark instances of the traveling salesman problem (TSP) and capacitated\nvehicle routing problem (CVRP) demonstrate the effectiveness of our method. The\ncode is available at: https://github.com/RoyalSkye/Omni-VRP.",
                "Disentangling and Operationalizing AI Fairness at LinkedIn\nOperationalizing AI fairness at LinkedIn's scale is challenging not only\nbecause there are multiple mutually incompatible definitions of fairness but\nalso because determining what is fair depends on the specifics and context of\nthe product where AI is deployed. Moreover, AI practitioners need clarity on\nwhat fairness expectations need to be addressed at the AI level. In this paper,\nwe present the evolving AI fairness framework used at LinkedIn to address these\nthree challenges. The framework disentangles AI fairness by separating out\nequal treatment and equitable product expectations. Rather than imposing a\ntrade-off between these two commonly opposing interpretations of fairness, the\nframework provides clear guidelines for operationalizing equal AI treatment\ncomplemented with a product equity strategy. This paper focuses on the equal AI\ntreatment component of LinkedIn's AI fairness framework, shares the principles\nthat support it, and illustrates their application through a case study. We\nhope this paper will encourage other big tech companies to join us in sharing\ntheir approach to operationalizing AI fairness at scale, so that together we\ncan keep advancing this constantly evolving field.",
                "Machine Learning Applications in Cascading Failure Analysis in Power\n  Systems: A Review\nCascading failures pose a significant threat to power grids and have garnered\nconsiderable research interest in the power system domain. The inherent\nuncertainty and severe impact associated with cascading failures have raised\nconcerns, prompting the development of various techniques to study these\ncomplex phenomena. In recent years, advancements in monitoring technologies and\nthe availability of large volumes of data from power systems, coupled with the\nemergence of intelligent algorithms, have made machine learning (ML) techniques\nincreasingly attractive for addressing cascading failure problems. This survey\nprovides a comprehensive overview of ML-based techniques for analyzing\ncascading failures in power systems. The survey categorizes these techniques\nbased on the evolutionary phases of the cascade process in power systems, as\nwell as studies focusing on cascade resiliency before the occurrence of\ncascades and problems related to cascades after their termination. By\norganizing and presenting these works into relevant categories, this survey\naims to offer insights and a systematic understanding the role of ML in\nmitigating cascading failures in power systems.",
                "Evaluating GPT's Programming Capability through CodeWars' Katas\nIn the burgeoning field of artificial intelligence (AI), understanding the\ncapabilities and limitations of programming-oriented models is crucial. This\npaper presents a novel evaluation of the programming proficiency of Generative\nPretrained Transformer (GPT) models, specifically GPT-3.5 and GPT-4, against\ncoding problems of varying difficulty levels drawn from Codewars. The\nexperiments reveal a distinct boundary at the 3kyu level, beyond which these\nGPT models struggle to provide solutions. These findings led to the proposal of\na measure for coding problem complexity that incorporates both problem\ndifficulty and the time required for solution. The research emphasizes the need\nfor validation and creative thinking capabilities in AI models to better\nemulate human problem-solving techniques. Future work aims to refine this\nproposed complexity measure, enhance AI models with these suggested\ncapabilities, and develop an objective measure for programming problem\ndifficulty. The results of this research offer invaluable insights for\nimproving AI programming capabilities and advancing the frontier of AI\nproblem-solving abilities.",
                "Necessary and Sufficient Conditions for Optimal Decision Trees using\n  Dynamic Programming\nGlobal optimization of decision trees has shown to be promising in terms of\naccuracy, size, and consequently human comprehensibility. However, many of the\nmethods used rely on general-purpose solvers for which scalability remains an\nissue. Dynamic programming methods have been shown to scale much better because\nthey exploit the tree structure by solving subtrees as independent subproblems.\nHowever, this only works when an objective can be optimized separately for\nsubtrees. We explore this relationship in detail and show the necessary and\nsufficient conditions for such separability and generalize previous dynamic\nprogramming approaches into a framework that can optimize any combination of\nseparable objectives and constraints. Experiments on five application domains\nshow the general applicability of this framework, while outperforming the\nscalability of general-purpose solvers by a large margin.",
                "GPT Models in Construction Industry: Opportunities, Limitations, and a\n  Use Case Validation\nLarge Language Models(LLMs) trained on large data sets came into prominence\nin 2018 after Google introduced BERT. Subsequently, different LLMs such as GPT\nmodels from OpenAI have been released. These models perform well on diverse\ntasks and have been gaining widespread applications in fields such as business\nand education. However, little is known about the opportunities and challenges\nof using LLMs in the construction industry. Thus, this study aims to assess GPT\nmodels in the construction industry. A critical review, expert discussion and\ncase study validation are employed to achieve the study objectives. The\nfindings revealed opportunities for GPT models throughout the project\nlifecycle. The challenges of leveraging GPT models are highlighted and a use\ncase prototype is developed for materials selection and optimization. The\nfindings of the study would be of benefit to researchers, practitioners and\nstakeholders, as it presents research vistas for LLMs in the construction\nindustry."
            ],
            "interesting paper": 1
        },
        {
            "papers": [
                "A Computational Account Of Self-Supervised Visual Learning From\n  Egocentric Object Play\nResearch in child development has shown that embodied experience handling\nphysical objects contributes to many cognitive abilities, including visual\nlearning. One characteristic of such experience is that the learner sees the\nsame object from several different viewpoints. In this paper, we study how\nlearning signals that equate different viewpoints -- e.g., assigning similar\nrepresentations to different views of a single object -- can support robust\nvisual learning. We use the Toybox dataset, which contains egocentric videos of\nhumans manipulating different objects, and conduct experiments using a computer\nvision framework for self-supervised contrastive learning. We find that\nrepresentations learned by equating different physical viewpoints of an object\nbenefit downstream image classification accuracy. Further experiments show that\nthis performance improvement is robust to variations in the gaps between\nviewpoints, and that the benefits transfer to several different image\nclassification tasks.",
                "Contextual Vision Transformers for Robust Representation Learning\nWe introduce Contextual Vision Transformers (ContextViT), a method designed\nto generate robust image representations for datasets experiencing shifts in\nlatent factors across various groups. Derived from the concept of in-context\nlearning, ContextViT incorporates an additional context token to encapsulate\ngroup-specific information. This integration allows the model to adjust the\nimage representation in accordance with the group-specific context.\nSpecifically, for a given input image, ContextViT maps images with identical\ngroup membership into this context token, which is appended to the input image\ntokens. Additionally, we introduce a context inference network to predict such\ntokens on-the-fly, given a batch of samples from the group. This enables\nContextViT to adapt to new testing distributions during inference time. We\ndemonstrate the efficacy of ContextViT across a wide range of applications. In\nsupervised fine-tuning, we show that augmenting pre-trained ViTs with our\nproposed context conditioning mechanism results in consistent improvements in\nout-of-distribution generalization on iWildCam and FMoW. We also investigate\nself-supervised representation learning with ContextViT. Our experiments on the\nCamelyon17 pathology imaging benchmark and the JUMP-CP microscopy imaging\nbenchmark demonstrate that ContextViT excels in learning stable image\nfeaturizations amidst distribution shift, consistently outperforming its ViT\ncounterpart.",
                "Spotlight Attention: Robust Object-Centric Learning With a Spatial\n  Locality Prior\nThe aim of object-centric vision is to construct an explicit representation\nof the objects in a scene. This representation is obtained via a set of\ninterchangeable modules called \\emph{slots} or \\emph{object files} that compete\nfor local patches of an image. The competition has a weak inductive bias to\npreserve spatial continuity; consequently, one slot may claim patches scattered\ndiffusely throughout the image. In contrast, the inductive bias of human vision\nis strong, to the degree that attention has classically been described with a\nspotlight metaphor. We incorporate a spatial-locality prior into\nstate-of-the-art object-centric vision models and obtain significant\nimprovements in segmenting objects in both synthetic and real-world datasets.\nSimilar to human visual attention, the combination of image content and spatial\nconstraints yield robust unsupervised object-centric learning, including less\nsensitivity to model hyperparameters.",
                "DENTEX: An Abnormal Tooth Detection with Dental Enumeration and\n  Diagnosis Benchmark for Panoramic X-rays\nPanoramic X-rays are frequently used in dentistry for treatment planning, but\ntheir interpretation can be both time-consuming and prone to error. Artificial\nintelligence (AI) has the potential to aid in the analysis of these X-rays,\nthereby improving the accuracy of dental diagnoses and treatment plans.\nNevertheless, designing automated algorithms for this purpose poses significant\nchallenges, mainly due to the scarcity of annotated data and variations in\nanatomical structure. To address these issues, the Dental Enumeration and\nDiagnosis on Panoramic X-rays Challenge (DENTEX) has been organized in\nassociation with the International Conference on Medical Image Computing and\nComputer-Assisted Intervention (MICCAI) in 2023. This challenge aims to promote\nthe development of algorithms for multi-label detection of abnormal teeth,\nusing three types of hierarchically annotated data: partially annotated\nquadrant data, partially annotated quadrant-enumeration data, and fully\nannotated quadrant-enumeration-diagnosis data, inclusive of four different\ndiagnoses. In this paper, we present the results of evaluating participant\nalgorithms on the fully annotated data, additionally investigating performance\nvariation for quadrant, enumeration, and diagnosis labels in the detection of\nabnormal teeth. The provision of this annotated dataset, alongside the results\nof this challenge, may lay the groundwork for the creation of AI-powered tools\nthat can offer more precise and efficient diagnosis and treatment planning in\nthe field of dentistry. The evaluation code and datasets can be accessed at\nhttps://github.com/ibrahimethemhamamci/DENTEX",
                "FedCSD: A Federated Learning Based Approach for Code-Smell Detection\nThis paper proposes a Federated Learning Code Smell Detection (FedCSD)\napproach that allows organizations to collaboratively train federated ML models\nwhile preserving their data privacy. These assertions have been supported by\nthree experiments that have significantly leveraged three manually validated\ndatasets aimed at detecting and examining different code smell scenarios. In\nexperiment 1, which was concerned with a centralized training experiment,\ndataset two achieved the lowest accuracy (92.30%) with fewer smells, while\ndatasets one and three achieved the highest accuracy with a slight difference\n(98.90% and 99.5%, respectively). This was followed by experiment 2, which was\nconcerned with cross-evaluation, where each ML model was trained using one\ndataset, which was then evaluated over the other two datasets. Results from\nthis experiment show a significant drop in the model's accuracy (lowest\naccuracy: 63.80\\%) where fewer smells exist in the training dataset, which has\na noticeable reflection (technical debt) on the model's performance. Finally,\nthe last and third experiments evaluate our approach by splitting the dataset\ninto 10 companies. The ML model was trained on the company's site, then all\nmodel-updated weights were transferred to the server. Ultimately, an accuracy\nof 98.34% was achieved by the global model that has been trained using 10\ncompanies for 100 training rounds. The results reveal a slight difference in\nthe global model's accuracy compared to the highest accuracy of the centralized\nmodel, which can be ignored in favour of the global model's comprehensive\nknowledge, lower training cost, preservation of data privacy, and avoidance of\nthe technical debt problem.",
                "A rule-general abductive learning by rough sets\nIn real-world tasks, there is usually a large amount of unlabeled data and\nlabeled data. The task of combining the two to learn is known as\nsemi-supervised learning. Experts can use logical rules to label unlabeled\ndata, but this operation is costly. The combination of perception and reasoning\nhas a good effect in processing such semi-supervised tasks with domain\nknowledge. However, acquiring domain knowledge and the correction, reduction\nand generation of rules remain complex problems to be solved. Rough set theory\nis an important method for solving knowledge processing in information systems.\nIn this paper, we propose a rule general abductive learning by rough set\n(RS-ABL). By transforming the target concept and sub-concepts of rules into\ninformation tables, rough set theory is used to solve the acquisition of domain\nknowledge and the correction, reduction and generation of rules at a lower\ncost. This framework can also generate more extensive negative rules to enhance\nthe breadth of the knowledge base. Compared with the traditional\nsemi-supervised learning method, RS-ABL has higher accuracy in dealing with\nsemi-supervised tasks."
            ],
            "interesting paper": 2
        },
        {
            "papers": [
                "Responsible Design Patterns for Machine Learning Pipelines\nIntegrating ethical practices into the AI development process for artificial\nintelligence (AI) is essential to ensure safe, fair, and responsible operation.\nAI ethics involves applying ethical principles to the entire life cycle of AI\nsystems. This is essential to mitigate potential risks and harms associated\nwith AI, such as algorithm biases. To achieve this goal, responsible design\npatterns (RDPs) are critical for Machine Learning (ML) pipelines to guarantee\nethical and fair outcomes. In this paper, we propose a comprehensive framework\nincorporating RDPs into ML pipelines to mitigate risks and ensure the ethical\ndevelopment of AI systems. Our framework comprises new responsible AI design\npatterns for ML pipelines identified through a survey of AI ethics and data\nmanagement experts and validated through real-world scenarios with expert\nfeedback. The framework guides AI developers, data scientists, and\npolicy-makers to implement ethical practices in AI development and deploy\nresponsible AI systems in production.",
                "A Virtual-Force Based Swarm Algorithm for Balanced Circular Bin Packing\n  Problems\nBalanced circular bin packing problems consist in positioning a given number\nof weighted circles in order to minimize the radius of a circular container\nwhile satisfying equilibrium constraints. These problems are NP-hard, highly\nconstrained and dimensional. This paper describes a swarm algorithm based on a\nvirtual-force system in order to solve balanced circular bin packing problems.\nIn the proposed approach, a system of forces is applied to each component\nallowing to take into account the constraints and minimizing the objective\nfunction using the fundamental principle of dynamics. The proposed algorithm is\nexperimented and validated on benchmarks of various balanced circular bin\npacking problems with up to 300 circles. The reported results allow to assess\nthe effectiveness of the proposed approach compared to existing results from\nthe literature.",
                "Credit Card Fraud Detection Using Asexual Reproduction Optimization\nAs the number of credit card users has increased, detecting fraud in this\ndomain has become a vital issue. Previous literature has applied various\nsupervised and unsupervised machine learning methods to find an effective fraud\ndetection system. However, some of these methods require an enormous amount of\ntime to achieve reasonable accuracy. In this paper, an Asexual Reproduction\nOptimization (ARO) approach was employed, which is a supervised method to\ndetect credit card fraud. ARO refers to a kind of production in which one\nparent produces some offspring. By applying this method and sampling just from\nthe majority class, the effectiveness of the classification is increased. A\ncomparison to Artificial Immune Systems (AIS), which is one of the best methods\nimplemented on current datasets, has shown that the proposed method is able to\nremarkably reduce the required training time and at the same time increase the\nrecall that is important in fraud detection problems. The obtained results show\nthat ARO achieves the best cost in a short time, and consequently, it can be\nconsidered a real-time fraud detection system.",
                "Constrained Causal Bayesian Optimization\nWe propose constrained causal Bayesian optimization (cCBO), an approach for\nfinding interventions in a known causal graph that optimize a target variable\nunder some constraints. cCBO first reduces the search space by exploiting the\ngraph structure and, if available, an observational dataset; and then solves\nthe restricted optimization problem by modelling target and constraint\nquantities using Gaussian processes and by sequentially selecting interventions\nvia a constrained expected improvement acquisition function. We propose\ndifferent surrogate models that enable to integrate observational and\ninterventional data while capturing correlation among effects with increasing\nlevels of sophistication. We evaluate cCBO on artificial and real-world causal\ngraphs showing successful trade off between fast convergence and percentage of\nfeasible interventions.",
                "BetaZero: Belief-State Planning for Long-Horizon POMDPs using Learned\n  Approximations\nReal-world planning problems, including autonomous driving and sustainable\nenergy applications like carbon storage and resource exploration, have recently\nbeen modeled as partially observable Markov decision processes (POMDPs) and\nsolved using approximate methods. To solve high-dimensional POMDPs in practice,\nstate-of-the-art methods use online planning with problem-specific heuristics\nto reduce planning horizons and make the problems tractable. Algorithms that\nlearn approximations to replace heuristics have recently found success in\nlarge-scale fully observable domains. The key insight is the combination of\nonline Monte Carlo tree search with offline neural network approximations of\nthe optimal policy and value function. In this work, we bring this insight to\npartially observable domains and propose BetaZero, a belief-state planning\nalgorithm for high-dimensional POMDPs. BetaZero learns offline approximations\nthat replace heuristics to enable online decision making in long-horizon\nproblems. We address several challenges inherent in large-scale partially\nobservable domains; namely challenges of transitioning in stochastic\nenvironments, prioritizing action branching with a limited search budget, and\nrepresenting beliefs as input to the network. To formalize the use of all\nlimited search information, we train against a novel $Q$-weighted visit counts\npolicy. We test BetaZero on various well-established POMDP benchmarks found in\nthe literature and a real-world problem of critical mineral exploration.\nExperiments show that BetaZero outperforms state-of-the-art POMDP solvers on a\nvariety of tasks.",
                "Representation-Driven Reinforcement Learning\nWe present a representation-driven framework for reinforcement learning. By\nrepresenting policies as estimates of their expected values, we leverage\ntechniques from contextual bandits to guide exploration and exploitation.\nParticularly, embedding a policy network into a linear feature space allows us\nto reframe the exploration-exploitation problem as a\nrepresentation-exploitation problem, where good policy representations enable\noptimal exploration. We demonstrate the effectiveness of this framework through\nits application to evolutionary and policy gradient-based approaches, leading\nto significantly improved performance compared to traditional methods. Our\nframework provides a new perspective on reinforcement learning, highlighting\nthe importance of policy representation in determining optimal\nexploration-exploitation strategies."
            ],
            "interesting paper": 5
        },
        {
            "papers": [
                "Quantifying Representation Reliability in Self-Supervised Learning\n  Models\nSelf-supervised learning models extract general-purpose representations from\ndata. Quantifying the reliability of these representations is crucial, as many\ndownstream models rely on them as input for their own tasks. To this end, we\nintroduce a formal definition of representation reliability: the representation\nfor a given test point is considered to be reliable if the downstream models\nbuilt on top of that representation can consistently generate accurate\npredictions for that test point. However, accessing downstream data to quantify\nthe representation reliability is often infeasible or restricted due to privacy\nconcerns. We propose an ensemble-based method for estimating the representation\nreliability without knowing the downstream tasks a priori. Our method is based\non the concept of neighborhood consistency across distinct pre-trained\nrepresentation spaces. The key insight is to find shared neighboring points as\nanchors to align these representation spaces before comparing them. We\ndemonstrate through comprehensive numerical experiments that our method\neffectively captures the representation reliability with a high degree of\ncorrelation, achieving robust and favorable performance compared with baseline\nmethods.",
                "Doubly Robust Self-Training\nSelf-training is an important technique for solving semi-supervised learning\nproblems. It leverages unlabeled data by generating pseudo-labels and combining\nthem with a limited labeled dataset for training. The effectiveness of\nself-training heavily relies on the accuracy of these pseudo-labels. In this\npaper, we introduce doubly robust self-training, a novel semi-supervised\nalgorithm that provably balances between two extremes. When the pseudo-labels\nare entirely incorrect, our method reduces to a training process solely using\nlabeled data. Conversely, when the pseudo-labels are completely accurate, our\nmethod transforms into a training process utilizing all pseudo-labeled data and\nlabeled data, thus increasing the effective sample size. Through empirical\nevaluations on both the ImageNet dataset for image classification and the\nnuScenes autonomous driving dataset for 3D object detection, we demonstrate the\nsuperiority of the doubly robust loss over the standard self-training baseline.",
                "SSL-CPCD: Self-supervised learning with composite pretext-class\n  discrimination for improved generalisability in endoscopic image analysis\nData-driven methods have shown tremendous progress in medical image analysis.\nIn this context, deep learning-based supervised methods are widely popular.\nHowever, they require a large amount of training data and face issues in\ngeneralisability to unseen datasets that hinder clinical translation.\nEndoscopic imaging data incorporates large inter- and intra-patient variability\nthat makes these models more challenging to learn representative features for\ndownstream tasks. Thus, despite the publicly available datasets and datasets\nthat can be generated within hospitals, most supervised models still\nunderperform. While self-supervised learning has addressed this problem to some\nextent in natural scene data, there is a considerable performance gap in the\nmedical image domain. In this paper, we propose to explore patch-level\ninstance-group discrimination and penalisation of inter-class variation using\nadditive angular margin within the cosine similarity metrics. Our novel\napproach enables models to learn to cluster similar representative patches,\nthereby improving their ability to provide better separation between different\nclasses. Our results demonstrate significant improvement on all metrics over\nthe state-of-the-art (SOTA) methods on the test set from the same and diverse\ndatasets. We evaluated our approach for classification, detection, and\nsegmentation. SSL-CPCD achieves 79.77% on Top 1 accuracy for ulcerative colitis\nclassification, 88.62% on mAP for polyp detection, and 82.32% on dice\nsimilarity coefficient for segmentation tasks are nearly over 4%, 2%, and 3%,\nrespectively, compared to the baseline architectures. We also demonstrate that\nour method generalises better than all SOTA methods to unseen datasets,\nreporting nearly 7% improvement in our generalisability assessment.",
                "MERT: Acoustic Music Understanding Model with Large-Scale\n  Self-supervised Training\nSelf-supervised learning (SSL) has recently emerged as a promising paradigm\nfor training generalisable models on large-scale data in the fields of vision,\ntext, and speech. Although SSL has been proven effective in speech and audio,\nits application to music audio has yet to be thoroughly explored. This is\npartially due to the distinctive challenges associated with modelling musical\nknowledge, particularly tonal and pitched characteristics of music. To address\nthis research gap, we propose an acoustic Music undERstanding model with\nlarge-scale self-supervised Training (MERT), which incorporates teacher models\nto provide pseudo labels in the masked language modelling (MLM) style acoustic\npre-training. In our exploration, we identified an effective combination of\nteacher models, which outperforms conventional speech and audio approaches in\nterms of performance. This combination includes an acoustic teacher based on\nResidual Vector Quantisation - Variational AutoEncoder (RVQ-VAE) and a musical\nteacher based on the Constant-Q Transform (CQT). Furthermore, we explore a wide\nrange of settings to overcome the instability in acoustic language model\npre-training, which allows our designed paradigm to scale from 95M to 330M\nparameters. Experimental results indicate that our model can generalise and\nperform well on 14 music understanding tasks and attain state-of-the-art (SOTA)\noverall scores.",
                "Feature Learning in Image Hierarchies using Functional Maximal\n  Correlation\nThis paper proposes the Hierarchical Functional Maximal Correlation Algorithm\n(HFMCA), a hierarchical methodology that characterizes dependencies across two\nhierarchical levels in multiview systems. By framing view similarities as\ndependencies and ensuring contrastivity by imposing orthonormality, HFMCA\nachieves faster convergence and increased stability in self-supervised\nlearning. HFMCA defines and measures dependencies within image hierarchies,\nfrom pixels and patches to full images. We find that the network topology for\napproximating orthonormal basis functions aligns with a vanilla CNN, enabling\nthe decomposition of density ratios between neighboring layers of feature maps.\nThis approach provides powerful interpretability, revealing the resemblance\nbetween supervision and self-supervision through the lens of internal\nrepresentations.",
                "Primal-Attention: Self-attention through Asymmetric Kernel SVD in Primal\n  Representation\nRecently, a new line of works has emerged to understand and improve\nself-attention in Transformers by treating it as a kernel machine. However,\nexisting works apply the methods for symmetric kernels to the asymmetric\nself-attention, resulting in a nontrivial gap between the analytical\nunderstanding and numerical implementation. In this paper, we provide a new\nperspective to represent and optimize self-attention through asymmetric Kernel\nSingular Value Decomposition (KSVD), which is also motivated by the low-rank\nproperty of self-attention normally observed in deep layers. Through asymmetric\nKSVD, $i$) a primal-dual representation of self-attention is formulated, where\nthe optimization objective is cast to maximize the projection variances in the\nattention outputs; $ii$) a novel attention mechanism, i.e., Primal-Attention,\nis proposed via the primal representation of KSVD, avoiding explicit\ncomputation of the kernel matrix in the dual; $iii$) with KKT conditions, we\nprove that the stationary solution to the KSVD optimization in Primal-Attention\nyields a zero-value objective. In this manner, KSVD optimization can be\nimplemented by simply minimizing a regularization loss, so that low-rank\nproperty is promoted without extra decomposition. Numerical experiments show\nstate-of-the-art performance of our Primal-Attention with improved efficiency.\nMoreover, we demonstrate that the deployed KSVD optimization regularizes\nPrimal-Attention with a sharper singular value decay than that of the canonical\nself-attention, further verifying the great potential of our method. To the\nbest of our knowledge, this is the first work that provides a primal-dual\nrepresentation for the asymmetric kernel in self-attention and successfully\napplies it to modeling and optimization."
            ],
            "interesting paper": 3
        },
        {
            "papers": [
                "A Modular Test Bed for Reinforcement Learning Incorporation into\n  Industrial Applications\nThis application paper explores the potential of using reinforcement learning\n(RL) to address the demands of Industry 4.0, including shorter time-to-market,\nmass customization, and batch size one production. Specifically, we present a\nuse case in which the task is to transport and assemble goods through a model\nfactory following predefined rules. Each simulation run involves placing a\nspecific number of goods of random color at the entry point. The objective is\nto transport the goods to the assembly station, where two rivets are installed\nin each product, connecting the upper part to the lower part. Following the\ninstallation of rivets, blue products must be transported to the exit, while\ngreen products are to be transported to storage. The study focuses on the\napplication of reinforcement learning techniques to address this problem and\nimprove the efficiency of the production process.",
                "AI Liability Insurance With an Example in AI-Powered E-diagnosis System\nArtificial Intelligence (AI) has received an increasing amount of attention\nin multiple areas. The uncertainties and risks in AI-powered systems have\ncreated reluctance in their wild adoption. As an economic solution to\ncompensate for potential damages, AI liability insurance is a promising market\nto enhance the integration of AI into daily life. In this work, we use an\nAI-powered E-diagnosis system as an example to study AI liability insurance. We\nprovide a quantitative risk assessment model with evidence-based numerical\nanalysis. We discuss the insurability criteria for AI technologies and suggest\nnecessary adjustments to accommodate the features of AI products. We show that\nAI liability insurance can act as a regulatory mechanism to incentivize\ncompliant behaviors and serve as a certificate of high-quality AI systems.\nFurthermore, we suggest premium adjustment to reflect the dynamic evolution of\nthe inherent uncertainty in AI. Moral hazard problems are discussed and\nsuggestions for AI liability insurance are provided.",
                "An Architecture for Deploying Reinforcement Learning in Industrial\n  Environments\nIndustry 4.0 is driven by demands like shorter time-to-market, mass\ncustomization of products, and batch size one production. Reinforcement\nLearning (RL), a machine learning paradigm shown to possess a great potential\nin improving and surpassing human level performance in numerous complex tasks,\nallows coping with the mentioned demands. In this paper, we present an OPC UA\nbased Operational Technology (OT)-aware RL architecture, which extends the\nstandard RL setting, combining it with the setting of digital twins. Moreover,\nwe define an OPC UA information model allowing for a generalized plug-and-play\nlike approach for exchanging the RL agent used. In conclusion, we demonstrate\nand evaluate the architecture, by creating a proof of concept. By means of\nsolving a toy example, we show that this architecture can be used to determine\nthe optimal policy using a real control system.",
                "Deep Q-Learning versus Proximal Policy Optimization: Performance\n  Comparison in a Material Sorting Task\nThis paper presents a comparison between two well-known deep Reinforcement\nLearning (RL) algorithms: Deep Q-Learning (DQN) and Proximal Policy\nOptimization (PPO) in a simulated production system. We utilize a Petri Net\n(PN)-based simulation environment, which was previously proposed in related\nwork. The performance of the two algorithms is compared based on several\nevaluation metrics, including average percentage of correctly assembled and\nsorted products, average episode length, and percentage of successful episodes.\nThe results show that PPO outperforms DQN in terms of all evaluation metrics.\nThe study highlights the advantages of policy-based algorithms in problems with\nhigh-dimensional state and action spaces. The study contributes to the field of\ndeep RL in context of production systems by providing insights into the\neffectiveness of different algorithms and their suitability for different\ntasks.",
                "Large-Batch, Iteration-Efficient Neural Bayesian Design Optimization\nBayesian optimization (BO) provides a powerful framework for optimizing\nblack-box, expensive-to-evaluate functions. It is therefore an attractive tool\nfor engineering design problems, typically involving multiple objectives.\nThanks to the rapid advances in fabrication and measurement methods as well as\nparallel computing infrastructure, querying many design problems can be heavily\nparallelized. This class of problems challenges BO with an unprecedented setup\nwhere it has to deal with very large batches, shifting its focus from sample\nefficiency to iteration efficiency. We present a novel Bayesian optimization\nframework specifically tailored to address these limitations. Our key\ncontribution is a highly scalable, sample-based acquisition function that\nperforms a non-dominated sorting of not only the objectives but also their\nassociated uncertainty. We show that our acquisition function in combination\nwith different Bayesian neural network surrogates is effective in\ndata-intensive environments with a minimal number of iterations. We demonstrate\nthe superiority of our method by comparing it with state-of-the-art\nmulti-objective optimizations. We perform our evaluation on two real-world\nproblems -- airfoil design and 3D printing -- showcasing the applicability and\nefficiency of our approach. Our code is available at:\nhttps://github.com/an-on-ym-ous/lbn_mobo",
                "Federated Learning Games for Reconfigurable Intelligent Surfaces via\n  Causal Representations\nIn this paper, we investigate the problem of robust Reconfigurable\nIntelligent Surface (RIS) phase-shifts configuration over heterogeneous\ncommunication environments. The problem is formulated as a distributed learning\nproblem over different environments in a Federated Learning (FL) setting.\nEquivalently, this corresponds to a game played between multiple RISs, as\nlearning agents, in heterogeneous environments. Using Invariant Risk\nMinimization (IRM) and its FL equivalent, dubbed FL Games, we solve the RIS\nconfiguration problem by learning invariant causal representations across\nmultiple environments and then predicting the phases. The solution corresponds\nto playing according to Best Response Dynamics (BRD) which yields the Nash\nEquilibrium of the FL game. The representation learner and the phase predictor\nare modeled by two neural networks, and their performance is validated via\nsimulations against other benchmarks from the literature. Our results show that\ncausality-based learning yields a predictor that is 15% more accurate in unseen\nOut-of-Distribution (OoD) environments."
            ],
            "interesting paper": 4
        },
        {
            "papers": [
                "Scaling Up Semi-supervised Learning with Unconstrained Unlabelled Data\nWe propose UnMixMatch, a semi-supervised learning framework which can learn\neffective representations from unconstrained unlabelled data in order to scale\nup performance. Most existing semi-supervised methods rely on the assumption\nthat labelled and unlabelled samples are drawn from the same distribution,\nwhich limits the potential for improvement through the use of free-living\nunlabeled data. Consequently, the generalizability and scalability of\nsemi-supervised learning are often hindered by this assumption. Our method aims\nto overcome these constraints and effectively utilize unconstrained unlabelled\ndata in semi-supervised learning. UnMixMatch consists of three main components:\na supervised learner with hard augmentations that provides strong\nregularization, a contrastive consistency regularizer to learn underlying\nrepresentations from the unlabelled data, and a self-supervised loss to enhance\nthe representations that are learnt from the unlabelled data. We perform\nextensive experiments on 4 commonly used datasets and demonstrate superior\nperformance over existing semi-supervised methods with a performance boost of\n4.79%. Extensive ablation and sensitivity studies show the effectiveness and\nimpact of each of the proposed components of our method.",
                "Efficient Failure Pattern Identification of Predictive Algorithms\nGiven a (machine learning) classifier and a collection of unlabeled data, how\ncan we efficiently identify misclassification patterns presented in this\ndataset? To address this problem, we propose a human-machine collaborative\nframework that consists of a team of human annotators and a sequential\nrecommendation algorithm. The recommendation algorithm is conceptualized as a\nstochastic sampler that, in each round, queries the annotators a subset of\nsamples for their true labels and obtains the feedback information on whether\nthe samples are misclassified. The sampling mechanism needs to balance between\ndiscovering new patterns of misclassification (exploration) and confirming the\npotential patterns of classification (exploitation). We construct a\ndeterminantal point process, whose intensity balances the\nexploration-exploitation trade-off through the weighted update of the posterior\nat each round to form the generator of the stochastic sampler. The numerical\nresults empirically demonstrate the competitive performance of our framework on\nmultiple datasets at various signal-to-noise ratios.",
                "Rotating Features for Object Discovery\nThe binding problem in human cognition, concerning how the brain represents\nand connects objects within a fixed network of neural connections, remains a\nsubject of intense debate. Most machine learning efforts addressing this issue\nin an unsupervised setting have focused on slot-based methods, which may be\nlimiting due to their discrete nature and difficulty to express uncertainty.\nRecently, the Complex AutoEncoder was proposed as an alternative that learns\ncontinuous and distributed object-centric representations. However, it is only\napplicable to simple toy data. In this paper, we present Rotating Features, a\ngeneralization of complex-valued features to higher dimensions, and a new\nevaluation procedure for extracting objects from distributed representations.\nAdditionally, we show the applicability of our approach to pre-trained\nfeatures. Together, these advancements enable us to scale distributed\nobject-centric representations from simple toy to real-world data. We believe\nthis work advances a new paradigm for addressing the binding problem in machine\nlearning and has the potential to inspire further innovation in the field.",
                "Federated Domain Generalization: A Survey\nMachine learning typically relies on the assumption that training and testing\ndistributions are identical and that data is centrally stored for training and\ntesting. However, in real-world scenarios, distributions may differ\nsignificantly and data is often distributed across different devices,\norganizations, or edge nodes. Consequently, it is imperative to develop models\nthat can effectively generalize to unseen distributions where data is\ndistributed across different domains. In response to this challenge, there has\nbeen a surge of interest in federated domain generalization (FDG) in recent\nyears. FDG combines the strengths of federated learning (FL) and domain\ngeneralization (DG) techniques to enable multiple source domains to\ncollaboratively learn a model capable of directly generalizing to unseen\ndomains while preserving data privacy. However, generalizing the federated\nmodel under domain shifts is a technically challenging problem that has\nreceived scant attention in the research area so far. This paper presents the\nfirst survey of recent advances in this area. Initially, we discuss the\ndevelopment process from traditional machine learning to domain adaptation and\ndomain generalization, leading to FDG as well as provide the corresponding\nformal definition. Then, we categorize recent methodologies into four classes:\nfederated domain alignment, data manipulation, learning strategies, and\naggregation optimization, and present suitable algorithms in detail for each\ncategory. Next, we introduce commonly used datasets, applications, evaluations,\nand benchmarks. Finally, we conclude this survey by providing some potential\nresearch topics for the future.",
                "Learning Causally Disentangled Representations via the Principle of\n  Independent Causal Mechanisms\nLearning disentangled causal representations is a challenging problem that\nhas gained significant attention recently due to its implications for\nextracting meaningful information for downstream tasks. In this work, we define\na new notion of causal disentanglement from the perspective of independent\ncausal mechanisms. We propose ICM-VAE, a framework for learning causally\ndisentangled representations supervised by causally related observed labels. We\nmodel causal mechanisms using nonlinear learnable flow-based diffeomorphic\nfunctions to map noise variables to latent causal variables. Further, to\npromote the disentanglement of causal factors, we propose a causal\ndisentanglement prior learned from auxiliary labels and the latent causal\nstructure. We theoretically show the identifiability of causal factors and\nmechanisms up to permutation and elementwise reparameterization. We empirically\ndemonstrate that our framework induces highly disentangled causal factors,\nimproves interventional robustness, and is compatible with counterfactual\ngeneration.",
                "Joint Learning of Label and Environment Causal Independence for Graph\n  Out-of-Distribution Generalization\nWe tackle the problem of graph out-of-distribution (OOD) generalization.\nExisting graph OOD algorithms either rely on restricted assumptions or fail to\nexploit environment information in training data. In this work, we propose to\nsimultaneously incorporate label and environment causal independence (LECI) to\nfully make use of label and environment information, thereby addressing the\nchallenges faced by prior methods on identifying causal and invariant\nsubgraphs. We further develop an adversarial training strategy to jointly\noptimize these two properties for causal subgraph discovery with theoretical\nguarantees. Extensive experiments and analysis show that LECI significantly\noutperforms prior methods on both synthetic and real-world datasets,\nestablishing LECI as a practical and effective solution for graph OOD\ngeneralization.\n  Our code is available at https://github.com/divelab/LECI."
            ],
            "interesting paper": 3
        }
    ],
    "Alex Garcia": [
        {
            "papers": [
                "Distributed Online Rollout for Multivehicle Routing in Unmapped\n  Environments\nIn this work we consider a generalization of the well-known multivehicle\nrouting problem: given a network, a set of agents occupying a subset of its\nnodes, and a set of tasks, we seek a minimum cost sequence of movements subject\nto the constraint that each task is visited by some agent at least once. The\nclassical version of this problem assumes a central computational server that\nobserves the entire state of the system perfectly and directs individual agents\naccording to a centralized control scheme. In contrast, we assume that there is\nno centralized server and that each agent is an individual processor with no a\npriori knowledge of the underlying network (including task and agent\nlocations). Moreover, our agents possess strictly local communication and\nsensing capabilities (restricted to a fixed radius around their respective\nlocations), aligning more closely with several real-world multiagent\napplications. These restrictions introduce many challenges that are overcome\nthrough local information sharing and direct coordination between agents. We\npresent a fully distributed, online, and scalable reinforcement learning\nalgorithm for this problem whereby agents self-organize into local clusters and\nindependently apply a multiagent rollout scheme locally to each cluster. We\ndemonstrate empirically via extensive simulations that there exists a critical\nsensing radius beyond which the distributed rollout algorithm begins to improve\nover a greedy base policy. This critical sensing radius grows proportionally to\nthe $\\log^*$ function of the size of the network, and is, therefore, a small\nconstant for any relevant network. Our decentralized reinforcement learning\nalgorithm achieves approximately a factor of two cost improvement over the base\npolicy for a range of radii bounded from below and above by two and three times\nthe critical sensing radius, respectively.",
                "Cooperative Control of Multi-Channel Linear Systems with Self-Organizing\n  Private Agents\nCooperative behavior design for multi-agent systems with collective tasks is\na critical issue in promoting swarm intelligence. This paper investigates\ncooperative control for a multi-channel system, where each channel is managed\nby an agent expected to self-organize a controller to stabilize the system\ncollaboratively by communicating with neighbors in a network. Integrating a\nstate decomposition technique and a fusion approach, a fully distributed\nprivacy-preserving mechanism is proposed to shield agents' private information\nfrom neighbors' eavesdropping. Moreover, the cost of introducing the\nprivacy-preserving mechanism and the benefit of adding more channels to the\nsystem are quantitatively analyzed. Finally, comparative simulation examples\nare provided to demonstrate the effectiveness of the theoretical results.",
                "Discounting in Strategy Logic\nDiscounting is an important dimension in multi-agent systems as long as we\nwant to reason about strategies and time. It is a key aspect in economics as it\ncaptures the intuition that the far-away future is not as important as the near\nfuture. Traditional verification techniques allow to check whether there is a\nwinning strategy for a group of agents but they do not take into account the\nfact that satisfying a goal sooner is different from satisfying it after a long\nwait. In this paper, we augment Strategy Logic with future discounting over a\nset of discounted functions D, denoted SLdisc[D]. We consider \"until\" operators\nwith discounting functions: the satisfaction value of a specification in\nSLdisc[D] is a value in [0, 1], where the longer it takes to fulfill\nrequirements, the smaller the satisfaction value is. We motivate our approach\nwith classical examples from Game Theory and study the complexity of\nmodel-checking SLdisc[D]-formulas.",
                "SPRING: Studying the Paper and Reasoning to Play Games\nOpen-world survival games pose significant challenges for AI algorithms due\nto their multi-tasking, deep exploration, and goal prioritization requirements.\nDespite reinforcement learning (RL) being popular for solving games, its high\nsample complexity limits its effectiveness in complex open-world games like\nCrafter or Minecraft. We propose a novel approach, SPRING, to read the game's\noriginal academic paper and use the knowledge learned to reason and play the\ngame through a large language model (LLM). Prompted with the LaTeX source as\ngame context and a description of the agent's current observation, our SPRING\nframework employs a directed acyclic graph (DAG) with game-related questions as\nnodes and dependencies as edges. We identify the optimal action to take in the\nenvironment by traversing the DAG and calculating LLM responses for each node\nin topological order, with the LLM's answer to final node directly translating\nto environment actions. In our experiments, we study the quality of in-context\n\"reasoning\" induced by different forms of prompts under the setting of the\nCrafter open-world environment. Our experiments suggest that LLMs, when\nprompted with consistent chain-of-thought, have great potential in completing\nsophisticated high-level trajectories. Quantitatively, SPRING with GPT-4\noutperforms all state-of-the-art RL baselines, trained for 1M steps, without\nany training. Finally, we show the potential of games as a test bed for LLMs.",
                "Local control for the collective dynamics of self-propelled particles\nUtilizing a paradigmatic model for the motion of interacting self-propelled\nparticles, we demonstrate that local accelerations at the level of individual\nparticles can drive transitions between different collective dynamics, leading\nto a control process. We find that the ability to trigger such transitions is\nhierarchically distributed among the particles and can form distinctive spatial\npatterns within the collective. Chaotic dynamics occur during the transitions,\nwhich can be attributed to fractal basin boundaries mediating the control\nprocess. The particle hierarchies described in this study offer decentralized\ncapabilities for controlling artificial swarms.",
                "A Fast Algorithm for Consistency Checking Partially Ordered Time\nPartially ordered models of time occur naturally in applications where agents\nor processes cannot perfectly communicate with each other, and can be traced\nback to the seminal work of Lamport. In this paper we consider the problem of\ndeciding if a (likely incomplete) description of a system of events is\nconsistent, the network consistency problem for the point algebra of partially\nordered time (POT). While the classical complexity of this problem has been\nfully settled, comparably little is known of the fine-grained complexity of POT\nexcept that it can be solved in $O^*((0.368n)^n)$ time by enumerating ordered\npartitions. We construct a much faster algorithm with a run-time bounded by\n$O^*((0.26n)^n)$. This is achieved by a sophisticated enumeration of structures\nsimilar to total orders, which are then greedily expanded toward a solution.\nWhile similar ideas have been explored earlier for related problems it turns\nout that the analysis for POT is non-trivial and requires significant new\nideas."
            ],
            "interesting paper": 1
        },
        {
            "papers": [
                "On Computing Universal Plans for Partially Observable Multi-Agent Path\n  Finding\nMulti-agent routing problems have drawn significant attention nowadays due to\ntheir broad industrial applications in, e.g., warehouse robots, logistics\nautomation, and traffic control. Conventionally, they are modelled as classical\nplanning problems. In this paper, we argue that it is beneficial to formulate\nthem as universal planning problems. We therefore propose universal plans, also\nknown as policies, as the solution concepts, and implement a system called\nASP-MAUPF (Answer Set Programming for Multi-Agent Universal Plan Finding) for\ncomputing them. Given an arbitrary two-dimensional map and a profile of goals\nfor the agents, the system finds a feasible universal plan for each agent that\nensures no collision with others. We use the system to conduct some\nexperiments, and make some observations on the types of goal profiles and\nenvironments that will have feasible policies, and how they may depend on\nagents' sensors. We also demonstrate how users can customize action preferences\nto compute more efficient policies, even (near-)optimal ones.",
                "Fine-Grained Complexity Analysis of Multi-Agent Path Finding on 2D Grids\nMulti-Agent Path Finding (MAPF) is a fundamental motion coordination problem\narising in multi-agent systems with a wide range of applications. The problem's\nintractability has led to extensive research on improving the scalability of\nsolvers for it. Since optimal solvers can struggle to scale, a major challenge\nthat arises is understanding what makes MAPF hard. We tackle this challenge\nthrough a fine-grained complexity analysis of time-optimal MAPF on 2D grids,\nthereby closing two gaps and identifying a new tractability frontier. First, we\nshow that 2-colored MAPF, i.e., where the agents are divided into two teams,\neach with its own set of targets, remains NP-hard. Second, for the flowtime\nobjective (also called sum-of-costs), we show that it remains NP-hard to find a\nsolution in which agents have an individually optimal cost, which we call an\nindividually optimal solution. The previously tightest results for these MAPF\nvariants are for (non-grid) planar graphs. We use a single hardness\nconstruction that replaces, strengthens, and unifies previous proofs. We\nbelieve that it is also simpler than previous proofs for the planar case as it\nemploys minimal gadgets that enable its full visualization in one figure.\nFinally, for the flowtime objective, we establish a tractability frontier based\non the number of directions agents can move in. Namely, we complement our\nhardness result, which holds for three directions, with an efficient algorithm\nfor finding an individually optimal solution if only two directions are\nallowed. This result sheds new light on the structure of optimal solutions,\nwhich may help guide algorithm design for the general problem.",
                "Individuality in Swarm Robots with the Case Study of Kilobots: Noise,\n  Bug, or Feature?\nInter-individual differences are studied in natural systems, such as fish,\nbees, and humans, as they contribute to the complexity of both individual and\ncollective behaviors. However, individuality in artificial systems, such as\nrobotic swarms, is undervalued or even overlooked. Agent-specific deviations\nfrom the norm in swarm robotics are usually understood as mere noise that can\nbe minimized, for example, by calibration. We observe that robots have\nconsistent deviations and argue that awareness and knowledge of these can be\nexploited to serve a task. We measure heterogeneity in robot swarms caused by\nindividual differences in how robots act, sense, and oscillate. Our use case is\nKilobots and we provide example behaviors where the performance of robots\nvaries depending on individual differences. We show a non-intuitive example of\nphototaxis with Kilobots where the non-calibrated Kilobots show better\nperformance than the calibrated supposedly ``ideal\" one. We measure the\ninter-individual variations for heterogeneity in sensing and oscillation, too.\nWe briefly discuss how these variations can enhance the complexity of\ncollective behaviors. We suggest that by recognizing and exploring this new\nperspective on individuality, and hence diversity, in robotic swarms, we can\ngain a deeper understanding of these systems and potentially unlock new\npossibilities for their design and implementation of applications.",
                "Almost Envy-Free Allocations of Indivisible Goods or Chores with\n  Entitlements\nWe here address the problem of fairly allocating indivisible goods or chores\nto $n$ agents with weights that define their entitlement to the set of\nindivisible resources. Stemming from well-studied fairness concepts such as\nenvy-freeness up to one good (EF1) and envy-freeness up to any good (EFX) for\nagents with equal entitlements, we present, in this study, the first set of\nimpossibility results alongside algorithmic guarantees for fairness among\nagents with unequal entitlements.\n  Within this paper, we expand the concept of envy-freeness up to any good or\nchore to the weighted context (WEFX and XWEF respectively), demonstrating that\nthese allocations are not guaranteed to exist for two or three agents. Despite\nthese negative results, we develop a WEFX procedure for two agents with integer\nweights, and furthermore, we devise an approximate WEFX procedure for two\nagents with normalized weights. We further present a polynomial-time algorithm\nthat guarantees a weighted envy-free allocation up to one chore (1WEF) for any\nnumber of agents with additive cost functions. Our work underscores the\nheightened complexity of the weighted fair division problem when compared to\nits unweighted counterpart.",
                "MULTIGAIN 2.0: MDP controller synthesis for multiple mean-payoff, LTL\n  and steady-state constraints\nWe present MULTIGAIN 2.0, a major extension to the controller synthesis tool\nMULTIGAIN, built on top of the probabilistic model checker PRISM. This new\nversion extends MULTIGAIN's multi-objective capabilities, by allowing for the\nformal verification and synthesis of controllers for probabilistic systems with\nmulti-dimensional long-run average reward structures, steady-state constraints,\nand linear temporal logic properties. Additionally, MULTIGAIN 2.0 can modify\nthe underlying linear program to prevent unbounded-memory and other unintuitive\nsolutions and visualizes Pareto curves, in the two- and three-dimensional\ncases, to facilitate trade-off analysis in multi-objective scenarios.",
                "Exploring the Adaptive Behaviors of Particle Lenia: A\n  Perturbation-Response Analysis for Computational Agency\nA firm cognitive subject or ``individual'' is presupposed for the emergence\nof mind. However, with the development of recent information technology, the\n``individual'' has become more dispersed in society and the cognitive subject\nhas become increasingly unstable and adaptive, necessitating an update in our\nunderstanding of the ``individual''. Autopoiesis serves as a model of the\ncognitive subject, which is unstable and requires effort to maintain itself to\nadapt to the environment. In this study, we evaluated adaptivity for a highly\nextensible multi-particle system model Particle Lenia through the response\nperturbation. As a result, we found that Particle Lenia has a particle\nconfiguration that is both temporally unstable and has multiple stable states.\nThis result suggests that Particle Lenia can express adaptive characteristics\nand is expected to be used as a computational model toward building an\nautopoietic cognitive agent."
            ],
            "interesting paper": 3
        },
        {
            "papers": [
                "MADiff: Offline Multi-agent Learning with Diffusion Models\nDiffusion model (DM) recently achieved huge success in various scenarios\nincluding offline reinforcement learning, where the diffusion planner learn to\ngenerate desired trajectories during online evaluations. However, despite the\neffectiveness in single-agent learning, it remains unclear how DMs can operate\nin multi-agent problems, where agents can hardly complete teamwork without good\ncoordination by independently modeling each agent's trajectories. In this\npaper, we propose MADiff, a novel generative multi-agent learning framework to\ntackle this problem. MADiff is realized with an attention-based diffusion model\nto model the complex coordination among behaviors of multiple agents. To the\nbest of our knowledge, MADiff is the first diffusion-based multi-agent learning\nframework, which behaves as both a decentralized policy and a centralized\ncontroller. During decentralized executions, MADiff simultaneously performs\nteammate modeling, and the centralized controller can also be applied in\nmulti-agent trajectory predictions. Our experiments show the superior\nperformance of MADiff compared to baseline algorithms in a wide range of\nmulti-agent learning tasks, which emphasizes the effectiveness of MADiff in\nmodeling complex multi-agent interactions. Our code is available at\nhttps://github.com/zbzhu99/madiff.",
                "Formal Modelling for Multi-Robot Systems Under Uncertainty\nPurpose of Review: To effectively synthesise and analyse multi-robot\nbehaviour, we require formal task-level models which accurately capture\nmulti-robot execution. In this paper, we review modelling formalisms for\nmulti-robot systems under uncertainty, and discuss how they can be used for\nplanning, reinforcement learning, model checking, and simulation.\n  Recent Findings: Recent work has investigated models which more accurately\ncapture multi-robot execution by considering different forms of uncertainty,\nsuch as temporal uncertainty and partial observability, and modelling the\neffects of robot interactions on action execution. Other strands of work have\npresented approaches for reducing the size of multi-robot models to admit more\nefficient solution methods. This can be achieved by decoupling the robots under\nindependence assumptions, or reasoning over higher level macro actions.\n  Summary: Existing multi-robot models demonstrate a trade off between\naccurately capturing robot dependencies and uncertainty, and being small enough\nto tractably solve real world problems. Therefore, future research should\nexploit realistic assumptions over multi-robot behaviour to develop smaller\nmodels which retain accurate representations of uncertainty and robot\ninteractions; and exploit the structure of multi-robot problems, such as\nfactored state spaces, to develop scalable solution methods.",
                "Is Centralized Training with Decentralized Execution Framework\n  Centralized Enough for MARL?\nCentralized Training with Decentralized Execution (CTDE) has recently emerged\nas a popular framework for cooperative Multi-Agent Reinforcement Learning\n(MARL), where agents can use additional global state information to guide\ntraining in a centralized way and make their own decisions only based on\ndecentralized local policies. Despite the encouraging results achieved, CTDE\nmakes an independence assumption on agent policies, which limits agents to\nadopt global cooperative information from each other during centralized\ntraining. Therefore, we argue that existing CTDE methods cannot fully utilize\nglobal information for training, leading to an inefficient joint-policy\nexploration and even suboptimal results. In this paper, we introduce a novel\nCentralized Advising and Decentralized Pruning (CADP) framework for multi-agent\nreinforcement learning, that not only enables an efficacious message exchange\namong agents during training but also guarantees the independent policies for\nexecution. Firstly, CADP endows agents the explicit communication channel to\nseek and take advices from different agents for more centralized training. To\nfurther ensure the decentralized execution, we propose a smooth model pruning\nmechanism to progressively constraint the agent communication into a closed one\nwithout degradation in agent cooperation capability. Empirical evaluations on\nStarCraft II micromanagement and Google Research Football benchmarks\ndemonstrate that the proposed framework achieves superior performance compared\nwith the state-of-the-art counterparts. Our code will be made publicly\navailable.",
                "A Model-Based Solution to the Offline Multi-Agent Reinforcement Learning\n  Coordination Problem\nTraining multiple agents to coordinate is an essential problem with\napplications in robotics, game theory, economics, and social sciences. However,\nmost existing Multi-Agent Reinforcement Learning (MARL) methods are online and\nthus impractical for real-world applications in which collecting new\ninteractions is costly or dangerous. While these algorithms should leverage\noffline data when available, doing so gives rise to what we call the offline\ncoordination problem. Specifically, we identify and formalize the strategy\nagreement (SA) and the strategy fine-tuning (SFT) coordination challenges, two\nissues at which current offline MARL algorithms fail. Concretely, we reveal\nthat the prevalent model-free methods are severely deficient and cannot handle\ncoordination-intensive offline multi-agent tasks in either toy or MuJoCo\ndomains. To address this setback, we emphasize the importance of inter-agent\ninteractions and propose the very first model-based offline MARL method. Our\nresulting algorithm, Model-based Offline Multi-Agent Proximal Policy\nOptimization (MOMA-PPO) generates synthetic interaction data and enables agents\nto converge on a strategy while fine-tuning their policies accordingly. This\nsimple model-based solution solves the coordination-intensive offline tasks,\nsignificantly outperforming the prevalent model-free methods even under severe\npartial observability and with learned world models.",
                "Attention Schema in Neural Agents\nAttention has become a common ingredient in deep learning architectures. It\nadds a dynamical selection of information on top of the static selection of\ninformation supported by weights. In the same way, we can imagine a\nhigher-order informational filter built on top of attention: an Attention\nSchema (AS), namely, a descriptive and predictive model of attention. In\ncognitive neuroscience, Attention Schema Theory (AST) supports this idea of\ndistinguishing attention from AS. A strong prediction of this theory is that an\nagent can use its own AS to also infer the states of other agents' attention\nand consequently enhance coordination with other agents. As such, multi-agent\nreinforcement learning would be an ideal setting to experimentally test the\nvalidity of AST. We explore different ways in which attention and AS interact\nwith each other. Our preliminary results indicate that agents that implement\nthe AS as a recurrent internal control achieve the best performance. In\ngeneral, these exploratory experiments suggest that equipping artificial agents\nwith a model of attention can enhance their social intelligence.",
                "Reinforcement Learning With Reward Machines in Stochastic Games\nWe investigate multi-agent reinforcement learning for stochastic games with\ncomplex tasks, where the reward functions are non-Markovian. We utilize reward\nmachines to incorporate high-level knowledge of complex tasks. We develop an\nalgorithm called Q-learning with reward machines for stochastic games (QRM-SG),\nto learn the best-response strategy at Nash equilibrium for each agent. In\nQRM-SG, we define the Q-function at a Nash equilibrium in augmented state\nspace. The augmented state space integrates the state of the stochastic game\nand the state of reward machines. Each agent learns the Q-functions of all\nagents in the system. We prove that Q-functions learned in QRM-SG converge to\nthe Q-functions at a Nash equilibrium if the stage game at each time step\nduring learning has a global optimum point or a saddle point, and the agents\nupdate Q-functions based on the best-response strategy at this point. We use\nthe Lemke-Howson method to derive the best-response strategy given current\nQ-functions. The three case studies show that QRM-SG can learn the\nbest-response strategies effectively. QRM-SG learns the best-response\nstrategies after around 7500 episodes in Case Study I, 1000 episodes in Case\nStudy II, and 1500 episodes in Case Study III, while baseline methods such as\nNash Q-learning and MADDPG fail to converge to the Nash equilibrium in all\nthree case studies."
            ],
            "interesting paper": 1
        },
        {
            "papers": [
                "On the Value of Myopic Behavior in Policy Reuse\nLeveraging learned strategies in unfamiliar scenarios is fundamental to human\nintelligence. In reinforcement learning, rationally reusing the policies\nacquired from other tasks or human experts is critical for tackling problems\nthat are difficult to learn from scratch. In this work, we present a framework\ncalled Selective Myopic bEhavior Control~(SMEC), which results from the insight\nthat the short-term behaviors of prior policies are sharable across tasks. By\nevaluating the behaviors of prior policies via a hybrid value function\narchitecture, SMEC adaptively aggregates the sharable short-term behaviors of\nprior policies and the long-term behaviors of the task policy, leading to\ncoordinated decisions. Empirical results on a collection of manipulation and\nlocomotion tasks demonstrate that SMEC outperforms existing methods, and\nvalidate the ability of SMEC to leverage related prior policies.",
                "Integrating Action Knowledge and LLMs for Task Planning and Situation\n  Handling in Open Worlds\nTask planning systems have been developed to help robots use human knowledge\n(about actions) to complete long-horizon tasks. Most of them have been\ndeveloped for \"closed worlds\" while assuming the robot is provided with\ncomplete world knowledge. However, the real world is generally open, and the\nrobots frequently encounter unforeseen situations that can potentially break\nthe planner's completeness. Could we leverage the recent advances on\npre-trained Large Language Models (LLMs) to enable classical planning systems\nto deal with novel situations?\n  This paper introduces a novel framework, called COWP, for open-world task\nplanning and situation handling. COWP dynamically augments the robot's action\nknowledge, including the preconditions and effects of actions, with\ntask-oriented commonsense knowledge. COWP embraces the openness from LLMs, and\nis grounded to specific domains via action knowledge. For systematic\nevaluations, we collected a dataset that includes 1,085 execution-time\nsituations. Each situation corresponds to a state instance wherein a robot is\npotentially unable to complete a task using a solution that normally works.\nExperimental results show that our approach outperforms competitive baselines\nfrom the literature in the success rate of service tasks. Additionally, we have\ndemonstrated COWP using a mobile manipulator. Supplementary materials are\navailable at: https://cowplanning.github.io/",
                "Probing reaction channels via reinforcement learning\nWe propose a reinforcement learning based method to identify important\nconfigurations that connect reactant and product states along chemical reaction\npaths. By shooting multiple trajectories from these configurations, we can\ngenerate an ensemble of configurations that concentrate on the transition path\nensemble. This configuration ensemble can be effectively employed in a neural\nnetwork-based partial differential equation solver to obtain an approximation\nsolution of a restricted Backward Kolmogorov equation, even when the dimension\nof the problem is very high. The resulting solution, known as the committor\nfunction, encodes mechanistic information for the reaction and can in turn be\nused to evaluate reaction rates.",
                "Observation Denoising in CYRUS Soccer Simulation 2D Team For RoboCup\n  2023\nThe RoboCup competitions hold various leagues, and the Soccer Simulation 2D\nLeague is a major one among them. Soccer Simulation 2D (SS2D) match involves\ntwo teams, including 11 players and a coach, competing against each other. The\nplayers can only communicate with the Soccer Simulation Server during the game.\nThis paper presents the latest research of the CYRUS soccer simulation 2D team,\nthe champion of RoboCup 2021. We will explain our denoising idea powered by\nlong short-term memory networks (LSTM) and deep neural networks (DNN). The\nCYRUS team uses the CYRUS2D base code that was developed based on the Helios\nand Gliders bases.",
                "Optimization's Neglected Normative Commitments\nOptimization is offered as an objective approach to resolving complex,\nreal-world decisions involving uncertainty and conflicting interests. It drives\nbusiness strategies as well as public policies and, increasingly, lies at the\nheart of sophisticated machine learning systems. A paradigm used to approach\npotentially high-stakes decisions, optimization relies on abstracting the real\nworld to a set of decision(s), objective(s) and constraint(s). Drawing from the\nmodeling process and a range of actual cases, this paper describes the\nnormative choices and assumptions that are necessarily part of using\noptimization. It then identifies six emergent problems that may be neglected:\n1) Misspecified values can yield optimizations that omit certain imperatives\naltogether or incorporate them incorrectly as a constraint or as part of the\nobjective, 2) Problematic decision boundaries can lead to faulty modularity\nassumptions and feedback loops, 3) Failing to account for multiple agents'\ndivergent goals and decisions can lead to policies that serve only certain\nnarrow interests, 4) Mislabeling and mismeasurement can introduce bias and\nimprecision, 5) Faulty use of relaxation and approximation methods,\nunaccompanied by formal characterizations and guarantees, can severely impede\napplicability, and 6) Treating optimization as a justification for action,\nwithout specifying the necessary contextual information, can lead to ethically\ndubious or faulty decisions. Suggestions are given to further understand and\ncurb the harms that can arise when optimization is used wrongfully.",
                "Assumption Generation for the Verification of Learning-Enabled\n  Autonomous Systems\nProviding safety guarantees for autonomous systems is difficult as these\nsystems operate in complex environments that require the use of\nlearning-enabled components, such as deep neural networks (DNNs) for visual\nperception. DNNs are hard to analyze due to their size (they can have thousands\nor millions of parameters), lack of formal specifications (DNNs are typically\nlearnt from labeled data, in the absence of any formal requirements), and\nsensitivity to small changes in the environment. We present an assume-guarantee\nstyle compositional approach for the formal verification of system-level safety\nproperties of such autonomous systems. Our insight is that we can analyze the\nsystem in the absence of the DNN perception components by automatically\nsynthesizing assumptions on the DNN behaviour that guarantee the satisfaction\nof the required safety properties. The synthesized assumptions are the weakest\nin the sense that they characterize the output sequences of all the possible\nDNNs that, plugged into the autonomous system, guarantee the required safety\nproperties. The assumptions can be leveraged as run-time monitors over a\ndeployed DNN to guarantee the safety of the overall system; they can also be\nmined to extract local specifications for use during training and testing of\nDNNs. We illustrate our approach on a case study taken from the autonomous\nairplanes domain that uses a complex DNN for perception."
            ],
            "interesting paper": 2
        },
        {
            "papers": [
                "The Computational Complexity of Single-Player Imperfect-Recall Games\nWe study single-player extensive-form games with imperfect recall, such as\nthe Sleeping Beauty problem or the Absentminded Driver game. For such games,\ntwo natural equilibrium concepts have been proposed as alternative solution\nconcepts to ex-ante optimality. One equilibrium concept uses generalized double\nhalving (GDH) as a belief system and evidential decision theory (EDT), and\nanother one uses generalized thirding (GT) as a belief system and causal\ndecision theory (CDT). Our findings relate those three solution concepts of a\ngame to solution concepts of a polynomial maximization problem: global optima,\noptimal points with respect to subsets of variables and Karush-Kuhn-Tucker\n(KKT) points. Based on these correspondences, we are able to settle various\ncomplexity-theoretic questions on the computation of such strategies. For\nex-ante optimality and (EDT,GDH)-equilibria, we obtain NP-hardness and\ninapproximability, and for (CDT,GT)-equilibria we obtain CLS-completeness\nresults.",
                "Action valuation of on- and off-ball soccer players based on multi-agent\n  deep reinforcement learning\nAnalysis of invasive sports such as soccer is challenging because the game\nsituation changes continuously in time and space, and multiple agents\nindividually recognize the game situation and make decisions. Previous studies\nusing deep reinforcement learning have often considered teams as a single agent\nand valued the teams and players who hold the ball in each discrete event. Then\nit was challenging to value the actions of multiple players, including players\nfar from the ball, in a spatiotemporally continuous state space. In this paper,\nwe propose a method of valuing possible actions for on- and off-ball soccer\nplayers in a single holistic framework based on multi-agent deep reinforcement\nlearning. We consider a discrete action space in a continuous state space that\nmimics that of Google research football and leverages supervised learning for\nactions in reinforcement learning. In the experiment, we analyzed the\nrelationships with conventional indicators, season goals, and game ratings by\nexperts, and showed the effectiveness of the proposed method. Our approach can\nassess how multiple players move continuously throughout the game, which is\ndifficult to be discretized or labeled but vital for teamwork, scouting, and\nfan engagement.",
                "The Leximin Approach for a Sequence of Collective Decisions\nIn many situations, several agents need to make a sequence of decisions. For\nexample, a group of workers that needs to decide where their weekly meeting\nshould take place. In such situations, a decision-making mechanism must\nconsider fairness notions. In this paper, we analyze the fairness of three\nknown mechanisms: round-robin, maximum Nash welfare, and leximin. We consider\nboth offline and online settings, and concentrate on the fairness notion of\nproportionality and its relaxations. Specifically, in the offline setting, we\nshow that the three mechanisms fail to find a proportional or\napproximate-proportional outcome, even if such an outcome exists. We thus\nintroduce a new fairness property that captures this requirement, and show that\na variant of the leximin mechanism satisfies the new fairness property. In the\nonline setting, we show that it is impossible to guarantee proportionality or\nits relaxations. We thus consider a natural restriction on the agents'\npreferences, and show that the leximin mechanism guarantees the best possible\nadditive approximation to proportionality and satisfies all the relaxations of\nproportionality.",
                "Taming AI Bots: Controllability of Neural States in Large Language\n  Models\nWe tackle the question of whether an agent can, by suitable choice of\nprompts, control an AI bot to any state. To that end, we first introduce a\nformal definition of ``meaning'' that is amenable to analysis. Then, we\ncharacterize ``meaningful data'' on which large language models (LLMs) are\nostensibly trained, and ``well-trained LLMs'' through conditions that are\nlargely met by today's LLMs. While a well-trained LLM constructs an embedding\nspace of meanings that is Euclidean, meanings themselves do not form a vector\n(linear) subspace, but rather a quotient space within. We then characterize the\nsubset of meanings that can be reached by the state of the LLMs for some input\nprompt, and show that a well-trained bot can reach any meaning albeit with\nsmall probability. We then introduce a stronger notion of controllability as\n{\\em almost certain reachability}, and show that, when restricted to the space\nof meanings, an AI bot is controllable. We do so after introducing a functional\ncharacterization of attentive AI bots, and finally derive necessary and\nsufficient conditions for controllability. The fact that AI bots are\ncontrollable means that an adversary could steer them towards any state.\nHowever, the sampling process can be designed to counteract adverse actions and\navoid reaching undesirable regions of state space before their boundary is\ncrossed.",
                "Diffusion Model is an Effective Planner and Data Synthesizer for\n  Multi-Task Reinforcement Learning\nDiffusion models have demonstrated highly-expressive generative capabilities\nin vision and NLP. Recent studies in reinforcement learning (RL) have shown\nthat diffusion models are also powerful in modeling complex policies or\ntrajectories in offline datasets. However, these works have been limited to\nsingle-task settings where a generalist agent capable of addressing multi-task\npredicaments is absent. In this paper, we aim to investigate the effectiveness\nof a single diffusion model in modeling large-scale multi-task offline data,\nwhich can be challenging due to diverse and multimodal data distribution.\nSpecifically, we propose Multi-Task Diffusion Model (\\textsc{MTDiff}), a\ndiffusion-based method that incorporates Transformer backbones and prompt\nlearning for generative planning and data synthesis in multi-task offline\nsettings. \\textsc{MTDiff} leverages vast amounts of knowledge available in\nmulti-task data and performs implicit knowledge sharing among tasks. For\ngenerative planning, we find \\textsc{MTDiff} outperforms state-of-the-art\nalgorithms across 50 tasks on Meta-World and 8 maps on Maze2D. For data\nsynthesis, \\textsc{MTDiff} generates high-quality data for testing tasks given\na single demonstration as a prompt, which enhances the low-quality datasets for\neven unseen tasks.",
                "Safety of autonomous vehicles: A survey on Model-based vs. AI-based\n  approaches\nThe growing advancements in Autonomous Vehicles (AVs) have emphasized the\ncritical need to prioritize the absolute safety of AV maneuvers, especially in\ndynamic and unpredictable environments or situations. This objective becomes\neven more challenging due to the uniqueness of every traffic\nsituation/condition. To cope with all these very constrained and complex\nconfigurations, AVs must have appropriate control architectures with reliable\nand real-time Risk Assessment and Management Strategies (RAMS). These targeted\nRAMS must lead to reduce drastically the navigation risks. However, the lack of\nsafety guarantees proves, which is one of the key challenges to be addressed,\nlimit drastically the ambition to introduce more broadly AVs on our roads and\nrestrict the use of AVs to very limited use cases. Therefore, the focus and the\nambition of this paper is to survey research on autonomous vehicles while\nfocusing on the important topic of safety guarantee of AVs. For this purpose,\nit is proposed to review research on relevant methods and concepts defining an\noverall control architecture for AVs, with an emphasis on the safety assessment\nand decision-making systems composing these architectures. Moreover, it is\nintended through this reviewing process to highlight researches that use either\nmodel-based methods or AI-based approaches. This is performed while emphasizing\nthe strengths and weaknesses of each methodology and investigating the research\nthat proposes a comprehensive multi-modal design that combines model-based and\nAI approaches. This paper ends with discussions on the methods used to\nguarantee the safety of AVs namely: safety verification techniques and the\nstandardization/generalization of safety frameworks."
            ],
            "interesting paper": 2
        },
        {
            "papers": [
                "Towards a Unifying Model of Rationality in Multiagent Systems\nMultiagent systems deployed in the real world need to cooperate with other\nagents (including humans) nearly as effectively as these agents cooperate with\none another. To design such AI, and provide guarantees of its effectiveness, we\nneed to clearly specify what types of agents our AI must be able to cooperate\nwith. In this work we propose a generic model of socially intelligent agents,\nwhich are individually rational learners that are also able to cooperate with\none another (in the sense that their joint behavior is Pareto efficient). We\ndefine rationality in terms of the regret incurred by each agent over its\nlifetime, and show how we can construct socially intelligent agents for\ndifferent forms of regret. We then discuss the implications of this model for\nthe development of \"robust\" MAS that can cooperate with a wide variety of\nsocially intelligent agents.",
                "Steering control of payoff-maximizing players in adaptive learning\n  dynamics\nEvolutionary game theory provides a mathematical foundation for\ncross-disciplinary fertilization, especially for integrating ideas from\nartificial intelligence and game theory. Such integration offers a transparent\nand rigorous approach to complex decision-making problems in a variety of\nimportant contexts, ranging from evolutionary computation to machine behavior.\nDespite the astronomically huge individual behavioral strategy space for\ninteractions in the iterated Prisoner's Dilemma (IPD) games, the so-called\nZero-Determinant (ZD) strategies is a set of rather simple memory-one\nstrategies yet can unilaterally set a linear payoff relationship between\nthemselves and their opponent. Although the witting of ZD strategies gives\nplayers an upper hand in the IPD games, we find and characterize unbending\nstrategies that can force ZD players to be fair in their own interest.\nMoreover, our analysis reveals the ubiquity of unbending properties in common\nIPD strategies which are previously overlooked. In this work, we demonstrate\nthe important steering role of unbending strategies in fostering fairness and\ncooperation in pairwise interactions. Our results will help bring a new\nperspective by means of combining game theory and multi-agent learning systems\nfor optimizing winning strategies that are robust to noises, errors, and\ndeceptions in non-zero-sum games.",
                "The Alternating-Time \u03bc-Calculus With Disjunctive Explicit Strategies\nAlternating-time temporal logic (ATL) and its extensions, including the\nalternating-time $\\mu$-calculus (AMC), serve the specification of the strategic\nabilities of coalitions of agents in concurrent game structures. The key\ningredient of the logic are path quantifiers specifying that some coalition of\nagents has a joint strategy to enforce a given goal. This basic setup has been\nextended to let some of the agents (revocably) commit to using certain named\nstrategies, as in ATL with explicit strategies (ATLES). In the present work, we\nextend ATLES with fixpoint operators and strategy disjunction, arriving at the\nalternating-time $\\mu$-calculus with disjunctive explicit strategies (AMCDES),\nwhich allows for a more flexible formulation of temporal properties (e.g.\nfairness) and, through strategy disjunction, a form of controlled\nnondeterminism in commitments. Our main result is an ExpTime upper bound for\nsatisfiability checking (which is thus ExpTime-complete). We also prove upper\nbounds QP (quasipolynomial time) and NP $\\cap$ coNP for model checking under\nfixed interpretations of explicit strategies, and NP under open interpretation.\nOur key technical tool is a treatment of the AMCDES within the generic\nframework of coalgebraic logic, which in particular reduces the analysis of\nmost reasoning tasks to the treatment of a very simple one-step logic featuring\nonly propositional operators and next-step operators without nesting; we give a\nnew model construction principle for this one-step logic that relies on a\nset-valued variant of first-order resolution.",
                "Split Federated Learning: Speed up Model Training in Resource-Limited\n  Wireless Networks\nIn this paper, we propose a novel distributed learning scheme, named\ngroup-based split federated learning (GSFL), to speed up artificial\nintelligence (AI) model training. Specifically, the GSFL operates in a\nsplit-then-federated manner, which consists of three steps: 1) Model\ndistribution, in which the access point (AP) splits the AI models and\ndistributes the client-side models to clients; 2) Model training, in which each\nclient executes forward propagation and transmit the smashed data to the edge\nserver. The edge server executes forward and backward propagation and then\nreturns the gradient to the clients for updating local client-side models; and\n3) Model aggregation, in which edge servers aggregate the server-side and\nclient-side models. Simulation results show that the GSFL outperforms vanilla\nsplit learning and federated learning schemes in terms of overall training\nlatency while achieving satisfactory accuracy.",
                "Subequivariant Graph Reinforcement Learning in 3D Environments\nLearning a shared policy that guides the locomotion of different agents is of\ncore interest in Reinforcement Learning (RL), which leads to the study of\nmorphology-agnostic RL. However, existing benchmarks are highly restrictive in\nthe choice of starting point and target point, constraining the movement of the\nagents within 2D space. In this work, we propose a novel setup for\nmorphology-agnostic RL, dubbed Subequivariant Graph RL in 3D environments\n(3D-SGRL). Specifically, we first introduce a new set of more practical yet\nchallenging benchmarks in 3D space that allows the agent to have full\nDegree-of-Freedoms to explore in arbitrary directions starting from arbitrary\nconfigurations. Moreover, to optimize the policy over the enlarged state-action\nspace, we propose to inject geometric symmetry, i.e., subequivariance, into the\nmodeling of the policy and Q-function such that the policy can generalize to\nall directions, improving exploration efficiency. This goal is achieved by a\nnovel SubEquivariant Transformer (SET) that permits expressive message\nexchange. Finally, we evaluate the proposed method on the proposed benchmarks,\nwhere our method consistently and significantly outperforms existing approaches\non single-task, multi-task, and zero-shot generalization scenarios. Extensive\nablations are also conducted to verify our design. Code and videos are\navailable on our project page: https://alpc91.github.io/SGRL/.",
                "Partially Personalized Federated Learning: Breaking the Curse of Data\n  Heterogeneity\nWe present a partially personalized formulation of Federated Learning (FL)\nthat strikes a balance between the flexibility of personalization and\ncooperativeness of global training. In our framework, we split the variables\ninto global parameters, which are shared across all clients, and individual\nlocal parameters, which are kept private. We prove that under the right split\nof parameters, it is possible to find global parameters that allow each client\nto fit their data perfectly, and refer to the obtained problem as\noverpersonalized. For instance, the shared global parameters can be used to\nlearn good data representations, whereas the personalized layers are fine-tuned\nfor a specific client. Moreover, we present a simple algorithm for the\npartially personalized formulation that offers significant benefits to all\nclients. In particular, it breaks the curse of data heterogeneity in several\nsettings, such as training with local steps, asynchronous training, and\nByzantine-robust training."
            ],
            "interesting paper": 2
        },
        {
            "papers": [
                "Intent-aligned AI systems deplete human agency: the need for agency\n  foundations research in AI safety\nThe rapid advancement of artificial intelligence (AI) systems suggests that\nartificial general intelligence (AGI) systems may soon arrive. Many researchers\nare concerned that AIs and AGIs will harm humans via intentional misuse\n(AI-misuse) or through accidents (AI-accidents). In respect of AI-accidents,\nthere is an increasing effort focused on developing algorithms and paradigms\nthat ensure AI systems are aligned to what humans intend, e.g. AI systems that\nyield actions or recommendations that humans might judge as consistent with\ntheir intentions and goals. Here we argue that alignment to human intent is\ninsufficient for safe AI systems and that preservation of long-term agency of\nhumans may be a more robust standard, and one that needs to be separated\nexplicitly and a priori during optimization. We argue that AI systems can\nreshape human intention and discuss the lack of biological and psychological\nmechanisms that protect humans from loss of agency. We provide the first formal\ndefinition of agency-preserving AI-human interactions which focuses on\nforward-looking agency evaluations and argue that AI systems - not humans -\nmust be increasingly tasked with making these evaluations. We show how agency\nloss can occur in simple environments containing embedded agents that use\ntemporal-difference learning to make action recommendations. Finally, we\npropose a new area of research called \"agency foundations\" and pose four\ninitial topics designed to improve our understanding of agency in AI-human\ninteractions: benevolent game theory, algorithmic foundations of human rights,\nmechanistic interpretability of agency representation in neural-networks and\nreinforcement learning from internal states.",
                "DHRL-FNMR: An Intelligent Multicast Routing Approach Based on Deep\n  Hierarchical Reinforcement Learning in SDN\nThe optimal multicast tree problem in the Software-Defined Networking (SDN)\nmulticast routing is an NP-hard combinatorial optimization problem. Although\nexisting SDN intelligent solution methods, which are based on deep\nreinforcement learning, can dynamically adapt to complex network link state\nchanges, these methods are plagued by problems such as redundant branches,\nlarge action space, and slow agent convergence. In this paper, an SDN\nintelligent multicast routing algorithm based on deep hierarchical\nreinforcement learning is proposed to circumvent the aforementioned problems.\nFirst, the multicast tree construction problem is decomposed into two\nsub-problems: the fork node selection problem and the construction of the\noptimal path from the fork node to the destination node. Second, based on the\ninformation characteristics of SDN global network perception, the multicast\ntree state matrix, link bandwidth matrix, link delay matrix, link packet loss\nrate matrix, and sub-goal matrix are designed as the state space of intrinsic\nand meta controllers. Then, in order to mitigate the excessive action space,\nour approach constructs different action spaces at the upper and lower levels.\nThe meta-controller generates an action space using network nodes to select the\nfork node, and the intrinsic controller uses the adjacent edges of the current\nnode as its action space, thus implementing four different action selection\nstrategies in the construction of the multicast tree. To facilitate the\nintelligent agent in constructing the optimal multicast tree with greater\nspeed, we developed alternative reward strategies that distinguish between\nsingle-step node actions and multi-step actions towards multiple destination\nnodes.",
                "Best of Both Distortion Worlds\nWe study the problem of designing voting rules that take as input the ordinal\npreferences of $n$ agents over a set of $m$ alternatives and output a single\nalternative, aiming to optimize the overall happiness of the agents. The input\nto the voting rule is each agent's ranking of the alternatives from most to\nleast preferred, yet the agents have more refined (cardinal) preferences that\ncapture the intensity with which they prefer one alternative over another. To\nquantify the extent to which voting rules can optimize over the cardinal\npreferences given access only to the ordinal ones, prior work has used the\ndistortion measure, i.e., the worst-case approximation ratio between a voting\nrule's performance and the best performance achievable given the cardinal\npreferences.\n  The work on the distortion of voting rules has been largely divided into two\nworlds: utilitarian distortion and metric distortion. In the former, the\ncardinal preferences of the agents correspond to general utilities and the goal\nis to maximize a normalized social welfare. In the latter, the agents' cardinal\npreferences correspond to costs given by distances in an underlying metric\nspace and the goal is to minimize the (unnormalized) social cost. Several\ndeterministic and randomized voting rules have been proposed and evaluated for\neach of these worlds separately, gradually improving the achievable distortion\nbounds, but none of the known voting rules perform well in both worlds\nsimultaneously.\n  In this work, we prove that one can achieve the best of both worlds by\ndesigning new voting rules, that simultaneously achieve near-optimal distortion\nguarantees in both distortion worlds. We also prove that this positive result\ndoes not generalize to the case where the voting rule is provided with the\nrankings of only the top-$t$ alternatives of each agent, for $t<m$.",
                "Strategic Reasoning with Language Models\nStrategic reasoning enables agents to cooperate, communicate, and compete\nwith other agents in diverse situations. Existing approaches to solving\nstrategic games rely on extensive training, yielding strategies that do not\ngeneralize to new scenarios or games without retraining. Large Language Models\n(LLMs), with their ability to comprehend and generate complex, context-rich\nlanguage, could prove powerful as tools for strategic gameplay. This paper\nintroduces an approach that uses pretrained LLMs with few-shot chain-of-thought\nexamples to enable strategic reasoning for AI agents. Our approach uses\nsystematically generated demonstrations of reasoning about states, values, and\nbeliefs to prompt the model. Using extensive variations of simple matrix games,\nwe show that strategies that are derived based on systematically generated\nprompts generalize almost perfectly to new game structures, alternate\nobjectives, and hidden information. Additionally, we demonstrate our approach\ncan lead to human-like negotiation strategies in realistic scenarios without\nany extra training or fine-tuning. Our results highlight the ability of LLMs,\nguided by systematic reasoning demonstrations, to adapt and excel in diverse\nstrategic scenarios.",
                "Bigger, Better, Faster: Human-level Atari with human-level efficiency\nWe introduce a value-based RL agent, which we call BBF, that achieves\nsuper-human performance in the Atari 100K benchmark. BBF relies on scaling the\nneural networks used for value estimation, as well as a number of other design\nchoices that enable this scaling in a sample-efficient manner. We conduct\nextensive analyses of these design choices and provide insights for future\nwork. We end with a discussion about updating the goalposts for\nsample-efficient RL research on the ALE. We make our code and data publicly\navailable at\nhttps://github.com/google-research/google-research/tree/master/bigger_better_faster.",
                "NetHack is Hard to Hack\nNeural policy learning methods have achieved remarkable results in various\ncontrol problems, ranging from Atari games to simulated locomotion. However,\nthese methods struggle in long-horizon tasks, especially in open-ended\nenvironments with multi-modal observations, such as the popular dungeon-crawler\ngame, NetHack. Intriguingly, the NeurIPS 2021 NetHack Challenge revealed that\nsymbolic agents outperformed neural approaches by over four times in median\ngame score. In this paper, we delve into the reasons behind this performance\ngap and present an extensive study on neural policy learning for NetHack. To\nconduct this study, we analyze the winning symbolic agent, extending its\ncodebase to track internal strategy selection in order to generate one of the\nlargest available demonstration datasets. Utilizing this dataset, we examine\n(i) the advantages of an action hierarchy; (ii) enhancements in neural\narchitecture; and (iii) the integration of reinforcement learning with\nimitation learning. Our investigations produce a state-of-the-art neural agent\nthat surpasses previous fully neural policies by 127% in offline settings and\n25% in online settings on median game score. However, we also demonstrate that\nmere scaling is insufficient to bridge the performance gap with the best\nsymbolic models or even the top human players."
            ],
            "interesting paper": 1
        },
        {
            "papers": [
                "EMOTE: An Explainable architecture for Modelling the Other Through\n  Empathy\nWe can usually assume others have goals analogous to our own. This assumption\ncan also, at times, be applied to multi-agent games - e.g. Agent 1's attraction\nto green pellets is analogous to Agent 2's attraction to red pellets. This\n\"analogy\" assumption is tied closely to the cognitive process known as empathy.\nInspired by empathy, we design a simple and explainable architecture to model\nanother agent's action-value function. This involves learning an \"Imagination\nNetwork\" to transform the other agent's observed state in order to produce a\nhuman-interpretable \"empathetic state\" which, when presented to the learning\nagent, produces behaviours that mimic the other agent. Our approach is\napplicable to multi-agent scenarios consisting of a single learning agent and\nother (independent) agents acting according to fixed policies. This\narchitecture is particularly beneficial for (but not limited to) algorithms\nusing a composite value or reward function. We show our method produces better\nperformance in multi-agent games, where it robustly estimates the other's model\nin different environment configurations. Additionally, we show that the\nempathetic states are human interpretable, and thus verifiable.",
                "Environmental Memory Boosts Group Formation of Clueless Individuals\nThe formation of groups of interacting individuals improves performance and\nfitness in many decentralised systems, from micro-organisms to social insects,\nfrom robotic swarms to artificial intelligence algorithms. Often, group\nformation and high-level coordination in these systems emerge from individuals\nwith limited information-processing capabilities implementing low-level rules\nof communication to signal to each other. Here, we show that, even in a\ncommunity of clueless individuals incapable of processing information and\ncommunicating, a dynamic environment can coordinate group formation by\ntransiently storing memory of the earlier passage of individuals. Our results\nidentify a new mechanism of indirect coordination via shared memory that is\nprimarily promoted and reinforced by dynamic environmental factors, thus\novershadowing the need for any form of explicit signalling between individuals.\nWe expect this pathway to group formation to be relevant for understanding and\ncontrolling self-organisation and collective decision making in both living and\nartificial active matter in real-life environments.",
                "Human Control: Definitions and Algorithms\nHow can humans stay in control of advanced artificial intelligence systems?\nOne proposal is corrigibility, which requires the agent to follow the\ninstructions of a human overseer, without inappropriately influencing them. In\nthis paper, we formally define a variant of corrigibility called shutdown\ninstructability, and show that it implies appropriate shutdown behavior,\nretention of human autonomy, and avoidance of user harm. We also analyse the\nrelated concepts of non-obstruction and shutdown alignment, three previously\nproposed algorithms for human control, and one new algorithm.",
                "Provably Efficient Generalized Lagrangian Policy Optimization for Safe\n  Multi-Agent Reinforcement Learning\nWe examine online safe multi-agent reinforcement learning using constrained\nMarkov games in which agents compete by maximizing their expected total rewards\nunder a constraint on expected total utilities. Our focus is confined to an\nepisodic two-player zero-sum constrained Markov game with independent\ntransition functions that are unknown to agents, adversarial reward functions,\nand stochastic utility functions. For such a Markov game, we employ an approach\nbased on the occupancy measure to formulate it as an online constrained\nsaddle-point problem with an explicit constraint. We extend the Lagrange\nmultiplier method in constrained optimization to handle the constraint by\ncreating a generalized Lagrangian with minimax decision primal variables and a\ndual variable. Next, we develop an upper confidence reinforcement learning\nalgorithm to solve this Lagrangian problem while balancing exploration and\nexploitation. Our algorithm updates the minimax decision primal variables via\nonline mirror descent and the dual variable via projected gradient step and we\nprove that it enjoys sublinear rate $ O((|X|+|Y|) L \\sqrt{T(|A|+|B|)}))$ for\nboth regret and constraint violation after playing $T$ episodes of the game.\nHere, $L$ is the horizon of each episode, $(|X|,|A|)$ and $(|Y|,|B|)$ are the\nstate/action space sizes of the min-player and the max-player, respectively. To\nthe best of our knowledge, we provide the first provably efficient online safe\nreinforcement learning algorithm in constrained Markov games.",
                "Speaking the Language of Your Listener: Audience-Aware Adaptation via\n  Plug-and-Play Theory of Mind\nDialogue participants may have varying levels of knowledge about the topic\nunder discussion. In such cases, it is essential for speakers to adapt their\nutterances by taking their audience into account. Yet, it is an open question\nhow such adaptation can be modelled in computational agents. In this paper, we\nmodel a visually grounded referential game between a knowledgeable speaker and\na listener with more limited visual and linguistic experience. Inspired by\npsycholinguistic theories, we endow our speaker with the ability to adapt its\nreferring expressions via a simulation module that monitors the effectiveness\nof planned utterances from the listener's perspective. We propose an adaptation\nmechanism building on plug-and-play approaches to controlled language\ngeneration, where utterance generation is steered on the fly by the simulator\nwithout finetuning the speaker's underlying language model. Our results and\nanalyses show that our approach is effective: the speaker's utterances become\ncloser to the listener's domain of expertise, which leads to higher\ncommunicative success.",
                "Latent Exploration for Reinforcement Learning\nIn Reinforcement Learning, agents learn policies by exploring and interacting\nwith the environment. Due to the curse of dimensionality, learning policies\nthat map high-dimensional sensory input to motor output is particularly\nchallenging. During training, state of the art methods (SAC, PPO, etc.) explore\nthe environment by perturbing the actuation with independent Gaussian noise.\nWhile this unstructured exploration has proven successful in numerous tasks, it\ncan be suboptimal for overactuated systems. When multiple actuators, such as\nmotors or muscles, drive behavior, uncorrelated perturbations risk diminishing\neach other's effect, or modifying the behavior in a task-irrelevant way. While\nsolutions to introduce time correlation across action perturbations exist,\nintroducing correlation across actuators has been largely ignored. Here, we\npropose LATent TIme-Correlated Exploration (Lattice), a method to inject\ntemporally-correlated noise into the latent state of the policy network, which\ncan be seamlessly integrated with on- and off-policy algorithms. We demonstrate\nthat the noisy actions generated by perturbing the network's activations can be\nmodeled as a multivariate Gaussian distribution with a full covariance matrix.\nIn the PyBullet locomotion tasks, Lattice-SAC achieves state of the art\nresults, and reaches 18% higher reward than unstructured exploration in the\nHumanoid environment. In the musculoskeletal control environments of MyoSuite,\nLattice-PPO achieves higher reward in most reaching and object manipulation\ntasks, while also finding more energy-efficient policies with reductions of\n20-60%. Overall, we demonstrate the effectiveness of structured action noise in\ntime and actuator space for complex motor control tasks. The code is available\nat: https://github.com/amathislab/lattice."
            ],
            "interesting paper": 4
        },
        {
            "papers": [
                "On the Existence of Information Bottlenecks in Living and Non-Living\n  Systems\nIn many complex systems, we observe that `interesting behaviour' is often the\nconsequence of a system exploiting the existence of an Information Bottleneck\n(IB). These bottlenecks can occur at different scales, between individuals or\ncomponents of a system, and sometimes within individuals themselves.\nOftentimes, we regard these bottlenecks negatively; as merely the limitations\nof the individual's physiology and something that ought to be overcome when\ndesigning and implementing artificial systems. However, we suggest instead that\nIBs may serve a purpose beyond merely providing a minimally-viable channel for\ncoordination in collective systems. More specifically, we suggest that\ninteresting or novel behaviour occurs when the individuals in a system are\nconstrained or limited in their ability to share information and must discover\nnovel ways to exploit existing mechanisms, which are inherently bottlenecked,\nrather than circumventing or otherwise avoiding those mechanisms entirely.",
                "An Architecture for Deploying Reinforcement Learning in Industrial\n  Environments\nIndustry 4.0 is driven by demands like shorter time-to-market, mass\ncustomization of products, and batch size one production. Reinforcement\nLearning (RL), a machine learning paradigm shown to possess a great potential\nin improving and surpassing human level performance in numerous complex tasks,\nallows coping with the mentioned demands. In this paper, we present an OPC UA\nbased Operational Technology (OT)-aware RL architecture, which extends the\nstandard RL setting, combining it with the setting of digital twins. Moreover,\nwe define an OPC UA information model allowing for a generalized plug-and-play\nlike approach for exchanging the RL agent used. In conclusion, we demonstrate\nand evaluate the architecture, by creating a proof of concept. By means of\nsolving a toy example, we show that this architecture can be used to determine\nthe optimal policy using a real control system.",
                "Federated Learning Games for Reconfigurable Intelligent Surfaces via\n  Causal Representations\nIn this paper, we investigate the problem of robust Reconfigurable\nIntelligent Surface (RIS) phase-shifts configuration over heterogeneous\ncommunication environments. The problem is formulated as a distributed learning\nproblem over different environments in a Federated Learning (FL) setting.\nEquivalently, this corresponds to a game played between multiple RISs, as\nlearning agents, in heterogeneous environments. Using Invariant Risk\nMinimization (IRM) and its FL equivalent, dubbed FL Games, we solve the RIS\nconfiguration problem by learning invariant causal representations across\nmultiple environments and then predicting the phases. The solution corresponds\nto playing according to Best Response Dynamics (BRD) which yields the Nash\nEquilibrium of the FL game. The representation learner and the phase predictor\nare modeled by two neural networks, and their performance is validated via\nsimulations against other benchmarks from the literature. Our results show that\ncausality-based learning yields a predictor that is 15% more accurate in unseen\nOut-of-Distribution (OoD) environments.",
                "Multi-Robot Path Planning Combining Heuristics and Multi-Agent\n  Reinforcement Learning\nMulti-robot path finding in dynamic environments is a highly challenging\nclassic problem. In the movement process, robots need to avoid collisions with\nother moving robots while minimizing their travel distance. Previous methods\nfor this problem either continuously replan paths using heuristic search\nmethods to avoid conflicts or choose appropriate collision avoidance strategies\nbased on learning approaches. The former may result in long travel distances\ndue to frequent replanning, while the latter may have low learning efficiency\ndue to low sample exploration and utilization, and causing high training costs\nfor the model. To address these issues, we propose a path planning method,\nMAPPOHR, which combines heuristic search, empirical rules, and multi-agent\nreinforcement learning. The method consists of two layers: a real-time planner\nbased on the multi-agent reinforcement learning algorithm, MAPPO, which embeds\nempirical rules in the action output layer and reward functions, and a\nheuristic search planner used to create a global guiding path. During movement,\nthe heuristic search planner replans new paths based on the instructions of the\nreal-time planner. We tested our method in 10 different conflict scenarios. The\nexperiments show that the planning performance of MAPPOHR is better than that\nof existing learning and heuristic methods. Due to the utilization of empirical\nknowledge and heuristic search, the learning efficiency of MAPPOHR is higher\nthan that of existing learning methods.",
                "Knowledge-based Reasoning and Learning under Partial Observability in Ad\n  Hoc Teamwork\nAd hoc teamwork refers to the problem of enabling an agent to collaborate\nwith teammates without prior coordination. Data-driven methods represent the\nstate of the art in ad hoc teamwork. They use a large labeled dataset of prior\nobservations to model the behavior of other agent types and to determine the ad\nhoc agent's behavior. These methods are computationally expensive, lack\ntransparency, and make it difficult to adapt to previously unseen changes,\ne.g., in team composition. Our recent work introduced an architecture that\ndetermined an ad hoc agent's behavior based on non-monotonic logical reasoning\nwith prior commonsense domain knowledge and predictive models of other agents'\nbehavior that were learned from limited examples. In this paper, we\nsubstantially expand the architecture's capabilities to support: (a) online\nselection, adaptation, and learning of the models that predict the other\nagents' behavior; and (b) collaboration with teammates in the presence of\npartial observability and limited communication. We illustrate and\nexperimentally evaluate the capabilities of our architecture in two simulated\nmultiagent benchmark domains for ad hoc teamwork: Fort Attack and Half Field\nOffense. We show that the performance of our architecture is comparable or\nbetter than state of the art data-driven baselines in both simple and complex\nscenarios, particularly in the presence of limited training data, partial\nobservability, and changes in team composition.",
                "chemSKI with tokens: world building and economy in the SKI universe\nchemSKI with tokens is a confluent graph rewrite system where all rewrites\nare local, which moreover can be used to do SKI calculus reductions. The graph\nrewrites of chemSKI are made conservative by the use of tokens. We thus achieve\nseveral goals: conservative rewrites in a chemical style, a solution to the\nproblem of new edge names in a distributed, decentralized graphical reduction\nand a new estimation of the cost of a combinatory calculus computation. This\nformalism can be used either as an artificial chemistry or as a model of a\nvirtual decentralized machine which performs only local reductions. A programs\nrepository and the same article with simulations are available at github at\nhttps://mbuliga.github.io/chemski/chemski-with-tokens.html"
            ],
            "interesting paper": 4
        }
    ],
    "Alex Hernandez": [
        {
            "papers": [
                "Behavior quantification as the missing link between fields: Tools for\n  digital psychiatry and their role in the future of neurobiology\nThe great behavioral heterogeneity observed between individuals with the same\npsychiatric disorder and even within one individual over time complicates both\nclinical practice and biomedical research. However, modern technologies are an\nexciting opportunity to improve behavioral characterization. Existing\npsychiatry methods that are qualitative or unscalable, such as patient surveys\nor clinical interviews, can now be collected at a greater capacity and analyzed\nto produce new quantitative measures. Furthermore, recent capabilities for\ncontinuous collection of passive sensor streams, such as phone GPS or\nsmartwatch accelerometer, open avenues of novel questioning that were\npreviously entirely unrealistic. Their temporally dense nature enables a\ncohesive study of real-time neural and behavioral signals.\n  To develop comprehensive neurobiological models of psychiatric disease, it\nwill be critical to first develop strong methods for behavioral quantification.\nThere is huge potential in what can theoretically be captured by current\ntechnologies, but this in itself presents a large computational challenge --\none that will necessitate new data processing tools, new machine learning\ntechniques, and ultimately a shift in how interdisciplinary work is conducted.\nIn my thesis, I detail research projects that take different perspectives on\ndigital psychiatry, subsequently tying ideas together with a concluding\ndiscussion on the future of the field. I also provide software infrastructure\nwhere relevant, with extensive documentation.\n  Major contributions include scientific arguments and proof of concept results\nfor daily free-form audio journals as an underappreciated psychiatry research\ndatatype, as well as novel stability theorems and pilot empirical success for a\nproposed multi-area recurrent neural network architecture.",
                "Using Models Based on Cognitive Theory to Predict Human Behavior in\n  Traffic: A Case Study\nThe development of automated vehicles has the potential to revolutionize\ntransportation, but they are currently unable to ensure a safe and\ntime-efficient driving style. Reliable models predicting human behavior are\nessential for overcoming this issue. While data-driven models are commonly used\nto this end, they can be vulnerable in safety-critical edge cases. This has led\nto an interest in models incorporating cognitive theory, but as such models are\ncommonly developed for explanatory purposes, this approach's effectiveness in\nbehavior prediction has remained largely untested so far. In this article, we\ninvestigate the usefulness of the \\emph{Commotions} model -- a novel\ncognitively plausible model incorporating the latest theories of human\nperception, decision-making, and motor control -- for predicting human behavior\nin gap acceptance scenarios, which entail many important traffic interactions\nsuch as lane changes and intersections. We show that this model can compete\nwith or even outperform well-established data-driven prediction models across\nseveral naturalistic datasets. These results demonstrate the promise of\nincorporating cognitive theory in behavior prediction models for automated\nvehicles.",
                "On the Planning Abilities of Large Language Models : A Critical\n  Investigation\nIntrigued by the claims of emergent reasoning capabilities in LLMs trained on\ngeneral web corpora, in this paper, we set out to investigate their planning\ncapabilities. We aim to evaluate (1) the effectiveness of LLMs in generating\nplans autonomously in commonsense planning tasks and (2) the potential of LLMs\nin LLM-Modulo settings where they act as a source of heuristic guidance for\nexternal planners and verifiers. We conduct a systematic study by generating a\nsuite of instances on domains similar to the ones employed in the International\nPlanning Competition and evaluate LLMs in two distinct modes: autonomous and\nheuristic. Our findings reveal that LLMs' ability to generate executable plans\nautonomously is rather limited, with the best model (GPT-4) having an average\nsuccess rate of ~12% across the domains. However, the results in the LLM-Modulo\nsetting show more promise. In the LLM-Modulo setting, we demonstrate that\nLLM-generated plans can improve the search process for underlying sound\nplanners and additionally show that external verifiers can help provide\nfeedback on the generated plans and back-prompt the LLM for better plan\ngeneration.",
                "A Mini Review on the utilization of Reinforcement Learning with OPC UA\nReinforcement Learning (RL) is a powerful machine learning paradigm that has\nbeen applied in various fields such as robotics, natural language processing\nand game playing achieving state-of-the-art results. Targeted to solve\nsequential decision making problems, it is by design able to learn from\nexperience and therefore adapt to changing dynamic environments. These\ncapabilities make it a prime candidate for controlling and optimizing complex\nprocesses in industry. The key to fully exploiting this potential is the\nseamless integration of RL into existing industrial systems. The industrial\ncommunication standard Open Platform Communications UnifiedArchitecture (OPC\nUA) could bridge this gap. However, since RL and OPC UA are from different\nfields,there is a need for researchers to bridge the gap between the two\ntechnologies. This work serves to bridge this gap by providing a brief\ntechnical overview of both technologies and carrying out a semi-exhaustive\nliterature review to gain insights on how RL and OPC UA are applied in\ncombination. With this survey, three main research topics have been identified,\nfollowing the intersection of RL with OPC UA. The results of the literature\nreview show that RL is a promising technology for the control and optimization\nof industrial processes, but does not yet have the necessary standardized\ninterfaces to be deployed in real-world scenarios with reasonably low effort.",
                "Asking Before Acting: Gather Information in Embodied Decision Making\n  with Language Models\nWith strong capabilities of reasoning and a broad understanding of the world,\nLarge Language Models (LLMs) have demonstrated immense potential in building\nversatile embodied decision-making agents capable of executing a wide array of\ntasks. Nevertheless, when deployed in unfamiliar environments, we show that LLM\nagents encounter challenges in efficiently gathering essential information,\nleading to suboptimal performance. Conversely, human individuals often seek\nadditional information from their peers prior to taking action, harnessing\nexternal knowledge to avoid unnecessary trial and error. Drawing inspiration\nfrom this behavior, we propose \\textit{Asking Before Acting} (ABA), a method\nthat empowers the agent to proactively inquire with external sources for\npertinent information using natural language during their interactions within\nthe environment. In this way, the agent is able to enhance its efficiency and\nperformance by circumventing potentially laborious steps and combating the\ndifficulties associated with exploration in unfamiliar environments and\nvagueness of the instructions. We conduct extensive experiments involving a\nspectrum of environments including text-based household everyday tasks, robot\narm manipulation tasks, and real world open domain image based embodied tasks.\nThe experiments involve various models from Vicuna to GPT-4. The results\ndemonstrate that, even with modest prompts modifications, ABA exhibits\nsubstantial advantages on both performance and efficiency over baseline LLM\nagents. Further finetuning ABA with reformulated metadata (ABA-FT) faciliates\nlearning the rationale for asking and allows for additional enhancements\nespecially in tasks that baselines struggle to solve.",
                "ISLE: An Intelligent Streaming Framework for High-Throughput AI\n  Inference in Medical Imaging\nAs the adoption of Artificial Intelligence (AI) systems within the clinical\nenvironment grows, limitations in bandwidth and compute can create\ncommunication bottlenecks when streaming imaging data, leading to delays in\npatient care and increased cost. As such, healthcare providers and AI vendors\nwill require greater computational infrastructure, therefore dramatically\nincreasing costs. To that end, we developed ISLE, an intelligent streaming\nframework for high-throughput, compute- and bandwidth- optimized, and cost\neffective AI inference for clinical decision making at scale. In our\nexperiments, ISLE on average reduced data transmission by 98.02% and decoding\ntime by 98.09%, while increasing throughput by 2,730%. We show that ISLE\nresults in faster turnaround times, and reduced overall cost of data,\ntransmission, and compute, without negatively impacting clinical decision\nmaking using AI systems."
            ],
            "interesting paper": 3
        },
        {
            "papers": [
                "Exploring the Adaptive Behaviors of Particle Lenia: A\n  Perturbation-Response Analysis for Computational Agency\nA firm cognitive subject or ``individual'' is presupposed for the emergence\nof mind. However, with the development of recent information technology, the\n``individual'' has become more dispersed in society and the cognitive subject\nhas become increasingly unstable and adaptive, necessitating an update in our\nunderstanding of the ``individual''. Autopoiesis serves as a model of the\ncognitive subject, which is unstable and requires effort to maintain itself to\nadapt to the environment. In this study, we evaluated adaptivity for a highly\nextensible multi-particle system model Particle Lenia through the response\nperturbation. As a result, we found that Particle Lenia has a particle\nconfiguration that is both temporally unstable and has multiple stable states.\nThis result suggests that Particle Lenia can express adaptive characteristics\nand is expected to be used as a computational model toward building an\nautopoietic cognitive agent.",
                "Voyager: An Open-Ended Embodied Agent with Large Language Models\nWe introduce Voyager, the first LLM-powered embodied lifelong learning agent\nin Minecraft that continuously explores the world, acquires diverse skills, and\nmakes novel discoveries without human intervention. Voyager consists of three\nkey components: 1) an automatic curriculum that maximizes exploration, 2) an\never-growing skill library of executable code for storing and retrieving\ncomplex behaviors, and 3) a new iterative prompting mechanism that incorporates\nenvironment feedback, execution errors, and self-verification for program\nimprovement. Voyager interacts with GPT-4 via blackbox queries, which bypasses\nthe need for model parameter fine-tuning. The skills developed by Voyager are\ntemporally extended, interpretable, and compositional, which compounds the\nagent's abilities rapidly and alleviates catastrophic forgetting. Empirically,\nVoyager shows strong in-context lifelong learning capability and exhibits\nexceptional proficiency in playing Minecraft. It obtains 3.3x more unique\nitems, travels 2.3x longer distances, and unlocks key tech tree milestones up\nto 15.3x faster than prior SOTA. Voyager is able to utilize the learned skill\nlibrary in a new Minecraft world to solve novel tasks from scratch, while other\ntechniques struggle to generalize. We open-source our full codebase and prompts\nat https://voyager.minedojo.org/.",
                "A Decentralized Spike-based Learning Framework for Sequential Capture in\n  Discrete Perimeter Defense Problem\nThis paper proposes a novel Decentralized Spike-based Learning (DSL)\nframework for the discrete Perimeter Defense Problem (d-PDP). A team of\ndefenders is operating on the perimeter to protect the circular territory from\nradially incoming intruders. At first, the d-PDP is formulated as a\nspatio-temporal multi-task assignment problem (STMTA). The problem of STMTA is\nthen converted into a multi-label learning problem to obtain labels of segments\nthat defenders have to visit in order to protect the perimeter. The DSL\nframework uses a Multi-Label Classifier using Synaptic Efficacy Function\nspiking neuRON (MLC-SEFRON) network for deterministic multi-label learning.\nEach defender contains a single MLC-SEFRON network. Each MLC-SEFRON network is\ntrained independently using input from its own perspective for decentralized\noperations. The input spikes to the MLC-SEFRON network can be directly obtained\nfrom the spatio-temporal information of defenders and intruders without any\nextra pre-processing step. The output of MLC-SEFRON contains the labels of\nsegments that a defender has to visit in order to protect the perimeter. Based\non the multi-label output from the MLC-SEFRON a trajectory is generated for a\ndefender using a Consensus-Based Bundle Algorithm (CBBA) in order to capture\nthe intruders. The target multi-label output for training MLC-SEFRON is\nobtained from an expert policy. Also, the MLC-SEFRON trained for a defender can\nbe directly used for obtaining labels of segments assigned to another defender\nwithout any retraining. The performance of MLC-SEFRON has been evaluated for\nfull observation and partial observation scenarios of the defender. The overall\nperformance of the DSL framework is then compared with expert policy along with\nother existing learning algorithms. The scalability of the DSL has been\nevaluated using an increasing number of defenders.",
                "Exploiting Noise as a Resource for Computation and Learning in Spiking\n  Neural Networks\n$\\textbf{Formal version available at}$\nhttps://cell.com/patterns/fulltext/S2666-3899(23)00200-3\n  Networks of spiking neurons underpin the extraordinary information-processing\ncapabilities of the brain and have become pillar models in neuromorphic\nartificial intelligence. Despite extensive research on spiking neural networks\n(SNNs), most studies are established on deterministic models, overlooking the\ninherent non-deterministic, noisy nature of neural computations. This study\nintroduces the noisy spiking neural network (NSNN) and the noise-driven\nlearning rule (NDL) by incorporating noisy neuronal dynamics to exploit the\ncomputational advantages of noisy neural processing. NSNN provides a\ntheoretical framework that yields scalable, flexible, and reliable computation.\nWe demonstrate that NSNN leads to spiking neural models with competitive\nperformance, improved robustness against challenging perturbations than\ndeterministic SNNs, and better reproducing probabilistic computations in neural\ncoding. This study offers a powerful and easy-to-use tool for machine learning,\nneuromorphic intelligence practitioners, and computational neuroscience\nresearchers.",
                "Enhancing Human Capabilities through Symbiotic Artificial Intelligence\n  with Shared Sensory Experiences\nThe merging of human intelligence and artificial intelligence has long been a\nsubject of interest in both science fiction and academia. In this paper, we\nintroduce a novel concept in Human-AI interaction called Symbiotic Artificial\nIntelligence with Shared Sensory Experiences (SAISSE), which aims to establish\na mutually beneficial relationship between AI systems and human users through\nshared sensory experiences. By integrating multiple sensory input channels and\nprocessing human experiences, SAISSE fosters a strong human-AI bond, enabling\nAI systems to learn from and adapt to individual users, providing personalized\nsupport, assistance, and enhancement. Furthermore, we discuss the incorporation\nof memory storage units for long-term growth and development of both the AI\nsystem and its human user. As we address user privacy and ethical guidelines\nfor responsible AI-human symbiosis, we also explore potential biases and\ninequalities in AI-human symbiosis and propose strategies to mitigate these\nchallenges. Our research aims to provide a comprehensive understanding of the\nSAISSE concept and its potential to effectively support and enhance individual\nhuman users through symbiotic AI systems. This position article aims at\ndiscussing poteintial AI-human interaction related topics within the scientific\ncommunity, rather than providing experimental or theoretical results.",
                "Understanding the Capabilities of Large Language Models for Automated\n  Planning\nAutomated planning is concerned with developing efficient algorithms to\ngenerate plans or sequences of actions to achieve a specific goal in a given\nenvironment. Emerging Large Language Models (LLMs) can answer questions, write\nhigh-quality programming code, and predict protein folding, showcasing their\nversatility in solving various tasks beyond language-based problems. In this\npaper, we aim to explore how LLMs can also be used for automated planning. To\ndo so, we seek to answer four key questions. Firstly, we want to understand the\nextent to which LLMs can be used for plan generation. Secondly, we aim to\nidentify which pre-training data is most effective in facilitating plan\ngeneration. Thirdly, we investigate whether fine-tuning or prompting is a more\neffective approach for plan generation. Finally, we explore whether LLMs are\ncapable of plan generalization. By answering these questions, the study seeks\nto shed light on the capabilities of LLMs in solving complex planning problems\nand provide insights into the most effective approaches for using LLMs in this\ncontext."
            ],
            "interesting paper": 2
        },
        {
            "papers": [
                "Inferring the Future by Imagining the Past\nA single panel of a comic book can say a lot: it can depict not only where\nthe characters currently are, but also their motions, their motivations, their\nemotions, and what they might do next. More generally, humans routinely infer\ncomplex sequences of past and future events from a *static snapshot* of a\n*dynamic scene*, even in situations they have never seen before.\n  In this paper, we model how humans make such rapid and flexible inferences.\nBuilding on a long line of work in cognitive science, we offer a Monte Carlo\nalgorithm whose inferences correlate well with human intuitions in a wide\nvariety of domains, while only using a small, cognitively-plausible number of\nsamples. Our key technical insight is a surprising connection between our\ninference problem and Monte Carlo path tracing, which allows us to apply\ndecades of ideas from the computer graphics community to this\nseemingly-unrelated theory of mind task.",
                "Towards Cognitive Bots: Architectural Research Challenges\nSoftware bots operating in multiple virtual digital platforms must understand\nthe platforms' affordances and behave like human users. Platform affordances or\nfeatures differ from one application platform to another or through a life\ncycle, requiring such bots to be adaptable. Moreover, bots in such platforms\ncould cooperate with humans or other software agents for work or to learn\nspecific behavior patterns. However, present-day bots, particularly chatbots,\nother than language processing and prediction, are far from reaching a human\nuser's behavior level within complex business information systems. They lack\nthe cognitive capabilities to sense and act in such virtual environments,\nrendering their development a challenge to artificial general intelligence\nresearch. In this study, we problematize and investigate assumptions in\nconceptualizing software bot architecture by directing attention to significant\narchitectural research challenges in developing cognitive bots endowed with\ncomplex behavior for operation on information systems. As an outlook, we\npropose alternate architectural assumptions to consider in future bot design\nand bot development frameworks.",
                "SwiftSage: A Generative Agent with Fast and Slow Thinking for Complex\n  Interactive Tasks\nWe introduce SwiftSage, a novel agent framework inspired by the dual-process\ntheory of human cognition, designed to excel in action planning for complex\ninteractive reasoning tasks. SwiftSage integrates the strengths of behavior\ncloning and prompting large language models (LLMs) to enhance task completion\nperformance. The framework comprises two primary modules: the Swift module,\nrepresenting fast and intuitive thinking, and the Sage module, emulating\ndeliberate thought processes. The Swift module is a small encoder-decoder LM\nfine-tuned on the oracle agent's action trajectories, while the Sage module\nemploys LLMs such as GPT-4 for subgoal planning and grounding. We develop a\nheuristic method to harmoniously integrate the two modules, resulting in a more\nefficient and robust problem-solving process. In 30 tasks from the ScienceWorld\nbenchmark, SwiftSage significantly outperforms other methods such as SayCan,\nReAct, and Reflexion, demonstrating its effectiveness in solving complex\ninteractive tasks.",
                "Input-Aware Dynamic Timestep Spiking Neural Networks for Efficient\n  In-Memory Computing\nSpiking Neural Networks (SNNs) have recently attracted widespread research\ninterest as an efficient alternative to traditional Artificial Neural Networks\n(ANNs) because of their capability to process sparse and binary spike\ninformation and avoid expensive multiplication operations. Although the\nefficiency of SNNs can be realized on the In-Memory Computing (IMC)\narchitecture, we show that the energy cost and latency of SNNs scale linearly\nwith the number of timesteps used on IMC hardware. Therefore, in order to\nmaximize the efficiency of SNNs, we propose input-aware Dynamic Timestep SNN\n(DT-SNN), a novel algorithmic solution to dynamically determine the number of\ntimesteps during inference on an input-dependent basis. By calculating the\nentropy of the accumulated output after each timestep, we can compare it to a\npredefined threshold and decide if the information processed at the current\ntimestep is sufficient for a confident prediction. We deploy DT-SNN on an IMC\narchitecture and show that it incurs negligible computational overhead. We\ndemonstrate that our method only uses 1.46 average timesteps to achieve the\naccuracy of a 4-timestep static SNN while reducing the energy-delay-product by\n80%.",
                "Exploiting Large Neuroimaging Datasets to Create Connectome-Constrained\n  Approaches for more Robust, Efficient, and Adaptable Artificial Intelligence\nDespite the progress in deep learning networks, efficient learning at the\nedge (enabling adaptable, low-complexity machine learning solutions) remains a\ncritical need for defense and commercial applications. We envision a pipeline\nto utilize large neuroimaging datasets, including maps of the brain which\ncapture neuron and synapse connectivity, to improve machine learning\napproaches. We have pursued different approaches within this pipeline\nstructure. First, as a demonstration of data-driven discovery, the team has\ndeveloped a technique for discovery of repeated subcircuits, or motifs. These\nwere incorporated into a neural architecture search approach to evolve network\narchitectures. Second, we have conducted analysis of the heading direction\ncircuit in the fruit fly, which performs fusion of visual and angular velocity\nfeatures, to explore augmenting existing computational models with new insight.\nOur team discovered a novel pattern of connectivity, implemented a new model,\nand demonstrated sensor fusion on a robotic platform. Third, the team analyzed\ncircuitry for memory formation in the fruit fly connectome, enabling the design\nof a novel generative replay approach. Finally, the team has begun analysis of\nconnectivity in mammalian cortex to explore potential improvements to\ntransformer networks. These constraints increased network robustness on the\nmost challenging examples in the CIFAR-10-C computer vision robustness\nbenchmark task, while reducing learnable attention parameters by over an order\nof magnitude. Taken together, these results demonstrate multiple potential\napproaches to utilize insight from neural systems for developing robust and\nefficient machine learning techniques.",
                "Chain-of-Thought Hub: A Continuous Effort to Measure Large Language\n  Models' Reasoning Performance\nAs large language models (LLMs) are continuously being developed, their\nevaluation becomes increasingly important yet challenging. This work proposes\nChain-of-Thought Hub, an open-source evaluation suite on the multi-step\nreasoning capabilities of large language models. We are interested in this\nsetting for two reasons: (1) from the behavior of GPT and PaLM model family, we\nobserve that complex reasoning is likely to be a key differentiator between\nweaker and stronger LLMs; (2) we envisage large language models to become the\nnext-generation computational platform and foster an ecosystem of LLM-based new\napplications, this naturally requires the foundation models to perform complex\ntasks that often involve the composition of linguistic and logical operations.\nOur approach is to compile a suite of challenging reasoning benchmarks to track\nthe progress of LLMs. Our current results show that: (1) model scale clearly\ncorrelates with reasoning capabilities; (2) As of May 2023, Claude-v1.3 and\nPaLM-2 are the only two models that are comparable with GPT-4, while\nopen-sourced models still lag behind; (3) LLaMA-65B performs closely to\ncode-davinci-002, indicating that with successful further development such as\nreinforcement learning from human feedback (RLHF), it has great potential to be\nclose to GPT-3.5-Turbo. Our results also suggest that for the open-source\nefforts to catch up, the community may focus more on building better base\nmodels and exploring RLHF."
            ],
            "interesting paper": 3
        },
        {
            "papers": [
                "Synthesizing a Progression of Subtasks for Block-Based Visual\n  Programming Tasks\nBlock-based visual programming environments play an increasingly important\nrole in introducing computing concepts to K-12 students. In recent years, they\nhave also gained popularity in neuro-symbolic AI, serving as a benchmark to\nevaluate general problem-solving and logical reasoning skills. The open-ended\nand conceptual nature of these visual programming tasks make them challenging,\nboth for state-of-the-art AI agents as well as for novice programmers. A\nnatural approach to providing assistance for problem-solving is breaking down a\ncomplex task into a progression of simpler subtasks; however, this is not\ntrivial given that the solution codes are typically nested and have non-linear\nexecution behavior. In this paper, we formalize the problem of synthesizing\nsuch a progression for a given reference block-based visual programming task.\nWe propose a novel synthesis algorithm that generates a progression of subtasks\nthat are high-quality, well-spaced in terms of their complexity, and solving\nthis progression leads to solving the reference task. We show the utility of\nour synthesis algorithm in improving the efficacy of AI agents (in this case,\nneural program synthesizers) for solving tasks in the Karel programming\nenvironment. Then, we conduct a user study to demonstrate that our synthesized\nprogression of subtasks can assist a novice programmer in solving tasks in the\nHour of Code: Maze Challenge by Code-dot-org.",
                "Emergent Modularity in Pre-trained Transformers\nThis work examines the presence of modularity in pre-trained Transformers, a\nfeature commonly found in human brains and thought to be vital for general\nintelligence. In analogy to human brains, we consider two main characteristics\nof modularity: (1) functional specialization of neurons: we evaluate whether\neach neuron is mainly specialized in a certain function, and find that the\nanswer is yes. (2) function-based neuron grouping: we explore finding a\nstructure that groups neurons into modules by function, and each module works\nfor its corresponding function. Given the enormous amount of possible\nstructures, we focus on Mixture-of-Experts as a promising candidate, which\npartitions neurons into experts and usually activates different experts for\ndifferent inputs. Experimental results show that there are functional experts,\nwhere clustered are the neurons specialized in a certain function. Moreover,\nperturbing the activations of functional experts significantly affects the\ncorresponding function. Finally, we study how modularity emerges during\npre-training, and find that the modular structure is stabilized at the early\nstage, which is faster than neuron stabilization. It suggests that Transformers\nfirst construct the modular structure and then learn fine-grained neuron\nfunctions. Our code and data are available at\nhttps://github.com/THUNLP/modularity-analysis.",
                "In-Context Analogical Reasoning with Pre-Trained Language Models\nAnalogical reasoning is a fundamental capacity of human cognition that allows\nus to reason abstractly about novel situations by relating them to past\nexperiences. While it is thought to be essential for robust reasoning in AI\nsystems, conventional approaches require significant training and/or\nhard-coding of domain knowledge to be applied to benchmark tasks. Inspired by\ncognitive science research that has found connections between human language\nand analogy-making, we explore the use of intuitive language-based abstractions\nto support analogy in AI systems. Specifically, we apply large pre-trained\nlanguage models (PLMs) to visual Raven's Progressive Matrices (RPM), a common\nrelational reasoning test. By simply encoding the perceptual features of the\nproblem into language form, we find that PLMs exhibit a striking capacity for\nzero-shot relational reasoning, exceeding human performance and nearing\nsupervised vision-based methods. We explore different encodings that vary the\nlevel of abstraction over task features, finding that higher-level abstractions\nfurther strengthen PLMs' analogical reasoning. Our detailed analysis reveals\ninsights on the role of model complexity, in-context learning, and prior\nknowledge in solving RPM tasks.",
                "On the Value of Myopic Behavior in Policy Reuse\nLeveraging learned strategies in unfamiliar scenarios is fundamental to human\nintelligence. In reinforcement learning, rationally reusing the policies\nacquired from other tasks or human experts is critical for tackling problems\nthat are difficult to learn from scratch. In this work, we present a framework\ncalled Selective Myopic bEhavior Control~(SMEC), which results from the insight\nthat the short-term behaviors of prior policies are sharable across tasks. By\nevaluating the behaviors of prior policies via a hybrid value function\narchitecture, SMEC adaptively aggregates the sharable short-term behaviors of\nprior policies and the long-term behaviors of the task policy, leading to\ncoordinated decisions. Empirical results on a collection of manipulation and\nlocomotion tasks demonstrate that SMEC outperforms existing methods, and\nvalidate the ability of SMEC to leverage related prior policies.",
                "Evolving Connectivity for Recurrent Spiking Neural Networks\nRecurrent spiking neural networks (RSNNs) hold great potential for advancing\nartificial general intelligence, as they draw inspiration from the biological\nnervous system and show promise in modeling complex dynamics. However, the\nwidely-used surrogate gradient-based training methods for RSNNs are inherently\ninaccurate and unfriendly to neuromorphic hardware. To address these\nlimitations, we propose the evolving connectivity (EC) framework, an\ninference-only method for training RSNNs. The EC framework reformulates\nweight-tuning as a search into parameterized connection probability\ndistributions, and employs Natural Evolution Strategies (NES) for optimizing\nthese distributions. Our EC framework circumvents the need for gradients and\nfeatures hardware-friendly characteristics, including sparse boolean\nconnections and high scalability. We evaluate EC on a series of standard\nrobotic locomotion tasks, where it achieves comparable performance with deep\nneural networks and outperforms gradient-trained RSNNs, even solving the\ncomplex 17-DoF humanoid task. Additionally, the EC framework demonstrates a two\nto three fold speedup in efficiency compared to directly evolving parameters.\nBy providing a performant and hardware-friendly alternative, the EC framework\nlays the groundwork for further energy-efficient applications of RSNNs and\nadvances the development of neuromorphic devices.",
                "Optimization's Neglected Normative Commitments\nOptimization is offered as an objective approach to resolving complex,\nreal-world decisions involving uncertainty and conflicting interests. It drives\nbusiness strategies as well as public policies and, increasingly, lies at the\nheart of sophisticated machine learning systems. A paradigm used to approach\npotentially high-stakes decisions, optimization relies on abstracting the real\nworld to a set of decision(s), objective(s) and constraint(s). Drawing from the\nmodeling process and a range of actual cases, this paper describes the\nnormative choices and assumptions that are necessarily part of using\noptimization. It then identifies six emergent problems that may be neglected:\n1) Misspecified values can yield optimizations that omit certain imperatives\naltogether or incorporate them incorrectly as a constraint or as part of the\nobjective, 2) Problematic decision boundaries can lead to faulty modularity\nassumptions and feedback loops, 3) Failing to account for multiple agents'\ndivergent goals and decisions can lead to policies that serve only certain\nnarrow interests, 4) Mislabeling and mismeasurement can introduce bias and\nimprecision, 5) Faulty use of relaxation and approximation methods,\nunaccompanied by formal characterizations and guarantees, can severely impede\napplicability, and 6) Treating optimization as a justification for action,\nwithout specifying the necessary contextual information, can lead to ethically\ndubious or faulty decisions. Suggestions are given to further understand and\ncurb the harms that can arise when optimization is used wrongfully."
            ],
            "interesting paper": 3
        },
        {
            "papers": [
                "Determinantal Point Process Attention Over Grid Cell Code Supports Out\n  of Distribution Generalization\nDeep neural networks have made tremendous gains in emulating human-like\nintelligence, and have been used increasingly as ways of understanding how the\nbrain may solve the complex computational problems on which this relies.\nHowever, these still fall short of, and therefore fail to provide insight into\nhow the brain supports strong forms of generalization of which humans are\ncapable. One such case is out-of-distribution (OOD) generalization-successful\nperformance on test examples that lie outside the distribution of the training\nset. Here, we identify properties of processing in the brain that may\ncontribute to this ability. We describe a two-part algorithm that draws on\nspecific features of neural computation to achieve OOD generalization, and\nprovide a proof of concept by evaluating performance on two challenging\ncognitive tasks. First we draw on the fact that the mammalian brain represents\nmetric spaces using grid cell code (e.g., in the entorhinal cortex): abstract\nrepresentations of relational structure, organized in recurring motifs that\ncover the representational space. Second, we propose an attentional mechanism\nthat operates over the grid cell code using Determinantal Point Process (DPP),\nthat we call DPP attention (DPP-A) -- a transformation that ensures maximum\nsparseness in the coverage of that space. We show that a loss function that\ncombines standard task-optimized error with DPP-A can exploit the recurring\nmotifs in the grid cell code, and can be integrated with common architectures\nto achieve strong OOD generalization performance on analogy and arithmetic\ntasks. This provides both an interpretation of how the grid cell code in the\nmammalian brain may contribute to generalization performance, and at the same\ntime a potential means for improving such capabilities in artificial neural\nnetworks.",
                "Universal Mechanical Polycomputation in Granular Matter\nUnconventional computing devices are increasingly of interest as they can\noperate in environments hostile to silicon-based electronics, or compute in\nways that traditional electronics cannot. Mechanical computers, wherein\ninformation processing is a material property emerging from the interaction of\ncomponents with the environment, are one such class of devices. This\ninformation processing can be manifested in various physical substrates, one of\nwhich is granular matter. In a granular assembly, vibration can be treated as\nthe information-bearing mode. This can be exploited to realize \"polycomputing\":\nmaterials can be evolved such that a single grain within them can report the\nresult of multiple logical operations simultaneously at different frequencies,\nwithout recourse to quantum effects. Here, we demonstrate the evolution of a\nmaterial in which one grain acts simultaneously as two different NAND gates at\ntwo different frequencies. NAND gates are of interest as any logical operations\ncan be built from them. Moreover, they are nonlinear thus demonstrating a step\ntoward general-purpose, computationally dense mechanical computers.\nPolycomputation was found to be distributed across each evolved material,\nsuggesting the material's robustness. With recent advances in material\nsciences, hardware realization of these materials may eventually provide\ndevices that challenge the computational density of traditional computers.",
                "Key-Value Transformer\nTransformers have emerged as the prevailing standard solution for various AI\ntasks, including computer vision and natural language processing. The widely\nadopted Query, Key, and Value formulation (QKV) has played a significant role\nin this. Nevertheless, no research has examined the essentiality of these three\ncomponents for transformer performance. Therefore, we conducted an evaluation\nof the key-value formulation (KV), which generates symmetric attention maps,\nalong with an asymmetric version that incorporates a 2D positional encoding\ninto the attention matrix. Remarkably, this transformer requires fewer\nparameters and computation than the original one. Through experiments\nencompassing three task types -- synthetics (such as reversing or sorting a\nlist), vision (mnist or cifar classification), and NLP (character generation\nand translation) -- we discovered that the KV transformer occasionally\noutperforms the QKV transformer. However, it also exhibits instances of\nunderperformance compared to QKV, making it challenging to draw a definitive\nconclusion. Nonetheless, we consider the reported results to be encouraging and\nanticipate that they may pave the way for more efficient transformers in the\nfuture.",
                "The Digital Divide in Process Safety: Quantitative Risk Analysis of\n  Human-AI Collaboration\nDigital technologies have dramatically accelerated the digital transformation\nin process industries, boosted new industrial applications, upgraded the\nproduction system, and enhanced operational efficiency. In contrast, the\nchallenges and gaps between human and artificial intelligence (AI) have become\nmore and more prominent, whereas the digital divide in process safety is\naggregating. The study attempts to address the following questions: (i)What is\nAI in the process safety context? (ii)What is the difference between AI and\nhumans in process safety? (iii)How do AI and humans collaborate in process\nsafety? (iv)What are the challenges and gaps in human-AI collaboration? (v)How\nto quantify the risk of human-AI collaboration in process safety? Qualitative\nrisk analysis based on brainstorming and literature review, and quantitative\nrisk analysis based on layer of protection analysis (LOPA) and Bayesian network\n(BN), were applied to explore and model. The importance of human reliability\nshould be stressed in the digital age, not usually to increase the reliability\nof AI, and human-centered AI design in process safety needs to be propagated.",
                "Re-imagining health and well-being in low resource African settings\n  using an augmented AI system and a 3D digital twin\nThis paper discusses and explores the potential and relevance of recent\ndevelopments in artificial intelligence (AI) and digital twins for health and\nwell-being in low-resource African countries. We use the case of public health\nemergency response to disease outbreaks and epidemic control. There is\npotential to take advantage of the increasing availability of data and\ndigitization to develop advanced AI methods for analysis and prediction. Using\nan AI systems perspective, we review emerging trends in AI systems and digital\ntwins and propose an initial augmented AI system architecture to illustrate how\nan AI system can work with a 3D digital twin to address public health goals. We\nhighlight scientific knowledge discovery, continual learning, pragmatic\ninteroperability, and interactive explanation and decision-making as essential\nresearch challenges for AI systems and digital twins.",
                "Continual Task Allocation in Meta-Policy Network via Sparse Prompting\nHow to train a generalizable meta-policy by continually learning a sequence\nof tasks? It is a natural human skill yet challenging to achieve by current\nreinforcement learning: the agent is expected to quickly adapt to new tasks\n(plasticity) meanwhile retaining the common knowledge from previous tasks\n(stability). We address it by \"Continual Task Allocation via Sparse Prompting\n(CoTASP)\", which learns over-complete dictionaries to produce sparse masks as\nprompts extracting a sub-network for each task from a meta-policy network.\nCoTASP trains a policy for each task by optimizing the prompts and the\nsub-network weights alternatively. The dictionary is then updated to align the\noptimized prompts with tasks' embedding, thereby capturing tasks' semantic\ncorrelations. Hence, relevant tasks share more neurons in the meta-policy\nnetwork due to similar prompts while cross-task interference causing forgetting\nis effectively restrained. Given a meta-policy and dictionaries trained on\nprevious tasks, new task adaptation reduces to highly efficient sparse\nprompting and sub-network finetuning. In experiments, CoTASP achieves a\npromising plasticity-stability trade-off without storing or replaying any past\ntasks' experiences. It outperforms existing continual and multi-task RL methods\non all seen tasks, forgetting reduction, and generalization to unseen tasks."
            ],
            "interesting paper": 1
        },
        {
            "papers": [
                "Dissecting Chain-of-Thought: Compositionality through In-Context\n  Filtering and Learning\nChain-of-thought (CoT) is a method that enables language models to handle\ncomplex reasoning tasks by decomposing them into simpler steps. Despite its\nsuccess, the underlying mechanics of CoT are not yet fully understood. In an\nattempt to shed light on this, our study investigates the impact of CoT on the\nability of transformers to in-context learn a simple to study, yet general\nfamily of compositional functions: multi-layer perceptrons (MLPs). In this\nsetting, we find that the success of CoT can be attributed to breaking down\nin-context learning of a compositional function into two distinct phases:\nfocusing on and filtering data related to each step of the composition and\nin-context learning the single-step composition function. Through both\nexperimental and theoretical evidence, we demonstrate how CoT significantly\nreduces the sample complexity of in-context learning (ICL) and facilitates the\nlearning of complex functions that non-CoT methods struggle with. Furthermore,\nwe illustrate how transformers can transition from vanilla in-context learning\nto mastering a compositional function with CoT by simply incorporating\nadditional layers that perform the necessary data-filtering for CoT via the\nattention mechanism. In addition to these test-time benefits, we show CoT helps\naccelerate pretraining by learning shortcuts to represent complex functions and\nfiltering plays an important role in this process. These findings collectively\nprovide insights into the mechanics of CoT, inviting further investigation of\nits role in complex reasoning tasks.",
                "Code Prompting: a Neural Symbolic Method for Complex Reasoning in Large\n  Language Models\nLarge language models (LLMs) have scaled up to unlock a wide range of complex\nreasoning tasks with the aid of various prompting methods. However, current\nprompting methods generate natural language intermediate steps to help\nreasoning, which can cause imperfect task reduction and confusion. To mitigate\nsuch limitations, we explore code prompting, a neural symbolic prompting method\nwith both zero-shot and few-shot versions which triggers code as intermediate\nsteps. We conduct experiments on 7 widely-used benchmarks involving symbolic\nreasoning and arithmetic reasoning. Code prompting generally outperforms\nchain-of-thought (CoT) prompting. To further understand the performance and\nlimitations of code prompting, we perform extensive ablation studies and error\nanalyses, and identify several exclusive advantages of using symbolic\npromptings compared to natural language. We also consider the ensemble of code\nprompting and CoT prompting to combine the strengths of both. Finally, we show\nthrough experiments how code annotations and their locations affect code\nprompting.",
                "Towards a Unifying Model of Rationality in Multiagent Systems\nMultiagent systems deployed in the real world need to cooperate with other\nagents (including humans) nearly as effectively as these agents cooperate with\none another. To design such AI, and provide guarantees of its effectiveness, we\nneed to clearly specify what types of agents our AI must be able to cooperate\nwith. In this work we propose a generic model of socially intelligent agents,\nwhich are individually rational learners that are also able to cooperate with\none another (in the sense that their joint behavior is Pareto efficient). We\ndefine rationality in terms of the regret incurred by each agent over its\nlifetime, and show how we can construct socially intelligent agents for\ndifferent forms of regret. We then discuss the implications of this model for\nthe development of \"robust\" MAS that can cooperate with a wide variety of\nsocially intelligent agents.",
                "Low Precision Quantization-aware Training in Spiking Neural Networks\n  with Differentiable Quantization Function\nDeep neural networks have been proven to be highly effective tools in various\ndomains, yet their computational and memory costs restrict them from being\nwidely deployed on portable devices. The recent rapid increase of edge\ncomputing devices has led to an active search for techniques to address the\nabove-mentioned limitations of machine learning frameworks. The quantization of\nartificial neural networks (ANNs), which converts the full-precision synaptic\nweights into low-bit versions, emerged as one of the solutions. At the same\ntime, spiking neural networks (SNNs) have become an attractive alternative to\nconventional ANNs due to their temporal information processing capability,\nenergy efficiency, and high biological plausibility. Despite being driven by\nthe same motivation, the simultaneous utilization of both concepts has yet to\nbe thoroughly studied. Therefore, this work aims to bridge the gap between\nrecent progress in quantized neural networks and SNNs. It presents an extensive\nstudy on the performance of the quantization function, represented as a linear\ncombination of sigmoid functions, exploited in low-bit weight quantization in\nSNNs. The presented quantization function demonstrates the state-of-the-art\nperformance on four popular benchmarks, CIFAR10-DVS, DVS128 Gesture,\nN-Caltech101, and N-MNIST, for binary networks (64.05\\%, 95.45\\%, 68.71\\%, and\n99.43\\% respectively) with small accuracy drops and up to 31$\\times$ memory\nsavings, which outperforms existing methods.",
                "An Emergency Disposal Decision-making Method with Human--Machine\n  Collaboration\nRapid developments in artificial intelligence technology have led to unmanned\nsystems replacing human beings in many fields requiring high-precision\npredictions and decisions. In modern operational environments, all job plans\nare affected by emergency events such as equipment failures and resource\nshortages, making a quick resolution critical. The use of unmanned systems to\nassist decision-making can improve resolution efficiency, but their\ndecision-making is not interpretable and may make the wrong decisions. Current\nunmanned systems require human supervision and control. Based on this, we\npropose a collaborative human--machine method for resolving unplanned events\nusing two phases: task filtering and task scheduling. In the task filtering\nphase, we propose a human--machine collaborative decision-making algorithm for\ndynamic tasks. The GACRNN model is used to predict the state of the job nodes,\nlocate the key nodes, and generate a machine-predicted resolution task list. A\nhuman decision-maker supervises the list in real time and modifies and confirms\nthe machine-predicted list through the human--machine interface. In the task\nscheduling phase, we propose a scheduling algorithm that integrates human\nexperience constraints. The steps to resolve an event are inserted into the\nnormal job sequence to schedule the resolution. We propose several\nhuman--machine collaboration methods in each phase to generate steps to resolve\nan unplanned event while minimizing the impact on the original job plan.",
                "Chatbots put to the test in math and logic problems: A preliminary\n  comparison and assessment of ChatGPT-3.5, ChatGPT-4, and Google Bard\nA comparison between three chatbots which are based on large language models,\nnamely ChatGPT-3.5, ChatGPT-4 and Google Bard is presented, focusing on their\nability to give correct answers to mathematics and logic problems. In\nparticular, we check their ability to Understand the problem at hand; Apply\nappropriate algorithms or methods for its solution; and Generate a coherent\nresponse and a correct answer. We use 30 questions that are clear, without any\nambiguities, fully described with plain text only, and have a unique, well\ndefined correct answer. The questions are divided into two sets of 15 each. The\nquestions of Set A are 15 \"Original\" problems that cannot be found online,\nwhile Set B contains 15 \"Published\" problems that one can find online, usually\nwith their solution. Each question is posed three times to each chatbot. The\nanswers are recorded and discussed, highlighting their strengths and\nweaknesses. It has been found that for straightforward arithmetic, algebraic\nexpressions, or basic logic puzzles, chatbots may provide accurate solutions,\nalthough not in every attempt. However, for more complex mathematical problems\nor advanced logic tasks, their answers, although written in a usually\n\"convincing\" way, may not be reliable. Consistency is also an issue, as many\ntimes a chatbot will provide conflicting answers when given the same question\nmore than once. A comparative quantitative evaluation of the three chatbots is\nmade through scoring their final answers based on correctness. It was found\nthat ChatGPT-4 outperforms ChatGPT-3.5 in both sets of questions. Bard comes\nthird in the original questions of Set A, behind the other two chatbots, while\nit has the best performance (first place) in the published questions of Set B.\nThis is probably because Bard has direct access to the internet, in contrast to\nChatGPT chatbots which do not have any communication with the outside world."
            ],
            "interesting paper": 1
        },
        {
            "papers": [
                "Evaluating GPT's Programming Capability through CodeWars' Katas\nIn the burgeoning field of artificial intelligence (AI), understanding the\ncapabilities and limitations of programming-oriented models is crucial. This\npaper presents a novel evaluation of the programming proficiency of Generative\nPretrained Transformer (GPT) models, specifically GPT-3.5 and GPT-4, against\ncoding problems of varying difficulty levels drawn from Codewars. The\nexperiments reveal a distinct boundary at the 3kyu level, beyond which these\nGPT models struggle to provide solutions. These findings led to the proposal of\na measure for coding problem complexity that incorporates both problem\ndifficulty and the time required for solution. The research emphasizes the need\nfor validation and creative thinking capabilities in AI models to better\nemulate human problem-solving techniques. Future work aims to refine this\nproposed complexity measure, enhance AI models with these suggested\ncapabilities, and develop an objective measure for programming problem\ndifficulty. The results of this research offer invaluable insights for\nimproving AI programming capabilities and advancing the frontier of AI\nproblem-solving abilities.",
                "Uncovering multifunctional mechano-intelligence in and through phononic\n  metastructures harnessing physical reservoir computing\nThe recent advances in autonomous systems have prompted a strong demand for\nthe next generation of adaptive structures and materials to possess more\nbuilt-in intelligence in their mechanical domain, the so-called\nmechano-intelligence (MI). Previous MI attempts mainly focused on specific\ndesigns and case studies to realize limited aspects of MI, and there is a lack\nof a systematic foundation in constructing and integrating the different\nelements of intelligence in an effective and efficient manner. Here, we propose\na new approach to create the needed foundation in realizing integrated\nmultifunctional MI via a physical reservoir computing (PRC) framework. That is,\nto concurrently embody computing power and the various elements of\nintelligence, namely perception, decision-making, and commanding, directly in\nthe mechanical domain, advancing from conventional adaptive structures that\nrely solely on add-on digital computers and massive electronics to achieve\nintelligence. As an exemplar platform, we construct a mechanically intelligent\nphononic metastructure with the integrated elements of MI by harnessing the PRC\npower hidden in their high-degree-of-freedom nonlinear dynamics. Through\nanalyses and experimental investigations, we uncover multiple adaptive\nstructural functions ranging from self-tuning wave controls to wave-based logic\ngates. This research will provide the basis for creating future new structures\nthat would greatly surpass the state of the art - such as lower power\nconsumption, more direct interactions, and much better survivability in harsh\nenvironment or under cyberattacks. Moreover, it will enable the addition of new\nfunctions and autonomy to systems without overburdening the onboard computers.",
                "Probabilistic computation and uncertainty quantification with emerging\n  covariance\nBuilding robust, interpretable, and secure AI system requires quantifying and\nrepresenting uncertainty under a probabilistic perspective to mimic human\ncognitive abilities. However, probabilistic computation presents significant\nchallenges for most conventional artificial neural network, as they are\nessentially implemented in a deterministic manner. In this paper, we develop an\nefficient probabilistic computation framework by truncating the probabilistic\nrepresentation of neural activation up to its mean and covariance and construct\na moment neural network that encapsulates the nonlinear coupling between the\nmean and covariance of the underlying stochastic network. We reveal that when\nonly the mean but not the covariance is supervised during gradient-based\nlearning, the unsupervised covariance spontaneously emerges from its nonlinear\ncoupling with the mean and faithfully captures the uncertainty associated with\nmodel predictions. Our findings highlight the inherent simplicity of\nprobabilistic computation by seamlessly incorporating uncertainty into model\nprediction, paving the way for integrating it into large-scale AI systems.",
                "Recasting Self-Attention with Holographic Reduced Representations\nIn recent years, self-attention has become the dominant paradigm for sequence\nmodeling in a variety of domains. However, in domains with very long sequence\nlengths the $\\mathcal{O}(T^2)$ memory and $\\mathcal{O}(T^2 H)$ compute costs\ncan make using transformers infeasible. Motivated by problems in malware\ndetection, where sequence lengths of $T \\geq 100,000$ are a roadblock to deep\nlearning, we re-cast self-attention using the neuro-symbolic approach of\nHolographic Reduced Representations (HRR). In doing so we perform the same\nhigh-level strategy of the standard self-attention: a set of queries matching\nagainst a set of keys, and returning a weighted response of the values for each\nkey. Implemented as a ``Hrrformer'' we obtain several benefits including\n$\\mathcal{O}(T H \\log H)$ time complexity, $\\mathcal{O}(T H)$ space complexity,\nand convergence in $10\\times$ fewer epochs. Nevertheless, the Hrrformer\nachieves near state-of-the-art accuracy on LRA benchmarks and we are able to\nlearn with just a single layer. Combined, these benefits make our Hrrformer the\nfirst viable Transformer for such long malware classification sequences and up\nto $280\\times$ faster to train on the Long Range Arena benchmark. Code is\navailable at\n\\url{https://github.com/NeuromorphicComputationResearchProgram/Hrrformer}",
                "Unified Information Dynamic Analysis of Quantum Decision-Making and\n  Search Algorithms: Computational Intelligence Measure\nThere are important algorithms built upon a mixture of basic techniques\ndescribed; for example, the Fast Fourier Transform (FFT) employs both\nDivide-and-Conquer and Transform-and-Conquer techniques. In this article, the\nevolution of a quantum algorithm (QA) is examined from an information theory\nviewpoint. The complex vector entering the quantum algorithmic gate - QAG is\nconsidered as an information source both from the classical and the quantum\nlevel. The analysis of the classical and quantum information flow in\nDeutsch-Jozsa, Shor and Grover algorithms is used. It is shown that QAG, based\non superposition of states, quantum entanglement and interference, when acting\non the input vector, stores information into the system state, minimizing the\ngap between classical Shannon entropy and quantum von Neumann entropy.\nMinimizing of the gap between Shannon and von Neumann entropies is considered\nas a termination criterion of QA computational intelligence measure.",
                "SheetCopilot: Bringing Software Productivity to the Next Level through\n  Large Language Models\nComputer end users have spent billions of hours completing daily tasks like\ntabular data processing and project timeline scheduling. Most of these tasks\nare repetitive and error-prone, yet most end users lack the skill to automate\nthese burdensome works. With the advent of large language models (LLMs),\ndirecting software with natural language user requests become a reachable goal.\nIn this work, we propose a SheetCopilot agent that takes natural language task\nand control spreadsheet to fulfill the requirements. We propose a set of atomic\nactions as an abstraction of spreadsheet software functionalities. We further\ndesign a state machine-based task planning framework for LLMs to robustly\ninteract with spreadsheets. We curate a representative dataset containing 221\nspreadsheet control tasks and establish a fully automated evaluation pipeline\nfor rigorously benchmarking the ability of LLMs in software control tasks. Our\nSheetCopilot correctly completes 44.3\\% of tasks for a single generation,\noutperforming the strong code generation baseline by a wide margin. Our project\npage:https://sheetcopilot.github.io/."
            ],
            "interesting paper": 3
        },
        {
            "papers": [
                "Environmental Memory Boosts Group Formation of Clueless Individuals\nThe formation of groups of interacting individuals improves performance and\nfitness in many decentralised systems, from micro-organisms to social insects,\nfrom robotic swarms to artificial intelligence algorithms. Often, group\nformation and high-level coordination in these systems emerge from individuals\nwith limited information-processing capabilities implementing low-level rules\nof communication to signal to each other. Here, we show that, even in a\ncommunity of clueless individuals incapable of processing information and\ncommunicating, a dynamic environment can coordinate group formation by\ntransiently storing memory of the earlier passage of individuals. Our results\nidentify a new mechanism of indirect coordination via shared memory that is\nprimarily promoted and reinforced by dynamic environmental factors, thus\novershadowing the need for any form of explicit signalling between individuals.\nWe expect this pathway to group formation to be relevant for understanding and\ncontrolling self-organisation and collective decision making in both living and\nartificial active matter in real-life environments.",
                "Assessing Language Disorders using Artificial Intelligence: a Paradigm\n  Shift\nSpeech, language, and communication deficits are present in most\nneurodegenerative syndromes. They enable the early detection, diagnosis,\ntreatment planning, and monitoring of neurocognitive disease progression as\npart of traditional neurological assessment. Nevertheless, standard speech and\nlanguage evaluation is time-consuming and resource-intensive for clinicians. We\nargue that using machine learning methodologies, natural language processing,\nand modern artificial intelligence (AI) for Language Assessment is an\nimprovement over conventional manual assessment. Using these methodologies,\nComputational Language Assessment (CLA) accomplishes three goals: (i) provides\na neuro-cognitive evaluation of speech, language, and communication in elderly\nand high-risk individuals for dementia; (ii) facilitates the diagnosis,\nprognosis, and therapy efficacy in at-risk and language-impaired populations;\nand (iii) allows easier extensibility to assess patients from a wide range of\nlanguages. By employing AI models, CLA may inform neurocognitive theory on the\nrelationship between language symptoms and their neural bases. Finally, it\nsignals a paradigm shift by significantly advancing our ability to optimize the\nprevention and treatment of elderly individuals with communication disorders,\nallowing them to age gracefully with social engagement.",
                "Human Control: Definitions and Algorithms\nHow can humans stay in control of advanced artificial intelligence systems?\nOne proposal is corrigibility, which requires the agent to follow the\ninstructions of a human overseer, without inappropriately influencing them. In\nthis paper, we formally define a variant of corrigibility called shutdown\ninstructability, and show that it implies appropriate shutdown behavior,\nretention of human autonomy, and avoidance of user harm. We also analyse the\nrelated concepts of non-obstruction and shutdown alignment, three previously\nproposed algorithms for human control, and one new algorithm.",
                "Human or Not? A Gamified Approach to the Turing Test\nWe present \"Human or Not?\", an online game inspired by the Turing test, that\nmeasures the capability of AI chatbots to mimic humans in dialog, and of humans\nto tell bots from other humans. Over the course of a month, the game was played\nby over 1.5 million users who engaged in anonymous two-minute chat sessions\nwith either another human or an AI language model which was prompted to behave\nlike humans. The task of the players was to correctly guess whether they spoke\nto a person or to an AI. This largest scale Turing-style test conducted to date\nrevealed some interesting facts. For example, overall users guessed the\nidentity of their partners correctly in only 68% of the games. In the subset of\nthe games in which users faced an AI bot, users had even lower correct guess\nrates of 60% (that is, not much higher than chance). This white paper details\nthe development, deployment, and results of this unique experiment. While this\nexperiment calls for many extensions and refinements, these findings already\nbegin to shed light on the inevitable near future which will commingle humans\nand AI.",
                "EMOTE: An Explainable architecture for Modelling the Other Through\n  Empathy\nWe can usually assume others have goals analogous to our own. This assumption\ncan also, at times, be applied to multi-agent games - e.g. Agent 1's attraction\nto green pellets is analogous to Agent 2's attraction to red pellets. This\n\"analogy\" assumption is tied closely to the cognitive process known as empathy.\nInspired by empathy, we design a simple and explainable architecture to model\nanother agent's action-value function. This involves learning an \"Imagination\nNetwork\" to transform the other agent's observed state in order to produce a\nhuman-interpretable \"empathetic state\" which, when presented to the learning\nagent, produces behaviours that mimic the other agent. Our approach is\napplicable to multi-agent scenarios consisting of a single learning agent and\nother (independent) agents acting according to fixed policies. This\narchitecture is particularly beneficial for (but not limited to) algorithms\nusing a composite value or reward function. We show our method produces better\nperformance in multi-agent games, where it robustly estimates the other's model\nin different environment configurations. Additionally, we show that the\nempathetic states are human interpretable, and thus verifiable.",
                "Chain-Of-Thought Prompting Under Streaming Batch: A Case Study\nRecently, Large Language Models (LLMs) have demonstrated remarkable\ncapabilities. Chain-of-Thought (CoT) has been proposed as a way of assisting\nLLMs in performing complex reasoning. However, developing effective prompts can\nbe a challenging and labor-intensive task. Many studies come out of some way to\nautomatically construct CoT from test data. Most of them assume that all test\ndata is visible before testing and only select a small subset to generate\nrationales, which is an unrealistic assumption. In this paper, we present a\ncase study on how to construct and optimize chain-of-thought prompting using\nbatch data in streaming settings."
            ],
            "interesting paper": 4
        },
        {
            "papers": [
                "Integrated Sensing-Communication-Computation for Edge Artificial\n  Intelligence\nEdge artificial intelligence (AI) has been a promising solution towards 6G to\nempower a series of advanced techniques such as digital twins, holographic\nprojection, semantic communications, and auto-driving, for achieving\nintelligence of everything. The performance of edge AI tasks, including edge\nlearning and edge AI inference, depends on the quality of three highly coupled\nprocesses, i.e., sensing for data acquisition, computation for information\nextraction, and communication for information transmission. However, these\nthree modules need to compete for network resources for enhancing their own\nquality-of-services. To this end, integrated sensing-communication-computation\n(ISCC) is of paramount significance for improving resource utilization as well\nas achieving the customized goals of edge AI tasks. By investigating the\ninterplay among the three modules, this article presents various kinds of ISCC\nschemes for federated edge learning tasks and edge AI inference tasks in both\napplication and physical layers.",
                "Minding Language Models' (Lack of) Theory of Mind: A Plug-and-Play\n  Multi-Character Belief Tracker\nTheory of Mind (ToM)$\\unicode{x2014}$the ability to reason about the mental\nstates of other people$\\unicode{x2014}$is a key element of our social\nintelligence. Yet, despite their ever more impressive performance, large-scale\nneural language models still lack basic theory of mind capabilities\nout-of-the-box. We posit that simply scaling up models will not imbue them with\ntheory of mind due to the inherently symbolic and implicit nature of the\nphenomenon, and instead investigate an alternative: can we design a\ndecoding-time algorithm that enhances theory of mind of off-the-shelf neural\nlanguage models without explicit supervision? We present SymbolicToM, a\nplug-and-play approach to reason about the belief states of multiple characters\nin reading comprehension tasks via explicit symbolic representation. More\nconcretely, our approach tracks each entity's beliefs, their estimation of\nother entities' beliefs, and higher-order levels of reasoning, all through\ngraphical representations, allowing for more precise and interpretable\nreasoning than previous approaches. Empirical results on the well-known ToMi\nbenchmark (Le et al., 2019) demonstrate that SymbolicToM dramatically enhances\noff-the-shelf neural networks' theory of mind in a zero-shot setting while\nshowing robust out-of-distribution performance compared to supervised\nbaselines. Our work also reveals spurious patterns in existing theory of mind\nbenchmarks, emphasizing the importance of out-of-distribution evaluation and\nmethods that do not overfit a particular dataset.",
                "On the Existence of Information Bottlenecks in Living and Non-Living\n  Systems\nIn many complex systems, we observe that `interesting behaviour' is often the\nconsequence of a system exploiting the existence of an Information Bottleneck\n(IB). These bottlenecks can occur at different scales, between individuals or\ncomponents of a system, and sometimes within individuals themselves.\nOftentimes, we regard these bottlenecks negatively; as merely the limitations\nof the individual's physiology and something that ought to be overcome when\ndesigning and implementing artificial systems. However, we suggest instead that\nIBs may serve a purpose beyond merely providing a minimally-viable channel for\ncoordination in collective systems. More specifically, we suggest that\ninteresting or novel behaviour occurs when the individuals in a system are\nconstrained or limited in their ability to share information and must discover\nnovel ways to exploit existing mechanisms, which are inherently bottlenecked,\nrather than circumventing or otherwise avoiding those mechanisms entirely.",
                "An Architecture for Deploying Reinforcement Learning in Industrial\n  Environments\nIndustry 4.0 is driven by demands like shorter time-to-market, mass\ncustomization of products, and batch size one production. Reinforcement\nLearning (RL), a machine learning paradigm shown to possess a great potential\nin improving and surpassing human level performance in numerous complex tasks,\nallows coping with the mentioned demands. In this paper, we present an OPC UA\nbased Operational Technology (OT)-aware RL architecture, which extends the\nstandard RL setting, combining it with the setting of digital twins. Moreover,\nwe define an OPC UA information model allowing for a generalized plug-and-play\nlike approach for exchanging the RL agent used. In conclusion, we demonstrate\nand evaluate the architecture, by creating a proof of concept. By means of\nsolving a toy example, we show that this architecture can be used to determine\nthe optimal policy using a real control system.",
                "The feasibility of artificial consciousness through the lens of\n  neuroscience\nInteractions with large language models have led to the suggestion that these\nmodels may soon be conscious. From the perspective of neuroscience, this\nposition is difficult to defend. For one, the inputs to large language models\nlack the embodied, embedded information content characteristic of our sensory\ncontact with the world around us. Secondly, the architecture of large language\nmodels is missing key features of the thalamocortical system that have been\nlinked to conscious awareness in mammals. Finally, the evolutionary and\ndevelopmental trajectories that led to the emergence of living conscious\norganisms arguably have no parallels in artificial systems as envisioned today.\nThe existence of living organisms depends on their actions, and their survival\nis intricately linked to multi-level cellular, inter-cellular, and organismal\nprocesses culminating in agency and consciousness.",
                "Responsible Task Automation: Empowering Large Language Models as\n  Responsible Task Automators\nThe recent success of Large Language Models (LLMs) signifies an impressive\nstride towards artificial general intelligence. They have shown a promising\nprospect in automatically completing tasks upon user instructions, functioning\nas brain-like coordinators. The associated risks will be revealed as we\ndelegate an increasing number of tasks to machines for automated completion. A\nbig question emerges: how can we make machines behave responsibly when helping\nhumans automate tasks as personal copilots? In this paper, we explore this\nquestion in depth from the perspectives of feasibility, completeness and\nsecurity. In specific, we present Responsible Task Automation (ResponsibleTA)\nas a fundamental framework to facilitate responsible collaboration between\nLLM-based coordinators and executors for task automation with three empowered\ncapabilities: 1) predicting the feasibility of the commands for executors; 2)\nverifying the completeness of executors; 3) enhancing the security (e.g., the\nprotection of users' privacy). We further propose and compare two paradigms for\nimplementing the first two capabilities. One is to leverage the generic\nknowledge of LLMs themselves via prompt engineering while the other is to adopt\ndomain-specific learnable models. Moreover, we introduce a local memory\nmechanism for achieving the third capability. We evaluate our proposed\nResponsibleTA on UI task automation and hope it could bring more attentions to\nensuring LLMs more responsible in diverse scenarios."
            ],
            "interesting paper": 2
        }
    ],
    "Taylor Clark": [
        {
            "papers": [
                "Local control for the collective dynamics of self-propelled particles\nUtilizing a paradigmatic model for the motion of interacting self-propelled\nparticles, we demonstrate that local accelerations at the level of individual\nparticles can drive transitions between different collective dynamics, leading\nto a control process. We find that the ability to trigger such transitions is\nhierarchically distributed among the particles and can form distinctive spatial\npatterns within the collective. Chaotic dynamics occur during the transitions,\nwhich can be attributed to fractal basin boundaries mediating the control\nprocess. The particle hierarchies described in this study offer decentralized\ncapabilities for controlling artificial swarms.",
                "Cooperative Control of Multi-Channel Linear Systems with Self-Organizing\n  Private Agents\nCooperative behavior design for multi-agent systems with collective tasks is\na critical issue in promoting swarm intelligence. This paper investigates\ncooperative control for a multi-channel system, where each channel is managed\nby an agent expected to self-organize a controller to stabilize the system\ncollaboratively by communicating with neighbors in a network. Integrating a\nstate decomposition technique and a fusion approach, a fully distributed\nprivacy-preserving mechanism is proposed to shield agents' private information\nfrom neighbors' eavesdropping. Moreover, the cost of introducing the\nprivacy-preserving mechanism and the benefit of adding more channels to the\nsystem are quantitatively analyzed. Finally, comparative simulation examples\nare provided to demonstrate the effectiveness of the theoretical results.",
                "Selection for short-term empowerment accelerates the evolution of\n  homeostatic neural cellular automata\nEmpowerment -- a domain independent, information-theoretic metric -- has\npreviously been shown to assist in the evolutionary search for neural cellular\nautomata (NCA) capable of homeostasis when employed as a fitness function. In\nour previous study, we successfully extended empowerment, defined as maximum\ntime-lagged mutual information between agents' actions and future sensations,\nto a distributed sensorimotor system embodied as an NCA. However, the\ntime-delay between actions and their corresponding sensations was arbitrarily\nchosen. Here, we expand upon previous work by exploring how the time scale at\nwhich empowerment operates impacts its efficacy as an auxiliary objective to\naccelerate the discovery of homeostatic NCAs. We show that shorter time delays\nresult in marked improvements over empowerment with longer delays, when\ncompared to evolutionary selection only for homeostasis. Moreover, we evaluate\nstability and adaptability of evolved NCAs, both hallmarks of living systems\nthat are of interest to replicate in artificial ones. We find that short-term\nempowered NCA are more stable and are capable of generalizing better to unseen\nhomeostatic challenges. Taken together, these findings motivate the use of\nempowerment during the evolution of other artifacts, and suggest how it should\nbe incorporated to accelerate evolution of desired behaviors for them. Source\ncode for the experiments in this paper can be found at:\nhttps://github.com/caitlingrasso/empowered-nca-II.",
                "Lucy-SKG: Learning to Play Rocket League Efficiently Using Deep\n  Reinforcement Learning\nA successful tactic that is followed by the scientific community for\nadvancing AI is to treat games as problems, which has been proven to lead to\nvarious breakthroughs. We adapt this strategy in order to study Rocket League,\na widely popular but rather under-explored 3D multiplayer video game with a\ndistinct physics engine and complex dynamics that pose a significant challenge\nin developing efficient and high-performance game-playing agents. In this\npaper, we present Lucy-SKG, a Reinforcement Learning-based model that learned\nhow to play Rocket League in a sample-efficient manner, outperforming by a\nnotable margin the two highest-ranking bots in this game, namely Necto (2022\nbot champion) and its successor Nexto, thus becoming a state-of-the-art agent.\nOur contributions include: a) the development of a reward analysis and\nvisualization library, b) novel parameterizable reward shape functions that\ncapture the utility of complex reward types via our proposed Kinesthetic Reward\nCombination (KRC) technique, and c) design of auxiliary neural architectures\nfor training on reward prediction and state representation tasks in an\non-policy fashion for enhanced efficiency in learning speed and performance. By\nperforming thorough ablation studies for each component of Lucy-SKG, we showed\ntheir independent effectiveness in overall performance. In doing so, we\ndemonstrate the prospects and challenges of using sample-efficient\nReinforcement Learning techniques for controlling complex dynamical systems\nunder competitive team-based multiplayer conditions.",
                "Distributed Online Rollout for Multivehicle Routing in Unmapped\n  Environments\nIn this work we consider a generalization of the well-known multivehicle\nrouting problem: given a network, a set of agents occupying a subset of its\nnodes, and a set of tasks, we seek a minimum cost sequence of movements subject\nto the constraint that each task is visited by some agent at least once. The\nclassical version of this problem assumes a central computational server that\nobserves the entire state of the system perfectly and directs individual agents\naccording to a centralized control scheme. In contrast, we assume that there is\nno centralized server and that each agent is an individual processor with no a\npriori knowledge of the underlying network (including task and agent\nlocations). Moreover, our agents possess strictly local communication and\nsensing capabilities (restricted to a fixed radius around their respective\nlocations), aligning more closely with several real-world multiagent\napplications. These restrictions introduce many challenges that are overcome\nthrough local information sharing and direct coordination between agents. We\npresent a fully distributed, online, and scalable reinforcement learning\nalgorithm for this problem whereby agents self-organize into local clusters and\nindependently apply a multiagent rollout scheme locally to each cluster. We\ndemonstrate empirically via extensive simulations that there exists a critical\nsensing radius beyond which the distributed rollout algorithm begins to improve\nover a greedy base policy. This critical sensing radius grows proportionally to\nthe $\\log^*$ function of the size of the network, and is, therefore, a small\nconstant for any relevant network. Our decentralized reinforcement learning\nalgorithm achieves approximately a factor of two cost improvement over the base\npolicy for a range of radii bounded from below and above by two and three times\nthe critical sensing radius, respectively.",
                "SPRING: Studying the Paper and Reasoning to Play Games\nOpen-world survival games pose significant challenges for AI algorithms due\nto their multi-tasking, deep exploration, and goal prioritization requirements.\nDespite reinforcement learning (RL) being popular for solving games, its high\nsample complexity limits its effectiveness in complex open-world games like\nCrafter or Minecraft. We propose a novel approach, SPRING, to read the game's\noriginal academic paper and use the knowledge learned to reason and play the\ngame through a large language model (LLM). Prompted with the LaTeX source as\ngame context and a description of the agent's current observation, our SPRING\nframework employs a directed acyclic graph (DAG) with game-related questions as\nnodes and dependencies as edges. We identify the optimal action to take in the\nenvironment by traversing the DAG and calculating LLM responses for each node\nin topological order, with the LLM's answer to final node directly translating\nto environment actions. In our experiments, we study the quality of in-context\n\"reasoning\" induced by different forms of prompts under the setting of the\nCrafter open-world environment. Our experiments suggest that LLMs, when\nprompted with consistent chain-of-thought, have great potential in completing\nsophisticated high-level trajectories. Quantitatively, SPRING with GPT-4\noutperforms all state-of-the-art RL baselines, trained for 1M steps, without\nany training. Finally, we show the potential of games as a test bed for LLMs."
            ],
            "interesting paper": 2
        },
        {
            "papers": [
                "Individuality in Swarm Robots with the Case Study of Kilobots: Noise,\n  Bug, or Feature?\nInter-individual differences are studied in natural systems, such as fish,\nbees, and humans, as they contribute to the complexity of both individual and\ncollective behaviors. However, individuality in artificial systems, such as\nrobotic swarms, is undervalued or even overlooked. Agent-specific deviations\nfrom the norm in swarm robotics are usually understood as mere noise that can\nbe minimized, for example, by calibration. We observe that robots have\nconsistent deviations and argue that awareness and knowledge of these can be\nexploited to serve a task. We measure heterogeneity in robot swarms caused by\nindividual differences in how robots act, sense, and oscillate. Our use case is\nKilobots and we provide example behaviors where the performance of robots\nvaries depending on individual differences. We show a non-intuitive example of\nphototaxis with Kilobots where the non-calibrated Kilobots show better\nperformance than the calibrated supposedly ``ideal\" one. We measure the\ninter-individual variations for heterogeneity in sensing and oscillation, too.\nWe briefly discuss how these variations can enhance the complexity of\ncollective behaviors. We suggest that by recognizing and exploring this new\nperspective on individuality, and hence diversity, in robotic swarms, we can\ngain a deeper understanding of these systems and potentially unlock new\npossibilities for their design and implementation of applications.",
                "AI Techniques in the Microservices Life-Cycle: A Survey\nMicroservices is a popular architectural style for the development of\ndistributed software, with an emphasis on modularity, scalability, and\nflexibility. Indeed, in microservice systems, functionalities are provided by\nloosely coupled, small services, each focusing on a specific business\ncapability. Building a system according to the microservices architectural\nstyle brings a number of challenges, mainly related to how the different\nmicroservices are deployed and coordinated and how they interact. In this\npaper, we provide a survey about how techniques in the area of Artificial\nIntelligence have been used to tackle these challenges.",
                "A Decentralized Spike-based Learning Framework for Sequential Capture in\n  Discrete Perimeter Defense Problem\nThis paper proposes a novel Decentralized Spike-based Learning (DSL)\nframework for the discrete Perimeter Defense Problem (d-PDP). A team of\ndefenders is operating on the perimeter to protect the circular territory from\nradially incoming intruders. At first, the d-PDP is formulated as a\nspatio-temporal multi-task assignment problem (STMTA). The problem of STMTA is\nthen converted into a multi-label learning problem to obtain labels of segments\nthat defenders have to visit in order to protect the perimeter. The DSL\nframework uses a Multi-Label Classifier using Synaptic Efficacy Function\nspiking neuRON (MLC-SEFRON) network for deterministic multi-label learning.\nEach defender contains a single MLC-SEFRON network. Each MLC-SEFRON network is\ntrained independently using input from its own perspective for decentralized\noperations. The input spikes to the MLC-SEFRON network can be directly obtained\nfrom the spatio-temporal information of defenders and intruders without any\nextra pre-processing step. The output of MLC-SEFRON contains the labels of\nsegments that a defender has to visit in order to protect the perimeter. Based\non the multi-label output from the MLC-SEFRON a trajectory is generated for a\ndefender using a Consensus-Based Bundle Algorithm (CBBA) in order to capture\nthe intruders. The target multi-label output for training MLC-SEFRON is\nobtained from an expert policy. Also, the MLC-SEFRON trained for a defender can\nbe directly used for obtaining labels of segments assigned to another defender\nwithout any retraining. The performance of MLC-SEFRON has been evaluated for\nfull observation and partial observation scenarios of the defender. The overall\nperformance of the DSL framework is then compared with expert policy along with\nother existing learning algorithms. The scalability of the DSL has been\nevaluated using an increasing number of defenders.",
                "Exploring the Adaptive Behaviors of Particle Lenia: A\n  Perturbation-Response Analysis for Computational Agency\nA firm cognitive subject or ``individual'' is presupposed for the emergence\nof mind. However, with the development of recent information technology, the\n``individual'' has become more dispersed in society and the cognitive subject\nhas become increasingly unstable and adaptive, necessitating an update in our\nunderstanding of the ``individual''. Autopoiesis serves as a model of the\ncognitive subject, which is unstable and requires effort to maintain itself to\nadapt to the environment. In this study, we evaluated adaptivity for a highly\nextensible multi-particle system model Particle Lenia through the response\nperturbation. As a result, we found that Particle Lenia has a particle\nconfiguration that is both temporally unstable and has multiple stable states.\nThis result suggests that Particle Lenia can express adaptive characteristics\nand is expected to be used as a computational model toward building an\nautopoietic cognitive agent.",
                "Ghost in the Minecraft: Generally Capable Agents for Open-World\n  Environments via Large Language Models with Text-based Knowledge and Memory\nThe captivating realm of Minecraft has attracted substantial research\ninterest in recent years, serving as a rich platform for developing intelligent\nagents capable of functioning in open-world environments. However, the current\nresearch landscape predominantly focuses on specific objectives, such as the\npopular \"ObtainDiamond\" task, and has not yet shown effective generalization to\na broader spectrum of tasks. Furthermore, the current leading success rate for\nthe \"ObtainDiamond\" task stands at around 20%, highlighting the limitations of\nReinforcement Learning (RL) based controllers used in existing methods. To\ntackle these challenges, we introduce Ghost in the Minecraft (GITM), a novel\nframework integrates Large Language Models (LLMs) with text-based knowledge and\nmemory, aiming to create Generally Capable Agents (GCAs) in Minecraft. These\nagents, equipped with the logic and common sense capabilities of LLMs, can\nskillfully navigate complex, sparse-reward environments with text-based\ninteractions. We develop a set of structured actions and leverage LLMs to\ngenerate action plans for the agents to execute. The resulting LLM-based agent\nmarkedly surpasses previous methods, achieving a remarkable improvement of\n+47.5% in success rate on the \"ObtainDiamond\" task, demonstrating superior\nrobustness compared to traditional RL-based controllers. Notably, our agent is\nthe first to procure all items in the Minecraft Overworld technology tree,\ndemonstrating its extensive capabilities. GITM does not need any GPU for\ntraining, but a single CPU node with 32 CPU cores is enough. This research\nshows the potential of LLMs in developing capable agents for handling\nlong-horizon, complex tasks and adapting to uncertainties in open-world\nenvironments. See the project website at https://github.com/OpenGVLab/GITM.",
                "On Computing Universal Plans for Partially Observable Multi-Agent Path\n  Finding\nMulti-agent routing problems have drawn significant attention nowadays due to\ntheir broad industrial applications in, e.g., warehouse robots, logistics\nautomation, and traffic control. Conventionally, they are modelled as classical\nplanning problems. In this paper, we argue that it is beneficial to formulate\nthem as universal planning problems. We therefore propose universal plans, also\nknown as policies, as the solution concepts, and implement a system called\nASP-MAUPF (Answer Set Programming for Multi-Agent Universal Plan Finding) for\ncomputing them. Given an arbitrary two-dimensional map and a profile of goals\nfor the agents, the system finds a feasible universal plan for each agent that\nensures no collision with others. We use the system to conduct some\nexperiments, and make some observations on the types of goal profiles and\nenvironments that will have feasible policies, and how they may depend on\nagents' sensors. We also demonstrate how users can customize action preferences\nto compute more efficient policies, even (near-)optimal ones."
            ],
            "interesting paper": 1
        },
        {
            "papers": [
                "Communication-Efficient Reinforcement Learning in Swarm Robotic Networks\n  for Maze Exploration\nSmooth coordination within a swarm robotic system is essential for the\neffective execution of collective robot missions. Having efficient\ncommunication is key to the successful coordination of swarm robots. This paper\nproposes a new communication-efficient decentralized cooperative reinforcement\nlearning algorithm for coordinating swarm robots. It is made efficient by\nhierarchically building on the use of local information exchanges. We consider\na case study application of maze solving through cooperation among a group of\nrobots, where the time and costs are minimized while avoiding inter-robot\ncollisions and path overlaps during exploration. With a solid theoretical\nbasis, we extensively analyze the algorithm with realistic CORE network\nsimulations and evaluate it against state-of-the-art solutions in terms of maze\ncoverage percentage and efficiency under communication-degraded environments.\nThe results demonstrate significantly higher coverage accuracy and efficiency\nwhile reducing costs and overlaps even in high packet loss and low\ncommunication range scenarios.",
                "MADiff: Offline Multi-agent Learning with Diffusion Models\nDiffusion model (DM) recently achieved huge success in various scenarios\nincluding offline reinforcement learning, where the diffusion planner learn to\ngenerate desired trajectories during online evaluations. However, despite the\neffectiveness in single-agent learning, it remains unclear how DMs can operate\nin multi-agent problems, where agents can hardly complete teamwork without good\ncoordination by independently modeling each agent's trajectories. In this\npaper, we propose MADiff, a novel generative multi-agent learning framework to\ntackle this problem. MADiff is realized with an attention-based diffusion model\nto model the complex coordination among behaviors of multiple agents. To the\nbest of our knowledge, MADiff is the first diffusion-based multi-agent learning\nframework, which behaves as both a decentralized policy and a centralized\ncontroller. During decentralized executions, MADiff simultaneously performs\nteammate modeling, and the centralized controller can also be applied in\nmulti-agent trajectory predictions. Our experiments show the superior\nperformance of MADiff compared to baseline algorithms in a wide range of\nmulti-agent learning tasks, which emphasizes the effectiveness of MADiff in\nmodeling complex multi-agent interactions. Our code is available at\nhttps://github.com/zbzhu99/madiff.",
                "Mindstorms in Natural Language-Based Societies of Mind\nBoth Minsky's \"society of mind\" and Schmidhuber's \"learning to think\" inspire\ndiverse societies of large multimodal neural networks (NNs) that solve problems\nby interviewing each other in a \"mindstorm.\" Recent implementations of NN-based\nsocieties of minds consist of large language models (LLMs) and other NN-based\nexperts communicating through a natural language interface. In doing so, they\novercome the limitations of single LLMs, improving multimodal zero-shot\nreasoning. In these natural language-based societies of mind (NLSOMs), new\nagents -- all communicating through the same universal symbolic language -- are\neasily added in a modular fashion. To demonstrate the power of NLSOMs, we\nassemble and experiment with several of them (having up to 129 members),\nleveraging mindstorms in them to solve some practical AI tasks: visual question\nanswering, image captioning, text-to-image synthesis, 3D generation, egocentric\nretrieval, embodied AI, and general language-based task solving. We view this\nas a starting point towards much larger NLSOMs with billions of agents-some of\nwhich may be humans. And with this emergence of great societies of\nheterogeneous minds, many new research questions have suddenly become paramount\nto the future of artificial intelligence. What should be the social structure\nof an NLSOM? What would be the (dis)advantages of having a monarchical rather\nthan a democratic structure? How can principles of NN economies be used to\nmaximize the total reward of a reinforcement learning NLSOM? In this work, we\nidentify, discuss, and try to answer some of these questions.",
                "Attention Schema in Neural Agents\nAttention has become a common ingredient in deep learning architectures. It\nadds a dynamical selection of information on top of the static selection of\ninformation supported by weights. In the same way, we can imagine a\nhigher-order informational filter built on top of attention: an Attention\nSchema (AS), namely, a descriptive and predictive model of attention. In\ncognitive neuroscience, Attention Schema Theory (AST) supports this idea of\ndistinguishing attention from AS. A strong prediction of this theory is that an\nagent can use its own AS to also infer the states of other agents' attention\nand consequently enhance coordination with other agents. As such, multi-agent\nreinforcement learning would be an ideal setting to experimentally test the\nvalidity of AST. We explore different ways in which attention and AS interact\nwith each other. Our preliminary results indicate that agents that implement\nthe AS as a recurrent internal control achieve the best performance. In\ngeneral, these exploratory experiments suggest that equipping artificial agents\nwith a model of attention can enhance their social intelligence.",
                "A Model-Based Solution to the Offline Multi-Agent Reinforcement Learning\n  Coordination Problem\nTraining multiple agents to coordinate is an essential problem with\napplications in robotics, game theory, economics, and social sciences. However,\nmost existing Multi-Agent Reinforcement Learning (MARL) methods are online and\nthus impractical for real-world applications in which collecting new\ninteractions is costly or dangerous. While these algorithms should leverage\noffline data when available, doing so gives rise to what we call the offline\ncoordination problem. Specifically, we identify and formalize the strategy\nagreement (SA) and the strategy fine-tuning (SFT) coordination challenges, two\nissues at which current offline MARL algorithms fail. Concretely, we reveal\nthat the prevalent model-free methods are severely deficient and cannot handle\ncoordination-intensive offline multi-agent tasks in either toy or MuJoCo\ndomains. To address this setback, we emphasize the importance of inter-agent\ninteractions and propose the very first model-based offline MARL method. Our\nresulting algorithm, Model-based Offline Multi-Agent Proximal Policy\nOptimization (MOMA-PPO) generates synthetic interaction data and enables agents\nto converge on a strategy while fine-tuning their policies accordingly. This\nsimple model-based solution solves the coordination-intensive offline tasks,\nsignificantly outperforming the prevalent model-free methods even under severe\npartial observability and with learned world models.",
                "Exploiting Large Neuroimaging Datasets to Create Connectome-Constrained\n  Approaches for more Robust, Efficient, and Adaptable Artificial Intelligence\nDespite the progress in deep learning networks, efficient learning at the\nedge (enabling adaptable, low-complexity machine learning solutions) remains a\ncritical need for defense and commercial applications. We envision a pipeline\nto utilize large neuroimaging datasets, including maps of the brain which\ncapture neuron and synapse connectivity, to improve machine learning\napproaches. We have pursued different approaches within this pipeline\nstructure. First, as a demonstration of data-driven discovery, the team has\ndeveloped a technique for discovery of repeated subcircuits, or motifs. These\nwere incorporated into a neural architecture search approach to evolve network\narchitectures. Second, we have conducted analysis of the heading direction\ncircuit in the fruit fly, which performs fusion of visual and angular velocity\nfeatures, to explore augmenting existing computational models with new insight.\nOur team discovered a novel pattern of connectivity, implemented a new model,\nand demonstrated sensor fusion on a robotic platform. Third, the team analyzed\ncircuitry for memory formation in the fruit fly connectome, enabling the design\nof a novel generative replay approach. Finally, the team has begun analysis of\nconnectivity in mammalian cortex to explore potential improvements to\ntransformer networks. These constraints increased network robustness on the\nmost challenging examples in the CIFAR-10-C computer vision robustness\nbenchmark task, while reducing learnable attention parameters by over an order\nof magnitude. Taken together, these results demonstrate multiple potential\napproaches to utilize insight from neural systems for developing robust and\nefficient machine learning techniques."
            ],
            "interesting paper": 1
        },
        {
            "papers": [
                "On the Value of Myopic Behavior in Policy Reuse\nLeveraging learned strategies in unfamiliar scenarios is fundamental to human\nintelligence. In reinforcement learning, rationally reusing the policies\nacquired from other tasks or human experts is critical for tackling problems\nthat are difficult to learn from scratch. In this work, we present a framework\ncalled Selective Myopic bEhavior Control~(SMEC), which results from the insight\nthat the short-term behaviors of prior policies are sharable across tasks. By\nevaluating the behaviors of prior policies via a hybrid value function\narchitecture, SMEC adaptively aggregates the sharable short-term behaviors of\nprior policies and the long-term behaviors of the task policy, leading to\ncoordinated decisions. Empirical results on a collection of manipulation and\nlocomotion tasks demonstrate that SMEC outperforms existing methods, and\nvalidate the ability of SMEC to leverage related prior policies.",
                "Probing reaction channels via reinforcement learning\nWe propose a reinforcement learning based method to identify important\nconfigurations that connect reactant and product states along chemical reaction\npaths. By shooting multiple trajectories from these configurations, we can\ngenerate an ensemble of configurations that concentrate on the transition path\nensemble. This configuration ensemble can be effectively employed in a neural\nnetwork-based partial differential equation solver to obtain an approximation\nsolution of a restricted Backward Kolmogorov equation, even when the dimension\nof the problem is very high. The resulting solution, known as the committor\nfunction, encodes mechanistic information for the reaction and can in turn be\nused to evaluate reaction rates.",
                "Emergent Modularity in Pre-trained Transformers\nThis work examines the presence of modularity in pre-trained Transformers, a\nfeature commonly found in human brains and thought to be vital for general\nintelligence. In analogy to human brains, we consider two main characteristics\nof modularity: (1) functional specialization of neurons: we evaluate whether\neach neuron is mainly specialized in a certain function, and find that the\nanswer is yes. (2) function-based neuron grouping: we explore finding a\nstructure that groups neurons into modules by function, and each module works\nfor its corresponding function. Given the enormous amount of possible\nstructures, we focus on Mixture-of-Experts as a promising candidate, which\npartitions neurons into experts and usually activates different experts for\ndifferent inputs. Experimental results show that there are functional experts,\nwhere clustered are the neurons specialized in a certain function. Moreover,\nperturbing the activations of functional experts significantly affects the\ncorresponding function. Finally, we study how modularity emerges during\npre-training, and find that the modular structure is stabilized at the early\nstage, which is faster than neuron stabilization. It suggests that Transformers\nfirst construct the modular structure and then learn fine-grained neuron\nfunctions. Our code and data are available at\nhttps://github.com/THUNLP/modularity-analysis.",
                "Observation Denoising in CYRUS Soccer Simulation 2D Team For RoboCup\n  2023\nThe RoboCup competitions hold various leagues, and the Soccer Simulation 2D\nLeague is a major one among them. Soccer Simulation 2D (SS2D) match involves\ntwo teams, including 11 players and a coach, competing against each other. The\nplayers can only communicate with the Soccer Simulation Server during the game.\nThis paper presents the latest research of the CYRUS soccer simulation 2D team,\nthe champion of RoboCup 2021. We will explain our denoising idea powered by\nlong short-term memory networks (LSTM) and deep neural networks (DNN). The\nCYRUS team uses the CYRUS2D base code that was developed based on the Helios\nand Gliders bases.",
                "Integrating Action Knowledge and LLMs for Task Planning and Situation\n  Handling in Open Worlds\nTask planning systems have been developed to help robots use human knowledge\n(about actions) to complete long-horizon tasks. Most of them have been\ndeveloped for \"closed worlds\" while assuming the robot is provided with\ncomplete world knowledge. However, the real world is generally open, and the\nrobots frequently encounter unforeseen situations that can potentially break\nthe planner's completeness. Could we leverage the recent advances on\npre-trained Large Language Models (LLMs) to enable classical planning systems\nto deal with novel situations?\n  This paper introduces a novel framework, called COWP, for open-world task\nplanning and situation handling. COWP dynamically augments the robot's action\nknowledge, including the preconditions and effects of actions, with\ntask-oriented commonsense knowledge. COWP embraces the openness from LLMs, and\nis grounded to specific domains via action knowledge. For systematic\nevaluations, we collected a dataset that includes 1,085 execution-time\nsituations. Each situation corresponds to a state instance wherein a robot is\npotentially unable to complete a task using a solution that normally works.\nExperimental results show that our approach outperforms competitive baselines\nfrom the literature in the success rate of service tasks. Additionally, we have\ndemonstrated COWP using a mobile manipulator. Supplementary materials are\navailable at: https://cowplanning.github.io/",
                "Synthesizing a Progression of Subtasks for Block-Based Visual\n  Programming Tasks\nBlock-based visual programming environments play an increasingly important\nrole in introducing computing concepts to K-12 students. In recent years, they\nhave also gained popularity in neuro-symbolic AI, serving as a benchmark to\nevaluate general problem-solving and logical reasoning skills. The open-ended\nand conceptual nature of these visual programming tasks make them challenging,\nboth for state-of-the-art AI agents as well as for novice programmers. A\nnatural approach to providing assistance for problem-solving is breaking down a\ncomplex task into a progression of simpler subtasks; however, this is not\ntrivial given that the solution codes are typically nested and have non-linear\nexecution behavior. In this paper, we formalize the problem of synthesizing\nsuch a progression for a given reference block-based visual programming task.\nWe propose a novel synthesis algorithm that generates a progression of subtasks\nthat are high-quality, well-spaced in terms of their complexity, and solving\nthis progression leads to solving the reference task. We show the utility of\nour synthesis algorithm in improving the efficacy of AI agents (in this case,\nneural program synthesizers) for solving tasks in the Karel programming\nenvironment. Then, we conduct a user study to demonstrate that our synthesized\nprogression of subtasks can assist a novice programmer in solving tasks in the\nHour of Code: Maze Challenge by Code-dot-org."
            ],
            "interesting paper": 3
        },
        {
            "papers": [
                "A Hybrid Framework of Reinforcement Learning and Convex Optimization for\n  UAV-Based Autonomous Metaverse Data Collection\nUnmanned aerial vehicles (UAVs) are promising for providing communication\nservices due to their advantages in cost and mobility, especially in the\ncontext of the emerging Metaverse and Internet of Things (IoT). This paper\nconsiders a UAV-assisted Metaverse network, in which UAVs extend the coverage\nof the base station (BS) to collect the Metaverse data generated at roadside\nunits (RSUs). Specifically, to improve the data collection efficiency, resource\nallocation and trajectory control are integrated into the system model. The\ntime-dependent nature of the optimization problem makes it non-trivial to be\nsolved by traditional convex optimization methods. Based on the proposed\nUAV-assisted Metaverse network system model, we design a hybrid framework with\nreinforcement learning and convex optimization to {cooperatively} solve the\ntime-sequential optimization problem. Simulation results show that the proposed\nframework is able to reduce the mission completion time with a given\ntransmission power resource.",
                "Taming AI Bots: Controllability of Neural States in Large Language\n  Models\nWe tackle the question of whether an agent can, by suitable choice of\nprompts, control an AI bot to any state. To that end, we first introduce a\nformal definition of ``meaning'' that is amenable to analysis. Then, we\ncharacterize ``meaningful data'' on which large language models (LLMs) are\nostensibly trained, and ``well-trained LLMs'' through conditions that are\nlargely met by today's LLMs. While a well-trained LLM constructs an embedding\nspace of meanings that is Euclidean, meanings themselves do not form a vector\n(linear) subspace, but rather a quotient space within. We then characterize the\nsubset of meanings that can be reached by the state of the LLMs for some input\nprompt, and show that a well-trained bot can reach any meaning albeit with\nsmall probability. We then introduce a stronger notion of controllability as\n{\\em almost certain reachability}, and show that, when restricted to the space\nof meanings, an AI bot is controllable. We do so after introducing a functional\ncharacterization of attentive AI bots, and finally derive necessary and\nsufficient conditions for controllability. The fact that AI bots are\ncontrollable means that an adversary could steer them towards any state.\nHowever, the sampling process can be designed to counteract adverse actions and\navoid reaching undesirable regions of state space before their boundary is\ncrossed.",
                "Diffusion Model is an Effective Planner and Data Synthesizer for\n  Multi-Task Reinforcement Learning\nDiffusion models have demonstrated highly-expressive generative capabilities\nin vision and NLP. Recent studies in reinforcement learning (RL) have shown\nthat diffusion models are also powerful in modeling complex policies or\ntrajectories in offline datasets. However, these works have been limited to\nsingle-task settings where a generalist agent capable of addressing multi-task\npredicaments is absent. In this paper, we aim to investigate the effectiveness\nof a single diffusion model in modeling large-scale multi-task offline data,\nwhich can be challenging due to diverse and multimodal data distribution.\nSpecifically, we propose Multi-Task Diffusion Model (\\textsc{MTDiff}), a\ndiffusion-based method that incorporates Transformer backbones and prompt\nlearning for generative planning and data synthesis in multi-task offline\nsettings. \\textsc{MTDiff} leverages vast amounts of knowledge available in\nmulti-task data and performs implicit knowledge sharing among tasks. For\ngenerative planning, we find \\textsc{MTDiff} outperforms state-of-the-art\nalgorithms across 50 tasks on Meta-World and 8 maps on Maze2D. For data\nsynthesis, \\textsc{MTDiff} generates high-quality data for testing tasks given\na single demonstration as a prompt, which enhances the low-quality datasets for\neven unseen tasks.",
                "Beyond the Meta: Leveraging Game Design Parameters for Patch-Agnostic\n  Esport Analytics\nEsport games comprise a sizeable fraction of the global games market, and is\nthe fastest growing segment in games. This has given rise to the domain of\nesports analytics, which uses telemetry data from games to inform players,\ncoaches, broadcasters and other stakeholders. Compared to traditional sports,\nesport titles change rapidly, in terms of mechanics as well as rules. Due to\nthese frequent changes to the parameters of the game, esport analytics models\ncan have a short life-spam, a problem which is largely ignored within the\nliterature. This paper extracts information from game design (i.e. patch notes)\nand utilises clustering techniques to propose a new form of character\nrepresentation. As a case study, a neural network model is trained to predict\nthe number of kills in a Dota 2 match utilising this novel character\nrepresentation technique. The performance of this model is then evaluated\nagainst two distinct baselines, including conventional techniques. Not only did\nthe model significantly outperform the baselines in terms of accuracy (85%\nAUC), but the model also maintains the accuracy in two newer iterations of the\ngame that introduced one new character and a brand new character type. These\nchanges introduced to the design of the game would typically break conventional\ntechniques that are commonly used within the literature. Therefore, the\nproposed methodology for representing characters can increase the life-spam of\nmachine learning models as well as contribute to a higher performance when\ncompared to traditional techniques typically employed within the literature.",
                "Continual Task Allocation in Meta-Policy Network via Sparse Prompting\nHow to train a generalizable meta-policy by continually learning a sequence\nof tasks? It is a natural human skill yet challenging to achieve by current\nreinforcement learning: the agent is expected to quickly adapt to new tasks\n(plasticity) meanwhile retaining the common knowledge from previous tasks\n(stability). We address it by \"Continual Task Allocation via Sparse Prompting\n(CoTASP)\", which learns over-complete dictionaries to produce sparse masks as\nprompts extracting a sub-network for each task from a meta-policy network.\nCoTASP trains a policy for each task by optimizing the prompts and the\nsub-network weights alternatively. The dictionary is then updated to align the\noptimized prompts with tasks' embedding, thereby capturing tasks' semantic\ncorrelations. Hence, relevant tasks share more neurons in the meta-policy\nnetwork due to similar prompts while cross-task interference causing forgetting\nis effectively restrained. Given a meta-policy and dictionaries trained on\nprevious tasks, new task adaptation reduces to highly efficient sparse\nprompting and sub-network finetuning. In experiments, CoTASP achieves a\npromising plasticity-stability trade-off without storing or replaying any past\ntasks' experiences. It outperforms existing continual and multi-task RL methods\non all seen tasks, forgetting reduction, and generalization to unseen tasks.",
                "Determinantal Point Process Attention Over Grid Cell Code Supports Out\n  of Distribution Generalization\nDeep neural networks have made tremendous gains in emulating human-like\nintelligence, and have been used increasingly as ways of understanding how the\nbrain may solve the complex computational problems on which this relies.\nHowever, these still fall short of, and therefore fail to provide insight into\nhow the brain supports strong forms of generalization of which humans are\ncapable. One such case is out-of-distribution (OOD) generalization-successful\nperformance on test examples that lie outside the distribution of the training\nset. Here, we identify properties of processing in the brain that may\ncontribute to this ability. We describe a two-part algorithm that draws on\nspecific features of neural computation to achieve OOD generalization, and\nprovide a proof of concept by evaluating performance on two challenging\ncognitive tasks. First we draw on the fact that the mammalian brain represents\nmetric spaces using grid cell code (e.g., in the entorhinal cortex): abstract\nrepresentations of relational structure, organized in recurring motifs that\ncover the representational space. Second, we propose an attentional mechanism\nthat operates over the grid cell code using Determinantal Point Process (DPP),\nthat we call DPP attention (DPP-A) -- a transformation that ensures maximum\nsparseness in the coverage of that space. We show that a loss function that\ncombines standard task-optimized error with DPP-A can exploit the recurring\nmotifs in the grid cell code, and can be integrated with common architectures\nto achieve strong OOD generalization performance on analogy and arithmetic\ntasks. This provides both an interpretation of how the grid cell code in the\nmammalian brain may contribute to generalization performance, and at the same\ntime a potential means for improving such capabilities in artificial neural\nnetworks."
            ],
            "interesting paper": 3
        },
        {
            "papers": [
                "Towards a Unifying Model of Rationality in Multiagent Systems\nMultiagent systems deployed in the real world need to cooperate with other\nagents (including humans) nearly as effectively as these agents cooperate with\none another. To design such AI, and provide guarantees of its effectiveness, we\nneed to clearly specify what types of agents our AI must be able to cooperate\nwith. In this work we propose a generic model of socially intelligent agents,\nwhich are individually rational learners that are also able to cooperate with\none another (in the sense that their joint behavior is Pareto efficient). We\ndefine rationality in terms of the regret incurred by each agent over its\nlifetime, and show how we can construct socially intelligent agents for\ndifferent forms of regret. We then discuss the implications of this model for\nthe development of \"robust\" MAS that can cooperate with a wide variety of\nsocially intelligent agents.",
                "Steering control of payoff-maximizing players in adaptive learning\n  dynamics\nEvolutionary game theory provides a mathematical foundation for\ncross-disciplinary fertilization, especially for integrating ideas from\nartificial intelligence and game theory. Such integration offers a transparent\nand rigorous approach to complex decision-making problems in a variety of\nimportant contexts, ranging from evolutionary computation to machine behavior.\nDespite the astronomically huge individual behavioral strategy space for\ninteractions in the iterated Prisoner's Dilemma (IPD) games, the so-called\nZero-Determinant (ZD) strategies is a set of rather simple memory-one\nstrategies yet can unilaterally set a linear payoff relationship between\nthemselves and their opponent. Although the witting of ZD strategies gives\nplayers an upper hand in the IPD games, we find and characterize unbending\nstrategies that can force ZD players to be fair in their own interest.\nMoreover, our analysis reveals the ubiquity of unbending properties in common\nIPD strategies which are previously overlooked. In this work, we demonstrate\nthe important steering role of unbending strategies in fostering fairness and\ncooperation in pairwise interactions. Our results will help bring a new\nperspective by means of combining game theory and multi-agent learning systems\nfor optimizing winning strategies that are robust to noises, errors, and\ndeceptions in non-zero-sum games.",
                "An Emergency Disposal Decision-making Method with Human--Machine\n  Collaboration\nRapid developments in artificial intelligence technology have led to unmanned\nsystems replacing human beings in many fields requiring high-precision\npredictions and decisions. In modern operational environments, all job plans\nare affected by emergency events such as equipment failures and resource\nshortages, making a quick resolution critical. The use of unmanned systems to\nassist decision-making can improve resolution efficiency, but their\ndecision-making is not interpretable and may make the wrong decisions. Current\nunmanned systems require human supervision and control. Based on this, we\npropose a collaborative human--machine method for resolving unplanned events\nusing two phases: task filtering and task scheduling. In the task filtering\nphase, we propose a human--machine collaborative decision-making algorithm for\ndynamic tasks. The GACRNN model is used to predict the state of the job nodes,\nlocate the key nodes, and generate a machine-predicted resolution task list. A\nhuman decision-maker supervises the list in real time and modifies and confirms\nthe machine-predicted list through the human--machine interface. In the task\nscheduling phase, we propose a scheduling algorithm that integrates human\nexperience constraints. The steps to resolve an event are inserted into the\nnormal job sequence to schedule the resolution. We propose several\nhuman--machine collaboration methods in each phase to generate steps to resolve\nan unplanned event while minimizing the impact on the original job plan.",
                "Controllable Path of Destruction\nPath of Destruction (PoD) is a self-supervised method for learning iterative\ngenerators. The core idea is to produce a training set by destroying a set of\nartifacts, and for each destructive step create a training instance based on\nthe corresponding repair action. A generator trained on this dataset can then\ngenerate new artifacts by repairing from arbitrary states. The PoD method is\nvery data-efficient in terms of original training examples and well-suited to\nfunctional artifacts composed of categorical data, such as game levels and\ndiscrete 3D structures. In this paper, we extend the Path of Destruction method\nto allow designer control over aspects of the generated artifacts.\nControllability is introduced by adding conditional inputs to the state-action\npairs that make up the repair trajectories. We test the controllable PoD method\nin a 2D dungeon setting, as well as in the domain of small 3D Lego cars.",
                "Split Federated Learning: Speed up Model Training in Resource-Limited\n  Wireless Networks\nIn this paper, we propose a novel distributed learning scheme, named\ngroup-based split federated learning (GSFL), to speed up artificial\nintelligence (AI) model training. Specifically, the GSFL operates in a\nsplit-then-federated manner, which consists of three steps: 1) Model\ndistribution, in which the access point (AP) splits the AI models and\ndistributes the client-side models to clients; 2) Model training, in which each\nclient executes forward propagation and transmit the smashed data to the edge\nserver. The edge server executes forward and backward propagation and then\nreturns the gradient to the clients for updating local client-side models; and\n3) Model aggregation, in which edge servers aggregate the server-side and\nclient-side models. Simulation results show that the GSFL outperforms vanilla\nsplit learning and federated learning schemes in terms of overall training\nlatency while achieving satisfactory accuracy.",
                "Magnetic field regression using artificial neural networks for cold atom\n  experiments\nAccurately measuring magnetic fields is essential for magnetic-field\nsensitive experiments in fields like atomic, molecular, and optical physics,\ncondensed matter experiments, and other areas. However, since many experiments\nare conducted in an isolated vacuum environment that is inaccessible to\nexperimentalists, it can be challenging to accurately determine the magnetic\nfield. Here, we propose an efficient method for detecting magnetic fields with\nthe assistance of an artificial neural network (NN). Instead of measuring the\nmagnetic field directly at the desired location, we detect magnetic fields at\nseveral surrounding positions, and a trained NN can accurately predict the\nmagnetic field at the target location. After training, we achieve a relative\nerror of magnetic field magnitude (magnitude of error over the magnitude of\nmagnetic field) below 0.3$\\%$, and we successfully apply this method to our\nerbium quantum gas apparatus. This approach significantly simplifies the\nprocess of determining magnetic fields in isolated vacuum environments and can\nbe applied to various research fields across a wide range of magnetic field\nmagnitudes."
            ],
            "interesting paper": 3
        },
        {
            "papers": [
                "NetHack is Hard to Hack\nNeural policy learning methods have achieved remarkable results in various\ncontrol problems, ranging from Atari games to simulated locomotion. However,\nthese methods struggle in long-horizon tasks, especially in open-ended\nenvironments with multi-modal observations, such as the popular dungeon-crawler\ngame, NetHack. Intriguingly, the NeurIPS 2021 NetHack Challenge revealed that\nsymbolic agents outperformed neural approaches by over four times in median\ngame score. In this paper, we delve into the reasons behind this performance\ngap and present an extensive study on neural policy learning for NetHack. To\nconduct this study, we analyze the winning symbolic agent, extending its\ncodebase to track internal strategy selection in order to generate one of the\nlargest available demonstration datasets. Utilizing this dataset, we examine\n(i) the advantages of an action hierarchy; (ii) enhancements in neural\narchitecture; and (iii) the integration of reinforcement learning with\nimitation learning. Our investigations produce a state-of-the-art neural agent\nthat surpasses previous fully neural policies by 127% in offline settings and\n25% in online settings on median game score. However, we also demonstrate that\nmere scaling is insufficient to bridge the performance gap with the best\nsymbolic models or even the top human players.",
                "Uncovering multifunctional mechano-intelligence in and through phononic\n  metastructures harnessing physical reservoir computing\nThe recent advances in autonomous systems have prompted a strong demand for\nthe next generation of adaptive structures and materials to possess more\nbuilt-in intelligence in their mechanical domain, the so-called\nmechano-intelligence (MI). Previous MI attempts mainly focused on specific\ndesigns and case studies to realize limited aspects of MI, and there is a lack\nof a systematic foundation in constructing and integrating the different\nelements of intelligence in an effective and efficient manner. Here, we propose\na new approach to create the needed foundation in realizing integrated\nmultifunctional MI via a physical reservoir computing (PRC) framework. That is,\nto concurrently embody computing power and the various elements of\nintelligence, namely perception, decision-making, and commanding, directly in\nthe mechanical domain, advancing from conventional adaptive structures that\nrely solely on add-on digital computers and massive electronics to achieve\nintelligence. As an exemplar platform, we construct a mechanically intelligent\nphononic metastructure with the integrated elements of MI by harnessing the PRC\npower hidden in their high-degree-of-freedom nonlinear dynamics. Through\nanalyses and experimental investigations, we uncover multiple adaptive\nstructural functions ranging from self-tuning wave controls to wave-based logic\ngates. This research will provide the basis for creating future new structures\nthat would greatly surpass the state of the art - such as lower power\nconsumption, more direct interactions, and much better survivability in harsh\nenvironment or under cyberattacks. Moreover, it will enable the addition of new\nfunctions and autonomy to systems without overburdening the onboard computers.",
                "Symmetry-Aware Robot Design with Structured Subgroups\nRobot design aims at learning to create robots that can be easily controlled\nand perform tasks efficiently. Previous works on robot design have proven its\nability to generate robots for various tasks. However, these works searched the\nrobots directly from the vast design space and ignored common structures,\nresulting in abnormal robots and poor performance. To tackle this problem, we\npropose a Symmetry-Aware Robot Design (SARD) framework that exploits the\nstructure of the design space by incorporating symmetry searching into the\nrobot design process. Specifically, we represent symmetries with the subgroups\nof the dihedral group and search for the optimal symmetry in structured\nsubgroups. Then robots are designed under the searched symmetry. In this way,\nSARD can design efficient symmetric robots while covering the original design\nspace, which is theoretically analyzed. We further empirically evaluate SARD on\nvarious tasks, and the results show its superior efficiency and\ngeneralizability.",
                "Bigger, Better, Faster: Human-level Atari with human-level efficiency\nWe introduce a value-based RL agent, which we call BBF, that achieves\nsuper-human performance in the Atari 100K benchmark. BBF relies on scaling the\nneural networks used for value estimation, as well as a number of other design\nchoices that enable this scaling in a sample-efficient manner. We conduct\nextensive analyses of these design choices and provide insights for future\nwork. We end with a discussion about updating the goalposts for\nsample-efficient RL research on the ALE. We make our code and data publicly\navailable at\nhttps://github.com/google-research/google-research/tree/master/bigger_better_faster.",
                "Intent-aligned AI systems deplete human agency: the need for agency\n  foundations research in AI safety\nThe rapid advancement of artificial intelligence (AI) systems suggests that\nartificial general intelligence (AGI) systems may soon arrive. Many researchers\nare concerned that AIs and AGIs will harm humans via intentional misuse\n(AI-misuse) or through accidents (AI-accidents). In respect of AI-accidents,\nthere is an increasing effort focused on developing algorithms and paradigms\nthat ensure AI systems are aligned to what humans intend, e.g. AI systems that\nyield actions or recommendations that humans might judge as consistent with\ntheir intentions and goals. Here we argue that alignment to human intent is\ninsufficient for safe AI systems and that preservation of long-term agency of\nhumans may be a more robust standard, and one that needs to be separated\nexplicitly and a priori during optimization. We argue that AI systems can\nreshape human intention and discuss the lack of biological and psychological\nmechanisms that protect humans from loss of agency. We provide the first formal\ndefinition of agency-preserving AI-human interactions which focuses on\nforward-looking agency evaluations and argue that AI systems - not humans -\nmust be increasingly tasked with making these evaluations. We show how agency\nloss can occur in simple environments containing embedded agents that use\ntemporal-difference learning to make action recommendations. Finally, we\npropose a new area of research called \"agency foundations\" and pose four\ninitial topics designed to improve our understanding of agency in AI-human\ninteractions: benevolent game theory, algorithmic foundations of human rights,\nmechanistic interpretability of agency representation in neural-networks and\nreinforcement learning from internal states.",
                "DHRL-FNMR: An Intelligent Multicast Routing Approach Based on Deep\n  Hierarchical Reinforcement Learning in SDN\nThe optimal multicast tree problem in the Software-Defined Networking (SDN)\nmulticast routing is an NP-hard combinatorial optimization problem. Although\nexisting SDN intelligent solution methods, which are based on deep\nreinforcement learning, can dynamically adapt to complex network link state\nchanges, these methods are plagued by problems such as redundant branches,\nlarge action space, and slow agent convergence. In this paper, an SDN\nintelligent multicast routing algorithm based on deep hierarchical\nreinforcement learning is proposed to circumvent the aforementioned problems.\nFirst, the multicast tree construction problem is decomposed into two\nsub-problems: the fork node selection problem and the construction of the\noptimal path from the fork node to the destination node. Second, based on the\ninformation characteristics of SDN global network perception, the multicast\ntree state matrix, link bandwidth matrix, link delay matrix, link packet loss\nrate matrix, and sub-goal matrix are designed as the state space of intrinsic\nand meta controllers. Then, in order to mitigate the excessive action space,\nour approach constructs different action spaces at the upper and lower levels.\nThe meta-controller generates an action space using network nodes to select the\nfork node, and the intrinsic controller uses the adjacent edges of the current\nnode as its action space, thus implementing four different action selection\nstrategies in the construction of the multicast tree. To facilitate the\nintelligent agent in constructing the optimal multicast tree with greater\nspeed, we developed alternative reward strategies that distinguish between\nsingle-step node actions and multi-step actions towards multiple destination\nnodes."
            ],
            "interesting paper": 3
        },
        {
            "papers": [
                "Environmental Memory Boosts Group Formation of Clueless Individuals\nThe formation of groups of interacting individuals improves performance and\nfitness in many decentralised systems, from micro-organisms to social insects,\nfrom robotic swarms to artificial intelligence algorithms. Often, group\nformation and high-level coordination in these systems emerge from individuals\nwith limited information-processing capabilities implementing low-level rules\nof communication to signal to each other. Here, we show that, even in a\ncommunity of clueless individuals incapable of processing information and\ncommunicating, a dynamic environment can coordinate group formation by\ntransiently storing memory of the earlier passage of individuals. Our results\nidentify a new mechanism of indirect coordination via shared memory that is\nprimarily promoted and reinforced by dynamic environmental factors, thus\novershadowing the need for any form of explicit signalling between individuals.\nWe expect this pathway to group formation to be relevant for understanding and\ncontrolling self-organisation and collective decision making in both living and\nartificial active matter in real-life environments.",
                "Space Net Optimization\nMost metaheuristic algorithms rely on a few searched solutions to guide later\nsearches during the convergence process for a simple reason: the limited\ncomputing resource of a computer makes it impossible to retain all the searched\nsolutions. This also reveals that each search of most metaheuristic algorithms\nis just like a ballpark guess. To help address this issue, we present a novel\nmetaheuristic algorithm called space net optimization (SNO). It is equipped\nwith a new mechanism called space net; thus, making it possible for a\nmetaheuristic algorithm to use most information provided by all searched\nsolutions to depict the landscape of the solution space. With the space net, a\nmetaheuristic algorithm is kind of like having a ``vision'' on the solution\nspace. Simulation results show that SNO outperforms all the other metaheuristic\nalgorithms compared in this study for a set of well-known single objective\nbound constrained problems in most cases.",
                "EMOTE: An Explainable architecture for Modelling the Other Through\n  Empathy\nWe can usually assume others have goals analogous to our own. This assumption\ncan also, at times, be applied to multi-agent games - e.g. Agent 1's attraction\nto green pellets is analogous to Agent 2's attraction to red pellets. This\n\"analogy\" assumption is tied closely to the cognitive process known as empathy.\nInspired by empathy, we design a simple and explainable architecture to model\nanother agent's action-value function. This involves learning an \"Imagination\nNetwork\" to transform the other agent's observed state in order to produce a\nhuman-interpretable \"empathetic state\" which, when presented to the learning\nagent, produces behaviours that mimic the other agent. Our approach is\napplicable to multi-agent scenarios consisting of a single learning agent and\nother (independent) agents acting according to fixed policies. This\narchitecture is particularly beneficial for (but not limited to) algorithms\nusing a composite value or reward function. We show our method produces better\nperformance in multi-agent games, where it robustly estimates the other's model\nin different environment configurations. Additionally, we show that the\nempathetic states are human interpretable, and thus verifiable.",
                "A Virtual-Force Based Swarm Algorithm for Balanced Circular Bin Packing\n  Problems\nBalanced circular bin packing problems consist in positioning a given number\nof weighted circles in order to minimize the radius of a circular container\nwhile satisfying equilibrium constraints. These problems are NP-hard, highly\nconstrained and dimensional. This paper describes a swarm algorithm based on a\nvirtual-force system in order to solve balanced circular bin packing problems.\nIn the proposed approach, a system of forces is applied to each component\nallowing to take into account the constraints and minimizing the objective\nfunction using the fundamental principle of dynamics. The proposed algorithm is\nexperimented and validated on benchmarks of various balanced circular bin\npacking problems with up to 300 circles. The reported results allow to assess\nthe effectiveness of the proposed approach compared to existing results from\nthe literature.",
                "Human Control: Definitions and Algorithms\nHow can humans stay in control of advanced artificial intelligence systems?\nOne proposal is corrigibility, which requires the agent to follow the\ninstructions of a human overseer, without inappropriately influencing them. In\nthis paper, we formally define a variant of corrigibility called shutdown\ninstructability, and show that it implies appropriate shutdown behavior,\nretention of human autonomy, and avoidance of user harm. We also analyse the\nrelated concepts of non-obstruction and shutdown alignment, three previously\nproposed algorithms for human control, and one new algorithm.",
                "Human or Not? A Gamified Approach to the Turing Test\nWe present \"Human or Not?\", an online game inspired by the Turing test, that\nmeasures the capability of AI chatbots to mimic humans in dialog, and of humans\nto tell bots from other humans. Over the course of a month, the game was played\nby over 1.5 million users who engaged in anonymous two-minute chat sessions\nwith either another human or an AI language model which was prompted to behave\nlike humans. The task of the players was to correctly guess whether they spoke\nto a person or to an AI. This largest scale Turing-style test conducted to date\nrevealed some interesting facts. For example, overall users guessed the\nidentity of their partners correctly in only 68% of the games. In the subset of\nthe games in which users faced an AI bot, users had even lower correct guess\nrates of 60% (that is, not much higher than chance). This white paper details\nthe development, deployment, and results of this unique experiment. While this\nexperiment calls for many extensions and refinements, these findings already\nbegin to shed light on the inevitable near future which will commingle humans\nand AI."
            ],
            "interesting paper": 1
        },
        {
            "papers": [
                "Federated Learning Games for Reconfigurable Intelligent Surfaces via\n  Causal Representations\nIn this paper, we investigate the problem of robust Reconfigurable\nIntelligent Surface (RIS) phase-shifts configuration over heterogeneous\ncommunication environments. The problem is formulated as a distributed learning\nproblem over different environments in a Federated Learning (FL) setting.\nEquivalently, this corresponds to a game played between multiple RISs, as\nlearning agents, in heterogeneous environments. Using Invariant Risk\nMinimization (IRM) and its FL equivalent, dubbed FL Games, we solve the RIS\nconfiguration problem by learning invariant causal representations across\nmultiple environments and then predicting the phases. The solution corresponds\nto playing according to Best Response Dynamics (BRD) which yields the Nash\nEquilibrium of the FL game. The representation learner and the phase predictor\nare modeled by two neural networks, and their performance is validated via\nsimulations against other benchmarks from the literature. Our results show that\ncausality-based learning yields a predictor that is 15% more accurate in unseen\nOut-of-Distribution (OoD) environments.",
                "Knowledge-based Reasoning and Learning under Partial Observability in Ad\n  Hoc Teamwork\nAd hoc teamwork refers to the problem of enabling an agent to collaborate\nwith teammates without prior coordination. Data-driven methods represent the\nstate of the art in ad hoc teamwork. They use a large labeled dataset of prior\nobservations to model the behavior of other agent types and to determine the ad\nhoc agent's behavior. These methods are computationally expensive, lack\ntransparency, and make it difficult to adapt to previously unseen changes,\ne.g., in team composition. Our recent work introduced an architecture that\ndetermined an ad hoc agent's behavior based on non-monotonic logical reasoning\nwith prior commonsense domain knowledge and predictive models of other agents'\nbehavior that were learned from limited examples. In this paper, we\nsubstantially expand the architecture's capabilities to support: (a) online\nselection, adaptation, and learning of the models that predict the other\nagents' behavior; and (b) collaboration with teammates in the presence of\npartial observability and limited communication. We illustrate and\nexperimentally evaluate the capabilities of our architecture in two simulated\nmultiagent benchmark domains for ad hoc teamwork: Fort Attack and Half Field\nOffense. We show that the performance of our architecture is comparable or\nbetter than state of the art data-driven baselines in both simple and complex\nscenarios, particularly in the presence of limited training data, partial\nobservability, and changes in team composition.",
                "On the Existence of Information Bottlenecks in Living and Non-Living\n  Systems\nIn many complex systems, we observe that `interesting behaviour' is often the\nconsequence of a system exploiting the existence of an Information Bottleneck\n(IB). These bottlenecks can occur at different scales, between individuals or\ncomponents of a system, and sometimes within individuals themselves.\nOftentimes, we regard these bottlenecks negatively; as merely the limitations\nof the individual's physiology and something that ought to be overcome when\ndesigning and implementing artificial systems. However, we suggest instead that\nIBs may serve a purpose beyond merely providing a minimally-viable channel for\ncoordination in collective systems. More specifically, we suggest that\ninteresting or novel behaviour occurs when the individuals in a system are\nconstrained or limited in their ability to share information and must discover\nnovel ways to exploit existing mechanisms, which are inherently bottlenecked,\nrather than circumventing or otherwise avoiding those mechanisms entirely.",
                "Multi-Robot Path Planning Combining Heuristics and Multi-Agent\n  Reinforcement Learning\nMulti-robot path finding in dynamic environments is a highly challenging\nclassic problem. In the movement process, robots need to avoid collisions with\nother moving robots while minimizing their travel distance. Previous methods\nfor this problem either continuously replan paths using heuristic search\nmethods to avoid conflicts or choose appropriate collision avoidance strategies\nbased on learning approaches. The former may result in long travel distances\ndue to frequent replanning, while the latter may have low learning efficiency\ndue to low sample exploration and utilization, and causing high training costs\nfor the model. To address these issues, we propose a path planning method,\nMAPPOHR, which combines heuristic search, empirical rules, and multi-agent\nreinforcement learning. The method consists of two layers: a real-time planner\nbased on the multi-agent reinforcement learning algorithm, MAPPO, which embeds\nempirical rules in the action output layer and reward functions, and a\nheuristic search planner used to create a global guiding path. During movement,\nthe heuristic search planner replans new paths based on the instructions of the\nreal-time planner. We tested our method in 10 different conflict scenarios. The\nexperiments show that the planning performance of MAPPOHR is better than that\nof existing learning and heuristic methods. Due to the utilization of empirical\nknowledge and heuristic search, the learning efficiency of MAPPOHR is higher\nthan that of existing learning methods.",
                "Egocentric Planning for Scalable Embodied Task Achievement\nEmbodied agents face significant challenges when tasked with performing\nactions in diverse environments, particularly in generalizing across object\ntypes and executing suitable actions to accomplish tasks. Furthermore, agents\nshould exhibit robustness, minimizing the execution of illegal actions. In this\nwork, we present Egocentric Planning, an innovative approach that combines\nsymbolic planning and Object-oriented POMDPs to solve tasks in complex\nenvironments, harnessing existing models for visual perception and natural\nlanguage processing. We evaluated our approach in ALFRED, a simulated\nenvironment designed for domestic tasks, and demonstrated its high scalability,\nachieving an impressive 36.07% unseen success rate in the ALFRED benchmark and\nwinning the ALFRED challenge at CVPR Embodied AI workshop. Our method requires\nreliable perception and the specification or learning of a symbolic description\nof the preconditions and effects of the agent's actions, as well as what object\ntypes reveal information about others. It is capable of naturally scaling to\nsolve new tasks beyond ALFRED, as long as they can be solved using the\navailable skills. This work offers a solid baseline for studying end-to-end and\nhybrid methods that aim to generalize to new tasks, including recent approaches\nrelying on LLMs, but often struggle to scale to long sequences of actions or\nproduce robust plans for novel tasks.",
                "An Architecture for Deploying Reinforcement Learning in Industrial\n  Environments\nIndustry 4.0 is driven by demands like shorter time-to-market, mass\ncustomization of products, and batch size one production. Reinforcement\nLearning (RL), a machine learning paradigm shown to possess a great potential\nin improving and surpassing human level performance in numerous complex tasks,\nallows coping with the mentioned demands. In this paper, we present an OPC UA\nbased Operational Technology (OT)-aware RL architecture, which extends the\nstandard RL setting, combining it with the setting of digital twins. Moreover,\nwe define an OPC UA information model allowing for a generalized plug-and-play\nlike approach for exchanging the RL agent used. In conclusion, we demonstrate\nand evaluate the architecture, by creating a proof of concept. By means of\nsolving a toy example, we show that this architecture can be used to determine\nthe optimal policy using a real control system."
            ],
            "interesting paper": 2
        }
    ],
    "Quinn Smith": [
        {
            "papers": [
                "Trends and Challenges Towards an Effective Data-Driven Decision Making\n  in UK SMEs: Case Studies and Lessons Learnt from the Analysis of 85 SMEs\nThe adoption of data science brings vast benefits to Small and Medium-sized\nEnterprises (SMEs) including business productivity, economic growth, innovation\nand jobs creation. Data Science can support SMEs to optimise production\nprocesses, anticipate customers' needs, predict machinery failures and deliver\nefficient smart services. Businesses can also harness the power of Artificial\nIntelligence (AI) and Big Data and the smart use of digital technologies to\nenhance productivity and performance, paving the way for innovation. However,\nintegrating data science decisions into an SME requires both skills and IT\ninvestments. In most cases, such expenses are beyond the means of SMEs due to\nlimited resources and restricted access to financing. This paper presents\ntrends and challenges towards an effective data-driven decision making for\norganisations based on a case study of 85 SMEs, mostly from the West Midlands\nregion of England. The work is supported as part of a 3 years ERDF (European\nRegional Development Funded project) in the areas of big data management,\nanalytics and business intelligence. We present two case studies that\ndemonstrates the potential of Digitisation, AI and Machine Learning and use\nthese as examples to unveil challenges and showcase the wealth of current\navailable opportunities for SMEs.",
                "Symbolic Regression via Control Variable Genetic Programming\nLearning symbolic expressions directly from experiment data is a vital step\nin AI-driven scientific discovery. Nevertheless, state-of-the-art approaches\nare limited to learning simple expressions. Regressing expressions involving\nmany independent variables still remain out of reach. Motivated by the control\nvariable experiments widely utilized in science, we propose Control Variable\nGenetic Programming (CVGP) for symbolic regression over many independent\nvariables. CVGP expedites symbolic expression discovery via customized\nexperiment design, rather than learning from a fixed dataset collected a\npriori. CVGP starts by fitting simple expressions involving a small set of\nindependent variables using genetic programming, under controlled experiments\nwhere other variables are held as constants. It then extends expressions\nlearned in previous generations by adding new independent variables, using new\ncontrol variable experiments in which these variables are allowed to vary.\nTheoretically, we show CVGP as an incremental building approach can yield an\nexponential reduction in the search space when learning a class of expressions.\nExperimentally, CVGP outperforms several baselines in learning symbolic\nexpressions involving multiple independent variables.",
                "An Experimental Investigation into the Evaluation of Explainability\n  Methods\nEXplainable Artificial Intelligence (XAI) aims to help users to grasp the\nreasoning behind the predictions of an Artificial Intelligence (AI) system.\nMany XAI approaches have emerged in recent years. Consequently, a subfield\nrelated to the evaluation of XAI methods has gained considerable attention,\nwith the aim to determine which methods provide the best explanation using\nvarious approaches and criteria. However, the literature lacks a comparison of\nthe evaluation metrics themselves, that one can use to evaluate XAI methods.\nThis work aims to fill this gap by comparing 14 different metrics when applied\nto nine state-of-the-art XAI methods and three dummy methods (e.g., random\nsaliency maps) used as references. Experimental results show which of these\nmetrics produces highly correlated results, indicating potential redundancy. We\nalso demonstrate the significant impact of varying the baseline hyperparameter\non the evaluation metric values. Finally, we use dummy methods to assess the\nreliability of metrics in terms of ranking, pointing out their limitations.",
                "ISLE: An Intelligent Streaming Framework for High-Throughput AI\n  Inference in Medical Imaging\nAs the adoption of Artificial Intelligence (AI) systems within the clinical\nenvironment grows, limitations in bandwidth and compute can create\ncommunication bottlenecks when streaming imaging data, leading to delays in\npatient care and increased cost. As such, healthcare providers and AI vendors\nwill require greater computational infrastructure, therefore dramatically\nincreasing costs. To that end, we developed ISLE, an intelligent streaming\nframework for high-throughput, compute- and bandwidth- optimized, and cost\neffective AI inference for clinical decision making at scale. In our\nexperiments, ISLE on average reduced data transmission by 98.02% and decoding\ntime by 98.09%, while increasing throughput by 2,730%. We show that ISLE\nresults in faster turnaround times, and reduced overall cost of data,\ntransmission, and compute, without negatively impacting clinical decision\nmaking using AI systems.",
                "Towards a Capability Assessment Model for the Comprehension and Adoption\n  of AI in Organisations\nThe comprehension and adoption of Artificial Intelligence (AI) are beset with\npractical and ethical problems. This article presents a 5-level AI Capability\nAssessment Model (AI-CAM) and a related AI Capabilities Matrix (AI-CM) to\nassist practitioners in AI comprehension and adoption. These practical tools\nwere developed with business executives, technologists, and other\norganisational stakeholders in mind. They are founded on a comprehensive\nconception of AI compared to those in other AI adoption models and are also\nopen-source artefacts. Thus, the AI-CAM and AI-CM present an accessible\nresource to help inform organisational decision-makers on the capability\nrequirements for (1) AI-based data analytics use cases based on machine\nlearning technologies; (2) Knowledge representation to engineer and represent\ndata, information and knowledge using semantic technologies; and (3) AI-based\nsolutions that seek to emulate human reasoning and decision-making. The AI-CAM\ncovers the core capability dimensions (business, data, technology,\norganisation, AI skills, risks, and ethical considerations) required at the\nfive capability maturity levels to achieve optimal use of AI in organisations.",
                "Patient Outcome Predictions Improve Operations at a Large Hospital\n  Network\nProblem definition: Access to accurate predictions of patients' outcomes can\nenhance medical staff's decision-making, which ultimately benefits all\nstakeholders in the hospitals. A large hospital network in the US has been\ncollaborating with academics and consultants to predict short-term and\nlong-term outcomes for all inpatients across their seven hospitals.\nMethodology/results: We develop machine learning models that predict the\nprobabilities of next 24-hr/48-hr discharge and intensive care unit transfers,\nend-of-stay mortality and discharge dispositions. All models achieve high\nout-of-sample AUC (75.7%-92.5%) and are well calibrated. In addition, combining\n48-hr discharge predictions with doctors' predictions simultaneously enables\nmore patient discharges (10%-28.7%) and fewer 7-day/30-day readmissions\n($p$-value $<0.001$). We implement an automated pipeline that extracts data and\nupdates predictions every morning, as well as user-friendly software and a\ncolor-coded alert system to communicate these patient-level predictions\n(alongside explanations) to clinical teams. Managerial implications: Since we\nhave been gradually deploying the tool, and training medical staff, over 200\ndoctors, nurses, and case managers across seven hospitals use it in their daily\npatient review process. We observe a significant reduction in the average\nlength of stay (0.67 days per patient) following its adoption and anticipate\nsubstantial financial benefits (between \\$55 and \\$72 million annually) for the\nhealthcare system."
            ],
            "interesting paper": 2
        },
        {
            "papers": [
                "Attention Paper: How Generative AI Reshapes Digital Shadow Industry?\nThe rapid development of digital economy has led to the emergence of various\nblack and shadow internet industries, which pose potential risks that can be\nidentified and managed through digital risk management (DRM) that uses\ndifferent techniques such as machine learning and deep learning. The evolution\nof DRM architecture has been driven by changes in data forms. However, the\ndevelopment of AI-generated content (AIGC) technology, such as ChatGPT and\nStable Diffusion, has given black and shadow industries powerful tools to\npersonalize data and generate realistic images and conversations for fraudulent\nactivities. This poses a challenge for DRM systems to control risks from the\nsource of data generation and to respond quickly to the fast-changing risk\nenvironment. This paper aims to provide a technical analysis of the challenges\nand opportunities of AIGC from upstream, midstream, and downstream paths of\nblack/shadow industries and suggest future directions for improving existing\nrisk control systems. The paper will explore the new black and shadow\ntechniques triggered by generative AI technology and provide insights for\nbuilding the next-generation DRM system.",
                "Alert of the Second Decision-maker: An Introduction to Human-AI Conflict\nThe collaboration between humans and artificial intelligence (AI) is a\nsignificant feature in this digital age. However, humans and AI may have\nobservation, interpretation, and action conflicts when working synchronously.\nThis phenomenon is often masked by faults and, unfortunately, overlooked. This\npaper systematically introduces the human-AI conflict concept, causes,\nmeasurement methods, and risk assessment. The results highlight that there is a\npotential second decision-maker besides the human, which is the AI; the\nhuman-AI conflict is a unique and emerging risk in digitalized process systems;\nand this is an interdisciplinary field that needs to be distinguished from\ntraditional fault and failure analysis; the conflict risk is significant and\ncannot be ignored.",
                "Language Models Can Improve Event Prediction by Few-Shot Abductive\n  Reasoning\nLarge language models have shown astonishing performance on a wide range of\nreasoning tasks. In this paper, we investigate whether they could reason about\nreal-world events and help improve the prediction performance of event sequence\nmodels. We design LAMP, a framework that integrates a large language model in\nevent prediction. Particularly, the language model performs abductive reasoning\nto assist an event sequence model: the event model proposes predictions on\nfuture events given the past; instructed by a few expert-annotated\ndemonstrations, the language model learns to suggest possible causes for each\nproposal; a search module finds out the previous events that match the causes;\na scoring function learns to examine whether the retrieved events could\nactually cause the proposal. Through extensive experiments on several\nchallenging real-world datasets, we demonstrate that our framework -- thanks to\nthe reasoning capabilities of large language models -- could significantly\noutperform the state-of-the-art event sequence models.",
                "Generating Synergistic Formulaic Alpha Collections via Reinforcement\n  Learning\nIn the field of quantitative trading, it is common practice to transform raw\nhistorical stock data into indicative signals for the market trend. Such\nsignals are called alpha factors. Alphas in formula forms are more\ninterpretable and thus favored by practitioners concerned with risk. In\npractice, a set of formulaic alphas is often used together for better modeling\nprecision, so we need to find synergistic formulaic alpha sets that work well\ntogether. However, most traditional alpha generators mine alphas one by one\nseparately, overlooking the fact that the alphas would be combined later. In\nthis paper, we propose a new alpha-mining framework that prioritizes mining a\nsynergistic set of alphas, i.e., it directly uses the performance of the\ndownstream combination model to optimize the alpha generator. Our framework\nalso leverages the strong exploratory capabilities of reinforcement\nlearning~(RL) to better explore the vast search space of formulaic alphas. The\ncontribution to the combination models' performance is assigned to be the\nreturn used in the RL process, driving the alpha generator to find better\nalphas that improve upon the current set. Experimental evaluations on\nreal-world stock market data demonstrate both the effectiveness and the\nefficiency of our framework for stock trend forecasting. The investment\nsimulation results show that our framework is able to achieve higher returns\ncompared to previous approaches.",
                "Passive learning of active causal strategies in agents and language\n  models\nWhat can be learned about causality and experimentation from passive data?\nThis question is salient given recent successes of passively-trained language\nmodels in interactive domains such as tool use. Passive learning is inherently\nlimited. However, we show that purely passive learning can in fact allow an\nagent to learn generalizable strategies for determining and using causal\nstructures, as long as the agent can intervene at test time. We formally\nillustrate that learning a strategy of first experimenting, then seeking goals,\ncan allow generalization from passive learning in principle. We then show\nempirically that agents trained via imitation on expert data can indeed\ngeneralize at test time to infer and use causal links which are never present\nin the training data; these agents can also generalize experimentation\nstrategies to novel variable sets never observed in training. We then show that\nstrategies for causal intervention and exploitation can be generalized from\npassive data even in a more complex environment with high-dimensional\nobservations, with the support of natural language explanations. Explanations\ncan even allow passive learners to generalize out-of-distribution from\nperfectly-confounded training data. Finally, we show that language models,\ntrained only on passive next-word prediction, can generalize causal\nintervention strategies from a few-shot prompt containing examples of\nexperimentation, together with explanations and reasoning. These results\nhighlight the surprising power of passive learning of active causal strategies,\nand may help to understand the behaviors and capabilities of language models.",
                "Transformative Effects of ChatGPT on Modern Education: Emerging Era of\n  AI Chatbots\nChatGPT, an AI-based chatbot, was released to provide coherent and useful\nreplies based on analysis of large volumes of data. In this article, leading\nscientists, researchers and engineers discuss the transformative effects of\nChatGPT on modern education. This research seeks to improve our knowledge of\nChatGPT capabilities and its use in the education sector, identifying potential\nconcerns and challenges. Our preliminary evaluation concludes that ChatGPT\nperformed differently in each subject area including finance, coding and maths.\nWhile ChatGPT has the ability to help educators by creating instructional\ncontent, offering suggestions and acting as an online educator to learners by\nanswering questions and promoting group work, there are clear drawbacks in its\nuse, such as the possibility of producing inaccurate or false data and\ncircumventing duplicate content (plagiarism) detectors where originality is\nessential. The often reported hallucinations within Generative AI in general,\nand also relevant for ChatGPT, can render its use of limited benefit where\naccuracy is essential. What ChatGPT lacks is a stochastic measure to help\nprovide sincere and sensitive communication with its users. Academic\nregulations and evaluation practices used in educational institutions need to\nbe updated, should ChatGPT be used as a tool in education. To address the\ntransformative effects of ChatGPT on the learning environment, educating\nteachers and students alike about its capabilities and limitations will be\ncrucial."
            ],
            "interesting paper": 3
        },
        {
            "papers": [
                "MADiff: Offline Multi-agent Learning with Diffusion Models\nDiffusion model (DM) recently achieved huge success in various scenarios\nincluding offline reinforcement learning, where the diffusion planner learn to\ngenerate desired trajectories during online evaluations. However, despite the\neffectiveness in single-agent learning, it remains unclear how DMs can operate\nin multi-agent problems, where agents can hardly complete teamwork without good\ncoordination by independently modeling each agent's trajectories. In this\npaper, we propose MADiff, a novel generative multi-agent learning framework to\ntackle this problem. MADiff is realized with an attention-based diffusion model\nto model the complex coordination among behaviors of multiple agents. To the\nbest of our knowledge, MADiff is the first diffusion-based multi-agent learning\nframework, which behaves as both a decentralized policy and a centralized\ncontroller. During decentralized executions, MADiff simultaneously performs\nteammate modeling, and the centralized controller can also be applied in\nmulti-agent trajectory predictions. Our experiments show the superior\nperformance of MADiff compared to baseline algorithms in a wide range of\nmulti-agent learning tasks, which emphasizes the effectiveness of MADiff in\nmodeling complex multi-agent interactions. Our code is available at\nhttps://github.com/zbzhu99/madiff.",
                "Towards Cognitive Bots: Architectural Research Challenges\nSoftware bots operating in multiple virtual digital platforms must understand\nthe platforms' affordances and behave like human users. Platform affordances or\nfeatures differ from one application platform to another or through a life\ncycle, requiring such bots to be adaptable. Moreover, bots in such platforms\ncould cooperate with humans or other software agents for work or to learn\nspecific behavior patterns. However, present-day bots, particularly chatbots,\nother than language processing and prediction, are far from reaching a human\nuser's behavior level within complex business information systems. They lack\nthe cognitive capabilities to sense and act in such virtual environments,\nrendering their development a challenge to artificial general intelligence\nresearch. In this study, we problematize and investigate assumptions in\nconceptualizing software bot architecture by directing attention to significant\narchitectural research challenges in developing cognitive bots endowed with\ncomplex behavior for operation on information systems. As an outlook, we\npropose alternate architectural assumptions to consider in future bot design\nand bot development frameworks.",
                "A Framework for Incentivized Collaborative Learning\nCollaborations among various entities, such as companies, research labs, AI\nagents, and edge devices, have become increasingly crucial for achieving\nmachine learning tasks that cannot be accomplished by a single entity alone.\nThis is likely due to factors such as security constraints, privacy concerns,\nand limitations in computation resources. As a result, collaborative learning\n(CL) research has been gaining momentum. However, a significant challenge in\npractical applications of CL is how to effectively incentivize multiple\nentities to collaborate before any collaboration occurs. In this study, we\npropose ICL, a general framework for incentivized collaborative learning, and\nprovide insights into the critical issue of when and why incentives can improve\ncollaboration performance. Furthermore, we show the broad applicability of ICL\nto specific cases in federated learning, assisted learning, and multi-armed\nbandit with both theory and experimental results.",
                "Attention Schema in Neural Agents\nAttention has become a common ingredient in deep learning architectures. It\nadds a dynamical selection of information on top of the static selection of\ninformation supported by weights. In the same way, we can imagine a\nhigher-order informational filter built on top of attention: an Attention\nSchema (AS), namely, a descriptive and predictive model of attention. In\ncognitive neuroscience, Attention Schema Theory (AST) supports this idea of\ndistinguishing attention from AS. A strong prediction of this theory is that an\nagent can use its own AS to also infer the states of other agents' attention\nand consequently enhance coordination with other agents. As such, multi-agent\nreinforcement learning would be an ideal setting to experimentally test the\nvalidity of AST. We explore different ways in which attention and AS interact\nwith each other. Our preliminary results indicate that agents that implement\nthe AS as a recurrent internal control achieve the best performance. In\ngeneral, these exploratory experiments suggest that equipping artificial agents\nwith a model of attention can enhance their social intelligence.",
                "Inferring the Future by Imagining the Past\nA single panel of a comic book can say a lot: it can depict not only where\nthe characters currently are, but also their motions, their motivations, their\nemotions, and what they might do next. More generally, humans routinely infer\ncomplex sequences of past and future events from a *static snapshot* of a\n*dynamic scene*, even in situations they have never seen before.\n  In this paper, we model how humans make such rapid and flexible inferences.\nBuilding on a long line of work in cognitive science, we offer a Monte Carlo\nalgorithm whose inferences correlate well with human intuitions in a wide\nvariety of domains, while only using a small, cognitively-plausible number of\nsamples. Our key technical insight is a surprising connection between our\ninference problem and Monte Carlo path tracing, which allows us to apply\ndecades of ideas from the computer graphics community to this\nseemingly-unrelated theory of mind task.",
                "How Do UX Practitioners Communicate AI as a Design Material? Artifacts,\n  Conceptions, and Propositions\nUX practitioners (UXPs) face novel challenges when working with and\ncommunicating artificial intelligence (AI) as a design material. We explore how\nUXPs communicate AI concepts when given hands-on experience training and\nexperimenting with AI models. To do so, we conducted a task-based design study\nwith 27 UXPs in which they prototyped and created a design presentation for a\nAI-enabled interface while having access to a simple AI model training tool.\nThrough analyzing UXPs' design presentations and post-activity interviews, we\nfound that although UXPs struggled to clearly communicate some AI concepts,\ntinkering with AI broadened common ground when communicating with technical\nstakeholders. UXPs also identified key risks and benefits of AI in their\ndesigns, and proposed concrete next steps for both UX and AI work. We conclude\nwith a sensitizing concept and recommendations for design and AI tools to\nenhance multi-stakeholder communication and collaboration when crafting\nhuman-centered AI experiences."
            ],
            "interesting paper": 1
        },
        {
            "papers": [
                "Modeling Dynamic Environments with Scene Graph Memory\nEmbodied AI agents that search for objects in large environments such as\nhouseholds often need to make efficient decisions by predicting object\nlocations based on partial information. We pose this as a new type of link\nprediction problem: link prediction on partially observable dynamic graphs. Our\ngraph is a representation of a scene in which rooms and objects are nodes, and\ntheir relationships are encoded in the edges; only parts of the changing graph\nare known to the agent at each timestep. This partial observability poses a\nchallenge to existing link prediction approaches, which we address. We propose\na novel state representation -- Scene Graph Memory (SGM) -- with captures the\nagent's accumulated set of observations, as well as a neural net architecture\ncalled a Node Edge Predictor (NEP) that extracts information from the SGM to\nsearch efficiently. We evaluate our method in the Dynamic House Simulator, a\nnew benchmark that creates diverse dynamic graphs following the semantic\npatterns typically seen at homes, and show that NEP can be trained to predict\nthe locations of objects in a variety of environments with diverse object\nmovement dynamics, outperforming baselines both in terms of new scene\nadaptability and overall accuracy. The codebase and more can be found at\nhttps://www.scenegraphmemory.com.",
                "Incentivizing honest performative predictions with proper scoring rules\nProper scoring rules incentivize experts to accurately report beliefs,\nassuming predictions cannot influence outcomes. We relax this assumption and\ninvestigate incentives when predictions are performative, i.e., when they can\ninfluence the outcome of the prediction, such as when making public predictions\nabout the stock market. We say a prediction is a fixed point if it accurately\nreflects the expert's beliefs after that prediction has been made. We show that\nin this setting, reports maximizing expected score generally do not reflect an\nexpert's beliefs, and we give bounds on the inaccuracy of such reports. We show\nthat, for binary predictions, if the influence of the expert's prediction on\noutcomes is bounded, it is possible to define scoring rules under which optimal\nreports are arbitrarily close to fixed points. However, this is impossible for\npredictions over more than two outcomes. We also perform numerical simulations\nin a toy setting, showing that our bounds are tight in some situations and that\nprediction error is often substantial (greater than 5-10%). Lastly, we discuss\nalternative notions of optimality, including performative stability, and show\nthat they incentivize reporting fixed points.",
                "A Comprehensive Overview and Comparative Analysis on Deep Learning\n  Models: CNN, RNN, LSTM, GRU\nDeep learning (DL) has emerged as a powerful subset of machine learning (ML)\nand artificial intelligence (AI), outperforming traditional ML methods,\nespecially in handling unstructured and large datasets. Its impact spans across\nvarious domains, including speech recognition, healthcare, autonomous vehicles,\ncybersecurity, predictive analytics, and more. However, the complexity and\ndynamic nature of real-world problems present challenges in designing effective\ndeep learning models. Consequently, several deep learning models have been\ndeveloped to address different problems and applications. In this article, we\nconduct a comprehensive survey of various deep learning models, including\nConvolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs),\nGenerative Models, Deep Reinforcement Learning (DRL), and Deep Transfer\nLearning. We examine the structure, applications, benefits, and limitations of\neach model. Furthermore, we perform an analysis using three publicly available\ndatasets: IMDB, ARAS, and Fruit-360. We compare the performance of six renowned\ndeep learning models: CNN, Simple RNN, Long Short-Term Memory (LSTM),\nBidirectional LSTM, Gated Recurrent Unit (GRU), and Bidirectional GRU.",
                "AI Coach Assist: An Automated Approach for Call Recommendation in\n  Contact Centers for Agent Coaching\nIn recent years, the utilization of Artificial Intelligence (AI) in the\ncontact center industry is on the rise. One area where AI can have a\nsignificant impact is in the coaching of contact center agents. By analyzing\ncall transcripts using Natural Language Processing (NLP) techniques, it would\nbe possible to quickly determine which calls are most relevant for coaching\npurposes. In this paper, we present AI Coach Assist, which leverages the\npre-trained transformer-based language models to determine whether a given call\nis coachable or not based on the quality assurance (QA) questions asked by the\ncontact center managers or supervisors. The system was trained and evaluated on\na large dataset collected from real-world contact centers and provides an\neffective way to recommend calls to the contact center managers that are more\nlikely to contain coachable moments. Our experimental findings demonstrate the\npotential of AI Coach Assist to improve the coaching process, resulting in\nenhancing the performance of contact center agents.",
                "Data Minimization at Inference Time\nIn domains with high stakes such as law, recruitment, and healthcare,\nlearning models frequently rely on sensitive user data for inference,\nnecessitating the complete set of features. This not only poses significant\nprivacy risks for individuals but also demands substantial human effort from\norganizations to verify information accuracy. This paper asks whether it is\nnecessary to use \\emph{all} input features for accurate predictions at\ninference time. The paper demonstrates that, in a personalized setting,\nindividuals may only need to disclose a small subset of their features without\ncompromising decision-making accuracy. The paper also provides an efficient\nsequential algorithm to determine the appropriate attributes for each\nindividual to provide. Evaluations across various learning tasks show that\nindividuals can potentially report as little as 10\\% of their information while\nmaintaining the same accuracy level as a model that employs the full set of\nuser information.",
                "The Curse of Recursion: Training on Generated Data Makes Models Forget\nStable Diffusion revolutionised image creation from descriptive text. GPT-2,\nGPT-3(.5) and GPT-4 demonstrated astonishing performance across a variety of\nlanguage tasks. ChatGPT introduced such language models to the general public.\nIt is now clear that large language models (LLMs) are here to stay, and will\nbring about drastic change in the whole ecosystem of online text and images. In\nthis paper we consider what the future might hold. What will happen to GPT-{n}\nonce LLMs contribute much of the language found online? We find that use of\nmodel-generated content in training causes irreversible defects in the\nresulting models, where tails of the original content distribution disappear.\nWe refer to this effect as Model Collapse and show that it can occur in\nVariational Autoencoders, Gaussian Mixture Models and LLMs. We build\ntheoretical intuition behind the phenomenon and portray its ubiquity amongst\nall learned generative models. We demonstrate that it has to be taken seriously\nif we are to sustain the benefits of training from large-scale data scraped\nfrom the web. Indeed, the value of data collected about genuine human\ninteractions with systems will be increasingly valuable in the presence of\ncontent generated by LLMs in data crawled from the Internet."
            ],
            "interesting paper": 1
        },
        {
            "papers": [
                "Re-imagining health and well-being in low resource African settings\n  using an augmented AI system and a 3D digital twin\nThis paper discusses and explores the potential and relevance of recent\ndevelopments in artificial intelligence (AI) and digital twins for health and\nwell-being in low-resource African countries. We use the case of public health\nemergency response to disease outbreaks and epidemic control. There is\npotential to take advantage of the increasing availability of data and\ndigitization to develop advanced AI methods for analysis and prediction. Using\nan AI systems perspective, we review emerging trends in AI systems and digital\ntwins and propose an initial augmented AI system architecture to illustrate how\nan AI system can work with a 3D digital twin to address public health goals. We\nhighlight scientific knowledge discovery, continual learning, pragmatic\ninteroperability, and interactive explanation and decision-making as essential\nresearch challenges for AI systems and digital twins.",
                "ChatGPT Informed Graph Neural Network for Stock Movement Prediction\nChatGPT has demonstrated remarkable capabilities across various natural\nlanguage processing (NLP) tasks. However, its potential for inferring dynamic\nnetwork structures from temporal textual data, specifically financial news,\nremains an unexplored frontier. In this research, we introduce a novel\nframework that leverages ChatGPT's graph inference capabilities to enhance\nGraph Neural Networks (GNN). Our framework adeptly extracts evolving network\nstructures from textual data, and incorporates these networks into graph neural\nnetworks for subsequent predictive tasks. The experimental results from stock\nmovement forecasting indicate our model has consistently outperformed the\nstate-of-the-art Deep Learning-based benchmarks. Furthermore, the portfolios\nconstructed based on our model's outputs demonstrate higher annualized\ncumulative returns, alongside reduced volatility and maximum drawdown. This\nsuperior performance highlights the potential of ChatGPT for text-based network\ninferences and underscores its promising implications for the financial sector.",
                "Diffusion Model is an Effective Planner and Data Synthesizer for\n  Multi-Task Reinforcement Learning\nDiffusion models have demonstrated highly-expressive generative capabilities\nin vision and NLP. Recent studies in reinforcement learning (RL) have shown\nthat diffusion models are also powerful in modeling complex policies or\ntrajectories in offline datasets. However, these works have been limited to\nsingle-task settings where a generalist agent capable of addressing multi-task\npredicaments is absent. In this paper, we aim to investigate the effectiveness\nof a single diffusion model in modeling large-scale multi-task offline data,\nwhich can be challenging due to diverse and multimodal data distribution.\nSpecifically, we propose Multi-Task Diffusion Model (\\textsc{MTDiff}), a\ndiffusion-based method that incorporates Transformer backbones and prompt\nlearning for generative planning and data synthesis in multi-task offline\nsettings. \\textsc{MTDiff} leverages vast amounts of knowledge available in\nmulti-task data and performs implicit knowledge sharing among tasks. For\ngenerative planning, we find \\textsc{MTDiff} outperforms state-of-the-art\nalgorithms across 50 tasks on Meta-World and 8 maps on Maze2D. For data\nsynthesis, \\textsc{MTDiff} generates high-quality data for testing tasks given\na single demonstration as a prompt, which enhances the low-quality datasets for\neven unseen tasks.",
                "The Digital Divide in Process Safety: Quantitative Risk Analysis of\n  Human-AI Collaboration\nDigital technologies have dramatically accelerated the digital transformation\nin process industries, boosted new industrial applications, upgraded the\nproduction system, and enhanced operational efficiency. In contrast, the\nchallenges and gaps between human and artificial intelligence (AI) have become\nmore and more prominent, whereas the digital divide in process safety is\naggregating. The study attempts to address the following questions: (i)What is\nAI in the process safety context? (ii)What is the difference between AI and\nhumans in process safety? (iii)How do AI and humans collaborate in process\nsafety? (iv)What are the challenges and gaps in human-AI collaboration? (v)How\nto quantify the risk of human-AI collaboration in process safety? Qualitative\nrisk analysis based on brainstorming and literature review, and quantitative\nrisk analysis based on layer of protection analysis (LOPA) and Bayesian network\n(BN), were applied to explore and model. The importance of human reliability\nshould be stressed in the digital age, not usually to increase the reliability\nof AI, and human-centered AI design in process safety needs to be propagated.",
                "ProcessGPT: Transforming Business Process Management with Generative\n  Artificial Intelligence\nGenerative Pre-trained Transformer (GPT) is a state-of-the-art machine\nlearning model capable of generating human-like text through natural language\nprocessing (NLP). GPT is trained on massive amounts of text data and uses deep\nlearning techniques to learn patterns and relationships within the data,\nenabling it to generate coherent and contextually appropriate text. This\nposition paper proposes using GPT technology to generate new process models\nwhen/if needed. We introduce ProcessGPT as a new technology that has the\npotential to enhance decision-making in data-centric and knowledge-intensive\nprocesses. ProcessGPT can be designed by training a generative pre-trained\ntransformer model on a large dataset of business process data. This model can\nthen be fine-tuned on specific process domains and trained to generate process\nflows and make decisions based on context and user input. The model can be\nintegrated with NLP and machine learning techniques to provide insights and\nrecommendations for process improvement. Furthermore, the model can automate\nrepetitive tasks and improve process efficiency while enabling knowledge\nworkers to communicate analysis findings, supporting evidence, and make\ndecisions. ProcessGPT can revolutionize business process management (BPM) by\noffering a powerful tool for process augmentation, automation and improvement.\nFinally, we demonstrate how ProcessGPT can be a powerful tool for augmenting\ndata engineers in maintaining data ecosystem processes within large bank\norganizations. Our scenario highlights the potential of this approach to\nimprove efficiency, reduce costs, and enhance the quality of business\noperations through the automation of data-centric and knowledge-intensive\nprocesses. These results underscore the promise of ProcessGPT as a\ntransformative technology for organizations looking to improve their process\nworkflows.",
                "Taming AI Bots: Controllability of Neural States in Large Language\n  Models\nWe tackle the question of whether an agent can, by suitable choice of\nprompts, control an AI bot to any state. To that end, we first introduce a\nformal definition of ``meaning'' that is amenable to analysis. Then, we\ncharacterize ``meaningful data'' on which large language models (LLMs) are\nostensibly trained, and ``well-trained LLMs'' through conditions that are\nlargely met by today's LLMs. While a well-trained LLM constructs an embedding\nspace of meanings that is Euclidean, meanings themselves do not form a vector\n(linear) subspace, but rather a quotient space within. We then characterize the\nsubset of meanings that can be reached by the state of the LLMs for some input\nprompt, and show that a well-trained bot can reach any meaning albeit with\nsmall probability. We then introduce a stronger notion of controllability as\n{\\em almost certain reachability}, and show that, when restricted to the space\nof meanings, an AI bot is controllable. We do so after introducing a functional\ncharacterization of attentive AI bots, and finally derive necessary and\nsufficient conditions for controllability. The fact that AI bots are\ncontrollable means that an adversary could steer them towards any state.\nHowever, the sampling process can be designed to counteract adverse actions and\navoid reaching undesirable regions of state space before their boundary is\ncrossed."
            ],
            "interesting paper": 2
        },
        {
            "papers": [
                "Shapley Based Residual Decomposition for Instance Analysis\nIn this paper, we introduce the idea of decomposing the residuals of\nregression with respect to the data instances instead of features. This allows\nus to determine the effects of each individual instance on the model and each\nother, and in doing so makes for a model-agnostic method of identifying\ninstances of interest. In doing so, we can also determine the appropriateness\nof the model and data in the wider context of a given study. The paper focuses\non the possible applications that such a framework brings to the relatively\nunexplored field of instance analysis in the context of Explainable AI tasks.",
                "Towards a Unifying Model of Rationality in Multiagent Systems\nMultiagent systems deployed in the real world need to cooperate with other\nagents (including humans) nearly as effectively as these agents cooperate with\none another. To design such AI, and provide guarantees of its effectiveness, we\nneed to clearly specify what types of agents our AI must be able to cooperate\nwith. In this work we propose a generic model of socially intelligent agents,\nwhich are individually rational learners that are also able to cooperate with\none another (in the sense that their joint behavior is Pareto efficient). We\ndefine rationality in terms of the regret incurred by each agent over its\nlifetime, and show how we can construct socially intelligent agents for\ndifferent forms of regret. We then discuss the implications of this model for\nthe development of \"robust\" MAS that can cooperate with a wide variety of\nsocially intelligent agents.",
                "An Emergency Disposal Decision-making Method with Human--Machine\n  Collaboration\nRapid developments in artificial intelligence technology have led to unmanned\nsystems replacing human beings in many fields requiring high-precision\npredictions and decisions. In modern operational environments, all job plans\nare affected by emergency events such as equipment failures and resource\nshortages, making a quick resolution critical. The use of unmanned systems to\nassist decision-making can improve resolution efficiency, but their\ndecision-making is not interpretable and may make the wrong decisions. Current\nunmanned systems require human supervision and control. Based on this, we\npropose a collaborative human--machine method for resolving unplanned events\nusing two phases: task filtering and task scheduling. In the task filtering\nphase, we propose a human--machine collaborative decision-making algorithm for\ndynamic tasks. The GACRNN model is used to predict the state of the job nodes,\nlocate the key nodes, and generate a machine-predicted resolution task list. A\nhuman decision-maker supervises the list in real time and modifies and confirms\nthe machine-predicted list through the human--machine interface. In the task\nscheduling phase, we propose a scheduling algorithm that integrates human\nexperience constraints. The steps to resolve an event are inserted into the\nnormal job sequence to schedule the resolution. We propose several\nhuman--machine collaboration methods in each phase to generate steps to resolve\nan unplanned event while minimizing the impact on the original job plan.",
                "Parity Calibration\nIn a sequential regression setting, a decision-maker may be primarily\nconcerned with whether the future observation will increase or decrease\ncompared to the current one, rather than the actual value of the future\nobservation. In this context, we introduce the notion of parity calibration,\nwhich captures the goal of calibrated forecasting for the increase-decrease (or\n\"parity\") event in a timeseries. Parity probabilities can be extracted from a\nforecasted distribution for the output, but we show that such a strategy leads\nto theoretical unpredictability and poor practical performance. We then observe\nthat although the original task was regression, parity calibration can be\nexpressed as binary calibration. Drawing on this connection, we use an online\nbinary calibration method to achieve parity calibration. We demonstrate the\neffectiveness of our approach on real-world case studies in epidemiology,\nweather forecasting, and model-based control in nuclear fusion.",
                "Chatbots to ChatGPT in a Cybersecurity Space: Evolution,\n  Vulnerabilities, Attacks, Challenges, and Future Recommendations\nChatbots shifted from rule-based to artificial intelligence techniques and\ngained traction in medicine, shopping, customer services, food delivery,\neducation, and research. OpenAI developed ChatGPT blizzard on the Internet as\nit crossed one million users within five days of its launch. However, with the\nenhanced popularity, chatbots experienced cybersecurity threats and\nvulnerabilities. This paper discussed the relevant literature, reports, and\nexplanatory incident attacks generated against chatbots. Our initial point is\nto explore the timeline of chatbots from ELIZA (an early natural language\nprocessing computer program) to GPT-4 and provide the working mechanism of\nChatGPT. Subsequently, we explored the cybersecurity attacks and\nvulnerabilities in chatbots. Besides, we investigated the ChatGPT, specifically\nin the context of creating the malware code, phishing emails, undetectable\nzero-day attacks, and generation of macros and LOLBINs. Furthermore, the\nhistory of cyberattacks and vulnerabilities exploited by cybercriminals are\ndiscussed, particularly considering the risk and vulnerabilities in ChatGPT.\nAddressing these threats and vulnerabilities requires specific strategies and\nmeasures to reduce the harmful consequences. Therefore, the future directions\nto address the challenges were presented.",
                "Learning Perturbations to Explain Time Series Predictions\nExplaining predictions based on multivariate time series data carries the\nadditional difficulty of handling not only multiple features, but also time\ndependencies. It matters not only what happened, but also when, and the same\nfeature could have a very different impact on a prediction depending on this\ntime information. Previous work has used perturbation-based saliency methods to\ntackle this issue, perturbing an input using a trainable mask to discover which\nfeatures at which times are driving the predictions. However these methods\nintroduce fixed perturbations, inspired from similar methods on static data,\nwhile there seems to be little motivation to do so on temporal data. In this\nwork, we aim to explain predictions by learning not only masks, but also\nassociated perturbations. We empirically show that learning these perturbations\nsignificantly improves the quality of these explanations on time series data."
            ],
            "interesting paper": 4
        },
        {
            "papers": [
                "Visual Exploratory Data Analysis of the Covid-19 Pandemic in Nigeria:\n  Two Years after the Outbreak\nThe outbreak of the coronavirus disease in Nigeria and all over the world in\n2019/2020 caused havoc on the world's economy and put a strain on global\nhealthcare facilities and personnel. It also threw up many opportunities to\nimprove processes using artificial intelligence techniques like big data\nanalytics and business intelligence. The need to speedily make decisions that\ncould have far-reaching effects is prompting the boom in data analytics which\nis achieved via exploratory data analysis (EDA) to see trends, patterns, and\nrelationships in the data. Today, big data analytics is revolutionizing\nprocesses and helping improve productivity and decision-making capabilities in\nall aspects of life. The large amount of heterogeneous and, in most cases,\nopaque data now available has made it possible for researchers and businesses\nof all sizes to effectively deploy data analytics to gain action-oriented\ninsights into various problems in real time. In this paper, we deployed\nMicrosoft Excel and Python to perform EDA of the covid-19 pandemic data in\nNigeria and presented our results via visualizations and a dashboard using\nTableau. The dataset is from the Nigeria Centre for Disease Control (NCDC)\nrecorded between February 28th, 2020, and July 19th, 2022. This paper aims to\nfollow the data and visually show the trends over the past 2 years and also\nshow the powerful capabilities of these data analytics tools and techniques.\nFurthermore, our findings contribute to the current literature on Covid-19\nresearch by showcasing how the virus has progressed in Nigeria over time and\nthe insights thus far.",
                "Traffic Prediction using Artificial Intelligence: Review of Recent\n  Advances and Emerging Opportunities\nTraffic prediction plays a crucial role in alleviating traffic congestion\nwhich represents a critical problem globally, resulting in negative\nconsequences such as lost hours of additional travel time and increased fuel\nconsumption. Integrating emerging technologies into transportation systems\nprovides opportunities for improving traffic prediction significantly and\nbrings about new research problems. In order to lay the foundation for\nunderstanding the open research challenges in traffic prediction, this survey\naims to provide a comprehensive overview of traffic prediction methodologies.\nSpecifically, we focus on the recent advances and emerging research\nopportunities in Artificial Intelligence (AI)-based traffic prediction methods,\ndue to their recent success and potential in traffic prediction, with an\nemphasis on multivariate traffic time series modeling. We first provide a list\nand explanation of the various data types and resources used in the literature.\nNext, the essential data preprocessing methods within the traffic prediction\ncontext are categorized, and the prediction methods and applications are\nsubsequently summarized. Lastly, we present primary research challenges in\ntraffic prediction and discuss some directions for future research.",
                "Intent-aligned AI systems deplete human agency: the need for agency\n  foundations research in AI safety\nThe rapid advancement of artificial intelligence (AI) systems suggests that\nartificial general intelligence (AGI) systems may soon arrive. Many researchers\nare concerned that AIs and AGIs will harm humans via intentional misuse\n(AI-misuse) or through accidents (AI-accidents). In respect of AI-accidents,\nthere is an increasing effort focused on developing algorithms and paradigms\nthat ensure AI systems are aligned to what humans intend, e.g. AI systems that\nyield actions or recommendations that humans might judge as consistent with\ntheir intentions and goals. Here we argue that alignment to human intent is\ninsufficient for safe AI systems and that preservation of long-term agency of\nhumans may be a more robust standard, and one that needs to be separated\nexplicitly and a priori during optimization. We argue that AI systems can\nreshape human intention and discuss the lack of biological and psychological\nmechanisms that protect humans from loss of agency. We provide the first formal\ndefinition of agency-preserving AI-human interactions which focuses on\nforward-looking agency evaluations and argue that AI systems - not humans -\nmust be increasingly tasked with making these evaluations. We show how agency\nloss can occur in simple environments containing embedded agents that use\ntemporal-difference learning to make action recommendations. Finally, we\npropose a new area of research called \"agency foundations\" and pose four\ninitial topics designed to improve our understanding of agency in AI-human\ninteractions: benevolent game theory, algorithmic foundations of human rights,\nmechanistic interpretability of agency representation in neural-networks and\nreinforcement learning from internal states.",
                "Less Likely Brainstorming: Using Language Models to Generate Alternative\n  Hypotheses\nA human decision-maker benefits the most from an AI assistant that corrects\nfor their biases. For problems such as generating interpretation of a radiology\nreport given findings, a system predicting only highly likely outcomes may be\nless useful, where such outcomes are already obvious to the user. To alleviate\nbiases in human decision-making, it is worth considering a broad differential\ndiagnosis, going beyond the most likely options. We introduce a new task, \"less\nlikely brainstorming,\" that asks a model to generate outputs that humans think\nare relevant but less likely to happen. We explore the task in two settings: a\nbrain MRI interpretation generation setting and an everyday commonsense\nreasoning setting. We found that a baseline approach of training with less\nlikely hypotheses as targets generates outputs that humans evaluate as either\nlikely or irrelevant nearly half of the time; standard MLE training is not\neffective. To tackle this problem, we propose a controlled text generation\nmethod that uses a novel contrastive learning strategy to encourage models to\ndifferentiate between generating likely and less likely outputs according to\nhumans. We compare our method with several state-of-the-art controlled text\ngeneration models via automatic and human evaluations and show that our models'\ncapability of generating less likely outputs is improved.",
                "Evaluating GPT's Programming Capability through CodeWars' Katas\nIn the burgeoning field of artificial intelligence (AI), understanding the\ncapabilities and limitations of programming-oriented models is crucial. This\npaper presents a novel evaluation of the programming proficiency of Generative\nPretrained Transformer (GPT) models, specifically GPT-3.5 and GPT-4, against\ncoding problems of varying difficulty levels drawn from Codewars. The\nexperiments reveal a distinct boundary at the 3kyu level, beyond which these\nGPT models struggle to provide solutions. These findings led to the proposal of\na measure for coding problem complexity that incorporates both problem\ndifficulty and the time required for solution. The research emphasizes the need\nfor validation and creative thinking capabilities in AI models to better\nemulate human problem-solving techniques. Future work aims to refine this\nproposed complexity measure, enhance AI models with these suggested\ncapabilities, and develop an objective measure for programming problem\ndifficulty. The results of this research offer invaluable insights for\nimproving AI programming capabilities and advancing the frontier of AI\nproblem-solving abilities.",
                "Using Data Analytics to Derive Business Intelligence: A Case Study\nThe data revolution experienced in recent times has thrown up new challenges\nand opportunities for businesses of all sizes in diverse industries. Big data\nanalytics is already at the forefront of innovations to help make meaningful\nbusiness decisions from the abundance of raw data available today. Business\nintelligence and analytics has become a huge trend in todays IT world as\ncompanies of all sizes are looking to improve their business processes and\nscale up using data driven solutions. This paper aims to demonstrate the data\nanalytical process of deriving business intelligence via the historical data of\na fictional bike share company seeking to find innovative ways to convert their\ncasual riders to annual paying registered members. The dataset used is freely\navailable as Chicago Divvy Bicycle Sharing Data on Kaggle. The authors used the\nRTidyverse library in RStudio to analyse the data and followed the six data\nanalysis steps of ask, prepare, process, analyse, share, and act to recommend\nsome actionable approaches the company could adopt to convert casual riders to\npaying annual members. The findings from this research serve as a valuable case\nexample, of a real world deployment of BIA technologies in the industry, and a\ndemonstration of the data analysis cycle for data practitioners, researchers,\nand other potential users."
            ],
            "interesting paper": 2
        },
        {
            "papers": [
                "Assessing the Generalizability of a Performance Predictive Model\nA key component of automated algorithm selection and configuration, which in\nmost cases are performed using supervised machine learning (ML) methods is a\ngood-performing predictive model. The predictive model uses the feature\nrepresentation of a set of problem instances as input data and predicts the\nalgorithm performance achieved on them. Common machine learning models struggle\nto make predictions for instances with feature representations not covered by\nthe training data, resulting in poor generalization to unseen problems. In this\nstudy, we propose a workflow to estimate the generalizability of a predictive\nmodel for algorithm performance, trained on one benchmark suite to another. The\nworkflow has been tested by training predictive models across benchmark suites\nand the results show that generalizability patterns in the landscape feature\nspace are reflected in the performance space.",
                "From Human-Centered to Social-Centered Artificial Intelligence:\n  Assessing ChatGPT's Impact through Disruptive Events\nLarge language models (LLMs) and dialogue agents have existed for years, but\nthe release of recent GPT models has been a watershed moment for artificial\nintelligence (AI) research and society at large. Immediately recognized for its\ngenerative capabilities and versatility, ChatGPT's impressive proficiency\nacross technical and creative domains led to its widespread adoption. While\nsociety grapples with the emerging cultural impacts of ChatGPT, critiques of\nChatGPT's impact within the machine learning community have coalesced around\nits performance or other conventional Responsible AI evaluations relating to\nbias, toxicity, and 'hallucination.' We argue that these latter critiques draw\nheavily on a particular conceptualization of the 'human-centered' framework,\nwhich tends to cast atomized individuals as the key recipients of both the\nbenefits and detriments of technology. In this article, we direct attention to\nanother dimension of LLMs and dialogue agents' impact: their effect on social\ngroups, institutions, and accompanying norms and practices. By illustrating\nChatGPT's social impact through three disruptive events, we challenge\nindividualistic approaches in AI development and contribute to ongoing debates\naround the ethical and responsible implementation of AI systems. We hope this\neffort will call attention to more comprehensive and longitudinal evaluation\ntools and compel technologists to go beyond human-centered thinking and ground\ntheir efforts through social-centered AI.",
                "Human or Not? A Gamified Approach to the Turing Test\nWe present \"Human or Not?\", an online game inspired by the Turing test, that\nmeasures the capability of AI chatbots to mimic humans in dialog, and of humans\nto tell bots from other humans. Over the course of a month, the game was played\nby over 1.5 million users who engaged in anonymous two-minute chat sessions\nwith either another human or an AI language model which was prompted to behave\nlike humans. The task of the players was to correctly guess whether they spoke\nto a person or to an AI. This largest scale Turing-style test conducted to date\nrevealed some interesting facts. For example, overall users guessed the\nidentity of their partners correctly in only 68% of the games. In the subset of\nthe games in which users faced an AI bot, users had even lower correct guess\nrates of 60% (that is, not much higher than chance). This white paper details\nthe development, deployment, and results of this unique experiment. While this\nexperiment calls for many extensions and refinements, these findings already\nbegin to shed light on the inevitable near future which will commingle humans\nand AI.",
                "Survey of Trustworthy AI: A Meta Decision of AI\nWhen making strategic decisions, we are often confronted with overwhelming\ninformation to process. The situation can be further complicated when some\npieces of evidence are contradicted each other or paradoxical. The challenge\nthen becomes how to determine which information is useful and which ones should\nbe eliminated. This process is known as meta-decision. Likewise, when it comes\nto using Artificial Intelligence (AI) systems for strategic decision-making,\nplacing trust in the AI itself becomes a meta-decision, given that many AI\nsystems are viewed as opaque \"black boxes\" that process large amounts of data.\nTrusting an opaque system involves deciding on the level of Trustworthy AI\n(TAI). We propose a new approach to address this issue by introducing a novel\ntaxonomy or framework of TAI, which encompasses three crucial domains:\narticulate, authentic, and basic for different levels of trust. To underpin\nthese domains, we create ten dimensions to measure trust:\nexplainability/transparency, fairness/diversity, generalizability, privacy,\ndata governance, safety/robustness, accountability, reproducibility,\nreliability, and sustainability. We aim to use this taxonomy to conduct a\ncomprehensive survey and explore different TAI approaches from a strategic\ndecision-making perspective.",
                "EMOTE: An Explainable architecture for Modelling the Other Through\n  Empathy\nWe can usually assume others have goals analogous to our own. This assumption\ncan also, at times, be applied to multi-agent games - e.g. Agent 1's attraction\nto green pellets is analogous to Agent 2's attraction to red pellets. This\n\"analogy\" assumption is tied closely to the cognitive process known as empathy.\nInspired by empathy, we design a simple and explainable architecture to model\nanother agent's action-value function. This involves learning an \"Imagination\nNetwork\" to transform the other agent's observed state in order to produce a\nhuman-interpretable \"empathetic state\" which, when presented to the learning\nagent, produces behaviours that mimic the other agent. Our approach is\napplicable to multi-agent scenarios consisting of a single learning agent and\nother (independent) agents acting according to fixed policies. This\narchitecture is particularly beneficial for (but not limited to) algorithms\nusing a composite value or reward function. We show our method produces better\nperformance in multi-agent games, where it robustly estimates the other's model\nin different environment configurations. Additionally, we show that the\nempathetic states are human interpretable, and thus verifiable.",
                "TorchRL: A data-driven decision-making library for PyTorch\nPyTorch has ascended as a premier machine learning framework, yet it lacks a\nnative and comprehensive library for decision and control tasks suitable for\nlarge development teams dealing with complex real-world data and environments.\nTo address this issue, we propose TorchRL, a generalistic control library for\nPyTorch that provides well-integrated, yet standalone components. We introduce\na new and flexible PyTorch primitive, the TensorDict, which facilitates\nstreamlined algorithm development across the many branches of Reinforcement\nLearning (RL) and control. We provide a detailed description of the building\nblocks and an extensive overview of the library across domains and tasks.\nFinally, we experimentally demonstrate its reliability and flexibility and show\ncomparative benchmarks to demonstrate its computational efficiency. TorchRL\nfosters long-term support and is publicly available on GitHub for greater\nreproducibility and collaboration within the research community. The code is\nopen-sourced on GitHub."
            ],
            "interesting paper": 2
        },
        {
            "papers": [
                "AI Liability Insurance With an Example in AI-Powered E-diagnosis System\nArtificial Intelligence (AI) has received an increasing amount of attention\nin multiple areas. The uncertainties and risks in AI-powered systems have\ncreated reluctance in their wild adoption. As an economic solution to\ncompensate for potential damages, AI liability insurance is a promising market\nto enhance the integration of AI into daily life. In this work, we use an\nAI-powered E-diagnosis system as an example to study AI liability insurance. We\nprovide a quantitative risk assessment model with evidence-based numerical\nanalysis. We discuss the insurability criteria for AI technologies and suggest\nnecessary adjustments to accommodate the features of AI products. We show that\nAI liability insurance can act as a regulatory mechanism to incentivize\ncompliant behaviors and serve as a certificate of high-quality AI systems.\nFurthermore, we suggest premium adjustment to reflect the dynamic evolution of\nthe inherent uncertainty in AI. Moral hazard problems are discussed and\nsuggestions for AI liability insurance are provided.",
                "The ethical ambiguity of AI data enrichment: Measuring gaps in research\n  ethics norms and practices\nThe technical progression of artificial intelligence (AI) research has been\nbuilt on breakthroughs in fields such as computer science, statistics, and\nmathematics. However, in the past decade AI researchers have increasingly\nlooked to the social sciences, turning to human interactions to solve the\nchallenges of model development. Paying crowdsourcing workers to generate or\ncurate data, or data enrichment, has become indispensable for many areas of AI\nresearch, from natural language processing to reinforcement learning from human\nfeedback (RLHF). Other fields that routinely interact with crowdsourcing\nworkers, such as Psychology, have developed common governance requirements and\nnorms to ensure research is undertaken ethically. This study explores how, and\nto what extent, comparable research ethics requirements and norms have\ndeveloped for AI research and data enrichment. We focus on the approach taken\nby two leading conferences: ICLR and NeurIPS, and journal publisher Springer.\nIn a longitudinal study of accepted papers, and via a comparison with\nPsychology and CHI papers, this work finds that leading AI venues have begun to\nestablish protocols for human data collection, but these are are inconsistently\nfollowed by authors. Whilst Psychology papers engaging with crowdsourcing\nworkers frequently disclose ethics reviews, payment data, demographic data and\nother information, similar disclosures are far less common in leading AI venues\ndespite similar guidance. The work concludes with hypotheses to explain these\ngaps in research ethics practices and considerations for its implications.",
                "AI and the creative realm: A short review of current and future\n  applications\nThis study explores the concept of creativity and artificial intelligence\n(AI) and their recent integration. While AI has traditionally been perceived as\nincapable of generating new ideas or creating art, the development of more\nsophisticated AI models and the proliferation of human-computer interaction\ntools have opened up new possibilities for AI in artistic creation. This study\ninvestigates the various applications of AI in a creative context,\ndifferentiating between the type of art, language, and algorithms used. It also\nconsiders the philosophical implications of AI and creativity, questioning\nwhether consciousness can be researched in machines and AI's potential\ninterests and decision-making capabilities. Overall, we aim to stimulate a\nreflection on AI's use and ethical implications in creative contexts.",
                "Cook-Gen: Robust Generative Modeling of Cooking Actions from Recipes\nAs people become more aware of their food choices, food computation models\nhave become increasingly popular in assisting people in maintaining healthy\neating habits. For example, food recommendation systems analyze recipe\ninstructions to assess nutritional contents and provide recipe recommendations.\nThe recent and remarkable successes of generative AI methods, such as\nauto-regressive large language models, can lead to robust methods for a more\ncomprehensive understanding of recipes for healthy food recommendations beyond\nsurface-level nutrition content assessments. In this study, we explore the use\nof generative AI methods to extend current food computation models, primarily\ninvolving the analysis of nutrition and ingredients, to also incorporate\ncooking actions (e.g., add salt, fry the meat, boil the vegetables, etc.).\nCooking actions are notoriously hard to model using statistical learning\nmethods due to irregular data patterns - significantly varying natural language\ndescriptions for the same action (e.g., marinate the meat vs. marinate the meat\nand leave overnight) and infrequently occurring patterns (e.g., add salt occurs\nfar more frequently than marinating the meat). The prototypical approach to\nhandling irregular data patterns is to increase the volume of data that the\nmodel ingests by orders of magnitude. Unfortunately, in the cooking domain,\nthese problems are further compounded with larger data volumes presenting a\nunique challenge that is not easily handled by simply scaling up. In this work,\nwe propose novel aggregation-based generative AI methods, Cook-Gen, that\nreliably generate cooking actions from recipes, despite difficulties with\nirregular data patterns, while also outperforming Large Language Models and\nother strong baselines.",
                "Minding Language Models' (Lack of) Theory of Mind: A Plug-and-Play\n  Multi-Character Belief Tracker\nTheory of Mind (ToM)$\\unicode{x2014}$the ability to reason about the mental\nstates of other people$\\unicode{x2014}$is a key element of our social\nintelligence. Yet, despite their ever more impressive performance, large-scale\nneural language models still lack basic theory of mind capabilities\nout-of-the-box. We posit that simply scaling up models will not imbue them with\ntheory of mind due to the inherently symbolic and implicit nature of the\nphenomenon, and instead investigate an alternative: can we design a\ndecoding-time algorithm that enhances theory of mind of off-the-shelf neural\nlanguage models without explicit supervision? We present SymbolicToM, a\nplug-and-play approach to reason about the belief states of multiple characters\nin reading comprehension tasks via explicit symbolic representation. More\nconcretely, our approach tracks each entity's beliefs, their estimation of\nother entities' beliefs, and higher-order levels of reasoning, all through\ngraphical representations, allowing for more precise and interpretable\nreasoning than previous approaches. Empirical results on the well-known ToMi\nbenchmark (Le et al., 2019) demonstrate that SymbolicToM dramatically enhances\noff-the-shelf neural networks' theory of mind in a zero-shot setting while\nshowing robust out-of-distribution performance compared to supervised\nbaselines. Our work also reveals spurious patterns in existing theory of mind\nbenchmarks, emphasizing the importance of out-of-distribution evaluation and\nmethods that do not overfit a particular dataset.",
                "An Overview on Generative AI at Scale with Edge-Cloud Computing\nAs a specific category of artificial intelligence (AI), generative artificial\nintelligence (GenAI) generates new content that resembles what is created by\nhumans. The rapid development of GenAI systems has created a huge amount of new\ndata on the Internet, posing new challenges to current computing and\ncommunication frameworks. Currently, GenAI services rely on the traditional\ncloud computing framework due to the need for large computation resources.\nHowever, such services will encounter high latency because of data transmission\nand a high volume of requests. On the other hand, edge-cloud computing can\nprovide adequate computation power and low latency at the same time through the\ncollaboration between edges and the cloud. Thus, it is attractive to build\nGenAI systems at scale by leveraging the edge-cloud computing paradigm. In this\noverview paper, we review recent developments in GenAI and edge-cloud\ncomputing, respectively. Then, we use two exemplary GenAI applications to\ndiscuss technical challenges in scaling up their solutions using edge-cloud\ncollaborative systems. Finally, we list design considerations for training and\ndeploying GenAI systems at scale and point out future research directions."
            ],
            "interesting paper": 4
        }
    ]
}